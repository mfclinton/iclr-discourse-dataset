The paper introduces a neural translation model that automatically discovers phrases . This idea is very interesting and tries to marry phrase - based statistical machine translation with neural methods in a principled way . However , the clarity of the paper could be improved .

The local reordering layer has the ability to swap inputs , however , how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others ?

Are all segments translated independently , or do you carry over the hidden state of the decoder RNN between segments ? In Figure 1 both a BRNN and SWAN layer are shown , is there another RNN in the SWAN layer , or does the BRNN emit the final outputs after the segments have been determined ?

Thank you for your valuable comments . We address the comments and questions below :
1 . The local reordering layer has the ability to swap inputs , however , how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others ?
< Response > : We do not have a guarantee that the layer forces to swap inputs as it is data driven . In Appendix A , we show an example translating from " can you translate it ? " to " können es übersetzen ? " to show that some input information is swapped . Note that the example needs to be reordered from " translate it " to " es übersetzen " . Each row of Figure 3 represents a window of size 7 that is centered at a source sentence word . We can observe that the gates mostly focus on the central word since the first part of the sentence only requires monotonic alignment . Interestingly , the model outputs " $ " ( empty ) when the model has the word " translate " in the center of the window . Then , the model outputs " es " when the model encounters " it " . Finally , in the last window ( top row ) , the model not only has a large gate value to the center input " ? " , but the model also has a relatively large gate value to the word " translate " in order to output the translation " übersetzen ? " . This shows an example of the reordering effect achieved by using the gating mechanism of the reordering layer .

2 . Are all segments translated independently , or do you carry over the hidden state of the decoder RNN between segments ?
< Response > : Yes , all the segments are translated independently . We do not carry over the hidden states between segments . Hence , the decoding can be parallelized . We are highlighting this part in the second to last paragraph of Section 2.2.

3 . In Figure 1 both a BRNN and a SWAN layer are shown , is there another RNN in the SWAN layer , or does the BRNN emit the final outputs after the segments have been determined ?
< Response > : In Figure 1 , the reordering layer and BRNN can be considered as the encoder of an input sequence . The SWAN is the decoder , which contains another unidirectional RNN for p( a_t|x_t ) in Eq . ( 1 ) . The BRNN emits x_t to SWAN . We added some clarification in defining x_t to address it .

Authors proposed a new neural - network based machine translation method that generates the target sentence by generating multiple partial segments in the target sentence from different positions in the source information . The model is based on the SWAN architecture which is previously proposed , and an additional " local reordering " layer to reshuffle source information to adjust those positions to the target sentence .

Using the SWAN architecture looks more reasonable than the conventional attention mechanism when the ground - truth word alignment is monotone . Also , the concept of local reordering mechanism looks well to improve the basic SWAN model to reconfigure it to the situation of machine translation tasks .

The " window size " of the local reordering layer looks like the " distortion limit " used in traditional phrase - based statistical machine translation methods , and this hyperparameter may impose a similar issue with that of the distortion limit into the proposed model ; small window sizes may drop information about long dependency . For example , verbs in German sentences sometimes move to the tail of the sentence and they introduce a dependency between some distant words in the sentence . Since reordering windows restrict the context of each position to a limited number of neighbors , it may not capture distant information enough . I expected that some observations about this point will be unveiled in the paper , but unfortunately , the paper described only a few BLEU scores with different window sizes which have not enough information about it . It is useful for all followers of this paper to provide some observations about this point .
In addition , it could be very meaningful to provide some experimental results on linguistically distant language pairs , such as Japanese and English , or simply reversing word orders in either source or target sentences ( this might work to simulate the case of distant reordering ) .

Authors argued some differences between conventional attention mechanism and the local reordering mechanism , but it is somewhat unclear that which ones are the definite difference between those approaches .

A super interesting and mysterious point of the proposed method is that it achieves better BLEU than conventional methods despite no any global language models ( Table 1 row 8 ) , and the language model options ( Table 1 row 9 and footnote 4 ) may reduce the model accuracy as well as it works not so effectively . This phenomenon definitely goes against the intuitions about developing most of the conventional machine translation models . Specifically , it is unclear how the model correctly treats word connections between segments without any global language model . Authors should pay attention to explain more detailed analysis about this point in the paper .

Eq . ( 1 ) is incorrect . According to Fig. 2 , the conditional probability in the product operator should be revised to p( a_t | x_{1:t} , a_{1:t - 1 } ) , and the independence approximation to remove a_{1:t - 1 } from the conditions should also be noted in the paper .
Nevertheless , the condition x_{1:t} could not be reduced because the source position is always conditioned by all previous positions through an RNN .



Thank you for your valuable comments . We address the comments and questions below :
1 . The " window size " of the local reordering layer looks like the " distortion limit " used in traditional phrase - based statistical machine translation methods , and this hyperparameter may impose a similar issue with that of the distortion limit into the proposed model .
< Response > : Thanks a lot for your suggestion . We add the reference [ Brown 1993 ] and discussion to the end of Section 2.3 . We believe the limit of local reordering is mitigated by using bidirectional RNN after that . Thus it is not very clear how to analyze the exact behavior of the local reordering layer . We are currently actively investigating new ways of doing so .

2 . It could be very meaningful to provide some experimental results on linguistically distant language pairs , such as Japanese and English , or simply reversing word orders in either source or target sentences .
< Response > : Thanks a lot for your suggestion . This is definitely one important direction we should investigate in future work .

3 . Authors argued some differences between conventional attention mechanism and the local reordering mechanism , but it is somewhat unclear that which ones are the definite difference between those approaches .
< Response > : We reiterate and reorganize the important differences here :
First , we do not have a query to begin with as in standard attention mechanisms . Second , unlike standard attention , which is top-down from a decoder state to encoder states , the reordering operation is bottom -up . Third , the weights {w_i}_{i=0 }^{2\tau} capture the relative positions of the input elements , whereas the weights are the same for different queries and encoder hidden states in the attention mechanism ( no positional information ) . The reordering layer performs locally similar to a convolutional layer and the positional information is encoded by a different parameter w_i for each relative position i in the window . Fourth , we do not normalize the weights for the input elements e_{t-\tau} , ... , e_t , ... , e_{t +\tau} . This provides the reordering capability and can potentially turn off everything if needed . Finally , the gate of any position i in the reordering window is determined by all input elements e_{t -\tau} , … , e_t , … , e_{t + \tau} in the window .

4 . Equation ( 1 ) is incorrect . According to Fig. 2 , the conditional probability in the product operator should be revised to p( a_t | x_{1:t} , a_{1:t - 1 } ) , and the independence approximation to remove a_{1:t - 1 } from the conditions should also be noted in the paper . Nevertheless , the condition x_{1:t} could not be reduced because the source position is always conditioned by all previous positions through an RNN .
< Response > : We respectfully disagree with this assessment . The Eq. ( 1 ) is not an approximation ; it is the way we model the output . This is motivated by Eqs . ( 2 ) and ( 3 ) of the CTC paper [ Graves 2006 ] , p( y_{1:T}|x_{1:T ’} ) = sum_{a_{1:T ’}} p( a_{1:T}|x_{1:T ’} ) marginalizes over the set of all possible segmentations . And a_{ 1: T ’} is a collection of the segments that , when concatenated , leads to y_{1 : T} . We also have p( a_{1:T}|x_{1:T ’} ) = \ prod{t=1 }^{T ’} p( a_t|x_t ) given the assumption that the outputs at different times are conditional independent given the input state x_t . Put it in another way , our approach can be described via a fully generative model :

For t=1 , T ’:
Using x_t as the initial state , sample target words from RNN until we reach the end of segment symbol . This gives us segment a_t .
Finally , concatenate {a_1 , ...a_ T ’} to obtain an output y_{1 : T}.

Since there are more than one way to obtain the same y_{1 : T} , its probability becomes p( y_{1:T}|x_{1:T ’} ) = sum_{a_{ 1 : T ’} , where a_{1:T’}\in S_y} p( a_{1:T}|x_{1:T ’} ) , the Eq. ( 1 ) in our paper . This explanation is also added in the updated paper .

[ Graves 2006 ] Graves , Alex , et al . " Connectionist temporal classification : labeling unsegmented sequence data with recurrent neural networks . " Proceedings of the 23rd international conference on Machine learning . ACM , 2006 .

5 . NPMT achieves better BLEU than conventional methods despite no any global language models ( Table 1 row 8 ) , and the language model options ( Table 1 row 9 and footnote 4 ) may reduce the model accuracy as well as it works not so effectively .
< Response > : We also believe this is a super interesting and exciting observation . Our current understanding is that phrases are important building blocks of the whole target sentence and these phrases are relatively independent . With the help of being able to see the entire input sentence through the encoder , the performance can still be quite good without modeling the connection between the phrases . This is exciting because decoding can be done in linear time and can also be parallelized . We also show that adding an n-gram LM during beam search did help improve performance ( Table 1 row 9 ) .

This paper introduces a new architecture for end to end neural machine translation . Inspired by the phrase based approach , the translation process is decomposed as follows : source words are embedded and then reordered ; a bilstm then encodes the reordered source ; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right .

This kind of approach is more related to ngram based machine translation than conventional phrase based one .

The idea is nice . The proposed approach does not rely on attention based model . This opens nice perpectives for better and faster inference .

My first concern is about the architecture description . For instance , the swan part is not really stand alone . For reader who does not already know this net , I 'm not sure this is really clear . Moreover , there is no link between notations used for the swan part and the ones used in the reordering part .

Then , one question arises . Why do n't you consider the reordering of the whole source sentence . Maybe you could motivate your choice at this point . This is the main contribution of the paper , since swan already exists .

Finally , the experimental part shows nice improvements but : 1 / you must provide baseline results with a well tuned phrase based mt system ; 2 / the datasets are small ones , as well as the vocabularies , you should try with larger datasets and bpe for sake of comparison .

Thank you for your valuable comments . We address the comments and questions below :
1 . For instance , the swan part is not really stand alone . For reader who does not already know this net , I 'm not sure this is really clear .
< Response > : We add some detailed explanations to address the confusion from Reviewer2 ( see responses to Reviewer2 ) and make SWAN more stand alone . For example , we added an explanation of SWAN via a probabilistic generative model in section 2.2.

2 . There is no link between notations used for the swan part and the ones used in the reordering part
< Response > : We clarify the description of symbols h and x , and indicate the connections between the two using the bi-directional RNN in Figure 1 ( a ) .

3 . Why do n't you consider the reordering of the whole source sentence ?
< Response > : Our proposed reordering layer is not limited to local reordering . Empirically , we found the results do not improve when we increase the window sizes in our experiments , see Appendix B for details . It might be due to the choice of language pairs , which are relative monotonic .

4 . The experimental part shows nice improvements but : 1 / you must provide baseline results with a well tuned phrase based mt system
< Response > : We have looked into this direction for comparison . However , in the IWSLT 2014 competition , test set tst2014 is not revealed . Hence , we cannot directly compare results with the all the teams on [ Cettolo 2014 ] . Given in IWSLT 2015 English - German task , NMT outperforms Phrase - Based MT by a large margin ( up to 5.2 BLEU score ) [ Luong 2015 ] . Our NPMT further outperforms the state - of - the-art NMT types of systems by up to 2.7 BLEU score in English - German task .

Reference :
[ Cettolo 2014 ] Mauro Cettolo , Jan Niehues , Sebastian St¨uker , Luisa Bentivogli , and Marcello Federico . Report on the 11th IWSLT evaluation campaign , IWSLT 2014 . In Proceedings of IWSLT , 2014 .
[ Luong 2015 ] Minh - Thang Luong and Christopher D. Manning . Stanford Neural Machine Translation Systems for Spoken Language Domain . IWSLT ’15 .

5 . 2 / the datasets are small ones , as well as the vocabularies , you should try with larger datasets and bpe for sake of comparison .
< Response > : We are actively working on improving the speed of the system and exploring this approach on WMT datasets with bpe vocabularies . We plan to open source our implementations to expedite this direction .

Summary : The paper addresses the actual problem of compression of deep neural networks . Authors propose to use another technique for sparse matrix storage . Namely , authors propose to use Bloomier filter for more efficient storage of sparse matrices obtained from Dynamic Network Surgery ( DNS ) method . Moreover , authors propose elegant and efficient trick for mitigating errors of Bloomier filter . Overall , the paper present clear and completed research of the proposed technique .
Clarity and Quality : The paper is well structured and easy to follow . Authors provide a reader with a large amount of technical details , which helps to reproduce their method . The paper contains detailed investigation of every step and aspect in the proposed pipeline , which made this research well - organized and complete .
Though the presentation of some results can be improved . Namely , core values for compression and improvement are presented only for two biggest layers in networks , but more important values are compression and improvement for whole networks .
Originality and Significance : The main contribution of the paper is the adaptation of Bloomier filter for sparse network obtained from almost any procedure of networks sparsification . However , this adaptation is almost straightforward , except the proposed trick of network fine - tuning for compensating false positive values of Bloomier filter . Significance of the results is hard to estimate because of several reasons :
Values of compression and improvement are presented only for two layers , not for the whole network .
According to Fig. 4 , encoding of sparse matrices via Bloomier filter is efficient ( compared to CSR ) only for matrices with nonzero ratio greater than 0.04 . So this method ca n’t be applied to all layers in network , that can significantly influence overall compression .

Other Comments :
The procedure of network evaluation is totally omitted in the paper . So a model supposed to be “ unpacked ” ( to the dense or CSR format ) before evaluation . Considering this , comparison with CSR could be made only for sending the model over a network . Since , CSR format can be efficiently used during evaluation .
Minor Comments :
( Page 5 ) “ Because we encode $ k$ clusters , $ t$ must be greater than $ \lceil \log_ 2 k\rceil $ ” . Perhaps , “ ... $ r$ must be greater than $ \lceil \log_2 k\rceil $ ” would be better for understanding .
( Page 7 ) “ Bloomier filter encoding increased the top - 1 accuracy by 2.0 percentage points ” . Perhaps , authors have meant top - 1 error .

" Values of compression and improvement are presented only for two layers , not for the whole network .
According to Fig. 4 , encoding of sparse matrices via Bloomier filter is efficient ( compared to CSR ) only for matrices with nonzero ratio greater than 0.04 . So this method ca n’t be applied to all layers in network , that can significantly influence overall compression . "

Figure 4 is meant to demonstrate that Weightless scales better with increasing sparsity than Deep Compression . The results we present in Table 2 show that Weightless does not require a non-zero ratio of 4 % ; CNN - 1 in LeNet5 with magnitude pruning has a non-zero ratio of 7 % and it still outperforms Deep Compression .


" The procedure of network evaluation is totally omitted in the paper . So a model supposed to be “ unpacked ” ( to the dense or CSR format ) before evaluation . Considering this , comparison with CSR could be made only for sending the model over a network . Since , CSR format can be efficiently used during evaluation . "

You are correct . In its current form , a model would need to be “ unpacked ” to the dense format before evaluation , but this paper is meant to focus solely on compression for over the wire transmission . We feel this is an important problem facing companies deploying deep learning models . We are currently investigating building specialized hardware for efficient sparse processing that would enable evaluating models in the encoded space .


The problem of lossy compression of neural networks is essentially important and relevant . The paper proposes an interesting usage of Bloomier filters in lossy compression of neural net weights . The Bloomier filter is proposed by others . It is a data structure that maps from sparse indices to their corresponding values with chances that returns incorrect values for non-existing indices . The paper compares its method with two baseline methods ( Magnitude and Dynamic network surgery DNS ) to demonstrates its performance .

I find the paper fairly interesting but still have some concerns in the technical part and experiments .

Pros :
1 . The paper seems the first to introduce Bloomier filter into the network compression problem . I think its contribution is novel and original . The paper may interest those who work in the network compression domain .
2 . The method works well in the demonstrated experimental cases .

Cons :
1 . The technical part is partially clear . It might be worthwhile to briefly describe the encoding / construction algorithm used in the paper . It is recommended to describe a bit more details about how such encoding / decoding methods are applied in reducing neural net weights .
2 . One drawback of the proposed method is that it has to work with sparse weights . That requires the method to be used together with network pruning methods , which seems limiting its applicability . I believe the paper can be further improved by including a study of the compression results without a pruning method ( e.g. , comparing with Huffman in table 3 ) .
3 . What is the reason there is no DNS results reported for VGG - 16 ? Is it because the network is deeper ?
4 . The experimental part can be improved by reporting the compression results for the whole network instead of a single layer .
5 . It seems the construction of Bloomier filter is costly and the proposed method has to construct Bloomier filters for all layers . What is the total time cost in terms of encoding and decoding those networks ( LeNet and VGG ) ? It would be nice to have a separate comparison on the time consumption of different methods .
6 . Figure 4 seems a bit misleading . The comparison should be conducted on the same accuracy level instead of the ratio of nonzero weights . I recommend producing another new figure of doing such comparison .
7 . The proposed idea seems somewhat related to using low rank factorization of weight matrices for compression . It might be worthwhile to compare the two approaches in experiments .
8 . I am specifically interested in discussions about the possibility of encoding the whole network instead of layer - by-layer retraining .

1 . The technical part is partially clear . It might be worthwhile to briefly describe the encoding / construction algorithm used in the paper . It is recommended to describe a bit more details about how such encoding / decoding methods are applied in reducing neural net weights .

We have included a brief description of construction in the appendix . Also , as now mentioned in the paper , we will release an implementation with the publication of the paper .

2 . One drawback of the proposed method is that it has to work with sparse weights . That requires the method to be used together with network pruning methods , which seems limiting its applicability . I believe the paper can be further improved by including a study of the compression results without a pruning method ( e.g. , comparing with Huffman in table 3 ) .

You are correct that sparsity is necessary . This is also true for competing encoding techniques ( namely Deep Compression ) . For the large VGG16 fully connected layer we ran Huffman encoding on un - pruned , clustered weights and got an 12.8 x compression factor , which is an order of magnitude less than the reported results .

3 . What is the reason there is no DNS results reported for VGG - 16 ? Is it because the network is deeper ?

No , it was because the weights were not made available and were unclear how to tune the DNS hyperparameters to effectively prune the VGG16 weights . We would like to include a DNS version of VGG16 as the suspected improvement in sparsity would likely significantly improve our results . We are actively working on more advanced pruning techniques , different model types , and datasets to demonstrate the benefits of lossy encoding . We feel this paper presents the core technique and benefits of the proposed method over the state - of - the-art .

4 . The experimental part can be improved by reporting the compression results for the whole network instead of a single layer .

We focused on the largest layers to get the most benefit . In the final version we can report the overall compression if the reviewers feel it is beneficial .

5 . It seems the construction of Bloomier filter is costly and the proposed method has to construct Bloomier filters for all layers . What is the total time cost in terms of encoding and decoding those networks ( LeNet and VGG ) ? It would be nice to have a separate comparison on the time consumption of different methods .

Construction times for LeNet300 - 100 , LeNet5 , and VGG16 are 6 seconds , 23 seconds , and 517 seconds , respectively . Decoding takes 11 , 12 , and 505 seconds for each of the aforementioned models . We believe that these one - time overheads are negligible considering the significant reductions in model size .

6 . Figure 4 seems a bit misleading . The comparison should be conducted on the same accuracy level instead of the ratio of nonzero weights . I recommend producing another new figure of doing such comparison .

Thank you for the suggestion . We believe that this suggestion can improve the paper . As a result , we conducted additional experiments on iso-accuracy comparison between Weightless and Deep Compression in Figure 7 ( appendix ) .

7 . The proposed idea seems somewhat related to using low rank factorization of weight matrices for compression . It might be worthwhile to compare the two approaches in experiments .

We believe the benefits of low rank factorization lie in efficient execution by reducing the number of computations requires . A byproduct of low rank factorization is compression on the order of 50 % . However , when specifically targeting over the wire compression , this is not competitive with existing techniques . If you feel that this is an important distinction that must be made , we will happily add it in the related work section .

8 . I am specifically interested in discussions about the possibility of encoding the whole network instead of layer - by-layer retraining .

We can encode the whole network by eliminating the retraining steps . However , this will come at the expense of either model accuracy ( if we use the same t value as with retraining ) or overall compression ( if we increase t ) . For example , without retraining , VGG16 can lose 2 % absolute accuracy as shown in Figure 5 ( appendix ) . Previously , we tried using an auxiliary data structure to fix false positives ( called exception lists ) , this proved to incur significant storage overheads . As a result , we strongly believe that retraining is an integral part of mitigating the effects of false positives .

This paper proposes an interesting approach to compress the weights of a network for storage or transmission purposes . My understanding is , at inference , the network is ' recovered ' therefore there is no difference in processing time ( slight differences in accuracy due to the approximation in recovering the weights ) .

- The idea is nice although it 's applicability is limited as it is only for distribution of the model and storing ( is storage really a problem ? ) .

Method :
- the idea of using the Bloomier filter is new to me . However , the paper is miss -leading as the filtering is a minor part of the complete process . The paper introduces a complete pipeline including quantization , and pruning to maximize the benefits of the filter and an additional ( optional ) step to achieve further compression .

- The method / idea seems simply and easy to reproduce ( except the subsequent steps that are not clearly detailed ) .

Clarity

- The paper could improve its clarity . At the moment , the Bloomier is the core but needs many other components to make it effective . Those components are not detailed to the level of being reproducible .
- One interesting point is the self - implementation of the Deep compression algorithm . The paper claims this is a competitive representation as it achieves better compression than the original one . However , those numbers are not clear in tables ( only in table 3 numbers seem to be equivalent to the ones in the text ) . This needs clarification , CSR achieves 81.8 % according to Table 2 and 119 according to the text .

Results :
- Current results are interesting . However I have several concerns :
1 ) it is not clear to me why assuming similar performance . While Bloomier is weightless the complete process involves many retraining steps involving performance loss . Analysis on this would be nice to see ( I doubt it ends exactly at the same number ) . Section 3 explicitly suggest there is the need of retraining to mitigate the effect of false positives which is then increased with pruning and quantization . Therefore , would be nice to see the impact in accuracy ( even it is not the main focus of the work ) .

2 ) Resutls are focused on fully connected layers which carry ( for the given models ) the larger number of weights ( and therefore it is easy to get large compression numbers ) . What would happen in newer models where the fully connected layer is minimal compared to conv . layers ? What about the accuracy impact there ? Let 's say in a Resnet -34 .
3 ) I would like to see further analysis on why Bloomier filter encoding improves accuracy ( or is a typo and meant to be error ? ) by 2 % . This is a large improvement without training from scractch .
4 ) It is interesting to me how the retraining process is ' hidden ' all over the paper . At the beginning it is claimed that it takes about one hour for VGG - 16 to compute the Bloomier filters . Howerver , that is only a minimal portion of the entire pipeline . Later in the experimental section it is mentioned that ' tens of epochs ' are needed for retraining ( assuming to compensate for errors ) after retraining for compensating l1 pruning ?.... tens of epochs is a significant portion of the entire training process assuming VGG is trained for 90epochs max .

5 ) Interestingly , as mentioned in the paper , this is ' static compression ' . That is , the model needs to be completely ' restored ' before inference . This is miss -leading as an embedded device will need the same requirements as any other at inferece time ( or maybe I am missing something ) . That is , the benefit is mainly for storing and transmission .

6 ) I would like to see the sensibility analysis with respect to t and the number of clusters .

7 ) As mentioned before , LeNet is great but would be nice to see more complicated models ( even resnet on CIFAR ) . These models are not only large in terms of parameters but also quite sensitive to modifications in the weight structure .

8 ) Results are focused on a single layer . What happens if all the layers are considered at the same time ? Here I am also concerned about the retraining process ( fixing one layer and retraining the deeper ones ) . How is this done using only fully connected layers ? What is the impact of doing it all over the network ( let 's say VGG - 16 from the first convolutional layer to the very last ) .

Summary :

All in all , the idea has potential but there are many missing details . I would like to see clearer and more comprehensive results in terms of modern models and in the complete model , not only in the FC layer , including accuracy impact .

" The paper could improve its clarity ... "
The reason for the brevity on pruning and clustering was because we viewed these aspects as prior work and did not want to spend time discussing materials we did not deem research contributions of our manuscript .

" One interesting point is the self - implementation of the Deep compression algorithm .... This needs clarification , CSR achieves 81.8 % according to Table 2 and 119 according to the text . "
We understand your confusion and will clarify the text , but these numbers are indeed correct . The 81.8 x is encoding only ( using CSR ) and the 119 x is CSR + Huffman . The distinction is there to compare Bloomier filters with Bloomier + LZMA ( i.e. , encoding in 4.1 with compression in 4.2 ) .

1 ) " It is not clear to me why assuming similar performance ... "
This is an excellent point and to address it we have added a plot ( see Figure 5 ) to the appendix of the paper that shows how model performance is regained with retraining .

" Analysis on this would be nice to see ... "
You are correct that it ’s not the exact same number ( see Figure 5 ) but we are careful to make sure that the final test - accuracy reported is the same as the baseline or better by a small amount ( e.g. , VGG16 experiences an absolute improvement of less than 0.1 % overall test accuracy ) .
Figure 7 further shows how Weightless offers better compression vs . error scaling than CSR .

2 ) " Resutls are focused on fully connected layers which carry ( for the given models ) ... "
The models we chose were done so as they are the ones most commonly used in the literature ( Deep Compression , DNS , and HashedNets ) . We also consider CNNs and show that Bloomier filters perform well on them ( see LeNet5 ) . Our findings suggest that so long as weights exhibit sufficient sparsity , the method is effective .

As future work , we are actively looking into more advanced pruning techniques to achieve the necessary sparsity to encode networks like ResNet - 34 . We recently evaluated magnitude pruning on ResNet - 34 , but saw substantial increase in model error which we felt would be an unfair comparison .

3 ) " I would like to see further analysis on why Bloomier filter encoding improves accuracy ( or is a typo and meant to be error ? ) by 2 % ... "
You are correct in that this is a typo . It should be error and this is corrected .

4 ) " It is interesting to me how the retraining process is ' hidden ' all over the paper ... "
We have now included a plot ( Figure 5 ) which shows how retraining recovers accuracy in encoded layers . We have also included numbers for construction and reconstruction for all the largest layers in the models used ( at the request of another reviewer ) . We find that on a modern machine , the longest construction takes is 8.5 minutes ; the machines we used originally were older and part of a cluster being used by others .

5 ) " Interestingly , as mentioned in the paper , this is ' static compression ' ... "
That is correct . We are looking into ways to compute in the compressed space with Weightless , but to be competitive it will likely require special hardware and require a deeper investigation .

We did not intend to mislead the reader , this is a compression paper for efficient weight transmission ( and storage ) . If there is a way we could fix this we will gladly amend the paper .

6 ) " I would like to see the sensibility analysis with respect to t and the number of clusters . "
We have added a plot ( Figure 6 ) to show this to the appendix .

7 ) " As mentioned before , LeNet is great but would be nice to see more complicated models ( even resnet on CIFAR ) ... "
See above .

8 ) " Results are focused on a single layer . What happens if all the layers are considered at the same time ?... "
If all the layers are considered ( i.e. , encoded ) at the same time , there is no opportunity for deeper layers to be retrained to compensate for errors in the earlier layers . In this scenario , one would likely have to increase the t value to mitigate false positives or incur a slight increase in model error .

If each layer is encoded individually , the process occurs precisely as specified in the paper . Each layer is encoded and the deeper layers are retrained around their false positives .


This paper introduces a new task that combines elements of instruction following
and visual question answering : agents must accomplish particular tasks in an
interactive environment while providing one - word answers to questions about
features of the environment . To solve this task , the paper also presents a new
model architecture that effectively computes a low -rank attention over both
positions and feature indices in the input image . It uses this attention as a
common bottleneck for downstream predictors that select actions and answers to
questions . The paper 's main claim is that this model architecture enables strong
generalization : it allows the model to succeed at the instruction following task
even when given words it has only seen in QA contexts , and vice-versa .
Experiments show that on the navigation task , the proposed approach outperforms
a variety of baselines under both a normal data condition and one requiring
strong generalization .

On the whole , I think this paper does paper does a good job of motivating the
proposed modeling decisions . The approach is likely to be useful for other
researchers working on related problems . I have a few questions about the
evaluation , but most of my comments are about presentation .

EVALUATION

Is it really the case that no results are presented for the QA task , or am I
misreading one of the charts here ? Given that this paper spends a lot of time
motivating the QA task as part of the training scenario , I was surprised not to
see it evaluated .

Additionally , when I first read the paper I thought that the ZS1 experiments
featured no QA training at all . However , your response to one of the sibling
comments suggests that it 's still a " mixed " training setting where the sampled
QA and NAV instances happen to cover the full space . This should be made more
clear in the paper . It would be nice to know ( 1 ) how the various models perform
at QA in both ZS1 and ZS2 settings , and ( 2 ) what the actual performance is NAV
alone ( even if the results are terrible ) .

MODEL PRESENTATION

I found section 2 difficult to read : in particular , the overloading of \ Phi
with different subscripts for different output types , the general fact that
e.g. x and \ Phi_x are used interchangeably , and the large number of different
variables . My best suggestions are to drop the \ Phis altogether and consider
using text subscripts rather than coming up with a new name for every variable ,
but there are probably other things that will also help .

OTHER NOTES

- This paper needs serious proofreading --- just in the first few pages the errors
I noticed were " in 2D environment " ( in the title ! ) , " such capability " , " this
characteristics " , " such language generalization problem " , " the agent need to " ,
" some early pioneering system " , " commands is " . I gave up on keeping track at
this point but there are many more .

- \phi in Fig 2 should be explained by the caption .

- Here 's another good paper to cite for the end of 2.2.1 :
https://arxiv.org/pdf/1707.00683.pdf.

- The mechanism in 2.2.4 feels a little like
http://aclweb.org/anthology/D17-1015

- I do n't think the content on pages 12 , 13 , and 14 adds much to the
paper --- consider moving these to an appendix .

Thanks for your comments ! They really help a lot .

First , thanks for suggesting adding the results for QA . Originally we intended to use QA as an auxiliary task to help train NAV . We did n't think of adding results for it ( although we indeed had some records showing how well different methods perform in QA during the training ) . In the revised paper , we have included the QA classification accuracies in the normal , ZS1 and ZS2 settings ( Figure 6 c , Figure 7 c and f ) . We believe that this addition actually demonstrates the generalization ability of our model even better ( not only in NAV but also in QA ) . Because now we also evaluate QA in the test , we modify all the related paragraphs across the paper to emphasize this addition .

We believe that the original text already clarifies ( section 4.4 when defining ZS1 ) that ZS1 is about excluding word pairs from both NAV commands and QA questions , but not about training NAV alone . Note that training both NAV and QA together does not necessarily imply that the sampled NAV and QA instances cover the full space . For ZS1 , a subspace of sentences ( containing certain word pairs ) is not covered . For ZS2 , a different subspace of sentences ( containing certain new words ) is not covered . In other words , our zero - shot setting is not achieved by turning off either NAV or QA , but instead is by excluding certain sentence patterns from the training ( for both NAV and QA ) .

As requested , we also added the performance of training NAV alone without QA in the normal language setting ( Figure 6 ) . This ablation is called NAVA in the revised experiment section . An analysis of this ablation was also added ( section 4.3 ) .

Thanks for suggesting citing [ de Vries et al 2017 ] and [ Kitaev and Klein 2017 ] . We find that they are indeed closely related to our work . We have cited and discussed them at the end of section 2.2.1 ( -> 2.2.2 ) and section 2.2.4 ( -> 2.2.5 ) , respectively .

We have simplified the notations in section 2 to keep the presentation concise as suggested . We moved the content of pages 12 , 13 , and 14 to Appendix A . We went through a careful round of proofreading of the revised paper . While we are still trying to get others into the proofreading process , we have uploaded the second version of the paper to facilitate possible discussions on the OpenView .


[ Overview ]
In this paper , the authors proposed a unified model for combining vision , language , and action . It is aimed at controlling an agent in a virtual environment to move to a specified location in a 2D map , and answer user 's questions as well . To address this problem , the authors proposed an explicit grounding way to connect the words in a sentence and spatial regions in the images . Specifically , By this way , the model could exploit the outputs of concept detection module to perform the actions and question answering as well jointly . In the experiments , the authors compared with several previous attention methods to show the effectiveness of the proposed concept detection module and demonstrated its superiority on several configurations , including in - domain and out -of-domain cases .

[ Strengths ]

1 . I think this paper proposed interesting tasks to combine the vision , language , and actions . As we know , in a realistic environment , all three components are necessary to complete a complex tasks which need the interactions with the physical environments . The authors should release the dataset to prompt the research in this area .

2 . The authors proposed a simple method to ground the language on visual input . Specifically , the authors grounded each word in a sentence to all locations of the visual map , and then perform a simple concept detection upon it . Then , the model used this intermediate representation to guide the navigation of agent in the 2D map and visual question answering as well .

3 . From the experiments , it is shown that the proposed model outperforms several baseline methods in both normal tasks and out -of-domain ones . According to the visualizations , the interpreter could generate meaningful attention map given a textual query .

[ Weakness ]

1 . The definition of explicit grounding is a bit misleading . Though the grounding or attention is performed for each word at each location of the visual map . It is a still kind of soft -attention , except that is performed for each word in a sentence . As far as I know , this has been done in several previous works , such as : ( a ) . Hierarchical question - image co-attention for visual question answering ( https://scholar.google.com/scholar?oi=bibs&cluster=15146345852176060026&btnI=1&hl=en). Lu et al . NIPS 2016 . ( b ) . Graph - Structured Representations for Visual Question Answering . Teney et al. arXiv 2016 . At most recent , we have seen some more explicit way for visual grounding like : ( c ) . Bottom - up and top-down attention for image captioning and VQA ( https://arxiv.org/abs/1707.07998). Anderson et al. arXiv 2017 .

2 . Since the model is aimed at grounding the language on the vision based on interactions , it is worth to show how well the final model could ground the text words to each of the visual objects . Say , show the affinity matrix between the words and the objects to indicate the correlations .

[ Summary ]

I think this is a good paper which integrates vision , language , and actions in a virtual environment . I would foresee more and more works will be devoted to this area , considering its close connection to our daily life . To address this problem , the authors proposed a simple model to ground words on visual signals , which prove to outperform previous methods , such as CA , SAN , etc . According to the visualization , the model could attend the right region of the image for finishing a navigation and QA task . As I said , the authors should rephrase the definition of explicit grounding , to make it clearly distinguished with the previous work I listed above . Also , the authors should definitely show the grounding attention results of words and visual signal jointly , i.e. , showing them together in one figure instead of separately in Figure 9 and Figure 10 .


Thanks for your comments !

We agree with the reviewer that our original definition of explicit
grounding had some ambiguity . Thus we added several paragraphs to
elaborate on this . Because then the original section 2.2.1 became so long that we divided it into two ( 2.2.1 and 2.2.2 ) . Specifically , we rephrased section 2.2.1 ( -> 2.2.2 ) by giving a detailed definition about what it means for a framework to have an explicit grounding strategy . We also discussed the similarities and differences of our grounding with the related work pointed out by the reviewer at the end of section 2.2.1 ( -> 2.2.2 ) .

In summary , our explicit grounding requires two extra properties on top
of the soft attention mechanism :

1 ) the grounding ( image attention ) of a sentence is computed based on
the grounding results of the individual words in that sentence ( i.e. , compositionality ) ;

2 ) in the framework , there are no other types of language- vision
fusions besides this kind of groundings by 1 )

One benefit of such an explicit grounding is that Eq 2 achieves a
language " bottleneck " for downstream predictors ( as Reviewer 2 pointed
out in the comment ) . This bottleneck is used for both NAV and QA . It
implies an " independence of path " property because given the image all
that matters for the NAV and QA tasks is the attention $ x $ ( Eq 2 ) . It
guarantees , via the model architecture , that the agent will perform
completely in the same way on the same image even given different
sentences as long as their $ x$ are the same . Also because $ x$ is
explicit , the roles played by the individual words of $ s$ in
generating $ x$ are interpretable . This is in contrast to Eq 1 where
the roles of individual words are unclear . The interpretability
provides a possibility of establishing a link between language
grounding and prediction . We argue that these are crucial reasons that
account for our strong generalization in both ZS1 and ZS2 settings .

We have modified the original Figure 10 so that the image attention is
visualized jointly with the word attention . More examples are shown
now after the modification . Because of a limited space , we moved this
part to Appendix A and divided it into six figures ( Figure 10 - Figure 15 ) .


The paper introduces XWORLD , a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks . Agents working in this setting therefore , learn the language of the " teacher " and efficiently ground words to their respective concepts in the environment . The work also propose a neat model motivated by the environment and outperform various baselines .

Further , the paper evaluates the language acquisition aspect via two zero - shot learning tasks -- ZS1 ) A setting consisting of previously seen concepts in unseen configurations ZS2 ) Contains new words that did not appear in the training phase .

The robustness to navigation commands in Section 4.5 is very forced and incorrect -- randomly inserting unseen words at crucial points might lead to totally different original navigation commands right ? As the paper says , a difference of one word can lead to completely different goals and so , the noise robustness experiments seem to test for the biases learned by the agent in some sense ( which is not desirable ) . Is there any justification for why this method of injecting noise was chosen ? Is it possible to use hard negatives as noisy / trick commands and evaluate against them for robustness ?

Overall , I think the paper proposes an interesting environment and task that is of interest to the community in general . The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks ( in 3D , say ) .

Thanks for your comments .

The experiment of robustness is aimed at testing the agent in a
scenario out of our control , such as executing navigation commands
elicited by human after the training is done . In such case , we simply
assume that the evaluator does not have any knowledge of the training
process . A natural - language sentence elicited by the human evaluator
might convey a meaning that is similar or same to a sentence generated
by our grammar , however , it might not be that well - formed ( e.g. ,
containing extra irrelevant words ) . One simple way of simulating this
scenario ( incompletely ) is to insert noisy word embeddings into the original sentence .

This preliminary experiment serves to provide some numbers to let the
readers have a rough idea about how well the agent will perform in an
uncontrollable setting . However , because of its minor significance and
a possible misunderstanding , we have removed this section ( 4.5 ) from
the original paper .


The authors propose a new measure to capture the inherent randomness of the performance of a neural net under different random initialisations and / or data inputs . Just reporting the best performance among many random realisations is clearly flawed yet still widely adopted . Instead , the authors propose to compute the so-called best- out - of -n performance , which is the expected best performance under n random initialisations .

Pros :
- The widespread reporting of just the best model is clearly leading to very biased results and does not help with reproducibility . Any effort to mitigate this problem is thus welcome .
- The proposed quantity is simple to compute if we have m realisations of the same model under different random inputs ( random initialisation or random data ) and will converge to a stable limit even if m is very large .

Cons :
- The best -out -of -n performance is well grounded if we have different random inputs such as random initial parameters or random batch processing . Arguably , there is even larger variance if the model parameters such as number of layers , layer size etc are varied . Yet these variations can not really be captured by the best- out -of -n performance indicator unless modelled as random variables ( which would lead to different sorts of problems ) .
- Computationally it requires to have a large number m of replications which is not always feasible .
- Most importantly : the proposed way is just one of many ways to reduce the distribution of performances to a single scalar quantity . Why is it better than just reporting a specific quantile , for example ? Perhaps any such attempt to reduce to a single scalar is flawed and we should report the full distribution ( or first and second moment , or several quantiles ) . For example : the boo -n performance gets better if the outcome is highly variable compared to a model where the mean performance is identical but the outcome much less variable . High variance of the performance can be negative or positive , depending on the application and the choice of boo -n is making a singular choice just as if we chose the mean or min or max or a specific quantile .














- We address the first point in the common response above .

- Yes , usage of the proposed method may require a large number of replications . However , this requirement stems from the degree of stochasticity in training . If we used any other statistical technique , we believe it would require a comparable amount of replications . If one really cannot afford to run so many replications , one should still try to estimate the resulting confidence interval and hence at least disclose the uncertainty in reported results . Today this uncertainty is still there , just unreported .

- Yes , we agree that researchers should publish as much information as possible about the performance distribution of their architecture , which may allow the reader to calculate the characteristic that interests her the most ( whether it be mean , Boo_n , or a quantile ) . However , we believe that scalar metrics do have their value as proxies for comparing models - this usage now has an important place in Machine Learning research . This is why we are trying to propose an improvement in this area .
As to why we consider Boo_n better than the alternatives ( e.g. mean , quantile ) , we believe that it best captures what may interest a practitioner intending to deploy the model : He may have the capacity to train n models and deploy the best one . Our score directly captures what performance to expect under such scenario .

This manuscript raises an important issue regarding the current lack of standardization regarding methods for evaluating and reporting algorithm performance in deep learning research . While I believe that raising this issue is important and that the method proposed is a step in the right direction , I have a number of concerns which I will list below . One risk is that if the proposed solution is not adequate or widely agreeable then we may find a proliferation of solutions from which different groups might pick and choose as it suits their results !

The method of choosing the best model under ' internal ' cross-validation to take through to ' external ' cross-validation against a second hold - out set should be regarded as one possible stochastic solution to the optimisation problem of hyper- parameter selection . The authors are right to emphasize that this should be considered part of the cost of the technique , but I would not suggest that one specify a ' benchmark ' number of trials ( n=5 ) for comparison . Rather I would suggest that this is a decision that needs to be explored and understood by the researchers presenting the method in order to understand the cost / benefit ratio for their algorithm provided by attempting to refine their guess of the optimal hyperparameters . This would then allow for other methods not based on internal cross-validation to be compared on a level footing .

I think that the fundamental issue of stochasticity of concern for repeatability and generalisability of these performance evaluation exercises is not in the stochastic optimisation search but in the use of a single hold - out sample . Would it not be wise to insist on a mean performance ( a mean Boo_n or other ) over multiple random partitions of the entire dataset into training and hold - out ? I wonder if in theory both the effect of increasing n and the mean hold - out performance could be learnt efficiently with a clever experimental design .

Finally , I am concerned with the issue of how to compute the suggested Boo_n score . Use of a parameteric Gaussian approximation is a strong assumption , while bootstrap methods for order statistics can be rather noisy . It would be interesting to see a comparison of the results from the parametric and non-parameteric Boo_n versions applied to the test problems .


- We admit that we add " yet another " method of evaluation . This does indeed create an opportunity for cherrypicking . However , we believe it has value to expand the pool of options in a situation where a large part of the community has settled on a standard we consider flawed . The situation may temporarily become messier , but we consider this a necessary step towards a new equilibrium , hopefully with better evaluation standards .

- We will add the parametric / non- parametric estimator comparison to the paper soon .

We add some additional remarks in the common response above .

This paper addresses multiple issues arising from the fact that commonly reported best model performance numbers are a single sample from a performance distribution . These problems are very real , and they deserve significant attention from the ML community . However , I feel that the proposed solution may actually compound the issues highlighted .

Firstly , the proposed metric requires calculation of multiple test set experiments for every evaluation . In the paper up to 100 experiments were used . This may be reasonable in scenarios where the test set is hidden , and individual test numbers are never revealed . It also may be reasonable if we cynically assume that researchers are already running many test - set evaluations . But I am very opposed to any suggestion that we should relax the maxim that the test set should be used only once , or as close to once as is possible . Even the idea of researchers knowing their test set variance makes me very uneasy .

Secondly , this paper tries to account for variation in results due to different degrees of hyper- parameter tuning . This is certainly an admirable aim , since different research groups have access to very different types of resources . However , the suggested approach relies on randomly picking hyper- parameters from " a range that we previously found to work reasonably well " . This randomization does not account for the many experiments that were required to find this range . And the randomization is also not extended to parameters controlling the model architecture ( I suspect that a number of experiments went into picking the 32 layers in the ResNet used by this paper ) . Without a solid and consistent basis for these hyper - parameter perturbations , I worry that this approach will fail to normalize the effect of experiment numbers while also giving researchers an excuse to avoid reporting their experimental process .

I think this is a nice idea and the metric does merge the stability and low variance of mean score with the aspirations of best score . The metric may be very useful at development time in helping researchers build a reasonable expectation of test time performance in cases where the dev and test sets are strongly correlated . However , for the reasons outlined above , I do n't think the proposed approach solves the problems that it addresses . Ultimately , the decision about this paper is a subjective one . Are we willing to increase the risk of inadvertent hyper - parameter tuning on the test set for the sake of a more stable metric ?

Regarding your concern with multiple test set evaluations : Yes , there are risks associated with it ; however , we do not think using Boo_n would significantly change the current situation , which already mostly relies on the honesty of researchers - this is to some extent unavoidable in science .
Even with Boo_n the researchers still can keep the test scores hidden aside and calculate the final Boo_n score only when they finished tuning the architecture and running the experiments ( or even run evaluations with trained models only in the end ) .

More importantly , we argue in the paper that reporting the test performance of a single model does not have that much scientific value . It 's just a single sample drawn from a distribution , so we think it is not an appropriate way to characterize the performance distribution .

To some extent , this draws on a distinction between doing science and simply competing on a challenge , e.g. on Kaggle . In the latter case , we should focus on fair conditions for competitors and your objections would be very appropriate . In the former case , we should mainly try to well characterize the behaviour of the model on which the researchers are publishing their findings . There , we believe that multiple test evaluation brings better insight .

We address hyper - parameter tuning in the common response above .

To increase robustness to adversarial attacks , the paper fundamentally proposes to transform an input image before feeding it to a convolutional network classifier . The purpose of the transformation is to erase the high - frequency signals potentially embedded by an adversarial attack .

Strong points :

* To my knowledge , the proposed defense strategy is novel ( even if the idea of transformation has been introduced at https://arxiv.org/abs/1612.01401).

* The writing is reasonably clear ( up to the terminology issues discussed among the weak points ) , and introduces properly the adversarial attacks considered in the work .

* The proposed approach really helps in a black - box scenario ( Figure 4 ) . As explained below , the presented investigation is however insufficient to assess whether the proposed defense helps in a true white - box scenario .


Weak points :

* The black - box versus white - box terminology is not appropriate , and confusing . In general , black - box means that the adversary ignores everything from the decision process . Hence , in this case , the adversary does not know about the classification model , nor the defensive method , when used . This corresponds to Figure 3 . On the contrary , white - box means that the adversary knows everything about the classification method , including the transformation implemented to make it more robust to attacks . Assimilating the parameters of the transform to a secret key is not correct because those parameters could be inferred by presenting many image samples to the transform and looking at the outcome of the transformation ( which is supposed to be available in a ' white - box ' paradigm ) for those samples .

* Using block diagrams would definitely help in presenting the training / testing and attack / defense schemes investigated in Figure 3 , 4 , and 5 .

* The paper does not discuss the impact of the denfense strategy on the classification performance in absence of adversity .

* The paper lacks of positioning with respect to recent related works , e.g. ' Adversary Resistant Deep Neural Networks with an Application to Malware Detection ' in KDD 2017 , or ' Building Adversary - Resistant Deep Neural Networks without
Security through Obscurity ' at https://arxiv.org/abs/1612.01401.

* In a white - box scenario , the adversary knows about the transformation and the classification model . Hence , an effective and realistic attack should exploit this knowledge . Designing an attack in case of a non differentiable transformation is obviously not trivial since back - propagation can not be used . However , since the proposed transformation primarily aim at removing the high frequency pattern induced by the attack , one could for example design an attack that account for a ( linear and differentiable ) low - pass filter transformation . Another example of attack that account for transformation knowledge ( and would hopefully be more robust than the attacks considered in the manuscript ) could be one that alternates between a conventional attack and the transformation .

* If I understand correctly , the classification model considered in Figure 3 has been trained on original images , while the one in Figure 4 has been trained on transformed images . However , in absence of attack , they both achieve 76 % accuracy . Is it correct ? Does it mean that the transformation does not affect the classification accuracy at all ?


Overall , the works investigates an interesting idea , but lacks maturity to be accepted . Therefore , I would only recommend acceptation if room .

Minor issues :

Typo on p7 : to change *s *
Clarify poor formulations :
* p 1 : 'enforce model - specific strategies that enforce model properties such as invariance and smoothness via the learning algorithm or regularization schemes ' .
* p 1 : ' too simple to remove adversarial perturbations from input images sufficiently '

Thank you for your insightful comments on our work , which have been very helpful in improving the paper !

* The black - box versus white - box terminology is not appropriate ...

As several public comments have pointed out , the white - box terminology can be misleading . Some of our experiments are performed in a " gray - box " setting in which the adversary has access to the network parameters , but not to the quilting database that acts as a kind of " secret key " . We believe that this gray - box setting is of practical interest because the quilting process is stochastic and because the adversary never directly observes the quilted images themselves : this makes it very difficult for the adversary to exactly reproduce the quilted images that the defender produces . Per your suggestion , we have clarified the learning - setting terminology in the revised version of the paper .

* Using block diagrams would definitely help in presenting the training / testing and attack / defense schemes investigated in Figure 3 , 4 , and 5 .

Per your suggestion , we have added block diagrams clarifying the workflow of our attack / defense schemes in the revised version of the paper .

* The paper does not discuss the impact of the defense strategy on the classification performance in absence of adversity .

The first row of Tables 1 and 2 present the accuracy of various defenses on non-adversarial images ( " no attack " ) . In Figures 3 , 4 and 5 , the y-axis value corresponding to normalized L2 -dissimilarity of 0 corresponds to the accuracy on non-adversarial images . We have emphasized this point in the table and figure captions in the revised version of the paper .

* The paper lacks of positioning with respect to recent related works , e.g. ' Adversary Resistant Deep Neural Networks with an Application to Malware Detection ' in KDD 2017 , or ' Building Adversary - Resistant Deep Neural Networks without Security through Obscurity ' at https://arxiv.org/abs/1612.01401.

Thank you for pointing out these references , which we were unaware of at the time of submission . Both approaches are similar to our defenses in the sense that they focus on non-differentiable , stochastic transformations . Having said that , there are also substantial differences between our study and those related works . The first paper relies on LLE to represent data points as a linear combination of nearest neighbors : this approach may certainly be suitable for certain kinds of data , but is unlikely to work very well in extremely high - dimensional spaces such as the Image Net pixel space . The second paper 's approach of randomly removing blocks of pixels is related to our image - cropping baseline defense , which is one of our baselines . We have included positioning with respect to these works in the revised version of the paper .

* In a white - box scenario , the adversary knows about the transformation and the classification model . Hence , an effective and realistic attack should exploit this knowledge ...

In white - box settings , it may , indeed , be possible to devise attacks that are tailored towards a particular defense . In our work , we have tried to make the development of such attacks non-trivial by making our defenses non-differentiable and stochastic . Having said that , it may certainly be possible to devise attack strategies that are successful nevertheless ( such as the strategy sketched in our response to AnonReviewer3 ) . We leave the investigation of attacks that are tailored to our defenses to future work .

* If I understand correctly , the classification model considered in Figure 3 has been trained on original images , while the one in Figure 4 has been trained on transformed images . However , in absence of attack , they both achieve 76 % accuracy . Is it correct ? Does it mean that the transformation does not affect the classification accuracy at all ?

The 76 % accuracy is obtained by a convolutional network that is trained and tested on images on which no defense ( i.e. , input transformation ) is applied . The " no defense " baseline is this exactly the same in both Figures 3 and 4 . For defenses such as TV minimization and quilting , the accuracy on non-adversarial images is lower ( both in Figure 3 and 4 ) , which shows that the transformations , indeed , do negatively impact classification accuracy on non-adversarial images .

Summary : This works proposes strategies to make neural networks less sensitive to adversarial attacks . They consist into applying different transformations to the images , such as quantization , JPEG compression , total variation minimization and image quilting . Four adversarial attacks strategies are considered to attack a Resnet50 model for classification of Imagenet images .
Experiments are conducted in a black box setting ( when the model to attack is unknown by the adversary ) or white box setting ( the model and defense strategy are known by the adversary ) .
60 % of attacks are countered in this last most difficult setting .
The previous best approach for this task consists in ensemble training and is attack specific . It is therefore pretty robust to the attack it was trained on but is largely outperformed by the authors methods that manage to reduce the classifier error drop below 25 % .

Comments : The paper is well written , the proposed methods are well adapted to the task and lead to satisfying results .

The discussion remarks are particularly interesting : the non differentiability of the total variation and image quilting methods seems to be the key to their best performance in practice .
Minor : the bibliography should be uniformed .

Thanks for your positive evaluation of our paper ! Per your suggestion , we have updated the bibliography entries to make them uniform .

The paper investigates using input transformation techniques as a defence against adversarial examples . The authors evaluate a number of simple defences that are based on input transformations such TV minimization and image quilting and compare it against previously proposed ideas of JPEG compression and decompression and random crops . The authors have evaluated their defences against four main kinds of adversarial attacks .

The main takeaways of the paper are to incorporate transformations that are non-differentiable and randomised . Both TV minimisation and image quilting have that property and show good performance in withstanding adversarial attacks in various settings .

One argument that I am not sure would be applicable perhaps and could be used by adversarial attacks is as follows : If the defence uses image quilting for instance and obtains an image $ P$ that approximates the original observation $ X$ , it could be possible to use a model based approach that obtains an observation $ Q$ that is close to $ P$ which can be attacked using adversarial attacks . Would this observation then be vulnerable to such attacks ? This could perhaps be explored in future .

The paper provides useful contributions in forming model agnostic defences that could be further investigated . The authors show that the simple input transformations advocated work against the major kind of attacks . The input transformations of TV minimization and image quilting share varying characteristics in terms of being sensitive to various kinds of attacks and therefore can be combined . The evaluation is carried out on Image Net dataset with large number of examples .

Thank you for your insightful comments , and positive evaluation of work !

Regarding model - based approaches for attacking our quilting defense : we agree this may the most viable option for attacking our defense . As you suggest , it may be possible for the adversary to construct its own patch database , and use it to construct quilted images that may be sufficiently similar to the quilted image created using our " secret database " . The remaining issue for the adversary is then to backpropagate gradients through the quilting transformation : the adversary may be able to do this by training a pixel - to- pixel network that learns to produce the quilted image given an original image , and using this network to approximate gradients . We intend to investigate such attack approaches in future work . We have updated our paragraph describing future work to reflect this .


This paper introduces a neural network architecture for generating sketch drawings . The authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts . I agree . The contribution of this paper of this paper is two -fold . Firstly , the paper introduces a large sketch dataset that future papers can rely on . Secondly , the paper introduces the model for generating sketch drawings .

The model is inspired by the variational autoencoder . However , the proposed method departs from the theory that justifies the variational autoencoder . I believe the following things would be interesting points to discuss / follow up :
- The paper preliminarily investigates the influence of the KL regularisation term on a validation data likelihood . It seems to have a negative impact for the range of values that are discussed . However , I would expect there to be an optimum . Does the KL term help prevent overfitting at some stage ? Answering this question may help understand what influence variational inference has on this model .
- The decoder model has randomness injected in it at every stage of the RNN . Because of this , the latent state actually encodes a distribution over drawings , rather than a single drawing . It seems plausible that this is one of the reasons that the model cannot obtain a high likelihood with a high KL regularisation term . Would it help to rephrase the model to make the mapping from latent representation to drawing more deterministic ? This definitely would bring it closer to the way the VAE was originally introduced .
- The unconditional generative model * only * relies on the " injected randomness " for generating drawings , as the initial state is initialised to 0 . This also is not in the spirit of the original VAE , where unconditional generation involves sampling from the prior over the latent space .

I believe the design choices made by the authors to be valid in order to get things to work . But it would be interesting to see why a more straightforward application of theory perhaps * does n't * work as well ( or whether it works better ) . This would help interesting applications inform what is wrong with current theoretical views .

Overall , I would argue that this paper is a clear accept .

It is true that our design choices were made to get things to work , and despite this , the current model still has many issues that can be improved upon in the future . For example , the model does not perform well for long sequence length . We needed to use the Ramer –Douglas – Peucker ( RDP ) algorithm to simplify the strokes , which also made the data more consistent for the RNN . We have included these details and tried to put information about model limitations in the A1 Dataset Details section .

With your feedback , and also along with the feedback from AnonReviewer3 , we have added a short section in A6 that examines the tradeoff between likelihood and KL . We examine what happens qualitatively to the sketches as we vary the weighting on the KL term . Hopefully this will be a good starting point for future work .

In future work we will explore in depth the regularisation methodology - perhaps KL is not the best one to use and we wish to explore alternative approaches , for example alternatives outlined in [ 1 ] .

[ 1 ] InfoVAE : Information Maximizing Variational Autoencoders ( https://arxiv.org/abs/1706.02262).

The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN - variational autoencoder . Each node is represented with ( dx , dy ) along with one - hot representation of three different drawing status . A bi-directional LSTM is used to encode latent space in the training stage . Auto-regressive VAE is used for decoding .

Similar to standard VAEs , log-likelihood has bee used as the data-term and the KL divergence between latent space and Gaussian prior is the regularisation term .

Pros :
- Good solution to an interesting problem .
- Very interesting dataset to be released .
- Intensive experiments to validate the performance .

Cons :
- I am wondering whether the dataset contains biases regarding ( dx , dy ) . In the data collection stage , how were the points lists generated from pen strokes ? Did each points are sampled from same travelling distance or according to the same time interval ? Are there any other potential biases brought because the data collection tools ?
- Is log-likelihood a good loss here ? Think about the case where the sketch is exactly the same but just more points are densely sampled along the pen stroke . How do you deal with this case ?
- Does the dataset contain more meta-info that could be used for other tasks beyond generation , e.g. segmentation , classification , identification , etc .?

Regarding the data collection , we have used the Ramer – Douglas – Peucker ( RDP ) algorithm as a pre-processing step to simplify the strokes in the dataset . Using RDP , line strokes drawn very slowly ( with many points ) and drawn very swiftly with look similar after the simplification process . For example , if the user holds his or her finger on the screen in one location for many seconds while sketching something , many points will be generated at a single location , but the simplification method will collapse those points as a single point . We put details of the data collection and stroke simplification in A1 . Dataset Details .

The dataset will contain meta-info , such as country information , timestamp , and class , so we hope it can be used for classification experiments , and even for exploring cultural biases in the way we draw .

You raise an interesting point about whether log-likelihood is a good loss , especially in the case " where the sketch is exactly the same but just more points are densely sampled along the pen stroke " . Based on your feedback , and also the feedback of AnonReviewer2 , we have added a section in A6 " Which Loss Controls Image Coherency ? " , where we look at whether the KL loss term helps in such cases .

We explore the tradeoff between varying weights of the KL loss term and see that increasing the KL weighting produces qualitatively better reconstructions , despite having a lower log-likelihood loss number . We will investigate alternative loss formulations in future work , perhaps looking at adversarial methods , but we hope this will be a good start in that direction .

The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches .

+ new and large dataset
+ novel algorithm
+ well written
- no evaluation of dataset
- virtually no evaluation of algorithm
- no baselines or comparison

The paper is well written , and easy to follow . The presented algorithm sketch - rnn seems novel and significantly different from prior work .
In addition , the authors collected the largest sketch dataset , I know of . This is exciting as it could significantly push the state of the art in sketch understanding and generation .

Unfortunately the evaluation falls short . If the authors were to push for their novel algorithm , I 'd have expected them to compare to prior state of the art on standard metrics , ablate their algorithm to show that each component is needed , and show where their algorithm shines and where it falls short .
For ablation , the bare minimum includes : removing the forward and / or reverse encoder and seeing performance drop . Remove the variational component , and phrasing it simply as an auto-encoder . Table 1 is good , but not sufficient . Training loss alone likely does not capture the quality of a sketch .
A comparison the Graves 2013 is absolutely required , more comparisons are desired .
Finally , it would be nice to see where the algorithm falls short , and where there is room for improvement .

If the authors wish to push their dataset , it would help to first evaluate the quality of the dataset . For example , how well do humans classify these sketches ? How diverse are the sketches ? Are there any obvious modes ? Does the discretization into strokes matter ?
Additionally , the authors should present a few standard evaluation metrics they would like to compare algorithms on ? Are there any good automated metrics , and how well do they correspond to human judgement ?

In summary , I 'm both excited about the dataset and new architecture , but at the same time the authors missed a huge opportunity by not establishing proper baselines , evaluating their algorithm , and pushing for a standardized evaluation protocol for their dataset . I recommend the authors to decide if they want to present a new algorithm , or a new dataset and focus on a proper evaluation .

We agree with the reviewer that we try to do many things in this paper - introduce a method for generating vector images and introducing a large dataset of vector drawings , and entered a less - explored area with few established evaluation metrics .

As the dataset , and area of vector image modelling is new , our architecture was designed with simplicity in mind to become a baseline for future work . You mentioned Graves 2013 , a work that we actually based our method on . Specifically : once we take away the encoder , and generate images unconditionally using the decoder model , it is identical to the autoregressive modelling approach taken in Graves 2013 . The only minor difference is we needed to model the " end of drawing " probability and have appended the model to output that as well . You are right to mention that we should compare our encoder to a forward - only RNN , to see the metrics drop , although in practice we would argue that most practitioners would choose the bi-directional method from the onset especially when the length of the data becomes longer , and the architecture and task makes it possible to use a non-causal model . For example , while Graves 2013 uses a unidirectional LSTM for decoder - only handwriting generation , Graves 2007 [ 1 ] uses a bidirectional LSTM for handwriting classification without comparing to a unidirectional one .

You make some great points about the metrics relating to the dataset . We are particularly interested in how the diversity and multi-modality of these drawings relate to issues like novelty and interpretability . In our view , these human perception and generation issues are complex and important enough to warrant their own future paper ( s ) . To make it more convenient for future work on this dataset , we have also standardized the format and limited each class to have exactly 70 K samples , 2.5 K validation and test samples , rather than using the full extent of the dataset . We hope this standardisation will encourage future experiments in not just generation , but also classification , and also examination into cultural biases , diversity , modes , and human performance .

[ 1 ] A. Graves , S. Fernández , M. Liwicki , H. Bunke and J. Schmidhuber . Unconstrained online handwriting recognition with recurrent neural networks . NIPS 2007 , Vancouver , Canada .

https://papers.nips.cc/paper/3213-unconstrained-on-line-handwriting-recognition-with-recurrent-neural-networks


The generative model comprises a real - valued matrix M ( with a multivariate normal prior ) that serves
as the memory for an episode ( an unordered set of datapoints ) . For each datapoint a marginally independent
latent variable y_t is used to index into M and realize a conditional density
of another latent variable z. z_t is used to generate the data .

The proposal of learning with a probabilistic memory is interesting and the framework proposed is elegant and cleanly explained . The model is evaluated on the following tasks :
* Qualitative results on denoising and one - shot generation using the Omniglot dataset .
* Qualitative results on sampling from the model using the CIFAR dataset .
* Likelihood estimation on the Omniglot dataset

Questions and concerns :

The model appears novel and is interesting , the experiments , however , are lacking in that they
do not compare against other any recently proposed memory augmented deep generative models [ Bornschein et al ] and [ Li et. al ] ( https://arxiv.org/pdf/1602.07416.pdf). At the very minimum , the paper should include a discussion and a comparison with the latter . Doing so will help better understand what is gained from using retaining a probabilistic form of memory versus a determinstic memory indexed with attention as in [ Li et. al ].

How does the model perform as a function of varying T ( size of episodes ) during training ? It would be interesting to see how well the model performs in the limiting case of T=1.

What is the task being solved in Section 4.4 by the DNC and the Kanerva machine ? Please state this in the main paper .

Training and Evaluation : There is a mismatch in the training and evaluation procedure the implications of which I do n't
fully understand yet . The text states that the model was trained where each observation in an episode comprised randomly sampled datapoints . This corresponds to a generative process where ( 1 ) a memory is randomly drawn , ( 2 ) each observation in the episode is an independent draws from the memory conditioned decoder . During training ,
points in an episode are randomly selected . At test time , ( if I understand correctly , please correct me if I have n't ) , the model is evaluated by having multiple copies of the same test point within an episode . Is that correct ? If so , does n't that correspond to evaluating the model under a different generative assumption ? Why is this OK ?

Likelihood evaluation : Could you expand on how the ELBO of 68.3 is computed under the model for a single test image in the Omniglot dataset ? The text says that the likelihood of each data-point was divided by T ( the length of the episode considered ) . This seems at odds with models , such as DRAW , evaluate the likelihood -- once at the end of the generative drawing process . What is the per-pixel likelihood obtained on the CIFAR dataset and what is the likelihood on a model where T=1 ( for omniglot / cifar ) ?

Using Labels : Following up on the previous point , what happens if labelled information from Omniglot or CIFAR is used to define points within an episode during the training procedure ? Does this help or hurt performance ?

For the denoising comparison , how do the results compare to those obtained if you simulate a Markov Chain ( sample latent state conditioned on noisy image , sample latent state , sample denoised observation , repeat using denoised observation ) using a VAE ?

The model appears novel and is interesting , the experiments , however , are lacking in that they
do not compare against other any recently proposed memory augmented deep generative models ... the paper should include a discussion and a comparison with the latter ...

-- We agree that these works should be better highlighted in our manuscript . We have added a new paragraph ( 3rd in the Discussion section ) to describe the relations with these 2 papers .
While both of these models share some commonalities with our model , they also have key differences which make direct experimental comparisons problematic . As we describe in more detail in the Discussion , our paper addresses a different , although related , problem --- updating memory optimally . As you mentioned , such update is not possible with the memory in Li et al. , which is fixed over the course of episodes . Similarly , the likelihoods from our model have a very different meaning from Bornschein et al. , since the only ambiguity in retrieving stored patterns in their model was in the categorical addressing variable ; their model stores images in memory in the form of raw-pixels . We instead store compressed embeddings in a distributed fashion . As a result , the objective function we use ( eq. 2 ) becomes a constant 0 for the model in Bornschein et al. , since the mutual information between the memory and an episode of images I ( X ; M ) is simply the entropy of these images H( X ) , when all these images are directly stored in the memory .

How does the model perform as a function of varying T ( size of episodes ) during training ? ... the limiting case of T=1.

-- The performance under varying T is shown in figure 6 ( right ) . There is a smooth rise of test loss with increasing T , and T=1 does not seem to be very different .

What is the task being solved in Section 4.4 by the DNC and the Kanerva machine ? Please state this in the main paper .

-- We now clarify this in the main paper . It is the same episode storage and retrieval task as earlier in the paper , only we now look at the test regime where episode lengths are longer .

Training and Evaluation : There is a mismatch in the training and evaluation procedure the implications of which I do n't fully understand yet …

-- With one exception there is no mismatch in training and evaluation , though we can now see where confusion may have crept in . We have revised the description in the paper to clarify this . In general , training and testing follow the same sampling procedure in constructing the episodes . The single exception is in the omniglot one - shot generation and comparison with DNC , where we control the number of Omniglot classes in an episode during testing for illustrative purpose only [ i.e. to illustrate what happens with different levels of redundancy in the input data ] . For all other comparisons the train and test losses and visualisation are identical .

Likelihood evaluation : ... how the ELBO of 68.3 is computed … This seems at odds with models , such as DRAW … What is the per-pixel likelihood obtained on the CIFAR dataset ...?

-- “ T ” has different meanings in our model and in DRAW . DRAW is an autoregressive model that uses T steps to construct * one * image ; in our model , T is the number of images , so we divide the total log-likelihood of T ( conditionally ) independent images by T for comparison . The likelihood of 5329 can be converted to the per-pixel bits 4.4973 .

-- To get the number of 68.3 , we first compute the ELBO for the conditional log-likelihood of an episode with 32 images , which is log P ( x_1 , x_2 , … x_32 | M ) = 2185.6 . Since log P ( x_1 , x_2 , … x_32 | M ) = log P( x_1|M ) + log P( x_1|M ) + ..+ log P( x_32|M ) ( conditional independence / exchangeability ) , we can compute the average ELBO for each image by dividing 2185.6 / 32 = 68.3.

Using Labels : ... what happens if labelled information from Omniglot or CIFAR is used to define points within an episode during the training procedure ?

-- This is an interesting point . We think it will help performance , since the additional label information may help the model further reduce redundancy . We only trained on the worst case scenario without such information .

How do the results compare to those obtained if you simulate a Markov Chain using a VAE ?

-- We tried iterative sampling using a VAE as well . However , iterative sampling did not improve performance with a VAE --- which is why , to the best of our knowledge , it has not been used in previous literature . In our model iterative sampling works because of the structure of the generative model ( section 3.5 ) . We now discuss this in the revision and illustrated it in a new figure ( Figure 8 in the Appendix ) .

Many thanks for the comments which have helped us improve the manuscript . If you still feel that there are issues with the manuscript that would prevent you from raising your score , please point these out so that we can address them .

The paper presents the Kanerva Machine , extending an interesting older conceptual memory model to modern usage . The review of Kanerva ’s sparse distributed memory in the appendix was appreciated . While the analyses and bounds of the original work were only proven when restricted to uniform and binary data , the extensions proposed bring it to modern domain of non- uniform and floating point data .

The iterative reading mechanism which provides denoising and reconstruction when within tolerable error bounds , whilst no longer analytically provable , is well shown experimentally .
The experiments and results on Omniglot and CIFAR provide an interesting insight to the model 's behaviour with the comparisons to VAE and DNC also seem well constructed .

The discussions regarding efficiency and potential optimizations of writing inference model were also interesting and indeed the low rank approximation of U seems an interesting future direction .

Overall I found the paper well written and reintroduced + reframed a relatively underutilized but well theoretically founded model for modern use .

We appreciate your comments . Please let us know if you have any additional suggestions for the text or experiments that would further improve our paper , and potentially lead you to increase your score .

This paper generalizes the sparse distributed memory model of Kanerva to the Kanerva Machine by formulating a variational generative model of episodes with memory as the prior .

Please discuss the difference from other papers that implement memory as a generative model , i.e. ( Bornschein , Mnih , Zoran , Rezende 2017 )

A probabilistic interpretation of Kanerva ’s model was given before ( Anderson , 1989 http://ieeexplore.ieee.org/document/118597/ ) and ( Abbott , Hamrick , Griffiths , 2013 ) . Please discuss .

I found the relation to Kanerva ’s original model interesting and well explained . The original model was motivated by human long term memory and neuroscience . It would be nice if the authors can provide what neuroscience implications their work has , and comment on its biological plausibility .

Thank you for your comments .

We agree these papers should be discussed and have added new text ( paragraphs 2 and 3 in the Discussion ) to describe this work .

With regard to biological plausibility , we believe our main contribution is at the computational ( rather than implementation ) level : i.e. by providing a model that can be used in the context of complex memory tasks . As we focused on developing a functional and useful machine learning model , we do n’t make any claims about biological plausibility beyond the relationship with Kanerva ’s model , whose distributed structure was motivated by understanding of the brain .

Please let us know if you have any additional suggestions for the text or experiments that would further improve our paper , and potentially lead you to increase your score .


Overview :
This paper proposes an approach to curriculum learning , where subsets of examples to train on are chosen during the training process . The proposed method is based on a submodular set function over the examples , which is intended to capture diversity of the included examples and is added to the training objective ( eq. 2 ) . The set is optimized to be as hard as possible ( maximize loss ) , which results in a min-max problem . This is in turn optimized ( approximately ) by alternating between gradient - based loss minimization and submodular maximization . The theoretical analysis shows that if the loss is strongly convex , then the algorithm returns a solution which is close to the optimal solution . Empirical results are presented for several benchmarks .
The paper is mostly clear and the idea seems nice . On the downside , there are some limitations to the theoretical analysis and optimization scheme ( see comments below ) .

Comments :
- The theoretical result ( thm . 1 ) studies the case of full optimization , which is different than the proposed algorithm ( running a fixed number of weight updates ) . It would be interesting to show results on sensitivity to the number of updates ( p ) .
- The algorithm requires tuning of quite a few hyperparameters ( sec. 3 ) .
- Approximating a cluster with a single sample ( sec. 2.3 ) seems rather crude . There should be some theoretical and / or empirical study of its effect on quality of the solution .

Minor / typos :
- what is G ( j|G\j ) in eq . ( 9 ) ?
- why cite Anonymous ( 2018 ) instead of Appendix ...?
- define V in Thm . 1.
- in eq . ( 4 ) it may be clearer to denote g_k ( w ) . Likewise in eq . ( 6 ) \hat{g}_\hat { A} ( w ) , and in eq . ( 14 ) \tilde{g}_{\cal { A}} ( w ) .
- figures readability can be improved .

In the new revision , we 've added a 4.5 - page analysis to show the convergence speed of both outer - loop and the whole algorithm . A summary of the newly added analysis can be found in our new uploaded comments .

Reply to Comments :

Theorem 3 analyzes the convergence rate of the whole algorithm presented in Algorithm 1 with a fixed number of weight updates $ p$ in each inner-loop . The first term in the bound exponentially decreases with power $ p$ .

The convergence bounds in both Theorem 2 and Theorem 3 are functions of all hyperparameters $ \lambda$ , $ \ Delta $ and $ p$ . They show that exponentially decreasing $ \lambda$ is sufficient to guarantee a linear rate of convergence , while choosing small $ \ Delta$ and $ p$ make the algorithm efficient in computation . These theoretical analysis allows us to tune the hyperparameters in relatively small ranges .

Instead of representing the whole cluster by the centroid everywhere , we only represent the hardness of a cluster by the loss on its centroid . By setting the number of clusters to be a large value , e.g. , 1000 clusters for 50000 samples in our experiments , this hardness representation is accurate enough . It not only saves computation spent on submodular maximization in practice , but also makes the algorithm more robust to outliers , because it avoids selecting a single ( or a few number of ) outliers with extremely large loss .

Reply to Minor / typos :

G ( j|G\j ) contains a typo , it should be G ( j|V\j ) = G ( V ) - G ( V\j ) , the marginal gain of element j conditioned on all the other elements in ground set V except j . Thanks for pointing this out !

In the revision , 1 ) we changed all citations to Anonymous ( 2018 ) to specific sections in Appendix ; 2 ) we define V in Theorem 1 and all other Theorems ; 3 ) for simplicity of representation , we use g( ) without subscript when it causes no confusion . For example , Theorem 1 and Lemma 2 holds for any iteration in outer- loop , so we ignore the subscript of g( ) . When discussing relationship between different iterations of outer- loop , we add subscript to w in g( w ) ( e.g. , in proof of Theorem 2 ) or add subscipt to g ( ) ( e.g. , in proof of Theorem 3 ) .

This paper introduces MiniMax Curriculum learning , as an approach for adaptively train models by providing it different subsets of data . The authors formulate the learning problem as a minimax problem which tries to choose diverse example and " hard " examples , where the diversity is captured via a Submodular Loss function and the hardness is captured via the Loss function . The authors formulate the problem as an iterative technique which involves solving a minimax objective at every iteration . The authors argue the convergence results on the minimax objective subproblem , but do not seem to give results on the general problem . The ideas for this paper are built on existing work in Curriculum learning , which attempts to provide the learner easy examples followed by harder examples later on . The belief is that this learning style mimics human learners .

Pros :
- The analysis of the minimax objective is novel and the proof technique introduces several interesting ideas .
- This is a very interesting application of joint convex and submodular optimization , and uses properties of both to show the final convergence results
- Even through the submodular objective is only approximately solvable , it still translates into a convergence result
- The experimental results seem to be complete for the most part . They argue how the submodular optimization does not really affect the performance and diversity seems to empirically bring improvement on the datasets tried .

Cons :
- The main algorithm MCL is only a hueristic . Though the MiniMax subproblem can converge , the authors use this in somewhat of a hueristic manner .
- It seems somewhat hand wavy in the way the authors describe the hyper parameters of MCL , and it seems unclear when the algorithm converge and how to increase / decrease it over iterations
- The objective function also seems somewhat non-intuitive . Though the experimental results seem to indicate that the idea works , I think the paper does not motivate the loss function and the algorithm well .
- It seems to me the authors have experimented with smaller datasets ( CIFAR , MNIST , 20NewsGroups ) . This being mainly an empirical paper , I would have expected results on a few larger datasets ( e.g. ImageNet , Celeb Faces etc. ) , particularly to see if the idea also scales to these more real world larger datasets .

Overall , I would like to see if the paper could have been stronger empirically . Nevertheless , I do think there are some interesting ideas theoretically and algorithmically . For this reason , I vote for a borderline accept .

In the new revision , we add 4.5 - page analysis to show the convergence speed for both the outer - loop and the whole algorithm . A summary of the newly added analysis can be found in our new uploaded comments .

Reply to Cons :

Theorem 3 in the new revision gives the convergence analysis for the whole algorithm , each of whose inner-loop uses fixed number of updates to approximately solve a minimax problem . It does not only show convergence , but also shows convergence rate for both the inner-loop and outer- loop .

In Theorem 2 and Theorem 3 , we show convergence bounds as functions of all hyperparameters . These results give strong intuition for how to choose the hyperparameters . They show that exponentially decreasing $ \lambda$ is sufficient to guarantee a linear rate of convergence , while choosing small $ \ Delta$ and $ p$ make the algorithm efficient computationally . In practice , we use grid search with small ranges to achieve the hyperparameters used in experiments .

The intuitions behind the objetive function can be found in the two paragraphs above Section 1.1 , the last two paragraphs of Section 1.1 , and the first paragraph of Section 2 . In these places , we provide evidence based on the nature of machine learning model / algorithms , the similarity to the human teaching / learning process , and the comparison to previous works . In addition , the objective function has nice theoretical properties . Our newly added theoretical analysis supports that decreasing diversity weight $ \lambda$ and increasing hardness $ k$ can improve the convergence bound . This provides further theoretical support .

Our experiments verify several advantages of the proposed minimax curriculum learning across three different models and datasets . Our basic goal is to prove the idea of decreasing diversity and increasing hardness for general machine learning problems . This idea has never been studied before , either theoretically or empirically , as far as we know . We are working on experiments for much larger datasets such as ImageNet and COCO , and will make the results available as soon as we can .

The main strength of this paper , I think , is the theoretical result in Theorem 1 . This result is quite nice . I wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result further .

The authors ended the discussion on thm 1 on page 7 ( just above Sec 2.3 ) by saying what is sufficiently close to w* . If one goes back to ( 10 ) , it is easy to see that what converges to w* when one of three things happen ( assuming beta is fixed once loss L is selected ) .

1 ) k goes to infinity
2 ) alpha goes to 1
3 ) g( w* ) goes to 0

The authors discussed how alpha is close to 1 by virtue of submodular optimization lower bounds there for what is close to w* . In fact this proof shows the situation is much better than that .

If we are really concerned about making what converge to w* , and if we are willing to tolerate the increasing computational complexity associated solving submodular problems with larger k , we can schedule k to increase over time which guarantees that both alpha goes to 1 and g( w* ) goes to zero .

There is also a remark that G( A ) tends to be modular when lambda is small which is useful .
From the algorithm , it seems clear that the authors recognized these two useful aspects of the objective and scheduled lambda to decrease exponentially and k to increase linearly .

It would be really nice to complete the analysis of Thm1 with a formal analysis of convergence speed for ||what - w*|| as lambda and k are scheduled in this fashion . Such an analysis would help practitioners make better choices for the hyper parameters gamma and Delta .

Thanks for your positive comments about the theoretical analysis and helpful suggestions to extend Theorem 1 ! In the new revision , we 've added a 4.5 - page analysis . This does not only complete the analysis of Theorem 1 , but also shows the convergence speed for both the outer- loop and the whole algorithm , and show bounds as functions of hyperparameters . The results support our scheduling strategy for $ \lambda$ and $ k$ . A summary of the newly added analysis can be found in our new uploaded comments above your review comments .


The paper proposes a GAN - based method for image generation that attempts to separate latent variables describing fixed " content " of objects from latent variables describing properties of " view " ( all dynamic properties such as lighting , viewpoint , accessories , etc ) . The model is further extended for conditional generation and demonstrated on a range of image benchmark data sets .

The core idea is to train the model on pairs of images corresponding to the same content but varying in views , using adversarial training to discriminate such examples from generated pairs . This is a reasonable procedure and it seems to work well , but also conceptually quite straightforward -- this is quite likely how most people working in the field would solve this problem , standard GAN techniques are used for training the generator and discriminator , and the network architecture is directly borrowed from Radford et al. ( 2015 ) and not even explained at all in the paper . The conditional variant is less obvious , requiring two kinds of negative images , and again the proposed approach seems technically sound .

Given the simplicity of the algorithmic choices , the potential novelty of the paper lies more in the problem formulation itself , which considers the question of separating two sets of latent variables from each other in setups where one of them ( the " view " ) can vary from pair to pair in arbitrary manner and no attributes characterising the view are provided . This is an interesting problem setup , but not novel as such and unfortunately the paper does not do a very good job in putting it into the right context . The work is contrasted only against recent GAN - based image generation literature ( where covariates for the views are often included ) and the aspects related to multi-view learning are described only at the level of general intuition , instead of relating to the existing literature on the topic . The only relevant work cited from this angle is Mathieu et al . ( 2016 ) , but even that is dismissed lightly by saying it is worse in generative tasks . How about the differences ( theoretical and empirical ) between the proposed approach and theirs in disentangling the latent variables ? One would expect to see more discussion on this , given the importance of this property as motivation for the method .

The generative story using three sets of latent variables , one shared , to describe a pair of objects corresponds to inter-battery factor analysis ( IBFA ) and is hence very closely related to canonical correlation analysis as well ( Tucker " An inter - battery method of factor analysis " , Psychometrika , 1958 ; Klami et al. " Bayesian canonical correlation analysis " , JMLR , 2013 ) . Linear CCA naturally would not be sufficient for generative modeling and its non-linear variants ( e.g. Wang et al . " Deep variational canonical correlation analysis " , arXiv:1610.03454 , 2016 ; Damianou et al. " Manifold relevance determination " , ICML , 2012 ) would not produce visually pleasing generative samples either , but the relationship is so close that these models have even been used for analysing setups identical to yours ( e.g. Li et al . " Cross -pose face recognition by canonical correlation analysis " , arXiv :1507.08076 , 2015 ) but with goals other than generation . Consequently , the reader would expect to learn something about the relationship between the proposed method and the earlier literature building on the same latent variable formulation . A particularly interesting question would be whether the proposed model actually is a direct GAN - based extension of IBFA , and if not then how does it differ . Use of adversarial training to encourage separation of latent variables is clearly a reasonable idea and quite likely does better job than the earlier solutions ( typically based on some sort of group-sparsity assumption in shared - private factorisation ) with the possible or even likely exception of Mathieu at al. ( 2016 ) , and aspects like this should be explicitly discussed to extend the contribution from pure image generation to multi-view literature in general .

The empirical experiments are somewhat non-informative , relying heavily on visual comparisons and only satisfying the minimum requirement of demonstrating that the method does its job . The results look aesthetically more pleasing than the baselines , but the reader does not learn much about how the method actually behaves in practice ; when does it break down , how sensitive it is to various choices ( network structure , learning algorithm , amount of data , how well the content and view can be disentangled from each other , etc . ) . In other words , the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work ; the authors implemented the model and ran it on a collection of public data sets , but did not venture further into scientific reporting of the merits and limitations of the approach .

Finally , Table 1 seems to have some min / max values the wrong way around .


Revision of the review in light of the author response :
The authors have adequately addressed my main remarks , and while doing so have improved both the positioning of the paper amongst relevant literature and the somewhat limited empirical comparisons . In particular , the authors now discuss alternative multi-view generative models not based on GANs and the revised paper includes considerably extended set of numerical comparisons that better illustrate the advantage over earlier techniques . I have increased my preliminary rating to account for these improvements .

We thank the reviewer for the comments and feedback . We apologize for the late reply due to the large number of experiments that have been made to improve the quality of the paper .

Concerning the fact that our generative model is “ conceptually quite straightforward ” , we would like to emphasis that the proposed paper is as far as we know the first paper to evaluate this idea of using a discriminator on pairs of outputs for the multiview problem , this discriminator being in charge of telling is the two outputs correspond to the same object .

We acknowledge the reviewer for pointing us this extensive literature on IBFA and on similar ideas in CCA and non linear variants of CCA . Of course our method is clearly related to this literature and we added this related work on the state if the art section . As suggested by the reviewer the assumption made by our method is very similar to the one made with IBFA models . The main difference being in the way the models are learned : by using ‘ strong ‘ regularization and particular factorization functions in the IBFA literature , or by using a discriminator in our case . Note also that most experiments in the IBFA literature are based on datasets where a limited finite number of possible views is provided while our model is evaluated on complex datasets with multiple possible views , without any available view supervision . A detailed discussion on this point has been added in Section 6 .

About Radford architecture . Yes we do reuse the architecture in [ Radford et al. , 2015 ] for the DCGAN architecture because the core idea of the paper is elsewhere , as it was the case for [ Mathieu et al. , 2016 ] . Actually the main features of our method , its ability to learn from data whose views are not aligned between objects and which are unlabeled comes from our particular learning scheme and the way we build pairs of examples . This is why we focus the presentation of our method on this particular way of constructing training examples for our models .

Please consider that we have added a large additional experimental section that objectively evaluates the quality of the generated samples of the different models ( GMV , CMGV , GANx , CGAN and Mathieu et al. ) in terms of quality of the outputs , and in terms of diversity of the generated samples , showing the superiority of our model w.r.t these baselines ( new section 5.3 , pages 11 to 13 of the new version )


This paper firstly proposes a GAN architecture that aim at decomposing the underlying distribution of a particular class into " content " and " view " . The content can be seen as an intrinsic instantiation of the class that is independent of certain types of variation ( eg viewpoint ) , and a view is the observation of the object under a particular variation . The authors additionally propose a second conditional GAN that learns to generate different views given a specific content .

I find the idea of separating content and view interesting and I like the GMV and CGMV architectures . Not relying on manual attribute / class annotation for the views is also positive . The approach seems to work well for a relatively clean setup such as the chair dataset , but for the other datasets the separation is not so apparent . For example , in figure 5 , what does each column represent in terms of view ? It seems that it depends heavily on the content . That raises the question of how useful it is to have such a separation between content and views ; for some datasets their diversity can be a bottleneck for this partition , making the interpretation of views difficult .

A missing ( supervised ) reference that considers also the separation of content and views .
[ A ] Learning to generate chairs with convolutional neural networks , Alexey Dosovitskiy , Jost Tobias Springenberg , Thomas Brox , CVPR 15

Q : Figure 5 , you mean " all images in a column were generated with the same view vector "
Q : Why on Figure 7 you use different examples for CGAN ?

We thank the reviewer for the comments and feedback . We apologize for the late reply due to the large number of experiments that have been made to improve the quality of the paper .

As far as we understand , the main concern is about the fact that the interpretation of the notion of view can be difficult depending on the nature of the dataset . We agree on that point . Indeed , what we call ‘ content ’ in this paper corresponds to the invariant factors contained in a set of images representing a same object , the view corresponding to the remaining ‘ changing ’ factors . This is the assumption also made for the IBFA and CCA based approaches ( see next review ) . We have added a discussion on this point in the paper in the literature review section . Note also that the more difficult interpretation of views in our work is the counterpart of the increased ability of the method to deal with various datasets .

Concerning the suggested reference , our related work is focused on models that are not based on view supervision .

Note that we have added a large additional experimental section that objectively evaluates the quality of the generated samples of the different models ( GMV , CMGV , GANx , CGAN and Mathieu et al. ) in terms of quality of the outputs , and in terms of diversity of the generated samples , showing the superiority of our model w.r.t these baselines ( new section 5.3 , pages 11 to 13 of the new version )


The paper proposes a new generative model based on the Generative Adversarial Network ( GAN ) . The method disentangles the content and the view of objects without view supervision . The proposed Generative Multi-View ( GMV ) model can be considered to be an extension of the traditional GAN , where the GMV takes the content latent vector and the view latent vector as input . In addition , the GMV is trained to generate a pair of objects that share the content but with different views . In this way , the GMV successfully models the content and the view of the objects without using view labels . The paper also extends GMV into a conditional generative model that takes an input image and generates different views of the object in the input image . Experiments are conducted on four different datasets to show the generative ability of the proposed method .

Positives :
- The proposed method is novel in disentangling the content and the view of objects in a GAN and training the GAN with pairs of objects . By using pairs that share the content but with different views , the model can be trained successfully without using view labels .

- The experimental results on the four datasets show that the proposed network is able to model the context and the view of objects when generating images of these objects .

Negatives :
- The paper only shows comparison between the proposed method and several baselines : DCGAN and CGAN . There is no comparison with methods that also disentangle the content from the view such as Mathieu et al . 2016 .

- For the comparison with CGAN in Figure 7 , it would be better to show the results of C - GMV and CGAN on the same input images . Then it is easier for the readers to see the differences in the results from the two methods .

We thank the reviewer for the comments and feedback . We apologize for the late reply due to the number of additional experiments that have been made to improve the quality of the paper .

The first concern of the reviewer is about the lack of comparisons with other techniques . We updated the paper with results obtained on the same tasks with the approach by Mathieu et al . 2016 which is the closest to ours . Note that we were able to obtain comparable quality of outputs using the Mathieu et al . model by carefully testing many different neural networks architectures , the ones being provided in the open-source implementation , provided by the authors being inefficient on our problems . The quality of the generated samples of the different models ( GMV , CMGV , GANx , CGAN and Mathieu et al. ) have been evaluated in terms of quality of the outputs , and in terms of diversity of the generated samples , showing the superiority of our model w.r.t these baselines ( new section 5.3 , pages 11 to 13 of the new version )

We have also taken care to illustrate samples of the different models based on the same input images to allow for a better qualitative comparison ( Figure 8 )


This paper propose a new " gate " function for LSTM to enable the values of the gates towards 0 or 1 . The motivation behind is a flat region of the loss surface is likely to generalize well . It shows the experimental results are comparable or better than vanilla LSTM and much more robust to low - precision approximation and low-rank approximation .

In section 3.2 , the paper claimed using a smaller temperature cannot guarantee the outputs to be close to the boundary . Is there any experimental evidence to show it 's not working ? It also claimed pushing output gate to 0 / 1 will drop the performance . It actually quite interesting because there are bunch of paper claimed output gate is not important for language modeling , e.g. https://openreview.net/pdf?id=HJOQ7MgAW .

In the sensitive analysis , what if apply rounding / low -rank for all the parameters ?

How was this approach compare to binarynet https://arxiv.org/abs/1602.02830 ? Applying the same idea , but only for forget gate / input gate . Also , can we apply this idea to the binarynet ?

Overall , I think it 's an interesting paper but I feel it should compare with some simple baseline to binarized the gate function .

Updates : Thanks a lot for all the clarification . It do improve the paper quality but I 'm still thinking it 's higher than " 6 " but lower than " 7 " . To me , improve ppl from " 52.8 " to " 52.1 " is n't very significant . For WMT , it improve on DE ->EN but not for EN ->DE ( although it improve both for the author 's own baseline ) . So I 'm not fully convinced this approach could improve the generalization . But I feel this work can have many other applications such as " binarynet " .

[ Regarding the small temperature experiment ]

Thanks for figure this out . First , we want to point out that theoretically it does n’t help : Simply consider function f_{W , b} ( x ) =sigmoid ( ( Wx + b ) / tau ) , where tau is the temperature , it is computationally equivalent to f_{W ’ , b ’} ( x ) =sigmoid ( W’x + b’ ) by setting W’=W / tau and b’ = b / tau . Then using a small temperature is equivalent to rescale the initial parameter as well as gradient to a larger range . Usually , setting an initial point in a larger range with a larger learning rate will harm the optimization process .

We also did a set of experiments and updated the paper to show it does n’t help in practice .

[ Regarding the binary net ]

Despite the different between the model structure ( gate -based LSTM v.s. CNN ) , the main difference is that we regularize the output of the activation of the gates to binary value only , but not to regularize the weights . One should notice that the accuracy of Binary Net is usually much worse than the baseline model . However , we show that ( 1 ) Our models generalize well among different tasks . ( 2 ) The accuracy of the models after low- rank / low - precision compression using our method is competitive to ( or even better than ) the baseline . Besides , our techniques can also be applied to binarynet training .

[ Regarding apply rounding / low -rank for all the parameters ]

We will do the experiment but as our proposed method is focusing on LSTM unit . We are not sure whether the performance will drop a lot when we apply rounding / low - rank to embedding and attention .

This paper aims to push the LSTM gates to be binary . To achieve this , the paper proposes to employ the recent Gumbel - Softmax trick to obtain end - to - end trainable categorical distribution ( taking 0 or 1 value ) . The resulted G2-LSTM is applied for language model and machine translation in the experiments .

The novelty of this paper is limited . Just directly apply the Gumbel - Softmax trick .

The motivation is not explained clearly and convincingly . Why need to pursue binary gates ? According to the paper , it may give better generalization performance . But there is no theoretical or experimental evidence provided by this paper to support this argument .

The results of the new G2- LSTM are not significantly better than baselines in the experiments .


[ Regarding the experiment ]

We are afraid that the reviewer makes a wrong judgement to the performance results , our model is much better than the baseline on two tasks .

For machine translation , we achieved the SOTA performance on German - > English task and the improvement is significate ( + about 1 point ) in the field of translation , not to mention that our model is much better than some other submissions https://openreview.net/forum?id=HktJec1RZ.

For language model , by leveraging several tricks in literature , we significantly improve the performance from 77.4 to 52.1 ( the best number as far as we know ) . This number is achieved without using any hyperparameter search method , we reported the detail in the paper .

[ Regarding the motivation ]

We have discussed in section 2.1 that there are a bunch of work empirically and theoretically studying the relationship between flat loss surface and generalization , not to mention that there are some continuous study and verification in ICLR 2018 submissions , e.g. , https://openreview.net/forum?id=HkmaTz-0W . Thus our method is well motivated : by pushing the softmax operator towards its flat region will lead to better generalization .

[ Regarding the novelty of the paper ]

We are regretful to see the reviewer claims that there is little novelty in the paper . First , we are the first to apply Gumbel - softmax trick for robust training of LSTM by pushing the value of the gate to the boundary . We empirically show that our method achieves better accuracy even achieves the SOTA performance in some tasks . Second , we show that by different low - precision / low -rank compressions , our model is even still comparable to the baseline models before compressions .


The paper argues for pushing the input and forget gate ’s output toward 0 or 1 , i.e. , the LSTM tends to reside in flat region of surface loss , which is likely to generalize well . To achieve that , the sigmoid function in the original LSTM is replaced by a function G that is continuous and differentiable with respect to the parameters ( by applying the Gumbel - Softmax trick ) . As a result , the model is still differentiable while the output gate is approximately binarized .

Pros :
- The paper is clearly written
- The method is new and somehow theoretically guaranteed by the proof of the Proposition 1
- The experiments are clearly explained with detailed configurations
- The performance of the method in the model compression task is promising

Cons :
- The “ simple deduction ” which states that pushing the gate values toward 0 or 1 correspond to the region of the overall loss surface may need more theoretical analysis
- It is confusing whether the output of the gate is sampled based on or computed directly by the function G
- The experiments lack many recent baselines on the same dataset ( Penn Treebank : Melis et al. ( 2017 ) – On the State of the Art of Evaluation in Neural Language Models ; WMT : Ashish et.al. ( 2017 ) – Attention Is All You Need )
- The experiment ’s result is only slightly better than the baseline ’s
- To be more persuasive , the author should include in the baselines other method that can “ binerize ” the gate values such as the one sharpening the sigmoid function .


In short , this work is worth a read . Although the experimental results are not quite persuasive , the method is nice and promising .



[ Regarding the computation of function G ]

During training , the output of the gate is computed directly by function G , while the function G contains some random noise U.

[ Regarding the sharpened sigmoid function experiment ]

Thanks for figure this out . First , we want to point out that theoretically it does n’t help : Simply consider function f_{W , b} ( x ) =sigmoid ( ( Wx + b ) / tau ) , where tau is the temperature , it is computationally equivalent to f_{W ’ , b ’} ( x ) =sigmoid ( W’x + b’ ) by setting W’=W / tau and b’ = b / tau . Then using a small temperature is equivalent to rescale the initial parameter as well as gradient to a larger range . Usually , setting an initial point in a larger range with a larger learning rate will harm the optimization process .

We also did a set of experiments and updated the paper to show it does n’t help in practice .

[ Regarding the significance of experimental results ]

For machine translation , we achieved the SOTA performance on German - > English task and the improvement is significate ( + about 1 point ) in the field of translation , not to mention that our model is much better than some other submissions https://openreview.net/forum?id=HktJec1RZ. For English - > German task , we noticed that “ Attention is all you need ” is the state of the art but it is not LSTM - based ; thus we did n’t list that result in the paper .

For language model , thanks for the reference , we have studied the papers . By leveraging several tricks in literature , we significantly improve the performance from 77.4 to 52.1 ( the best number as far as we know ) without using any hyperparameter search method , we reported the detail in the paper .



Summary :
- nets that rely on single directions are probably overfitting
- batch norm helps not having large single directions
- high class selectivity of single units is a bad measure to find " important " neurons that help a NN generalize .

The experiments that this paper does are quite interesting , somewhat confirming intuitions that the community had , and bringing new insights into generalization . The presentation is good overall , but many minor improvements could help with readability .


Remarks :
- The first thing you should say in this paper is what you mean by " single direction " , at least an intuition , to be refined later . The second sentence of section 2 could easily be plugged in your abstract .
- You should already mention in section 2.1 that you are using ReLUs , otherwise clamping to 0 might take a different sense .
- considering the lack of page limit at ICLR , making * all * your figures bigger would be beneficial to readability .
- Figure 2 's y values drop rapidly as a function of x , maybe make x have a log scale or something that zooms in near 0 would help readability .
- Figure 3b 's discrete regimes is very weird , did you actually look at how much these clusters converged to the same solution in parameter space ?
- Figure 4a is nice , but an additional figure zooming in on the first 2 epochs would be really great , because that AUC curve goes up really fast in the beginning .
- Arpit et al. find that there is more cross -class information being shared for true labels than random labels . Considering you find that low class selectivity is an indicator of good generalization , would it make sense to look at " cross - class selectivity " ? If a neuron learns a feature shared by 2 or more classes , then it has this interesting property of offering a discrimination potential for multiple classes at the same time , rather than just 1 , making it more " useful " potentially , maybe less adversary prone ?
- You say in the figure captions that you use random orderings of the features to perform ablation , but nowhere in the main text ( which would be nice ) .



We thank the reviewer for their kind comments and helpful feedback . We have incorporated the reviewer ’s suggestions into the manuscript , and feel that they have substantially improved the clarity of the work . We have also performed additional experiments to address the importance of units with information about multiple classes , as the reviewer suggests . Details of these changes are below :

" The first thing you should say in this paper is what you mean by ' single direction . ' "

We have now defined ‘ single directions ’ in the abstract as the reviewer suggests , as well as adding an additional definition in the Introduction . We agree that this improves the clarity of the paper substantially .

" You should already mention in section 2.1 that you are using ReLUs . "

We have moved section 2.3 to section 2.1 , thereby highlighting that we are using ReLU ’s from the start , as the reviewer suggests .

" considering the lack of page limit at ICLR , making * all * your figures bigger would be beneficial to readability . "

We were perhaps overly concerned about the ICLR 8 - page soft limit in the first draft . We have increased the size of all the figures , as the reviewer suggests , and indeed , this improves the presentation of the paper .

" Figure 2 's y values drop rapidly as a function of x , maybe make x have a log scale or something that zooms in near 0 would help readability . "

We have now re-plotted Figure 2 using a log scale for the x-axis . We feel it has substantially improved the figure . We thank the reviewer for the great suggestion !

" Figure 3b 's discrete regimes is very weird , did you actually look at how much these clusters converged to the same solution in parameter space ? "

We absolutely agree that these discrete regimes are very weird , and fully intend to chase down the cause , and more generally , evaluate empirical convergence properties of multiple networks with the same topology but different random seeds in future work . However , an initial investigation into the causes of these regimes suggests that the answer is not obvious , and we believe that this question is beyond the scope of the present work .

" Arpit et al. find that there is more cross -class information being shared for true labels than random labels . Considering you find that low class selectivity is an indicator of good generalization , would it make sense to look at " cross - class selectivity " ? If a neuron learns a feature shared by 2 or more classes , then it has this interesting property of offering a discrimination potential for multiple classes at the same time , rather than just 1 , making it more " useful " potentially , maybe less adversary prone ? "

We agree with the reviewer , and indeed , we had included a discussion of the downsides of class selectivity in section titled ‘ Quantifying class selectivity ’ . While class selectivity absolutely ignores units with information about multiple classes , it has been used extensively in neuroscience to find neurons with strong tuning properties ( e.g. , the cat neurons prominently featured in previous deep learning analyses ) . In contrast , a metric such as mutual information should highlight units that are informative about multiple classes ( with ‘ cross - class selectivity ’ ) , but not necessarily units that are obviously interpretable .

However , we agree that it would be worthwhile to assess the relationship between cross-class selectivity ( as measured by mutual information ) and importance . To this end , we have performed a series of additional experiments using mutual information ( Fig. 6 b ; A4 ; Section A.5 ) . We found that while mutual information was slightly more predictive of unit importance than class selectivity it is still not a good predictor of unit importance ( Fig. A4 , p.15 ) . Interestingly , while we had previously shown that batch normalization decreases class selectivity , we found that batch normalization actually increases mutual information ( Fig. 6 b , p.7 ) . This result suggests that batch normalization encourages representations that are distributed across units as opposed to representations in which information about single classes is concentrated in single units . We have added text discussing these results in sections 2.3 ( p.3 ) and 3.4 ( p.7 ) .

" You say in the figure captions that you use random orderings of the features to perform ablation , but nowhere in the main text ( which would be nice ) . "

We have now included a statement in the main text saying that each ablation curve contains multiple random orderings ( p.4 , first incomplete paragraph ) .

This is an " analyze why " style of paper : the authors attempt to explain the relationship between some network property ( in this case , " reliance on single directions " ) , and a desired performance metric ( in this case , generalization ability ) . The authors quantify a variety of related ways to measure " reliance on single directions " and show that the more reliant on a single directions a given network is , the less well it generalizes .

Clarity : The paper is fairly clearly written . Sometimes key details are in the footnotes ( e.g. see footnote 3 ) -- not sure why -- but on the whole , I think the followed the paper reasonably well .

Quality : The work makes a good -faith attempt to be fairly systematic -- e.g evaluating several different types of network structures , with reasonable numbers of random initializations , and also illustrates the main point in several different comparatively independent - seeming ways . I feel fairly confident that the results are basically right within the somewhat limited domain that the authors explore .

Originality : This work is one in a series of papers about the topic of trying to understand what leads to good generalization in deep neural networks . I do n't know that the concept of " reliance on a single direction " seems especially novel to me , but on the other hand , I ca n't think of another paper that precisely investigates this notion the way it is done here .

Significance : The work touches on some important issues . I think the demonstration that the existence of strongly class - selective neurons is not a good correlate for generalization is interesting . This point illustrates something that has made me a bit uncomfortable with the trend toward " interpretable machine learning " that has been arising recently : in many of those results , it is shown that some fraction of the units at various levels of a trained deepnet have optimal driving stimuli that seem somewhat interpretable , with the implication that the existence of such units is an important correlate of network performance . There has even been some claims that better-performing networks have more " single - direction " interpretable units [ 1 ] . The fact that the current results seem directly in contradiction to that line of work is interesting , and the connections to batch normalization and dropout are for the same reason interesting . However , I wish the authors had grappled more directly with the apparent contradiction with ( e.g. ) [ 1 ] . There is probably a kind of tradeoff here . The closer the training dataset is to what is being tested for " generalization " , the more likely that having single - direction units is useful ; and vice-versa . I guess the big question is : what types of generalization are actually demanded / desired in real deployed machine learning systems ( or in the brain ) ? How does those cases compare with the toy examples analyzed here ? The paper does n't go far enough in really addressing these questions , but it is sort of beginning to make an effort .

However , for me the main failing of the paper is that it 's fairly descriptive without being that prescriptive . Does using their metric of reliance on a single direction , as a regularizer in and of itself , add anything above any beyond existing regularizers ( e.g. batch normalization or dropout ) ? It does n't seem like they tried . This seems to me the key question to understanding the significance of their results . Is " reliance on single direction " actually a good regularizer as such , especially for " real " problems like ( e.g. ) training a deep Convnet on ( e.g. ) Image Net or some other challenging dataset ? Would penalizing for this quantity improve the generalization of a network trained on ImageNet to other visual datasets ( e.g. MS-COCO ) ? If so , this would be a very significant result and would make me really care about their idea of " reliance on a singe direction " . If such results do not hold , it seems to me like one more theoretical possibility that would bite the dust when tested at scale .

[ 1 ] http://netdissect.csail.mit.edu/final-network-dissection.pdf

We thank the reviewer for their constructive feedback and their thorough reading of our paper . We have performed additional experiments ( to show that the insights of this work can be used prescriptively ) and provided additional discussion to work towards addressing the concerns the reviewer has raised . We have provided detailed responses to these comments as well as pointers to changes in the paper below :

" Sometimes key details are in the footnotes ... "

We initially put these details in footnotes to stay below the soft page limit . We have now moved all footnotes containing key details into the main text as the reviewer has requested .

" Originality : This work is one in a series of papers about the topic of trying to understand what leads to good generalization in deep neural networks . I do n't know that the concept of " reliance on a single direction " seems especially novel to me , but on the other hand , I ca n't think of another paper that precisely investigates this notion the way it is done here . "

As we discuss in both the introduction and related work sections of our paper , the concept of single direction reliance is related to previous theoretical work such as flat minima . However , to our knowledge , single direction reliance has never been empirically tested explicitly . Nonetheless , if the reviewer would be willing to point us in the direction of any related papers that we may have omitted from our manuscript , we would greatly appreciate it as we want to ensure that our discussion of prior work is as complete as possible .

" There has even been some claims that better-performing networks have more " single - direction " interpretable units [ 1 ] . The fact that the current results seem directly in contradiction to that line of work is interesting , and the connections to batch normalization and dropout are for the same reason interesting . However , I wish the authors had grappled more directly with the apparent contradiction with ( e.g. ) [ 1 ] . "

We have included an additional paragraph in the related work section ( Section 4 , p.9 , third complete paragraph ) comparing our work more extensively to the work of Bau et al . [ 1 ] . We believe that Bau et al . is extremely interesting work , and we note that , in many cases , our results are largely consistent with what Bau et al. observed ; for example , we both found a relationship between selectivity and depth . However , we do acknowledge that they observed a correlation between network performance and the number of concept-selective units ( Fig. 12 in Bau et al . ) . We believe that there are three potential explanations for this discrepancy :

( 1 ) As we note at the end of Section 2.3 , class selectivity and feature selectivity ( akin to the concept selectivity used in Bau et al. ) may exhibit different properties .

( 2 ) Bau et al. compare networks with different numbers of filters ( e.g. , AlexNet , GoogleNet , VGG , and ResNet - 152s ) , but measure the absolute number of unique detectors . It is possible that the number of unique detectors in better performing networks , such as ResNets , is simply a function of these networks having more filters .

( 3 ) Finally , both Bau et al . and our work observed a relationship between selectivity and depth ( see Fig. 5 in Bau et al. , and Fig. A2 in our manuscript ) . As Bau et al. compared the number of unique detectors across networks with substantially different depths , the increase in the number of unique detectors may have been due to the different depths of these networks . In line with this observation ( as well as point 2 above ) , we note that in Fig. 12 in Bau et al. , which plots the number of unique detectors as a function of accuracy on the action40 dataset , there appears to be little relationship when comparing only across points from the same model architecture .

" ‘ The closer the training dataset is to what is being tested for " generalization " , the more likely that having single - direction units is useful ; and vice-versa . I guess the big question is : what types of generalization are actually demanded / desired in real deployed machine learning systems ( or in the brain ) ? "

We have now included an additional paragraph in the Discussion section ( p.9 last incomplete paragraph ) addressing the distinction between different types of generalization based on the overlap between the train and test distributions . We believe that understanding how single direction reliance varies based on this overlap is an extremely interesting question although we feel it is beyond the scope of the present work .

[ 1 ] David Bau , Bolei Zhou , Aditya Khosla , Aude Oliva , and Antonio Torralba . Network Dissection : Quantifying Interpretability of Deep Visual Representations . 2017 . doi : 10.1109/CVPR.2017 . 354 . URL http://arxiv.org/abs/1704.05796.






" However , for me the main failing of the paper is that it 's fairly descriptive without being that prescriptive . Does using their metric of reliance on a single direction , as a regularizer in and of itself , add anything above and beyond existing regularizers ( e.g. batch normalization or dropout ) ? "

Though we would like to note that the primary goal of this work is to understand what factors lead to good generalization performance rather than to engineer a new model , we agree with the reviewer that a demonstration that the insights from our work can be used to directly improve model performance would be extremely valuable . However , all of the most obvious methods to regularize single direction reliance seem to reduce to dropout or one of its close variants . This is not to say that we believe there is no such regularizer -- it is merely to say that it is not obviously apparent . We have added a sentence in the Discussion to this effect ( p.9 , last complete paragraph ) .

Nonetheless , we do note that the insights from our work can be used prescriptively to indirectly improve models , as they provide a way to assess generalization performance without the need for a held - out validation set . In the original draft , we explored this in Fig. 4a - b as a means for early stopping . To expand on the potential for the method in this direction , we have added an additional experiment , in which we show that single direction reliance can be used as an effective method for hyperparameter selection as well ( Fig. 4c , p.5 last complete paragraph ) . We believe that this approach may prove extremely useful , especially in situations in which labeled data is rare .

article summary :
The authors use ablation analyses to evaluate the reliance on single coordinate - aligned directions in activation space ( i.e. the activation of single units or feature maps ) as a function of memorization . They find that the performance of networks that memorize more are also more affected by ablations . This result holds even for identical networks trained on identical data . The dynamics of this reliance on single directions suggest that it could be used as a criterion for early stopping . The authors discuss this observation in relation to dropout and batch normalization . Although dropout is an effective regularizer to prevent memorization of random labels , it does not prevent over - reliance on single directions . Batch normalization does appear to reduce the reliance on single directions , providing an alternative explanation for the effectiveness of batch normalization . Networks trained without batch normalization also demonstrated a significantly higher amount of class selectivity in individual units compared to networks trained without batch normalization . Highly selective units were found to be no more important than units that were not selective to a particular class . These results suggest that highly selective units may actually be harmful to network performance .

* Quality : The paper presents thorough and careful empirical analyses to support their claims .
* Clarity : The paper is very clear and well - organized . Sufficient detail is provided to reproduce the results .
* Originality : This work is one of many recent papers trying to understand generalization in deep networks . Their description of the activation space of networks that generalize compared to those that memorize is novel . The authors throughly relate their findings to related work on generalization , regularization , and pruning . However , the authors may wish to relate their findings to recent reports in neuroscience observing similar phenomena ( see below ) .
* Significance : The paper provides valuable insight that helps to relate existing theories about generalization in deep networks . The insights of this paper will have a large impact on regularization , early stopping , generalization , and methods used to explain neural networks .

Pros :
* Observations are replicated for several network architectures and datasets .
* Observations are very clearly contextualized with respect to several active areas of deep learning research .
Cons :
* The class selectivity measure does not capture all class -related information that a unit may pass on .

Comments :
* Regarding the class selectivity of single units , there is a growing body of literature in neurophysiology and neuroimaging describing similar observations where the interpretation has been that a primary role of any neural pathway is to “ denoise ” or cancel out the “ distractor ” rather than just amplifying the “ signal ” of interest .
* Untuned But Not Irrelevant : The Role of Untuned Neurons In Sensory Information Coding , https://www.biorxiv.org/content/early/2017/09/21/134379
* Correlated variability modifies working memory fidelity in primate prefrontal neuronal ensembles https://www.ncbi.nlm.nih.gov/pubmed/28275096
* On the interpretation of weight vectors of linear models in multivariate neuroimaging http://www.sciencedirect.com/science/article/pii/S1053811913010914
* see also LEARNING HOW TO EXPLAIN NEURAL NETWORKS https://openreview.net/forum?id=Hkn7CBaTW
* Regarding the intuition in section 3.1 , " The minimal description length of the model should be larger for the memorizing network than for the structure - finding network . As a result , the memorizing network should use more of its capacity than the structure - finding network , and by extension , more single directions ” . Does reliance on single directions not also imply a local encoding scheme ? We know that for a fixed number of units , a distributed representation will be able to encode a larger number of unique items than a local one . Therefore if this behaviour was the result of needing to use up more of the capacity of the network , would n’t you expect to observe more distributed representations ?

Minor issues :
* In the first sentence of section 2.3 , you say you analyzed three models and then you only list two . It seems you forgot to include ResNet trained on ImageNet .

First off , we would like to thank the reviewer for the kind review and the helpful feedback , especially with respect to class selectivity and the relationship to neuroscience . We have provided detailed responses to these comments as well as pointers to changes in the paper below :

" The class selectivity measure does not capture all class - related information that a unit may pass on . "

We agree with the reviewer , and indeed , we had included a discussion of the downsides of class selectivity in section titled ‘ Quantifying class selectivity . ’ While class selectivity absolutely ignores units with information about multiple classes , it has been used extensively in neuroscience to find neurons with strong tuning properties ( e.g. , the cat neurons prominently featured in previous deep learning analyses ) . In contrast , a metric such as mutual information should highlight units that are informative about multiple classes , but not necessarily units that are obviously interpretable .

However , we agree that it would be worthwhile to assess the relationship between multi-class selectivity ( as measured by mutual information ) and importance . To this end , we have performed a series of additional experiments using mutual information ( Fig. 6 b ; A4 ; Section A.5 ) . We found that while mutual information was slightly more predictive of unit importance than class selectivity , it is still not a good predictor of unit importance ( Fig. A4 , p.15 ) . Interestingly , while we had previously shown that batch normalization decreases class selectivity , we found that batch normalization actually increases mutual information ( Fig. 6 b , p.7 ) . This result suggests that batch normalization encourages representations that are distributed across units as opposed to representations in which information about single classes is concentrated in single units . We have added text discussing these results in sections 2.3 ( p.3 ) and 3.4 ( p.7 ) .

" ... the authors may wish to relate their findings to recent reports in neuroscience ... "

We are strong advocates of the idea that methods and ideas from neuroscience are useful for understanding machine learning models , and so , we have also included an additional paragraph in our ‘ related work ’ section ( p.8 , first complete paragraph ) contextualizing our work in recent neuroscience developments regarding robustness to noise , distributed representations , and correlated variability , including references that the reviewer has provided and several other neuroscience papers that influenced our work .

" In the first sentence of section 2.3 , you say you analyzed three models and then you only list two . It seems you forgot to include ResNet trained on ImageNet . "

Great catch ! We have resolved this now .


Overall , the idea of this paper is simple but interesting . Via performing variational inference in a kind of online manner , one can address continual learning for deep discriminative or generative networks with considerations of model uncertainty .

The paper is written well , and literature review is sufficient . My comment is mainly about its importance for large - scale computer vision applications . The neural networks in the experiments are shallow .


Extensions to more complex tasks :

In the existing discriminative model experiments , we use shallow networks that are comparable to those considered in previous work ( Kirkpatrick et al. , 2017 ; Zenke et al. , 2017 ) so that our reimplementation fairly represents the previous work . In the updated version of the paper , we have added an additional Split notMNIST experiment ( see page 7 of the new version and Figure 5 ) . The notMNIST dataset is much larger and more noisy than the MNIST dataset . It contains 400,000 images of 10 characters written in different font styles , where each character has 40,000 images . This dataset is considered more difficult than the MNIST dataset . In this new experiment , we investigate a deeper network with 4 hidden layers and our method also performs well compared to EWC and SI .

Extension to computer vision applications :

The paper shows that VCL performs very well for MLPs in a variety of settings which we believe is an important contribution . To apply our method to many large - scale computer vision applications , the method needs to be extended to handle CNNs . In general , accurate approximate variational inference methods have not been developed for CNNs and this is an outstanding goal of the area of Bayesian Deep Learning . We therefore leave this development for future research . However , once a good general variational inference method has been developed for CNNs , it will be straightforward to apply the VCL framework .

Although MC dropout ( Gal & Ghahramani , 2016 ) is one candidate for Bayesian inference in CNNs , the nature of this approximation makes vanilla application of the VCL framework difficult . MC dropout uses a Gaussian prior over the weights and ( the limit of ) a mixture of Gaussians with shared parameters for the variational distribution . These two distributions are not of the same form and therefore a second approximation step would be required to apply VCL . Moreover , the impoverished representation of posterior uncertainty retained by MC dropout is likely to result in poor continual learning performance since nuanced and parameter specific information about parameter uncertainty is required in this setting . Approximations that employ a single global variance parameter in the q distribution , such as those employed by Kingma et al. , 2015 , will suffer similar problems .

References :

Y. Gal and Z. Ghahramani . Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning . ICML 2016 .

D.P. Kingma , T. Salimans , M. Welling . Variational Dropout and the Local Reparameterization Trick . NIPS 2015 .

This paper proposes a new method , called VCL , for continual learning . This method is a combination of the online variational inference for streaming environment with Monte Carlo method . The authors further propose to maintain a coreset which consists of representative data points from the past tasks . Such a coreset is used for the main aim of avoiding the catastrophic forgetting problem in continual learning . Extensive experiments shows that VCL performs very well , compared with some state - of - the- art methods .

The authors present two ideas for continual learning in this paper : ( 1 ) Combination of online variational inference and sampling method , ( 2 ) Use of coreset to deal with the catastrophic forgetting problem . Both ideas have been investigated in Bayesian literature , while ( 2 ) has been recently investigated in continual learning . Therefore , the authors seems to be the first to investigate the effectiveness of ( 1 ) for continual learning . From extensive experiments , the authors find that the first idea results in VCL which can outperform other state - of - the- art approaches , while the second idea plays little role .

The finding of the effectiveness of idea ( 1 ) seems to be significant . The authors did a good job when providing a clear presentation , a detailed analysis about related work , an employment to deep discriminative models and deep generative models , and a thorough investigation of empirical performance .

There are some concerns the authors should consider :
- Since the coreset plays little role in the superior performance of VCL , it might be better if the authors rephrase the title of the paper . When the coreset is empty , VCL turns out to be online variational inference [ Broderich et al. , 2013 ; Ghahramani & Attias , 2000 ] . Their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as well .
- It is unclear about the sensitivity of VCL with respect to the size of the coreset . The authors should investigate this aspect .
- What is the trade - off when the size of the coreset increases ?


New experiments showing that coresets can significantly improve VCL 's performance :

The use of a coreset can * significantly * improve VCL over the vanilla version . We have added a more comprehensive comparison to the updated version of the paper to make this completely clear ( see Figure 3 and the last paragraph on page 6 ) . For example , on the permuted MNIST task when the coreset size is 200 examples per task , the final accuracy of VCL improves from 90 % to 93 % and when the coreset size is increased to 5,000 examples per task , the performance further improves to 95.5 % . These are significant improvements for this dataset . Crucially , using just the coreset alone ( and no online inference ) still performs significantly worse . Thus , although we agree that VCL alone is effective for continual learning the combination with a coreset can be critical .

Moreover , as now noted in the paper , from a more general perspective , coreset VCL is equivalent to a message - passing implementation of variational inference in which the coreset data point message updates are scheduled last , only after the contributions from other data have been incorporated . This opens the door to versions of VCL which revisit the coreset points several times through learning ( rather than just at the end ) .

Novelty of VCL and contributions of the paper :

The novelty of our VCL method compared to online variational inference ( Broderich et al. , 2013 ; Ghahramani & Attias , 2000 ) is two -fold .

First , online VI has only previously been applied to simple conjugate models . Here instead we consider deep neural networks and variational auto-encoders . Indeed , a Bayesian treatment of the parameters of variational auto-encoders , in addition to the latent variables , is challenging in and of itself . These more complex models require a fusion of online VI and Monte Carlo VI which is technically challenging .

Second , previous work on online VI considers very simple tasks , most typically where the data arrive in iid fashion . Here instead , we consider much more general continual learning tasks that were not previously considered for online VI . The increased inhomogeneity in the data necessitated the development of coreset VI which is more natural and simpler than previous work on coresets for continual learning such as Lopez - Paz and Ranzato ( 2017 ) which requires an additional constraint on the optimization objective for every new task .

At a more general level , we also feel that it is important to point out to the continual learning community that standard methods of ( approximate ) Bayesian inference provide a rich mathematical and algorithmic framework for attacking continual learning that has hitherto been largely overlooked .

Appropriateness of the Title :

Given the two points addressed in the above responses , we believe that the title is appropriate . We have endeavoured to explain the relationship to prior work in the first line of the abstract , “ a simple but general framework for continual learning that fuses online variational inference ( VI ) and recent advances in Monte Carlo VI for neural networks ” , which we hope clearly explains the positioning of the paper .


The paper describes the problem of continual learning , the non-iid nature of most real - life data and point out to the catastrophic forgetting phenomena in deep learning . The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations .

The paper is well written , the problem is described neatly in conjunction with the past work , and the proposed algorithm is supported by experiments . The work is a useful addition to the community .

My main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. ( 2017 ) or the split CIFAR experiments in Zenke et. al. ( 2017 ) . Even though the experiments carried out in the paper are important , they fall short of justifying a major step in the direction of the solution for the continual learning problem .

New Experiment on a harder task :

In order to further assess the efficacy of VCL on larger scale and more complex tasks we have added an additional experiment to the paper : the new Split notMNIST task on page 7 of the updated paper and Figure 5 . The notMNIST dataset is much larger and more noisy than the MNIST dataset . It contains 400,000 images of 10 characters written in different font styles , where each character has 40,000 images . This dataset is generally considered more difficult than the MNIST dataset . In this new experiment , we investigate a deeper network and show that VCL still performs well compared to EWC and SI .

Deployment on tasks requiring CNNs :

The application of VCL to the Atari or Split CIFAR tasks is also a sensible suggestion . However , this requires the development of reliable variational inference methods for convolutional neural networks ( CNNs ) . This is still an outstanding research goal of Bayesian Deep Learning and so we leave this for future research . However , once a good variational inference method has been developed for CNNs , it is straightforward to apply the VCL framework to the above tasks .

Please see more relevant discussions of the points above in the response to Reviewer 3.

The paper present a RL based approach to walk on a knowledge graph to answer queries . The idea is novel , the paper is clear in its exposition , and the authors provide a number of experimental comparisons with prior work on a variety of datasets .

Pros :
1 . The approach is simple ( no pre-training , no reward shaping , just RL from scratch with terminal reward , uses LSTM for keeping track of past state ) , computationally efficient ( no computation over the full graph ) , and performs well in most of the experiments reported in the paper .
2 . It scales well to longer path lengths , and also outperforms other methods for partially structured queries .

Cons :
1 . You should elaborate more on the negative results on FB15 K and why this performance would not transfer to other KB datasets that exist . This seems especially important since it 's a large scale dataset , while the datasets a ) - c ) reported in the paper are small scale .
2 . It would also be good to see if your method also performed well on the Nations dataset where the baselines performed well . That said , if its a small scale dataset , it would be preferable to focus on strengthening the experimental analysis on larger datasets .
3 . In Section 4.2 , why have you only compared to NeuralLP and not compared with the other methods ?

Suggestions / Questions :
1 . In the datatset statistics , can you also add the average degree of the knowledge graphs , to get a rough sense of the difficulty of each task .
2 . The explanation of the knowledge graph and notation could be made cleaner . It would be easier to introduce the vertices as the entities , and edges as normal edges with a labelled relation on top . A quick example to explain the action space would also help .
3 . Did you try a model where instead of using A_t directly as the weight vector for the softmax , you use it as an extra input ? Using it as the weight matrix directly might be over regularizing / constraining your model .

Revision : I appreciate the effort by the authors to update the paper . All my concerns were adequately addressed , plus improvements were made to better understand the comparison with other work . I update my review to 7 : Good paper , accept .

Thank you for your helpful reviews . We have updated the paper with your suggestions .
- We have updated the paper with a detailed analysis of the negative results on Fb15k-237 ( sec 4.1 ) and more importantly , how this dataset differs from other KG datasets . ( To summarize , FB15 k -237 has very ( i ) low clustering coefficient ( ii ) the path types do n't repeat that often ( iii ) has a lot of 1 - Many query relations . )

- In section 4.2 ( Grid World ) , we actually compared to other baselines such as DistMult , but since they are not path based method their performance was very low . We decided to not report the results because it was making the plot look disproportionate . However for completion , here are the numbers - ( Path length 2 - 4 ) 0.2365 , ( 4 - 6 ) 0.1144 , ( 6 - 8 ) 0.0808 , ( 8 - 10 ) 0.0413

- We have added the avg and mean degree of nodes of knowledge graphs in table 1 . MINERVA performs well in KGs with both high / low out degree of nodes .

- Yes ! , we did consider using A_t as apart of the input but it comes with few complications w.r.t the implementation . First as the number and ordering of outgoing edges from a node varies , feeding A_t into the MLP is not straightforward . Also since the output probabilities should have support only on the outgoing edges ( which are not uniquely determined by only the relations , but also the neighboring entity ) , the masking logic also becomes tricky . Finally , the excessive amount of parameters required this way might lead to overfitting . Since we were getting promising results with the simpler approach , we decided to continue with the first design choice .




We appreciate your revision of scores and we thank you for your helpful reviews once again !

The paper proposes an approach for query answering / link prediction in KBs that uses RL to navigate the KB graph between a query entity and a potential answer entity . The main originality is that , unlike random walk models , the proposed approach learns to navigate the graph while being conditioned on the query relation type .

I find the method sound and efficient and the proposed experiments are solid and convincing ; for what they test for .

Indeed , for each relation type that one wants to be testing on , this type of approach needs many training examples of pairs of entities ( say e_1 , e_2 ) connected both by this relation type ( e_1 R e_2 ) and by alternative paths ( e_1 R ' R '' R ' ' ' e_2 ) . Because the model needs to discover and learn that R < = > R ' R '' R ' '' .

The proposed model seems to be able to do that well when the number of relation types remains low ( < 50 ) . But things get interesting in KBs when the number of relation types gets pretty large ( hundreds / thousands ) . Learning the kind of patterns described above gets much trickier then . The results on FB15 k are a bit worrying in that respect . Maybe this is a matter of the dataset FB15 k itself but then having experiments on another dataset with hundreds of relation types could be important .

NELL has indeed 200 relations but if I 'm not mistaken , the NELL dataset is used for fact prediction and not query answering . And as noted in the paper , fact prediction is much easier .



Thank you for your helpful reviews !

You raised an interesting point regarding the performance of MINERVA on KGs with large number of relation types . For a fair comparison , we ran query answering ( not fact prediction ) experiments on NELL -995 and compared to our implementation of DistMult ( which does very well on FB15 k - 237 ) . DistMult achieves a score of 79.5 whereas MINERVA achieves a score of 82.73 . Another important point to note is that MINERVA is much more efficient at inference time . NELL has ~ 75 k entities and algorithms such as DistMult have to rank against all entities to get the final score . However MINERVA just has to walk to the right answer . This can be seen by comparing the wall - clock running times 35 secs wrt 115 secs - ( sec 4.1 of the paper Query Answering on NELL - 995 )
This empirically shows that MINERVA works for relations with many relation types . We would additionally like to point out that MINERVA does well on WikiMovies . In WikiMovies the queries are partially structured and are in natural language . Hence the number of query types are actually quite large ( and potentially unbounded ) . This also supports our claim . Thanks for the excellent suggestion again .

We also have updated the paper with a detailed analysis of the negative results on Fb15k-237 ( sec 4.1 ) and more importantly , how this dataset differs from other KG datasets .


The paper proposes a new approach ( Minerva ) to perform query answering on knowledge bases via reinforcement learning . The method is intended to answer queries of the form ( e ,r , ? ) on knowledge graphs consisting of dyadic relations . Minerva is evaluated on a number of different datasets such as WN18 , NELL -995 , and WikiMovies .

The paper proposes interesting ideas to attack a challenging problem , i.e. , how to perform query answering on incomplete knowledge bases . While RL methods for KG completion have been proposed recently ( e.g. , DeepPath ) , Minerva improves over these approaches by not requiring the target entity . This property can be indeed be important to perform query answering efficiently . The proposed model seems technically reasonable and the paper is generally written well and good to understand . However , important parts of the paper seem currently unfinished and would benefit from a more detailed discussion and analysis .

Most importantly , I 'm currently missing a better motivation and especially a more thorough evaluation on how Minerva improves over non-RL methods . For instance , the authors mention multi-hop methods such as ( Neelakantan , 2015 ; Guu , 2015 ) in the introduction . Since these methods are closely related , it would be important to compare to them experimentally ( unfortunately , DeepPath does n't do this comparison either ) . For instance , eliminating the need to pre-compute paths might be irrelevant when it does n't improve actual performance . Similarly , the paper mentions improved inference time , which indeed is a nice feature . However , I 'm wondering , what is the training time and how does it compare to standard methods like ComplEx . Also , how robust is training using REINFORCE ?

With regard to the experimental results : The improvements over DeepPath on NELL and on WikiMovies are indeed promising . I found the later results the most convincing , as the setting is closest to the actual task of query answering . However , what is worrying is that Minerva does n't do well on WN18 and FB15 k -237 ( for which the results are , unfortunately , only reported in the appendix ) . On FB15k-237 ( which is harder than WN18 and arguably more relevant for real - world scenarios since it is a subset of a real - world knowledge graph ) , it is actually outperformed by the relatively simple DistMult method . From these results , I find it hard to justify that " MINERVA obtains state - of - the - art results on seven KB datasets , significantly outperforming prior methods " , as stated in the abstract .

Further comments :
- How are non-existing relations handled , i.e. , queries ( e , r , x ) where there is no valid x ? Does Minerva assume there is always a valid answer ?
- Comparison to DeepPath : Did you evaluate Minerva with fixed embeddings ? Since the experiments in DeepPath used fixed embeddings , it would be important to know how much of the improvements can be attributed to this difference .
- The experimental section covers quite a lot of different tasks and datasets ( Countries , UMLS , Nations , NELL , WN18RR , Gridworld , WikiMovies ) all with different combinations of methods . For instance , countries is evaluated against ComplEx , NeuralLP and NTP ; NELL against DeepPath ; WN18RR against ConvE , ComplEx , and DistMult ; WikiMovies against MemoryNetworks , QA and NeuralLP . A more focused evaluation with a consistent set of methods could make the experiments more insightful .

Thank you for your helpful reviews !

a ) Comparison with multi-hop models : Thanks to your suggestion , we have updated the paper ( sec 4.2 ) with a new experiment which explicitly compares to non-RL neural multihop models which precomputes a set of paths . Starting from the source entity , the model featurizes a set of paths ( using a LSTM ) and max- pools across them . This feature vector is then concatenated with the query relation and then fed to a feed forward network to score each target entity . This model is similar to that of Neelakantan et al. ( 2015 ) except for the fact that it was originally designed to work between a fixed set of source and target entity pair , but in our case the target entity is unknown . The model ( baseline ) is trained with a multi-class cross entropy objective based on observed triples and during inference we rank target entities according the the score given by the model .
As we can see , MINERVA outperforms this model in both freebase and NELL suggesting that RL based approach can effectively reduce the search space and focus on paths relevant to answer the query . Please see sec 4.2 for additional details and results .

b ) Training time and robustness of the model : - We actually found MINERVA to be very robust during training . We were able to achieve the reported results without much tuning and they are also very easy to reproduce . However to quantify the results , we report the variance of the results across three independent runs on the freebase and nell datasets . Also we report the learning curve of score on the development set wrt time . ( Please see sec 4.5 of the paper )

c ) Regarding peformance on WN18RR and FB15 k -237 :
MINERVA actually achieves state - of- the-art in the WN18RR dataset .
On FB15k-237 , MINERVA matches with all baseline model and is outperformed only by DistMult . We have updated the paper with a detailed analysis of the negative results on Fb15k-237 ( sec 4.1 ) and more importantly , how this dataset differs from other KG datasets . ( To summarize , FB15 k -237 has very ( i ) low clustering coefficient ( ii ) the path types do n't repeat that often ( iii ) has a lot of 1 - Many query relations . )

d ) Query Answering experiment on NELL - 995 : - We also added a query answering ( not fact prediction ) experiment on NELL -995 and compared to our implementation of DistMult ( which does very well on FB15 k - 237 ) . DistMult achieves a score of 79.5 whereas MINERVA achieves a score of 82.73 . Another important point to note is that MINERVA is much more efficient at inference time . NELL has ~ 75 k entities and algorithms such as DistMult have to rank against all entities to get the final score . However MINERVA just has to walk to the right answer . This can be seen by comparing the wall - clock running times 35 secs wrt 115 secs - ( sec 4.1 of the paper Query Answering on NELL - 995 )

Further comments :

a ) How are non-existing relations handled , i.e. , queries ( e , r , x ) where there is no valid x ? Does Minerva assume there is always a valid answer ? - That is a good point . Currently MINERVA does not support non existing relations and assumes there is always a valid answer . The ability to handle non-existing relations is definitely important and we plan to incorporate this in future work .

b ) Comparison to DeepPath : Did you evaluate Minerva with fixed embeddings ? Since the experiments in DeepPath used fixed embeddings , it would be important to know how much of the improvements can be attributed to this difference
We actually tried both cases - train randomly initialized embeddings from scratch and using fixed pretrained embeddings . We achieved similar results in both cases . For fixed embeddings , the model converged faster but to a similar score . However for uniformity across experiments , we reported results where we trained the embeddings .

c ) Consistent baselines : We will update the paper to cover as many reported baselines as possible . However we have made sure to the best of our abilities to compare with the models which have current state of the art results on each dataset .


Summary : The paper applies graph convolutions with deep neural networks to the problem of " variable misuse " ( putting the wrong variable name in a program statement ) in graphs created deterministically from source code . Graph structure is determined by program abstract syntax tree ( AST ) and next -token edges , as well as variable / function name identity , assignment and other deterministic semantic relations . Initial node embedding comes from both type and tokenized name information . Gated Graph Neural Networks ( GGNNs , trained by maximum likelihood objective ) are then run for 8 iterations at test time .

The evaluation is extensive and mostly very good . Substantial data set of 29 m lines of code . Reasonable baselines . Nice ablation studies . I would have liked to see separate precision and recall rather than accuracy . The current 82.1 % accuracy is nice to see , but if 18 % of my program variables were erroneously flagged as errors , the tool would be useless . I 'd like to know if you can tune the threshold to get a precision / recall tradeoff that has very few false warnings , but still catches some errors .

Nice work creating an implementation of fast GGNNs with large diverse graphs . Glad to see that the code will be released . Great to see that the method is fast --- it seems fast enough to use in practice in a real IDE .

The model ( GGNN ) is not particularly novel , but I 'm not much bothered by that . I 'm very happy to see good application papers at ICLR . I agree with your pair of sentences in the conclusion : " Although source code is well understood and studied within other disciplines such as programming language research , it is a relatively new domain for deep learning . It presents novel opportunities compared to textual or perceptual data , as its ( local ) semantics are well - defined and rich additional information can be extracted using well - known , efficient program analyses . " I 'd like to see work in this area encouraged . So I recommend acceptance . If it had better ( e.g. ROC curve ) evaluation and some modeling novelty , I would rate it higher still .

Small notes :
The paper uses the term " data flow structure " without defining it .
Your data set consisted of C# code . Perhaps future work will see if the results are much different in other languages .


Thank you for reviewing our work so kindly . Please note that the evaluation in our submission only covers 2.9M SLOC ( not 29M ) , even though since the submission we have performed additional experiments with similar results on the Roslyn project ( ~2M SLOC ) .

We have just updated our submission to also include ROC and PR curves for our main model in the appendix , which show that for a false positive rate of 10 % , our model achieves a true positive rate of 73 % on the SeenTestProj dataset and 69 % on UnseenTestProj . The PR curve indicates that setting a high certainty threshold for such highlighting should yield relatively few false positives . We are working on further improving these numbers by addressing common causes of mistakes ( i.e. , our model often proposes to use a class field “ _field ” when the ground truth is the corresponding getter property “ Field ” ; a simple alias analysis can take care of these case ) .


The paper introduces an application of Graph Neural Networks ( Li 's Gated Graph Neural Nets , GGNNs , specifically ) for reasoning about programs and programming . The core idea is to represent a program as a graph that a GGNN can take as input , and train the GGNN to make token - level predictions that depend on the semantic context . The two experimental tasks were : 1 ) identifying variable ( mis ) use , ie . identifying bugs in programs where the wrong variable is used , and 2 ) predicting a variable 's name by consider its semantic context .

The paper is generally well written , easy to read and understand , and the results are compelling . The proposed GGNN approach outperforms ( bi-) LSTMs on both tasks . Because the tasks are not widely explored in the literature , it could be difficult to know how crucial exploiting graphically structured information is , so the authors performed several ablation studies to analyze this out . Those results show that as structural information is removed , the GGNN 's performance diminishes , as expected . As a demonstration of the usefulness of their approach , the authors ran their model on an unnamed open - source project and claimed to find several bugs , at least one of which potentially reduced memory performance .

Overall the work is important , original , well - executed , and should open new directions for deep learning in program analysis . I recommend it be accepted .

Thank you for your kind review . We have updated our paper to discuss bugs found by the model in more detail and have privately reported more bugs found in Roslyn to the developers ( cf . https://github.com/dotnet/roslyn/pull/23437, and note that this GitHub issue does not de - anonymize the paper authors ) .

This paper presents a novel application of machine learning using Graph NN 's on ASTs to identify incorrect variable usage and predict variable names in context . It is evaluated on a corpus of 29M SLOC , which is a substantial strength of the paper .

The paper is to be commended for the following aspects :
1 ) Detailed description of GGNNs and their comparison to LSTMs
2 ) The inclusion of ablation studies to strengthen the analysis of the proposed technique
3 ) Validation on real - world software data
4 ) The performance of the technique is reasonable enough to actually be used .

In reviewing the paper the following questions come to mind :
1 ) Is the false positive rate too high to be practical ? How should this be tuned so developers would want to use the tool ?
2 ) How does the approach generalize to other languages ? ( Presumably well , but something to consider for future work . )

Despite these questions , though , this paper is a nice addition to deep learning applications on software data and I believe it should be accepted .



Thank you for reviewing our work so kindly . Please note that the evaluation in our submission only covers 2.9M SLOC ( not 29M ) , even though we have performed additional experiments with similar results on the Roslyn project ( ~2M SLOC ) .

Regarding your first question : We have just updated our submission to also include ROC and PR curves for our main model in the appendix , which show that for a false positive rate of 10 % , our model achieves a true positive rate of 73 % on the SeenTestProj dataset and 69 % on UnseenTestProj . We expect our system to be most useful in a code review setting , where locations in which the model disagrees with the ground truth are highlighted for a reviewer . The PR curve indicates that setting a high certainty threshold for such highlighting should yield relatively few false positives .

Regarding your second question : We have not tested our model on other languages so far . However , we expect similar performance on other strongly typed languages such as Java . An interesting research question will be to explore how the model could be adapted to gradually typed ( e.g. TypeScript ) or untyped ( e.g. JavaScript or Python ) languages .


Summary :

This paper extends the work of Balle et al. ( 2016 , 2017 ) on using certain types of variational autoencoders for image compression . After encoding pixels with a convolutional net with GDN nonlinearities , the quantized coefficients are entropy encoded . Where before the coefficients were independently encoded , the coefficients are now jointly modeled using a latent variable model . In particular , the model exploits dependencies in the scale of neighboring coefficients . The additional latent variables are used to efficiently represent these scales . Both the coefficients and the representation of the scales are quantized and encoded in the binary image representation .

Review :

Lossy image compression using neural networks is a rapidly advancing field and of considerable interest to the ICLR community . I like the approach of using a hierarchical entropy model , which may inspire further work in this direction . It is nice to see that the variational approach may be able to outperform the more complicated state - of - the- art approach of Rippel and Bourdev ( 2017 ) . That said , the evaluation is in terms of MS- SSIM and only the network directly optimized for MS- SSIM outperformed the adversarial approach of R&B . Since the reconstructions generated by a network optimized for MSE tend to look better than those of the MS- SSIM network ( Figure 6 ) , I am wondering if the proposed approach is indeed outperforming R&B or just exploiting a weakness of MS-SSIM . It would have been great if the authors included a comparison based on human judgments or at least a side - by-side comparison of reconstructions generated by the two approaches .

It might be interesting to relate the entropy model used here to other work involving scale mixtures , e.g. the field of Gaussian scale mixtures ( Lyu & Simoncelli , 2007 ) .

Another interesting comparison might be to other compression approaches where scale mixtures were used and pixels were encoded together with scales ( e.g. , van den Oord & Schrauwen , 2017 ) .

The authors combine their approach using MS- SSIM as distortion . Is this technically still a VAE ? Might be worth discussing .

I did not quite follow the motivation for convolving the prior distributions with a uniform distribution .

The paper is mostly well written and clear . Minor suggestions :

– On page 3 the paper talks about “ the true posterior ” of a model which has n’t been defined yet . Although most readers will not stumble here as they will be familiar with VAEs , perhaps mention first that the generative model is defined over both $ x $ and $ \tilde y$ .

– Below Equation 2 it sounds like the authors claim that the entropy of the uniform distribution is zero independent of its width .

– Equation 7 is missing some $ \tilde z$ .

– The operational diagram in Figure 3 is missing a “ | ” .

Thank you for the review and suggestions .

> Lossy image compression using neural networks is a rapidly advancing field and of considerable interest to the ICLR community . I like the approach of using a hierarchical entropy model , which may inspire further work in this direction . It is nice to see that the variational approach may be able to outperform the more complicated state - of - the- art approach of Rippel and Bourdev ( 2017 ) . That said , the evaluation is in terms of MS- SSIM and only the network directly optimized for MS- SSIM outperformed the adversarial approach of R&B . Since the reconstructions generated by a network optimized for MSE tend to look better than those of the MS- SSIM network ( Figure 6 ) , I am wondering if the proposed approach is indeed outperforming R&B or just exploiting a weakness of MS-SSIM . It would have been great if the authors included a comparison based on human judgments or at least a side - by-side comparison of reconstructions generated by the two approaches .

Thank you for thinking critically about distortion metrics . This is precisely one of the points we wanted to make with this paper - none of the metrics available today are perfect , and it is easy for ANN - based methods to overfit to whatever metric is used , resulting in good performance numbers but a loss of visual quality . That said , we would like to point out that neither we nor Rippel ( 2017 ) provide an evaluation based on human judgements . As such , it is unclear whether the adversarial loss they blend with an MS- SSIM loss is actually helping in terms of visual quality . Unfortunately , we ca n't systematically compare our method to theirs using human judgements , because they did not make their images available to us .

Regarding MS-SSIM vs. squared loss , we think it depends on the image which one is visually better . Because MS- SSIM has been very popular , we wanted to show an example that is challenging for MS- SSIM . Note that many of the images shown in the appendix ( side by side , one optimized for MS - SSIM and one for squared loss ) are compressed to roughly similar bit rates , allowing a crude comparison ( it is difficult in our current approach to match bit rates exactly ) . We lowered the bit rate of the images for this revision , to make the differences more visible .

> It might be interesting to relate the entropy model used here to other work involving scale mixtures , e.g. the field of Gaussian scale mixtures ( Lyu & Simoncelli , 2007 ) .

Thanks , we included this reference .

> Another interesting comparison might be to other compression approaches where scale mixtures were used and pixels were encoded together with scales ( e.g. , van den Oord & > Schrauwen , 2017 ) .

We were unable to pinpoint this paper , could you please provide a more detailed reference ?

> The authors combine their approach using MS- SSIM as distortion . Is this technically still a VAE ? Might be worth discussing .

We do n't know , and unfortunately , currently do n't have much to say about this point .

> I did not quite follow the motivation for convolving the prior distributions with a uniform distribution .

We tried to improve the explanation in appendix 6.2.

> – On page 3 the paper talks about “ the true posterior ” of a model which has n’t been defined yet . Although most readers will not stumble here as they will be familiar with > VAEs , perhaps mention first that the generative model is defined over both $ x $ and $ \tilde y$ .

We hope to have fixed this with the current revision .

> – Below Equation 2 it sounds like the authors claim that the entropy of the uniform distribution is zero independent of its width .

That was not our intention , and it should be fixed now .

> – Equation 7 is missing some $ \tilde z$ .

Fixed .

> – The operational diagram in Figure 3 is missing a “ | ” .

Fixed .



Authors propose a transform coding solution by extending the work in Balle 2016 . They define an hyperprior for the entropy coder to model the spatial relation between the transformed coefficients .

The paper is well written , although I had trouble following some parts . The results of the proposal are state - of - the -art , and there is an extremely exhaustive comparison with many methods .

In my opinion the work has a good quality to be presented at the ICLR . However , I think it could be excellent if some parts are improved . Below I detail some parts that I think could be improved .


*** MAIN ISSUES

I have two main concerns about motivation that are related . The first refers to hyperprior motivation . It is not clear why , if GDN was proposed to eliminate statistical dependencies between pixels in the image , the main motivation is that GDN coefficients are not independent . Perhaps this confusion could be resolved by broadening the explanation in Figure 2 . My second concern is that it is not clear why it is better to modify the probability distribution for the entropy encoder than to improve the GDN model . I think this is a very interesting issue , although it may be outside the scope of this work . As far as I know , there is no theoretical solution to find the right balance between the complexity of transformation and the entropy encoder . However , it would be interesting to discuss this as it is the main novelty of the work compared to other methods of image compression based on deep learning .

*** OTHER ISSUES

INTRODUCTION

- " ... because our models are optimized end - to - end , they minimize the total expected code length by learning to balance the amount of side information with the expected improvement of the entropy model . "
I think this point is very interesting , it would be good to see some numbers of how this happens for the results presented , and also during the training procedure . For example , a simple comparison of the number of bits in the signal and side information depending on the compression rate or the number of iterations during model training .


COMPRESSION WITH VARIATIONAL MODELS

- There is something missing in the sentence : " ... such as arithmetic coding ( ) and transmitted ... "

- Fig1 . To me it is not clear how to read the left hand schemes . Could it be possible to include the distributions specifically ? Also it is strange that there is a \tiled{y} in both schemes but with different conditional dependencies . Another thing is that the symbol ψ appears in this figure and is not used in section 2 .

- It would be easier to follow if change the symbols of the functions parameters by something like \theta_a and \theta_s .

- " Distortion is the expected difference between ... " Why is the " expected " word used here ?

- " ... and substituting additive uniform noise ... " is this phrase correct ? Are authors is Balle 2016 substituting additive uniform noise ?

- In equation ( 1 ) , is the first term zero or constant ? when talking about equation ( 7 ) authors say " Again , the first term is constant , ... " .

- The sentence " Most previous work assumes ... " sounds strange .

- The example in Fig. 2 is extremely important to understand the motivation behind the hyperprior but I think it needs a little more explanation . This example is so important that it may need to be explained at the beginning of the work . Is this a real example , of a model trained without and with normalization ? If so specify it please . Why is GDN not able to eliminate these spatial dependencies ? Would these dependencies be eliminated if normalization were applied between spatial coefficients ? Could you remove dependencies with more layers or different parameters in the GDN ?

INTRODUCTION OF A SCALE HYPERPRIOR

- TYPO " ... from the center pane of ... "

- " ... and propose the following extension of the model ( figure 3 ) : " there is nothing after the colon . Maybe there is something missing , or maybe it should be a dot instead of a colon . However to me there is a lack of explanation about the model .


RESULTS

- " ... , the probability mass functions P_ŷi need to be constructed “ on the fly ” ... "
How computationally costly is this ?

- " ... batch normalization or learning rate decay were found to have no beneficial effect ( this may be due to the local normalization properties of GDN , which contain global normalization as a special case ) . "

This is extremely interesting . I see the connection for batch normalization , but not for decay of the learning rate . Please , clarify it . Does this mean that when using GDN instead of regular nonlinearity we no longer need to use batch normalization ? Or in other words , do you think that batch normalization is useful only because it is special case of GSN ? It would be useful for the community to assess what are the benefits of local normalization versus global normalization .

- " ... each of these combinations with 8 different values of λ in order to cover a range of rate–distortion tradeoffs . "

Would it be possible with your methods including \lambda as an input and the model parameters as side information ?

- I guess you included the side information when computing the total entropy ( or number of bits ) , was there a different way of compressing the image and the side information ?

- Using the same metrics to train and to evaluate is a little bit misleading . Evaluation plots using a different perceptual metric would be helpful .

- " Since MS-SSIM yields values between 0 ( worst ) and 1 ( best ) , and most of the compared methods achieve values well above 0.9 , we converted the quantity to decibels in order to improve legibility . "
Are differences of MS- SSIM with this conversion significant ? Is this transformation necessary , I lose the intuition . Besides , probably is my fault but I have not being able to " unconvert " the dB to MS-SSIM units , for instance 20 * log10 ( 1 ) = 20 but most curves surpass this value .

- " ... , results differ substantially depending on which distortion metric is used in the loss function during training . "
It would be informative to understand how the parameters change depending on the metric employed , or at least get an intuition about which set of parameters adapt more g_a , g_s , h_a and h_s .

- Figs 5 , 8 and 9 . How are the curves aggregated for different images ? Is it the mean for each rate value ? Note that depending on how this is done it could be totally misleading .

- It would be nice to include results from other methods ( like the BPG and Rippel 2017 ) to compare with visually .

RELATED WORK

Balle et al. already published a work including a perceptual metric in the end - to - end training procedure , which I think is one of the main contributions of this work . Please include it in related work :

" End-to-end optimization of nonlinear transform codes for perceptual quality . " J. Ballé , V. Laparra , and E.P. Simoncelli . PCS : Picture Coding Symposium , ( 2016 )

DISCUSSION

First paragraphs of discussion section look more like a second section of " related work " .
I think it is more interesting if the authors discuss the relevance of putting effort into modelling hyperprior or the distribution of images ( or transformation ) . Are these things equivalent ? Or is there some reason why we ca n't include hyperprior modeling in the g_a transformation ? For me it is not clear why we should model the distribution of outputs as , in principle , the g_a transformation has to enforce ( using the training procedure ) that the transformed data follow the imposed distribution . Is it because the GDN is not powerful enough to make the outputs independent ? or is it because it is beneficial in compression to divide the problem into two parts ?

REFERENCES

- Balle 2016 and Theis 2017 seem to be published in the same conference the same year . Using different years for the references is confusing .

- There is something strange with these references

Ballé , J , V Laparra , and E P Simoncelli ( 2016 ) . “ Density Modeling of Images using a Generalized
Normalization Transformation ” . In : Int ’l. Conf. on Learning Representations ( ICLR2016 ) . URL :
https://arxiv.org/abs/1511.06281.
Ballé , Valero Laparra , and Eero P. Simoncelli ( 2015 ) . “ Density Modeling of Images Using a Gen-
eralized Normalization Transformation ” . In : arXiv e-prints . Published as a conference paper at
the 4th International Conference for Learning Representations , San Juan , 2016 . arXiv : 1511 .
06281 .
– ( 2016 ) . “ End - to-end Optimized Image Compression ” . In : arXiv e-prints . 5th Int. Conf. for Learn -
ing Representations .



Thank you for the review and suggestions .

> I have two main concerns about motivation that are related . The first refers to hyperprior motivation . It is not clear why , if GDN was proposed to eliminate statistical > dependencies between pixels in the image , the main motivation is that GDN coefficients are not independent . Perhaps this confusion could be resolved by broadening the > explanation in Figure 2 . My second concern is that it is not clear why it is better to modify the probability distribution for the entropy encoder than to improve the GDN model > . I think this is a very interesting issue , although it may be outside the scope of this work . As far as I know , there is no theoretical solution to find the right balance > between the complexity of transformation and the entropy encoder . However , it would be interesting to discuss this as it is the main novelty of the work compared to other > methods of image compression based on deep learning .

Thank you very much for pointing this out ! Our intention was to enable factorization of the latent representation as much as possible . However , the hyperprior models still significantly outperform the factorized prior models . We think of that result as an indication that statistical dependencies in the latent representation , at least for compression models , may actually be desirable . Some of our intuitions were not conveyed well in the original draft . We have rewritten large parts of the paper to make this much clearer . Please refer to the revised discussion , as well as the new section 6.3 in the appendix for details .

> - " ... because our models are optimized end - to - end , they minimize the total expected code length by learning to balance the amount of side information with the expected > improvement of the entropy model . "
> I think this point is very interesting , it would be good to see some numbers of how this happens for the results presented , and also during the training procedure . For > example , a simple comparison of the number of bits in the signal and side information depending on the compression rate or the number of iterations during model training .

We included a new plot about this , and a paragraph describing it , in the experimental results section . Generally , the amount of side information used is very low compared to the total bit rate .

> - There is something missing in the sentence : " ... such as arithmetic coding ( ) and transmitted ... "

Fixed .

> - Fig1 . To me it is not clear how to read the left hand schemes . Could it be possible to include the distributions specifically ? Also it is strange that there is a \tiled{y} > in both schemes but with different conditional dependencies . Another thing is that the symbol ψ appears in this figure and is not used in section 2 .

The schemes on the left hand are " graphical models " that are quite standard in the literature on Bayesian modeling ( for instance , refer to " Pattern Recognition and Machine Learning " by Christopher Bishop ) . They are not crucial for the understanding of the paper , but might provide a quick overview for someone familiar with them . Unfortunately , we think there is n't enough space in the paper to provide more detail . Regarding the symbol psi , we reordered sections 2 and 3 to address the problem .

> - It would be easier to follow if change the symbols of the functions parameters by something like \theta_a and \theta_s .

We are following an established convention in the VAE literature to name the parameters of the generative model theta and the parameters of the inference model phi . We understand that this may decrease readability for people with other backgrounds , but currently we think this is the best solution .

> - " Distortion is the expected difference between ... " Why is the " expected " word used here ?

This is meant in the sense of taking the expectation of the difference over the data distribution . We tried to clarify this in the current revision and hope it is clearer now .

> - " ... and substituting additive uniform noise ... " is this phrase correct ? Are authors is Balle 2016 substituting additive uniform noise ?

Yes , that is correct .

> - In equation ( 1 ) , is the first term zero or constant ? when talking about equation ( 7 ) authors say " Again , the first term is constant , ... " .

They are zero * and * constant in both cases . We changed the language to be more precise .

> - The sentence " Most previous work assumes ... " sounds strange .

We rewrote parts of the paper , which should have fixed this .


> - The example in Fig. 2 is extremely important to understand the motivation behind the hyperprior but I think it needs a little more explanation . This example is so important > that it may need to be explained at the beginning of the work . Is this a real example , of a model trained without and with normalization ? If so specify it please .

Yes , this is a real example . We made the description more precise , and we hope that our edits to the main text helped to convey the motivation better .

> Why is GDN not able to eliminate these spatial dependencies ? Would these dependencies be eliminated if normalization were applied between spatial coefficients ? Could you remove dependencies with more layers or different parameters in the GDN ?

We think that GDN is capable of removing more dependencies than what we observe remain , but a certain amount of dependency may actually be desirable in the context of rate -- distortion optimization . Unfortunately , it 's impossible to fully control for all other possible causes of the remaining statistical dependencies , but we are interested in researching this further .

> - TYPO " ... from the center pane of ... "

Fixed .

> - " ... and propose the following extension of the model ( figure 3 ) : " there is nothing after the colon . Maybe there is something missing , or maybe it should be a dot instead of > a colon . However to me there is a lack of explanation about the model .

Fixed .

> - " ... , the probability mass functions P_ŷi need to be constructed “ on the fly ” ... "
> How computationally costly is this ?

We are investigating this currently . Our implementation at this point is naive , in that it pre-generates the probability tables and fully stores them in memory before doing the arithmetic coding . The memory requirements can be substantial , slowing the process down artificially . A better way would be to inline these computations . We 're also working on a method to collect accurate timing data , and will update the paper once we have them .

> - " ... batch normalization or learning rate decay were found to have no beneficial effect ( this may be due to the local normalization properties of GDN , which contain global > normalization as a special case ) . "
> This is extremely interesting . I see the connection for batch normalization , but not for decay of the learning rate . Please , clarify it . Does this mean that when using GDN > instead of regular nonlinearity we no longer need to use batch normalization ? Or in other words , do you think that batch normalization is useful only because it is special > case of GSN ? It would be useful for the community to assess what are the benefits of local normalization versus global normalization .

We think that GDN has the potential to subsume the effects of batch normalization , as it implements local normalization , which is a generalization of global normalization . The Tensorflow implementation of GDN uses different default constants compared to the one described by Ballé ( 2017 ) , which we suspect may have something to do with the fact that we could n't get much gains out of applying a learning rate decay . However , this is speculative , and we are still researching these effects .

> - " ... each of these combinations with 8 different values of λ in order to cover a range of rate–distortion tradeoffs . "
> Would it be possible with your methods including \lambda as an input and the model parameters as side information ?

Yes , we could treat lambda as side information and have the decoder switch between different sets of model parameters based on that . All that would be required is an encoding scheme for lambda .

> - I guess you included the side information when computing the total entropy ( or number of bits ) , was there a different way of compressing the image and the side information ?

Yes , the reported rates are total bit rates for encoding y and z . We included a new figure in the experimental results section to show the fraction of side information compared to total bit rate .

> - Using the same metrics to train and to evaluate is a little bit misleading . Evaluation plots using a different perceptual metric would be helpful .

Why do you think it is misleading to train and evaluate on the same metric , could you elaborate ?


> - " Since MS-SSIM yields values between 0 ( worst ) and 1 ( best ) , and most of the compared methods achieve values well above 0.9 , we converted the quantity to decibels in order to > improve legibility . "
> Are differences of MS- SSIM with this conversion significant ? Is this transformation necessary , I lose the intuition . Besides , probably is my fault but I have not being able to > " unconvert " the dB to MS-SSIM units , for instance 20 * log10 ( 1 ) = 20 but most curves surpass this value .

We updated the paper to include the exact formula we used in the figure caption , thanks for pointing out this oversight . The rationale for using this transformation is that a difference of , say , 0.01 in the 0.99 MS- SSIM range is much more significant than the same difference around a value of 0.91 , for example . In a plot , the difference becomes harder and harder to see the closer the values approach 1 . The logarithm serves to provide a visually more balanced presentation .

> - " ... , results differ substantially depending on which distortion metric is used in the loss function during training . "
> It would be informative to understand how the parameters change depending on the metric employed , or at least get an intuition about which set of parameters adapt more g_a , > g_s , h_a and h_s .

We agree that this would be interesting , but lack a good way of measuring it . We will likely do more research in this direction in the future .

> - Figs 5 , 8 and 9 . How are the curves aggregated for different images ? Is it the mean for each rate value ? Note that depending on how this is done it could be totally > misleading .

Thank you for pointing this out . We have updated the paper to use interpolated rate aggregation for the MS-SSIM plots , in order to match Rippel ( 2017 ) , and to use lambda-aggregation for the PSNR plots , in order to effectively compare to HEVC ( the results did not change much ) . We discuss this in appendix 6.4.

> - It would be nice to include results from other methods ( like the BPG and Rippel 2017 ) to compare with visually .

We agree this would be desirable , but this is limited in practice as Rippel ( 2017 ) have not made their reconstructed images available to us . For visual comparisons , we need to match bit rates for the compared image , which is not easy given models trained for a discrete set of lambdas . ( Many images provided in the appendix approximately match , but not all of them . ) We ’ll attempt to prepare a visual comparison to BPG for the final paper , if time permits , or make it available online later .

> Balle et al. already published a work including a perceptual metric in the end - to - end training procedure , which I think is one of the main contributions of this work . Please > include it in related work :
>
> " End - to -end optimization of nonlinear transform codes for perceptual quality . " J. Ballé , V. Laparra , and E.P. Simoncelli . PCS : Picture Coding Symposium , ( 2016 )

Thanks - we fixed this . Note that their results are quite limited , as they use block transforms which do n't adapt to the data as well as deeper models .

> First paragraphs of discussion section look more like a second section of " related work " .
> I think it is more interesting if the authors discuss the relevance of putting effort into modelling hyperprior or the distribution of images ( or transformation ) . Are these things equivalent ? Or is there some reason why we ca n't include hyperprior modeling in the g_a transformation ? For me it is not clear why we should model the distribution of outputs as , in principle , the g_a transformation has to enforce ( using the training procedure ) that the transformed data follow the imposed distribution . Is it because the GDN is not powerful enough to make the outputs independent ? or is it because it is beneficial in compression to divide the problem into two parts ?

We think that it may be beneficial to divide the problem into two parts , as you say , and that our results provide a bit of evidence regarding that . However , we did n't do a good job of presenting this intuition in the first draft . We hope that the current revision is much clearer .

> - Balle 2016 and Theis 2017 seem to be published in the same conference the same year . Using different years for the references is confusing .

Fixed .

> - There is something strange with these references
>
> Ballé , J , V Laparra , and E P Simoncelli ( 2016 ) . “ Density Modeling of Images using a Generalized
> Normalization Transformation ” . In : Int ’l. Conf. on Learning Representations ( ICLR2016 ) . URL :
> https://arxiv.org/abs/1511.06281.
> Ballé , Valero Laparra , and Eero P. Simoncelli ( 2015 ) . “ Density Modeling of Images Using a Gen-
> eralized Normalization Transformation ” . In : arXiv e-prints . Published as a conference paper at
> the 4th International Conference for Learning Representations , San Juan , 2016 . arXiv : 1511 .
> 06281 .
> – ( 2016 ) . “ End - to-end Optimized Image Compression ” . In : arXiv e-prints . 5th Int. Conf. for Learn -
> ing Representations .

Fixed .


The paper is a step forward for image deep compression , at least when departing from the ( Balle et al. , 2017 ) scheme .
The proposed hyperpriors are especially useful for medium to high bpp and optimized for L2 / PSNR evaluation .

I find the description of the maths too laconic and hard to follow . For example , what ’s the U ( .|. ) operator in ( 5 ) ?

What ’s the motivation of using GDN as non linearity instead of e.g. ReLU ?

I am not getting the need of MSSSIM ( dB ) . How exactly was it defined / computed ?

Importance of training data ? The proposed models are trained on 1 million images while others like ( Theis et al , 2017 ) and [ Ref1 , Ref2 ] use smaller datasets for training .

I am missing a discussion about Runtime / complexity vs . other approaches ?

Why MSSSIM is a relevant measure ? The Fig. 6 seem to show better visual results for L2 loss ( PSNR ) than when optimized for MSSSIM , at least in my opinion .

What 's the reason to use 4:4:4 for BPG and 4:2:0 for JPEG ?

What is the relation between hyperprior and importance maps / content - weights [ Ref1 ] ?

What about reproducibility of the results ? Will be the codes / models made publicly available ?

Relevant literature :
[ Ref1 ] Learning Convolutional Networks for Content-weighted Image Compression ( https://arxiv.org/abs/1703.10553)
[ Ref2 ] Soft-to- Hard Vector Quantization for End-to- End Learned Compression of Images and Neural Networks ( https://arxiv.org/abs/1704.00648)


Thank you for the review and suggestions .

> I find the description of the maths too laconic and hard to follow . For example , what ’s the U ( .|. ) operator in ( 5 ) ?

We have completely rewritten some sections of the paper in order to improve clarity . U ( .|. ) indicates a uniform distribution ( this is stated in the text ) . We hope that the paper is now easier to follow . Please let us know if there are any other ( or new ) parts which you find hard to read , we are happy to make further improvements .

> What ’s the motivation of using GDN as non linearity instead of e.g. ReLU ?

We have found that GDN nonlinearities , while keeping all other architecture parameters constant , provides significantly better performance than ReLU in g_a and g_s . We have n't done any systematic experiments regarding nonlinearities used in h_a and h_s , and went with ReLU as a " default " choice ( note that the amount of side information overall is very small , so we might not benefit much by optimizing this part of the model ) .

> I am not getting the need of MSSSIM ( dB ) . How exactly was it defined / computed ?

MS-SSIM is defined in Wang , Simoncelli , et al. ( 2003 ) . It is one of the most widely used perceptual image quality metrics . Thank you for pointing out that we did n’t define how we converted to decibels . We included the exact definition in the figure caption .

> Importance of training data ? The proposed models are trained on 1 million images while others like ( Theis et al , 2017 ) and [ Ref1 , Ref2 ] use smaller datasets for training .

We think this can likely be ruled out as a source of performance gains . Compared to the factorized - prior model in Ballé ( 2017 ) , which was trained on ~ 7000 images and squared error , our squared - error factorized - prior model matches its performance on PSNR ( figure 10 ) and even underperforms on MS- SSIM ( figure 11 ) .

> I am missing a discussion about Runtime / complexity vs . other approaches ?

Our main goal here was to optimize for compression performance , and to control for the effect of capacity limitations of the model ( as a result of fewer filters ) , which may cause unnecessary statistical dependencies in the representation . We realize this intention was n't sufficiently clear in our first draft , which has lead to some confusion . We have rewritten parts of the paper , added a paragraph to the discussion , and added a supporting section in the appendix ( 6.3 ) to clarify .

Comparing the runtime of the encoding and decoding process is important when evaluating compression methods for deployment . To make a fair comparison , all of the components involved must be appropriately optimized , which has not been a priority in our research so far . In particular , we have only implemented the arithmetic coder in a naive way , writing a very large probability table in memory , which is simple to implement , but unnecessarily slows down the computation . An optimized implementation would inline the computation of the probability tables in eq . ( 11 ) . Some idea of complexity can be gathered from the architecture . Unfortunately , we omitted the number of filters in the transforms , which was an oversight . We now state this in the caption of the figure showing the architecture .

We are working on improving our methods to make accurate runtime measurements , and will be happy to provide them in the final paper or here , as soon as we have them . To give you an estimate for the current implementation : the factorized - prior model , which does not suffer from the naive implementation mentioned above , can encode one of the Kodak images in ~ 70 ms ( corresponding to a throughput of ~ 5.5 megapixels per second ) . We estimate the hyperprior model to require a longer runtime , mostly due to h_a and h_s . Note that due to further subsampling , however , the complexity of h_a and h_s should be significantly lower than g_a and g_s .



> Why MSSSIM is a relevant measure ? The Fig. 6 seem to show better visual results for L2 loss ( PSNR ) than when optimized for MSSSIM , at least in my opinion .

MS- SSIM is a widely used image quality index , and has been popular in previous papers presenting ANN - based compression methods . We wanted to understand how visual quality differs when optimizing for different metrics . The image we show here represents a challenge for MS - SSIM , which we felt was important to talk about given how popular the metric is . Other images , such as Kodak 15 , tend to be more challenging for squared - error optimized models . Note that in this revision of the paper , we lowered the bit rates of the example images in the appendix , to make artifacts more visible and to demonstrate this effect more clearly across a range of different images .

> What 's the reason to use 4:4:4 for BPG and 4:2:0 for JPEG ?

Because we optimized our models for squared error in the RGB representation ( rather than a luma - - chroma colorspace ) , BPG 4:4:4 is the appropriate method to compare to , as it optimizes the same metric . With respect to JPEG , the 4:4:4 format is not widely used , and we found that it also appears to perform much worse than 4:2:0 ( indicating it may not have been optimized as well ) .

> What is the relation between hyperprior and importance maps / content - weights [ Ref1 ] ?

The importance maps of Li et al. ( 2017 ) are primarily designed to provide an embedded code ( i.e. , a compressed representation of the image which allows accessing lower- quality versions of the image by decoding only a part of the bitstream ) . To do this , they employ binary quantization rather than integer ( i.e. multi-level ) quantization , among other techniques . Their entropy model corresponds to a Markov-style prior , similar to the ones used in Johnston et al. ( 2017 ) and Rippel et al . ( 2017 ) .

> What about reproducibility of the results ? Will be the codes / models made publicly available ?

We are striving to publish at least the full results and parts of the code / model parameters , but due to possible legal constraints , we cannot make any promises at this point . We hope that the description in the paper is self - contained and detailed enough to be useful . We 're also happy to answer any further questions .


Summary

This paper presents Neural Process Networks , an architecture for capturing procedural knowledge stated in texts that makes use of a differentiable memory , a sentence and word attention mechanism , as well as learning action representations and their effect on entity representations . The architecture is tested for tracking entities in recipes , as well as generating the natural language description for the next step in a recipe . It is compared against a suit of baselines , such as GRUs , Recurrent Entity Networks , Seq2Seq and the Neural Checklist Model . While I liked the overall paper , I am worried about the generality of the model , the qualitative analysis , as well as a fair comparison to Recurrent Entity Networks and non-neural baselines .

Strengths

I believe the authors made a good effort in comparing against existing neural baselines ( Recurrent Entity Networks , Neural Checklist Model ) * for their task * . That said , it is unclear to me how generally applicable the method is and whether the comparison against Recurrent Entity Networks is fair ( see Weaknesses ) .
I like the ablation study .

Weaknesses

While I find the Neural Process Networks architecture interesting and I acknowledge that it outperforms Recurrent Entity Networks for the presented tasks , after reading the paper it is not clear to me how generally applicable the architecture is . Some design choices seem rather tailored to the task at hand ( manual collection of actions MTurk annotation in section 3.1 ) and I am wondering where else the authors see their method being applied given that the architecture relies on all entities and actions being known in advance . My understanding is that the architecture could be applied to bAbI and CBT ( the two tasks used in the Recurrent Entity Networks paper ) . If that is the case , a fair comparison to Recurrent Entity Networks would have been to test against Recurrent Entity Networks on these tasks too . If they the architecture cannot be applied in these tasks , the authors should explain why .
I am not convinced by the qualitative analysis . Table 2 tells me that even for the best model the entity selection performance is rather unreliable ( only 55.39 % F1 ) , yet all examples shown in Table 3 look really good , missing only the two entities oil ( 1 ) and sprinkles ( 3 ) . This suggests that these examples were cherry - picked and I would like to see examples that are sampled randomly from the dev set . I have a similar concern regarding the generation task . First , it is not mentioned where the examples in Table 6 are taken from – is it the train , dev or test set ? Second , the overall BLEU score seems quite low even for the best model , yet the examples in Table 6 look really good . In my opinion , a good qualitative analysis should also discuss failure cases . Since the BLEU score is so low here , you might also want to compare perplexity of the models .
The qualitative analysis in Table 5 is not convincing either . In Appendix A.1 it is mentioned that word embeddings are initialized from word2vec trained on the training set . My suspicion is that one would get the clustering in Table 4 already from those pretrained vectors , maybe even when pretrained on the Google news corpus . Hence , it is not clear what propagating gradients through the Neural Process Networks into the action embeddings adds , or put differently , why does it have to be a differentiable architecture when an NLP pipeline might be enough ? This could easily be tested by another ablation where action embeddings are pretrained using word2vec and then fixed during training of the Neural Process Network . Moreover , in 3.3 it is mentioned that even the Action Selection is pretrained , which makes me wonder what is actually trained jointly in the architecture and what is not .
I think the difficulty of the task at hand needs to be discussed at some point , ideally early in the paper . Until examples on page 7 are shown , I did not have a sense for why a neural architecture is chosen . For example , in 2.3 it is mentioned that for " wash and cut " the two functions fwash and fcut need to be selected . For this example , this seems trivial as the functions have the same name ( and you could even have a function per name ! ) . As far as I understand , the point of the action selector is to only have a fixed number of learned actions and multiple words ( cut , slice etc. ) should select the same action fcut . Otherwise ( if there is little language ambiguity ) I would not see the need for a complex neural architecture . Related to that , a non-neural baseline for the entity selection task that in my opinion definitely needs to be added is extracting entities using a pretrained NER system and returning all of them as the selection .
p2 Footnote 1 : So if I understand this correctly , this work builds upon a dataset of over 65 k recipes from Kiddon et al . ( 2016 ) , but only for 875 of those detailed annotations were created ?

Minor Comments

p1 : The statement " most natural language understanding algorithms do not have the capacity … " should be backed by reference .
p2 : " context representation ht " – I would directly mention that this is a sentence encoding .
p3 : 2.4 : I have the impression what you are describing here is known in the literature as entity linking .
p3 Eq.3 : Is n't c3*0 always a vector of zeros ?
p4 Eq.6 : W4 is an order - 3 tensor , correct ?
p4 Eq.8 : What is YC and WC here and what are their dimensions ? I am confused by the softmax , as my understanding ( from reading the paragraph on the Action Selection Loss on p.5 ) was that the expression in the softmax here is a scalar ( as it is done for every possible action ) , so this should be a sigmoid to allow for multiple actions to attain a probability of 1 ?
p5 : " See Appendix for details " -> " see Appendix C for details "
p5 3.3 : Could you elaborate on the heuristic for extracting verb mentions ? Is only one verb mention per sentence extracted ?
p5 : " trained to minimize cross -entropy loss " -> " trained to minimize the cross -entropy loss "
p5 3.3 : What is the global loss ?
p6 : " been read ( § 2.5 . " -> " been read ( § 2.5 ) . "
p6 : " We encode these vectors using a bidirectional GRU " – I think you composing a fixed - dimensional vector from the entity vectors ? What 's eI ?
p7 : For which statement is ( Kim et al. 2016 ) the reference ? Surely , they did not invent the Hadamard product .
p8 : " Our model , in contrast " use " -> " Our model , in contrast , uses " .
p8 Related Work : I think it is important to mention that existing architectures such as Memory Netwroks could , in principle , learn to track entities and devote part of their parameters to learn the effect of actions . What Neural Process Networks are providing is a strong inductive bias for tracking entities and learning the effect of actions that is useful for the task considered in this paper . As mentioned in the weaknesses , this might however come at the price of a less general model , which should be discussed .

# Update after the rebuttal
Thanks for the clarifications and updating the paper . I am increasing my score by two points and expect to see the ablations as well as the NER baseline mentioned in the rebuttal in the next revision of the paper . Furthermore , I encourage the authors to include the analysis of pretrained word2vec embeddings vs the embeddings learned by this architecture into the paper .

Because our response was longer than 5000 characters , we separate our response into multiple parts to break the writing at natural breaks in the response .

--- Motivation with Respect to Related Work ---
We thank the reviewer for the question that prompts us to better clarify the key differences between previous approaches based on datasets such as b AbI , and the task proposed in our study . The motivation of our work is to probe a research direction where we make use of naturally existing text with no gold labels , and investigate the role of the modular architecture and intermediate loss functions ( with distant supervision ) for learning latent action dynamics . In sum , the key contributions of our work are ( 1 ) to introduce a new task and dataset ( including detailed annotations for evaluation ) that bring up unique challenges that previous datasets did not cover , and ( 2 ) to propose a new model that is better suited for this new challenge of reasoning about action dynamics .

As such , our newly introduced task actually complements work on densely labeled datasets such as b AbI . The bAbI dataset is synthetically constructed such that training labels cover the full spectrum of the semantics the model is expected to learn ( i.e. , # of training instances is extremely high for the # of words / concepts involved ) . When the training set provides sufficient inductive signals , it is possible to train an end - to - end model to extract the complex relations needed to do well on the task , and Recurrent Entity Networks are one of the best models architected . In our task , because the dataset alone does not provide sufficient inductive signals ( only 875 recipes are densely labeled for evaluation ) , we investigate methods to provide better inductive biases using intermediate losses guided by distant supervision . We view both types of research directions --- integrating inductive biases into datasets ( b AbI ) vs. models ( NPN ) --- as important to pursue . They are complementary to each other , and our work focuses on the latter that has been relatively less explored in the existing literature .

CBT is a cloze task based on children ’s stories . While CBT is based on real natural language text like ours , the nature of the task differs much from ours in that answering the cloze task often requires remembering the surface patterns associated with each entity throughout the story excerpt ( as has been also suggested by the Window Encoding used by prior approaches on this task ) . In contrast , our task focuses on the unspoken causal effects of actions , rather than explicitly mentioned descriptions about entities .

Given the key differences between our task and others , it seems beyond the scope to require our model to outperform on all other tasks with different modeling requirements . That said , we are happy to include a detailed and insightful narrative about these differences in our revision , along with side by side performance comparisons . At this time , our conjecture is that updating entity states only through action application is likely to be too restrictive for CBT and bAbI tasks where remembering surface patterns without corresponding actions is crucial . However , our neural process networks can be easily extended to directly connect the sentence encoding to the simulation module ( a minor change to one equation ) , in order to allow for updating entity representations even when there are no explicit actions associated .

--- Qualitative Examples ---
The examples in all tables were taken from the development set . We chose these examples to provide interesting case studies on some of the patterns that the model is able to learn by reading text and simulating the underlying world . We agree with the reviewer that an analysis of failure cases should have been included in the original submission and have updated the paper to include examples of similarly interesting cases the model misses . We intend to expand the model ’s capabilities to capture these in future work .



--- Learning Causality -aware Action Embeddings ---
We apologize for the confusion about the action selector pretraining . What we meant in this case is that the MLP used to select action embeddings is pretrained . The action embeddings themselves are learned jointly with the entity selector , the simulation module , state change predictors and sentence encoder .

We included Table 5 to show our action embeddings model semantic similarity between real - world actions . While word2vec embeddings would , no doubt , capture lexical semantics between these actions , the neural process network learns commonalities between actions that are n’t as extractable with a word2vec model . Looking at the action embeddings for ``bake ” , ``boil ” , and ``pour ” , for example , we list the cosine similarities between pairs below :

Skipgram :
boil - bake → 0.329
boil - pour → 0.548

NPN :
boil - bake → 0.687
boil - pour → - 0.119

The NPN learns action embedding representations based on the state changes those actions induce as opposed to the local context windows around the mentions of the action in text , thereby encoding different semantics in the learned representation . While we did not use pretrained skipgram embeddings to initialize the action embeddings in our work , it is possible that including them when training our model might even lead to better results on our task as the action embeddings could encode elements of both lexical ( word2vec ) and frame ( NPN ) semantics . Conversely , we would argue that using only pretrained action embeddings from a word2vec model with no additional training would cause the bilinear matrix from the simulation module to have to learn the simulation mapping functions on its own , which would make the model less expressive . We will include both additional ablations in our final paper .

For the moment , the action selector learns from distant supervision ( string matching in each sentence is used to extract verb mention ( s ) as labels ) , but the model is designed to generalize beyond this signal . For example , in the sentence “ Boil the water in the pot ” , the model is designed to be able to select a composite action that includes an action such as f_put because boiling water involves moving the location of water to the pot . For the moment , we initialize a single action embedding for each verb in the lexicon and let the model learn to map sentences to a mask over these action embeddings . We agree with the reviewer , however , that it would be an interesting investigation to make the action embeddings ``implicit , ” letting the model learn to select combinations of elementary actions . This approach is one of our current avenues of future work and could have the effect of generalizing the model similarly to the un-tied version of the REN .

The reviewer makes a good point about including an NER baseline in the evaluation and we will include it in the final paper . We do n’t anticipate the performance being much stronger than the GRU baseline , however , since current NER systems can only identify entities that are directly mentioned in the text , thereby missing elided , coreferent and composite mentions .


--- Minor Comments ---
We appreciate the reviewer ’s thought - provoking questions about the impact of our model . We ’ve updated the paper to extend the qualitative evaluation with additional examples and to clarify where our approach differentiates with the goals of general - purpose memory models such as Recurrent Entity Networks . We thank the reviewer for pointing out additional baselines and ablations to run to show the importance of the components of the model and will update the paper to incorporate them as we get the results . Finally , we appreciate the reviewer ’s comments pointing out minor corrections to be made in the paper , and have incorporated them in the revised version .

Below , we address minor comments made by the reviewer that were not addressed in the paragraphs above .

p3 : 2.4 : I have the impression what you are describing here is known in the literature as entity linking .

Assuming the reviewer is referring to the recurrent attention paragraph , we think coreference resolution would be a more accurate analogue to the task being handled as the goal of the recurrent attention mechanism is to tie connections between entity changes in the text without the use of an external KB . However , coreference tasks are defined only over explicitly mentioned entities in the text , while our task requires reasoning about implicit mentions as well , e.g. , “ Add water to the pot . Boil for 30 minutes ” ( where the implicit argument of Boil is water ) .

p3 Eq.3 : Is n't c3*0 always a vector of zeros ?

Yes , we included this option in the choice distribution as an easy short - circuit for the model to choose to include no entities in a particular step .

p4 Eq.6 : W4 is an order - 3 tensor , correct ?

Yes , W_4 is a bilinear projection tensor between the action embedding and the entity embedding . We ’ve clarified this in the new version of the paper

p4 Eq.8 : What is YC and WC here and what are their dimensions ? I am confused by the softmax , as my understanding ( from reading the paragraph on the Action Selection Loss on p.5 ) was that the expression in the softmax here is a scalar ( as it is done for every possible action ) , so this should be a sigmoid to allow for multiple actions to attain a probability of 1 ?

The softmax here predicts the end state for each state change in the lexicon . Each state change is predicted individually , so Y_c corresponds to the end state being predicted for an individual state change . W_c corresponds to a projection for each individual state change . Each state predictor is a separate multi-class classifier that predicts the end state of the entity from the output of the action applicator . These predictors are trained using the State Change loss in section 5 . Actions are selected by the sigmoid in Equation 1.

p6 : " We encode these vectors using a bidirectional GRU " – I think you composing a fixed - dimensional vector from the entity vectors ? What 's eI ?

e I is the concatenation of final time step hidden states from encoding the entity state vectors in both directions using a bidirectional GRU .

p7 : For which statement is ( Kim et al. 2016 ) the reference ? Surely , they did not invent the Hadamard product .

Kim et al. used the Hadamard product to jointly project two input representation in multimodal learning . We used their citation as a motivation for our decision to jointly project signal from the entity state vectors and word context representation . We ’ve removed the citation to get rid of this ambiguity .


SUMMARY .

The paper presents a novel approach to procedural language understanding .
The proposed model reads food recipes and updates the representation of the entities mentioned in the text in order to reflect the physical changes of the entities in the recipe .
The authors also propose a manually annotated dataset where each passage of a recipe is annotated with entities , actions performed over the entities , and the change in state of the entities after the action .
The authors tested their model on the proposed dataset and compared it with several baselines .


----------

OVERALL JUDGMENT
The paper is very well written and easy to read .
I enjoyed reading this paper , I found the proposed architecture very well thought for the proposed task .
I would have liked to see a little bit more of analysis on the results , it would be interesting to see what are the cases the model struggles the most .

I am wondering how the model would perform without intermediate losses i.e. , entity selection loss and action selection loss .
It would also be interesting to see the impact of the amount of ' intermediate ' supervision on the state change prediction .

The setup for generation is a bit unclear to me .
The authors mentioned to encode entity vectors with a biGRU , do the authors encode it in order of appearance in the text ? would not it be better to encode the entities with some structure - agnostic model like Deep Sets ?



We thank the reviewer for their positive feedback . We share the same excitement about the potential for knowledge - guided architectures that simulate world dynamics .

We ’ve edited the paper to show more analysis examples . We ’d originally shown examples that presented interesting case studies on the model ’s capabilities . We ’ve now added other interesting cases that the model fails to handle , but that future simulators would need to capture to correctly model the domain .

--- Intermediate Losses for learning with Distant Supervision ---
Thanks for the question about the impact of the intermediate modular loss as that was one of the key investigation points of our work : whether a neural network trained with a single loss ( with distant supervision ) could learn the internal dynamics of the task , or whether adding additional losses as guides ( with additional distant supervision ) would promote the architected inductive biases . This investigation point is a direct consequence of the fact that we do not assume a manually constructed dataset that provides sufficient annotated labels that support directly learning implied action dynamics . Instead , we make use of naturally existing data as is , and investigate the role of the modular architecture and distantly supervised intermediate losses for learning latent structure .

To provide more detailed comments about the intermediate loss : without the entity selection and the action selection loss , the model would not learn the necessary bias to use the correct actions and entities in predicting the final states . Pretraining the action selector was also especially useful as it allowed the model to use the correct action embeddings when predicting the state changes that were happening in each step . This allowed errors in predicting the final states to be backpropagated to the correct action embeddings from the start .

We also think it ’s an interesting question to see how many examples the model must see during training to learn to select entities and simulate state changes . We thought about including experiments that randomly dropped a percentage of the training set labels and will add these ablations in the final paper .

--- Generation Modeling Variations ---
We appreciate the reviewer ’s suggestion for using deep sets to encode the state vectors and agree that it seems like a better modeling fit at an intuitive level . While we did not try deep sets as an encoding method , in our pilot study , we explored several attention mechanisms over both the context words and the entity state vectors , and we found that the simple sequential encoding leads to the best performance , a conclusion that had also been found in prior work ( Kiddon et al. 2016 ) . We will look into deep sets as an encoding mechanism and report it in the final paper if helpful .


The paper studies procedural language , which can be very useful in applications such as robotics or online customer support . The system is designed to model knowledge of the procedural task using actions and their effect on entities . The proposed solution incorporates a structured representation of domain-specific knowledge that appears to improve performance in two evaluated tasks : tracking entities as the procedure evolves , and generating sentences to complete a procedure . The method is interesting and presents a good amount of evidence that it works , compared to relevant baseline solutions .

The proposed tasks of tracking entities and generating sentences are also interesting given the procedural context , and the authors introduce a new dataset with dense annotations for evaluating this task . Learning happens in a weakly supervised manner , which is very interesting too , indicating that the model introduces the right bias to produce better results .

The manual selection and curation of entities for the domain are reasonable assumptions , but may also limit the applicability or generality from the learning perspective . This selection may also explain part of the better performance , as the right bias is not just in the model , but in the construction of the " ontologies " to make it work .


--- Architectures with modular prior knowledge representations ---
It is correct that our method assumes that predefined sets of entities , actions , and their causal effects are given before initializing the simulation environment . One motivation behind this design choice is to investigate more explicit and modular representations of the world ( i.e. , entities , actions , and their causal effects ) , abstracting away from specific words that appeared in the input text . We postulated that this modular architecture would better support integration of prior knowledge about actions and their causal effects , which can be viewed as part of the common sense knowledge people start with that biases how they read and interpret text . We agree with the reviewer that an interesting future research direction would be fully automatic acquisition of ontological knowledge , which we felt was beyond the scope of this paper .

--- Mostly automatic acquisition of prior knowledge ---
We also wonder whether there might have been slight confusion about how we acquire the predefined sets of entities , actions , and their causal effects . Importantly , for the training set , we acquire entities and actions automatically from the training corpus . We manually annotated entities and actions only for the purpose of evaluation , but do not use them during training . It is correct that we manually curate the handful of dimensions of action causality , however , primarily because there does not seem to be an easy way to acquire them automatically .


The paper is clear , although there are many English mistakes ( that should be corrected ) .
The proposed method aggregates answers from multiple passages in the context of QA . The new method is motivated well and departs from prior work . Experiments on three datasets show the proposed method to be notably better than several baselines ( although two of the baselines , GA and BiDAF , appear tremendously weak ) . The analysis of the results is interesting and largely convincing , although a more dedicated error analysis or discussion of the limitation of the proposed approach would be welcome .

Minor point : in the description of Quasar -T , the IR model is described as lucene index . An index is not an IR model . Lucene is an IR system that implements various IR models . The terminology should be corrected here .


Thank you for your valuable comments ! We corrected the grammar and spelling issues and revised the Lucene description on Page 6 .

We provided additional discussion in the conclusion section . Our analysis shows that the instances which were incorrectly predicted require complex reasoning and sometimes commonsense knowledge to get right . We believe that further improvement in these areas has the potential to greatly improve performance in these difficult multi-passage reasoning scenarios .

About baselines :
The two baselines , GA and BiDAF , came from the dataset papers . Besides these two , we also compared with the R^3 baseline . This method is from the recent work ( Wang et al , 2017 ) , which improves previous state - of - the - art neural - based open-domain QA system ( Chen et al. , 2017 ) on 4 out of 5 public datasets . As a result , we believe that this baseline reflects the state - of - the -art , thus our experimental comparison is reasonable .


The authors propose an approach where they aggregate , for each candidate answer , text from supporting passages . They make use of two ranking components . A strength - based re-ranker captures how often a candidate answer would be selected while a coverage - based re-ranker aims to estimate the coverage of the question by the supporting passages . Potential answers are extracted using a machine comprehension model . A bi-LSTM model is used to estimate the coverage of the question . A weighted combination of the outputs of both components generates the final ranking ( using softmax ) .
This article is really well written and clearly describes the proposed scheme . Their experiments clearly indicate that the combination of the two re-ranking components outperforms raw machine comprehension approaches . The paper also provides an interesting analysis of various design issues . Finally they situate the contribution with respect to some related work pertaining to open domain QA . This paper seems to me like an interesting and significant contribution .


Thank you for your kind review . We have improved the presentation and added new discussions which we hope will further improve .

Traditional open-domain QA systems typically have two steps : passage retrieval and aggregating answers extracted from the retrieved passages . This paper essentially follows the same paradigm , but leverages the state - of - the- art reading comprehension models for answer extraction , and develops the neural network models for the aggregating component . Although the idea seems incremental , the experimental results do seem solid . The paper is generally easy to follow , but in several places the presentation can be further improved .

Detailed comments / questions :
1 . In Sec. 2.2 , the justification for adding H^{aq} and \bar{H}^{aq} is to downweigh the impact of stop word matching . I feel this is a somewhat indirect and less effective design , if avoiding stop words is really the reason . A standard preprocessing step may be better .
2 . In Sec. 2.3 , it seems that the final score is just the sum of three individual normalized scores . It 's not truly a " weighted " combination , where the weights are typically assumed to be tuned .
3 . Figure 3 : Connecting the dots in the two subfigures on the right does not make sense . Bar charts should be used instead .
4 . The end of Sec. 4.2 : I feel it 's a bad example , as the passage does not really support the answer . The fact that " Sesame Street " got picked is probably just because it 's more famous .
5 . It 'd be interesting to see how traditional IR answer aggregation methods perform , such as simple classifiers or heuristics by word matching ( or weighted by TFIDF ) and counting . This will demonstrates the true advantages of leveraging modern NN models .

Pros :
1 . Updating a traditional open-domain QA approach with neural models
2 . Experiments demonstrate solid positive results

Cons :
1 . The idea seems incremental
2 . Presentation could be improved


Thank you for your feedback and thorough review . We have revised the paper to address the issues you raised and fixed the presentation issues .

ABOUT THE NOVELTY :

Although traditional QA systems also have the answer re-ranking component , this paper focuses on a novel problem of ``text evidence aggregation ' ' : Here the problem is essentially modeling the relationship between the question and multiple passages ( i.e. , text evidence ) , where different passages could enhance or complement each other . For example , the proposed neural re-ranker models the complementary scenario , i.e. , whether the union of different passages could cover different facts in a question , thus the attention - based model is a natural fit .

In contrast , previous answer re-ranking research did not address the above problem : ( 1 ) traditional QA systems like ( Ferrucci et al. , 2010 ) used similar passage retrieval approach with answer candidates added to the queries . However they usually consider each passage individually for extracting features of answers , whereas we utilize the information of union / co-occurrence of multiple passages by composing them with neural networks . ( 2 ) KB - QA systems ( Bast and Haussmann , 2015 ; Yih et al. , 2015 ; Xu et al. , 2016 ) sometimes use text evidence to help answer re-ranking , where the features are also extracted on the pair of a question and a single - passage but ignored the union information among multiple passages .

We have added the above discussion to our paper ( Page 11 ) .

RESPONSE TO THE DETAILED QUESTIONS :

Q1 : In Sec. 2.2 , the justification for adding H^{aq} and \bar{H}^{aq} is to downweigh the impact of stop word matching . I feel this is a somewhat indirect and less effective design , if avoiding stop words is really the reason . A standard preprocessing step may be better .

A1 : We follow the model design in ( Wang and Jiang 2017 ) . The reason for adding H^{aq} and \bar{H}^{aq} is not only to downweigh the stop word matching , but also to take into consideration the semantic information at each position . Therefore , the sentence - level matching model ( Eq. ( 5 ) in the next paragraph ) could potentially learn to distinguish the effects of the element - wise comparison vectors with the original lexical information . We ’ve clarified this on Page 5 .

Q2 : In Sec. 2.3 , it seems that the final score is just the sum of three individual normalized scores . It 's not truly a " weighted " combination , where the weights are typically assumed to be tuned .

A2 : We did tune the assigned weights for the three types of normalized scores on the dev set . The tuned version gives some improvement on dev and results in slightly better test scores , compared to simply summing up the three scores .

Q3 : Figure 3 : Connecting the dots in the two subfigures on the right does not make sense . Bar charts should be used instead .

A3 : We have changed the subfigures to bar charts in the updated version .

Q4 : The end of Sec. 4.2 : I feel it 's a bad example , as the passage does not really support the answer . The fact that " Sesame Street " got picked is probably just because it 's more famous .

A4 : We agree that the passages in Table 6 do not provide full evidence to the question ( unlike the example in Figure 1 b where the passages fully support all the facts in the question ) . However , the “ Sesame Street ” got picked not because it is more famous , but because it has supporting evidence in the form of the " award - winning " and " children 's TV show " facts , while the candidate " Great Dane " only covers " 1969 " .

We selected this example in order to show another common case of realistic problems in Open -Domain QA , where the question is complex and the top - K retrieved passages can not provide full evidence . In this case , our model is able to select the candidate with evidence covering more facts in the question ( i.e. the candidate that is more likely to be approximately correct ) .


Q5 : It 'd be interesting to see how traditional IR answer aggregation methods perform , such as simple classifiers or heuristics by word matching ( or weighted by TFIDF ) and counting . This will demonstrate the true advantages of leveraging modern NN models .

A5 : Thank you for the valuable advice ! We ’ve added a baseline method with BM25 value to rerank the answers based on the aggregated passages , together with the analysis about it in the current version . In summary , the BM25 model improved the F1 scores but sometimes caused a decrease in the EM scores . This is mainly for two reasons : ( 1 ) BM25 relies on bag-of - word representation , so context information is not taken into consideration . Also it does not model the phrase similarities . ( 2 ) shorter answers are preferred by BM25 . For example when answer candidate A is a subsequence of B , then according to our way of collecting pseudo passages , the pseudo passage of A is always a superset of the pseudo passage of B. Therefore F1 scores are often improved while EM declines .


Summary :

This paper proposes a non-recurrent model for reading comprehension which used only convolutions and attention . The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference . Authors also propose a paraphrasing based data augmentation method which helps in improving the performance . Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference .

My Comments :

The proposed model is convincing and the paper is well written .

1 . Why do n’t you report your model performance without data augmentation in Table 1 ? Is it because it does not achieve SOTA ? The proposed data augmentation is a general one and it can be used to improve the performance of other models as well . So it does not make sense to compare your model + data augmentation against other models without data augmentation . I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models .

2 . Can you mention your leaderboard test accuracy in the rebuttal ?

3 . The paper can be significantly strengthened by adding at least one more reading comprehension dataset . That will show the generality of the proposed architecture . Given the sufficient time for rebuttal , I am willing to increase my score if authors report results in an additional dataset in the revision .

4 . Are you willing to release your code to reproduce the results ?


Minor comments :

1 . You mention 4X to 9X for inference speedup in abstract and then 4X to 10X speedup in Intro . Please be consistent .
2 . In the first contribution bullet point , “ that exclusive built upon ” should be “ that is exclusively built upon ” .


We thank the reviewer for your acknowledgement to our contributions ! We address the comments below .

Q : The reviewer asks “ Why do n’t you report your model performance without data augmentation in Table 1 ? ”
A : We thank the reviewer for the suggestion ! We have added this result in the revision . In summary , without data augmentation , our model gets 82.7 F1 on dev set , while with data augmentation , we get 83.8 F1 on dev . We only submitted the model with augmented data , and get 84.6 F1 on test set , which outperforms most of the existing models and is the best among all the published results , as of Dec 20 , 2017 .

Q : The reviewer asks “ Can you mention your leaderboard test accuracy in the rebuttal ? ”
A : We submitted our best model for test set evaluation on SQuAD , on Dec 20 , 2017 . Our single model ( named “ FRC ” ) is ranked 3rd among all single models in terms of F1 with F1/EM=84.6/76.2 ( https://rajpurkar.github.io/SQuAD-explorer/). The performance gain is because we add more regularization to the model . Note that the two single models ranked above us have NOT been published yet : “ BiDAF + Self Attention + ELMo ” & “ AttentionReader + ” .

Q : Results on one more dataset .
A : We have conducted experiments on another Q&A dataset , TriviaQA , to verify that the effectiveness and efficiency of our model is general . In a nutshell , again , our model is 4 x to 16 x times faster than the RNN counterparts , while outperforming the state - of - the- art single - paragraph - reading model by more than 3.0 in both F1 and EM . Please see the revision .

Q : The reviewer asks “ Are you willing to release your code to reproduce the results ? ”
A : Yes , we will release the code after the paper gets accepted .

Q : Minor comments .
A : Thank you . We addressed all of them in the latest revision .


As we have included the result on the triviaQA dataset as well , we hope the reviewer can reconsider the score , as promised in the original review . Thanks again for your suggestion to help us improve the paper !

This paper proposes two contributions : first , applying CNNs + self -attention modules instead of LSTMs , which could result in significant speedup and good RC performance ; second , enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model , which could improve the RC performance marginally .

Firstly , I suggest the authors rewrite the end of the introduction . The current version tends to mix everything together and makes the misleading claim . When I read the paper , I thought the speeding up mechanism could give both speed up and performance boost , and lead to the 82.2 F1 . But it turns out that the above improvements are achieved with at least three different ideas : ( 1 ) the CNN + self -attention module ; ( 2 ) the entire model architecture design ; and ( 3 ) the data augmentation method .

Secondly , none of the above three ideas are well evaluated in terms of both speedup and RC performance , and I will comment in details as follows :

( 1 ) The CNN + self -attention was mainly borrowing the idea from ( Vaswani et al. , 2017a ) from NMT to RC . The novelty is limited but it is a good idea to speed up the RC models . However , as the authors hoped to claim that this module could contribute to both speedup and RC performance , it will be necessary to show the RC performance of the same model architecture , but replacing the CNNs with LSTMs . Only if the proposed architecture still gives better results , the claims in the introduction can be considered correct .

( 2 ) I feel that the model design is the main reason for the good overall RC performance . However , in the paper there is no motivation about why the architecture was designed like this . Moreover , the whole model architecture is only evaluated on the SQuAD dataset . As a result , it is not convincing that the system design has good generalization . If in ( 1 ) it is observed that using LSTMs in the model instead of CNNs could give on par or better results , it will be necessary to test the proposed model architecture on multiple datasets , as well as conducting more ablation tests about the model architecture itself .

( 3 ) I like the idea of data augmentation with paraphrasing . Currently , the improvement is only marginal , but there seems many other things to play with . For example , training NMT models with larger parallel corpora ; training NMT models with different language pairs with English as the pivot ; and better strategies to select the generated passages for data augmentation .

I am looking forward to the test performance of this work on SQuAD .

We believe there are misunderstandings we have addressed below . We also have also included more experimental results .

Q : The reviewer said “ I suggest the authors rewrite the end of the introduction . The current version tends to mix everything together and makes the misleading claim . ”
A : Thank you for the suggestions ! We have revised the introduction to make our contributions clearer . Note that even though self - attention has already been used extensively in Vaswani et al , the combination of convolutions and self - attention is novel , and is significantly better than self - attention alone and gives 2.7 F1 gain in our experiments . The use of convolutions also allows us to take advantage of common regularization methods in ConvNets such as stochastic depth ( layer dropout ) , which gives an additional gain of 0.2 F1 in our experiments .
We would like to point out that in this paper the use of CNN + self - attention is to speed - up the model during training and inference . The speed - up leads to faster experimentation and allows us to train on more augmented data , contributing to the strong result on SQuAD .

Q : The reviewer comments “ I feel that the model design is the main reason for the good overall RC performance . However , in the paper there is no motivation about why the architecture was designed like this . ”
A : At a high level , our architecture is the standard “ embedding -> embedding encoder -> attention -> modeling encoder -> output ” architecture , shared by many neural reading comprehension models . Thus , we do NOT claim any novelty in the overall architecture . Traditionally , the encoder components are bidirectional LSTMs . Our motivation is to speed - up the architecture by replacing the bidirectional LSTMs with convolution + self - attention for the encoders for both embedding and modeling components . The context passages are over one hundred words long in SQuAD , so the parallel nature of CNN architectures leads to a significant speed boost for both training and inference . Replacing bidirectional LSTMs with convolution + self - attention is our main novelty .

Q : The reviewer comments “ it will be necessary to show the RC performance of the same model architecture , but replacing the CNNs with LSTMs . Only if the proposed architecture still gives better results , the claims in the introduction can be considered correct . ”
A : We think the reviewer might have misunderstood our claim . As mentioned above , we do NOT claim any novelty in the overall architecture , as it is a common reading comprehension model . We will make this point clearer in the revision . Our contribution , as we have emphasized several times , is to replace the LSTM encoders with convolution + self - attention , without changing the remaining components . We find the resulting model both fast and accurate . In fact , if we switch back to LSTM encoders , it will become BiDAF [ 1 ] or DCN [ 2 ] , which are both slower ( see our speedup experiments ) and less accurate ( see the leaderboard : https://rajpurkar.github.io/SQuAD-explorer/) than ours .

[ 1 ] Bidirectional Attention Flow for Machine Comprehension . In ICLR 2017 .
Minjoon Seo , Aniruddha Kembhavi , Ali Farhadi , Hannaneh Hajishirzi .
[ 2 ] Dynamic Coattention Networks For Question Answering . ICLR 2017 .
Caiming Xiong , Victor Zhong , Richard Socher .

Q : Results on one more dataset .
A : We have conducted experiments on another Q&A dataset , TriviaQA , to verify that the effectiveness and efficiency of our model is general . In a nutshell , again , our model is 4 x to 16 x times faster than the RNN counterparts , while outperforming the state - of - the- art single - paragraph - reading model by more than 3.0 in both F1 and EM . Please see the revision .

Q : More result on data augmentation .
A : Thanks for the suggestions ! We indeed put more experiments in the revision and here are some interesting findings :
Translating to more languages can lead to more diverse augmented data , which further result in better generalization . Currently we try both French and German .
The sampling ratio of ( original : English - French-English : English - German-English ) during training matters . The best empirical ratio is 3:1:1 .

Q : Leaderboard result .
A : We submitted our best model for test set evaluation on SQuAD , on Dec 20 , 2017 . Our single model ( named “ FRC ” ) is ranked 3rd among all single models in terms of F1 with F1/EM=84.6/76.2 ( https://rajpurkar.github.io/SQuAD-explorer/). The performance gain is because we add more regularization to the model . Note that the two single models ranked above us have NOT been published yet : “ BiDAF + Self Attention + ELMo ” & “ AttentionReader + ” .

This paper presents a reading comprehension model using convolutions and attention . This model does not use any recurrent operation but it is not per se simpler than a recurrent model . Furthermore , the authors proposed an interesting idea to augment additional training data by paraphrasing based on off - the-shelf neural machine translation . On SQuAD dataset , their results show some small improvements using the proposed augmentation technique . Their best results , however , do not outperform the best results reported on the leader board .

Overall , this is an interesting study on SQuAD dataset . I would like to see results on more datasets and more discussion on the data augmentation technique . At the moment , the description in section 3 is fuzzy in my opinion . Interesting information could be :
- how is the performance of the NMT system ?
- how many new data points are finally added into the training data set ?
- what do ‘ data aug ’ x 2 or x 3 exactly mean ?


From the reviewer ’s comments , it is not immediately clear to us the reviewer ’s rationale for rejection . What we only know is the reviewer wants to know more about the data augmentation approach . It would be great if the reviewer can elaborate more on the rejection rationale .

We believe our work is significant in the following aspects :
( a ) Our work is novel : we introduced a new architecture for reading comprehension and a data augmentation technique that yields non-trivial gain on a strong SQuAD model . Note that even though self - attention has already been used extensively in Vaswani et al , the combination of convolutions and self - attention is novel , and is significantly better than self - attention alone and gives 2.7 F1 gain in our experiments . The use of convolutions also allows us to take advantage of common regularization methods in ConvNets such as stochastic depth ( layer dropout ) , which gives an additional gain of 0.2 F1 in our experiments .

( b ) Our model is accurate : we are currently ranked 3rd by F1 score on the SQuAD leaderboard among single models ( note : the two single models ranked above us are not published yet ) .

( c ) Our model is fast : we achieve a speed - up of up to 13 x and 9x in training and inference respectively on SQuAD .

As stated above , we are disappointed with the low scores that our paper has received . Concurrent to our submission , there are two other papers on SQuAD , FusionNet [ 1 ] and DCN + [ 2 ] , which only tested on SQuAD and obtained much lower F1 scores ( 83.9 and 83.0 respectively ) compared to ours ( 84.6 ) . Their papers , however , received the averaged review scores of 6.33 and 7 respectively , which are much higher than our averaged review score of 5.33 . As such , we encourage the reviewers to reconsider their scores .

[ 1 ] https://openreview.net/forum?id=BJIgi_eCZ&noteId=BJIgi_eCZ
[ 2 ] https://openreview.net/forum?id=H1meywxRW

More detailed comments :
Q : Regarding “ simplicity ” .
A : Thanks for raising this point . By simplicity , we mean we do not use hand - crafted features such as POS tagging ( [ 3 ] ) , nor multiple reading pass ( [ 4 ] ) . We have made this point clear in the revision and tried not using “ simple ” to avoid confusion .

[ 3 ] Reading Wikipedia to Answer Open-Domain Questions . In ACL 2017 .
Danqi Chen , Adam Fisch , Jason Weston , Antoine Bordes .
[ 4 ] Reasonet : Learning to stop reading in machine comprehension . In KDD 2017 .
Yelong Shen , Po - Sen Huang , Jianfeng Gao , Weizhu Chen .

Q : Leaderboard result .
A : We submitted our best model for test set evaluation on SQuAD , on Dec 20 , 2017 . Our single model ( named “ FRC ” ) is ranked 3rd among all single models in terms of F1 with F1/EM=84.6/76.2 ( https://rajpurkar.github.io/SQuAD-explorer/). The performance gain is because we add more regularization to the model . Note that the two single models ranked above us have NOT been published yet : “ BiDAF + Self Attention + ELMo ” & “ AttentionReader + ” .

Q : Results on one more dataset .
A : We have conducted experiments on another Q&A dataset , TriviaQA , to verify that the effectiveness and efficiency of our model is general . In a nutshell , again , our model is 4 x to 16 x times faster than the RNN counterparts , while outperforming the state - of - the- art single - paragraph - reading model by more than 3.0 in both F1 and EM . Please see the revision .

Q : Section 3 and more discussion on the data augmentation .
A : We have revised the paper to give more details regarding our method and results with data augmentation . Here , we highlight a few major details that the reviewers asked , as well as several new findings :
a ) Performance of NMT systems :
English -German ( newstest2015 ) : 27.6 ( to German ) and 29.9 ( to English )
English - French ( newstest2014 ) : 36.7 ( to French ) and 35.9 ( to English )
b ) Note that in our Table , “ x2 ” means the total amount of the final training data is twice as large as the original data , i.e. the added amount is the same as the original . We have clarified this as well in the revision .
c ) New finding : translating to more languages can lead to more diverse augmented data , which further result in better generalization . Currently we try both English - French and English - German .
d ) New finding : we have shown in the revised experiment section that different ratios ( original : English - French-English : English - German-English ) would have different effects on the final performance . Empirically , when the ratio is 3:1:1 , we get the best result . We interpret this phenomenon as : the translation might bring noise to the augmented data that we should lay more weight to the original clean data .



This paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem . Taking the full dimensionality to be the total number of parameters of the network model , the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters ( corresponding to a low -dimensional subspace within the original parameter ) , and then training the original network while restricting the projections of its parameters to lie within this subspace . Performance on this subspace is then evaluated relative to that over the full parameter space ( the baseline ) . As an empirical standard , the authors focus on the subspace dimension that achieves a performance of 90 % of the baseline . The authors then test out their measure of intrinsic dimensionality for fully - connected networks and convolutional networks , for several well - known datasets , and draw some interesting conclusions .

Pros :

* This paper continues the recent research trend towards a better characterization of neural networks and their performance . The authors show a good awareness of the recent literature , and to the best of my knowledge , their empirical characterization of the number of latent parameters is original .

* The characterization of the number of latent variables is an important one , and their measure does perform in a way that one would intuitively expect . For example , as reported by the authors , when training a fully - connected network on the MNIST image dataset , shuffling pixels does not result in a change in their intrinsic dimensionality . For a convolutional network the observed 3 - fold rise in intrinsic dimension is explained by the authors as due to the need to accomplish the classification task while respecting the structural constraints of the convnet .

* The proposed measures seem very practical - training on random projections uses far fewer parameters than in the original space ( the baseline ) , and presumably the cost of determining the intrinsic dimensionality would presumably be only a fraction of the cost of this baseline training .

* Except for the occasional typo or grammatical error , the paper is well - written and organized . The issues are clearly identified , for the most part ( but see below ... ) .

Cons :

* In the main paper , the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections . Variance should be taken into account explicitly , in presenting experimental results and in the definition and analysis of the empirical intrinsic dimension itself . How often does a random projection lead to a high - quality solution , and how often does it not ?

* The authors are careful to point out that training in restricted subspaces cannot lead to an optimal solution for the full parameter domain unless the subspace intersects the optimal solution region ( which in general can not be guaranteed ) . In their experiments ( FC networks of varying depths and layer widths for the MNIST dataset ) , between projected and original solutions achieving 90 % of baseline performance , they find an order of magnitude gap in the number of parameters needed . This calls into question the validity of random projection as an empirical means of categorizing the intrinsic dimensionality of a neural network .

* The authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionality greater than or equal to the intrinsic dimension . However , I do n't think that they make a convincing case for this approach . Again , variation is the difficulty : two different projective subspaces of the same dimensionality can lead to solutions that are extremely different in character or quality . How then can we be sure that our compressed network can be reconstituted into a solution of reasonable quality , even when its dimensionality greatly exceeds the intrinsic dimension ?

* The authors argue for a relationship between intrinsic dimensionality and the minimum description length ( MDL ) of their solution , in that the intrinsic dimensionality should serve as an upper bound on the MDL . However they do n't formally acknowledge that there is no standard relationship between the number of parameters and the actual number of bits needed to represent the model - it varies from setting to setting , with some parameters potentially requiring many more bits than others . And given this uncertain connection , and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionality , it is hard to accept that " there is some rigor behind " their conclusion that LeNet is better than FC networks for classification on MNIST because its empirical intrinsic dimensionality score is lower .

* The experimental validation of their measure of intrinsic dimension could be made more extensive . In the main paper , they use three image datasets - MNIST , CIFAR - 10 and ImageNet . In the supplemental information , they report intrinsic dimensions for reinforcement learning and other training tasks on four other sets .

Overall , I think that this characterization does have the potential to give insights into the performance of neural networks , provided that variation across projections is properly taken into account . For now , more work is needed .

====================================================================================================
Addendum :

The authors have revised their paper to take into account the effect of variation across projections , with results that greatly strengthen their results and provide a much better justification of their approach . I 'm satisfied too with their explanations , and how they incorporated them into their revised version . I 've adjusted my rating of the paper accordingly .

One point , however : the revisions seem somewhat rushed , due to the many typos and grammatical errors in the updated sections . I would like to encourage the authors to check their manuscript once more , very carefully , before finalizing the paper .
====================================================================================================

Thanks for taking the time to provide such thorough comments and such detailed feedback . Much of the feedback alludes to elements of the work that had been critically omitted . We ’ve since updated the paper to include 7 missing or modified sections and experiments as described below .

> [ Summary and Pros ]

Thanks kindly !

> Cons :

( We ’ve slightly reordered these in response )

> In the main paper , the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections . Variance should be taken into account explicitly , in presenting experimental results and in the definition and analysis of the empirical intrinsic dimension itself . How often does a random projection lead to a high - quality solution , and how often does it not ?

This is an important question . Initial experiments showed that random projections of the sizes under consideration -- e.g. through a dense random matrix with shape ( 750 by 200,000 ) -- contain so many IID random elements ( e.g. 150,000,000 ) that the difference between the luckiest random projection and the least lucky random projection among N = 10 or 20 was tiny and thus possible to ignore . However , we should definitely have described these initial experiments in the paper and otherwise justified the assumption .

We ’ve rectified this omission by ( a ) repeating all FC MNIST experiments three times each and including error bars both on each individual measurement of performance at a given dimension ( see , for example , vertical error bars on each dot in the updated Fig S6 ) , and ( b ) by using these multiple measurements to produce a bootstrapped estimate of the error bars on measurements of intrinsic dimension ( see , for example , horizontal error bars in the updated Fig S6 ) .

As can be seen in Fig S6 , the variance of performance for a given subspace dimension is very small , and the mean of performance monotonically increases ( very similar to one run result ) . It indicates the luckiness in random projection has a little impact on the quality of solutions , while the subspace dimensionality has a great impact on the quality of solutions .

We further repeat our experiments three times for more network architectures and datasets , and report the mean and standard deviation ( std ) on bootstrapped samples .

FC ( depth=2 , width = 200 ) on MNIST : mean=802.25 , std=67.83
LeNet on MNIST : mean=290.0 , std=0.0
FC ( depth=2 , width = 200 ) on CIFAR : mean=8277.5 , std=1378.36
LeNet on CIFAR : mean=2840.0 , std=120.0

The std here is of our measurement of the intrinsic dimension based on one- run result . Even considering the interval report here , the numbers can still serve as sufficient evidences for the main interest of paper : providing the insights of understanding network behavior . Hence , we can rely on one run result for fast compute of the intrinsic dimension , though slightly more accurate solutions can be obtained via multiple runs and refined interval of subspace dimensionality . We ’ve explained this justification in Section S5 and Fig S6 in Supplementary Material ( see updated draft ) .

Further , in the cases where the randomness of the learning process has large performance variance ( apart from the random projection ) , we have adjusted and clarified our definition of “ intrinsic dimension ” to take account of the variance . In reinforcement learning tasks , where the large randomness of the tasks themselves leads to very different performance in different runs , we performed the experiments multiple times , and defined the intrinsic dimension as the dimension at which the mean reward crossed the threshold , where the mean reward is averaged over 30 runs for a given subspace dimension . ( Details given in Section 3.3 )

> The authors then go on to propose that compression of the network … two different projective subspaces of the same dimensionality can lead to solutions that are extremely different in character or quality .

As now more carefully quantified above , the performance variation across different random projections is minimal . ( Also : see next section ) .

> How then can we be sure that our compressed network can be reconstituted into a solution of reasonable quality , even when its dimensionality greatly exceeds the intrinsic dimension ?

We ’re not training full networks , compressing them , and then reconstituting them into networks we hope will happen still to perform well ( such an approach could indeed fail ) . Instead , networks are trained directly in the compressed space ( “ end to end ” ) and evaluated during training and validation exactly as they would be in production . So training and validation accuracies may be interpreted as faithfully as one would in a normal training scenario .

> The authors are careful to point out ... [ T]hey find an order of magnitude gap in the number of parameters needed . This calls into question the validity of random projection as an empirical means of categorizing the intrinsic dimensionality of a neural network .

Perhaps we misunderstand part of this objection , but if not , we believe the approach is still quite defensible . We may see this from two directions : empirical and theoretical .

Empirical : trying many different random projections tends to produce very similar results ( thanks for the suggestion to report this directly ! )

Theoretical : given two random hyperplanes of dimension m and n in a space of dimension D , m and n will intersect with probability 0 if m + n < D and probability 1 if m + n >= D . The transition from “ almost surely will not intersect ” to “ almost surely will intersect ” is sudden . This result is for the intersection of two random hyperplanes , not for the intersection of a structured solution set and a random hyperplane , but we expect ( and above measure ) similar sudden transitions for actual solution sets .

> The authors argue for a relationship between intrinsic dimensionality and the minimum description length ( MDL ) ... However there is no standard relationship between the number of parameters and the actual number of bits

We just give a loose upper bound : one where each parameter -- native or subspace -- requires 32 bits to represent in floating point format . It ’s true that some parameters may be represented using far fewer bits , but this does not make the upper bound any less valid . We ’ve clarified that the upper bound refers to 32 bit floats , which were used for all experiments .

> [ More extensive experimental validation using other datasets ]

The results presented in the paper ( not including replicates re-run thanks to the above suggestions ) derive from over 10,000 experimental runs from seven datasets / environments . Generally speaking we ’d love to include even more , but we believe the theory and method as presented has been sufficiently validated through experiments . We certainly do believe that researchers will be able to generate great insights by applying these methods to further datasets , but at this point we feel it is defensible to delegate that to future work .

Nonetheless , we note extra experimentation added to the current draft after the initial submission :

In Section S8 ( paragraph “ The role of regularizers ” ) , we investigate the regularization ability of subspace training vs traditional regularizers ( weight decay and Dropout ) and their combinations . It is shown that subspace training itself has strong regularization ability by reducing the dimensionality of solution set , and its combination with traditional regularizers can further improve validation performance . Regarding intrinsic dimension , stronger traditional regularizers generally lead to slightly larger measured intrinsic dimension , as regularizers restrict the expressiveness of the model , which must be overcome by extra dimensions .
In Section S10 , we apply intrinsic dimension to further understand the contribution of each component in convolutional networks for image classification task : local receptive fields and weight -tying ( i.e. , the two aspects that a convolutional network is a special case of a FC network )

All told : again , thanks for the helpful , critical feedback ! We think the paper as amended is much stronger than it was on submission and hope you will agree .


[ =============================== REVISION =========================================================]
My questions are answered , paper undergone some revision to clarify the presentation . I still maintain that it is a good paper and argue for acceptance - it provides a witty way of checking whether the network is overparameterized . Mnist with shuffled labels is a great example that demonstrates the value of the approach , I would though have moved the results of it into the main paper , instead of supplemental materials
[ ======================== END OF REVISION =========================================================]

Authors introduce ransom subspace training ( random subspace neural nets ) where for a fixed architecture , only a subset of the parameters is trained , and the update for all the parameters is derived via random projection which is fixed for the duration of the training . Using this type of a network , authors introduce a notion of intrinsic dimension of optimization problems - it is minimal dimension of a subset , for which random subset neural net already reaches best ( or comparable ) performance .
Authors mention that this can be used for compressing networks - one would need to store the seed for the random matrix and the # of params equal to the intrinsic dimension of the net .
They then demonstrate that the intrinsic dimension for the same problem stays the same when different architectures are chosen . Finally they mention neural nets with comparable number of params to intrinsic dimension but that do n’t use random subspace trick do n’t achieve comparable performance . This does not always hold for CNNs
Model with smaller intrinsic dimension is suggested to be better . They also suggest that intrinsic dimension might be a good approximation to Minimum Description Length metric

My main concern is computational efficiency . They state that if used for compressing , their method is different from post - train compression , and the authors state that they train once end - to-end . It is indeed the case that once they found model that performs well , it is easy to compress , however they do train a number of models ( up to a number of intrinsic dimension ) until they get to this admissible model , which i envision would be computationally very expensive .

Questions :
- Are covets always better on MNIST : did n’t understand when authors said that intrinsic dimension of FC on shuffled data stayed the same ( why ) and then say that it becomes 190K - which one is correct ?
- MNIST - state the input dimension size , not clear how you got to that number of parameters overall


Thanks for your kind and helpful comments ! Replies below :

> My main concern is computational efficiency [ to compute intrinsic dimension ]

You are correct that the computational cost can be high to obtain the intrinsic dimension . In practice , one may find the intrinsic dimension by either ( a ) running many training runs at different dimensions in parallel ( low wall clock time , as we used here ) , or ( b ) using binary search ( low total amount of computation ) . In the latter case , the precision of the measured intrinsic dimension is then related to the number of iterations in binary search . Faster methods to obtain intrinsic dimension could be an interesting direction of future work . Slight inefficiency notwithstanding , we see the approach as valuable overall for the insight into network behavior that it provides , even if this insight comes at a cost of a binary search or use of a cluster .

> Are covets always better on MNIST :

Yes , convnets are always better on MNIST than FC networks , EXCEPT in the case where the pixel order is shuffled ( see more on this below ) .

> did n’t understand when authors said that intrinsic dimension of FC on shuffled data stayed the same ( why ) and then say that it becomes 190K - which one is correct ?

Sorry for this confusion , and thanks for pointing it out . The setting will make sense for those that know the Zhang et al 2017 ICLR paper in detail , but for those not familiar , the wording as submitted was indeed quite confusing . We ’ve updated the text to explain the situation more completely ( See Section S5.3 ) .

To wit , there are two different shuffled MNIST datasets :

( a ) a “ shuffled pixel ” dataset in which the label for each example remains the same as the normal dataset , but a random permutation of pixels is chosen once and then applied to all images in the training and test sets . FC networks solve the shuffled pixel datasets exactly as easily as the base dataset , because there is no privileged ordering of input dimension in FC networks -- all orderings are equivalent . Convnets suffer here because they expect local structure but the local structure was destroyed by shuffling pixel locations .

( b ) a “ shuffled label ” dataset in which the images remain the same as the base dataset , but training labels are randomly shuffled for the entire training set . Here , as in [ Zhang et al , ICLR 2017 ] , we only evaluate training accuracy , as test set accuracy remains forever at chance level ( the training set X and y convey no information about test set p( y |X ) , because test set labels are shuffled independent of the training set ) .

The intrinsic dimension of FC nets on the shuffled - label training set becomes huge : 190K . This is because the network must memorize every training set label ( well , 90 % of them ) , and the capacity required to do so is large . This illustrates cleanly an important concept : while the standard dataset and the shuffled - label dataset are of exactly the same size , containing exactly the same bits , and providing exactly the same number of constraints on the learned p( y |X ) , the random version contains much higher entropy , a fact which we can * measure * by computing intrinsic dimension of neural network ! In the standard dataset , the constraints imposed by the image - label pairs in the same class are very similar , thus parameters representing those constraints can be shared . The number of unique constraints is small . In contrast , the shuffled - label dataset fully randomizes the well - structured relationship among image - label pairs . Each randomized pair provides a unique constraint for the model , and the neural network has to be optimized to satisfy all the unique constraints . Hence , the number of unique constraints is very large . The number of unique constraints reflects the intrinsic dimension we obtained for each dataset .

> MNIST - state the input dimension size , not clear how you got to that number of parameters overall

Thanks for your careful study ; it was a typo ! the FC network size is 784-200- 200 - 10 ( 200 , not 400 , as we had stated ) . The total number of parameters ( including “ + 1 ” for biases ) is ( 784 + 1 ) * 200 + ( 200 + 1 ) * 200 + ( 200 + 1 ) * 10 = 199210 . The draft has been updated to fix this .


While deep learning usually involves estimating a large number of variable , this paper suggests to reduce its number by assuming that these variable lie in a low- dimensional subspace . In practice , this subspace is chosen randomly . Simulations show the promise of the proposed method . In particular , figure 2 shows that the number of parameters could be greatly reduced while keeping 90 % of the performance ; and figure 4 shows that this method outperforms the standard method . The method is clearly written and the idea looks original .

A con that I have is about the comparison in figure 4 . While the proposed subspace method might have the same number of parameters as the direct method , I wonder if it is a fair comparison since the subspace method could still be more computational expensive , due to larger number of latent variables .



Thanks for your kind and helpful comments ! A few replies and thoughts are below :

> ... this paper suggests to reduce its number by assuming that these variable lie in a low- dimensional subspace . In practice , this subspace is chosen randomly .

Not to belabor minutiae , but it may be worth discussing a few subtleties here in case any were not already clear . First , the paper does n’t assume that parameters lie in a low - dimensional subspace ; instead , it asks whether they happen to ( it turns out they often do ) and whether if they do , we could measure the dimensionality of that subspace using a simple approach -- by intersecting the solution space with random subspaces ( this does seem to work ) . So there are two subspaces under consideration : the subspace of solutions , which is certainly far more structured than random , and the subspace in which we search for intersection , which we do choose to be random . For example , in the Figure 1 toy example , the solution space is 990 dimensional and highly structured , but the 10 dimensional subspace in which we find intersection is random .

> Simulations show the promise of the proposed method . In particular , figure 2 shows that the number of parameters could be greatly reduced while keeping 90 % of the performance ; and figure 4 shows that this method outperforms the standard method . The method is clearly written and the idea looks original .

Indeed , the random projection method “ outperforms the standard method ” if we consider more parsimonious models to outperform those with more parameters . However , note that while this is a fun by - product of the approach , we ’d like to emphasize that the primary importance of the work is that it provides a tool that can be used to analyze and measure network behavior . By using random projections , we obtain a window into the complex , high dimensional objective landscape that was n’t previously reported , and we think this will be quite useful for the field !

We ’ve rewritten parts of the introduction to emphasize more clearly that our paper is not primarily one proposing a better model , but a paper providing insights into network properties .

> A con that I have is about the comparison in figure 4 . While the proposed subspace method might have the same number of parameters as the direct method , I wonder if it is a fair comparison since the subspace method could still be more computationally expensive , due to larger number of latent variables .

Indeed , the subspace method is always at least a little more computationally expensive , though minimally so . And , as mentioned above , the paper is n’t so much about fast tricks for better training as it about teasing out subtleties of high dimensional landscapes . This latter undertaking would be worth it even if computationally inconvenient ; it just so happens that the approach ends up being computationally reasonable compared to the cost of ordinary training !


After reading the other reviews and the authors ' responses , I am satisfied that this paper is above the accept threshold . I think there are many areas of further discussion that the authors can flesh out ( as mentioned below and in other reviews ) , but overall the contribution seems solid . I also appreciate the reviewers ' efforts to run more experiments and flesh out the discussion in the revised version of the submission .

Final concluding thoughts :
-- Perhaps pi^ref was somehow better for the structured prediction problems than RL problems ?
-- Can one show regret bound for multi-deviation if one does n't have to learn x ( i.e. , we were given a good x a priori ) ?



---------------------------------------------
ORIGINAL REVIEW

First off , I think this paper is potentially above the accept threshold . The ideas presented are interesting and the results are potentially interesting as well . However , I have some reservations , a significant portion of which stem from not understanding aspects of the proposed approach and theoretical results , as outlined below .



The algorithm design and theoretical results in the appendix could be made substantially more rigorous . Specifically :

-- basic notations such as regret ( in Theorem 1 ) , the total reward ( J ) , Q-value ( Q ) , and value function ( V ) are not defined . While these concepts are fairly standard , it would be highly beneficial to define them formally .

-- I 'm not convinced that the " terms in the parentheses " ( Eq. 7 ) are " exactly the contextual bandit cost " . I would like to see a more rigorous derivation of the connection . For instance , one could imagine that the policy disadvantage should be the difference between the residual costs of the bandit algorithm and the reference policy , rather than just the residual cost of the bandit algorithm .

-- I 'm a little unclear in the proof of Theorem 1 where Q ( s , pi_n ) from Eq 7 fits into Eq 8.

-- The residual cost used for CB.update depends on estimated costs at other time steps h!=h_dev . Presumably , these estimated costs will change as learning progresses . How does one reconcile that ? I imagine that it could somehow work out using a bandit algorithm with adversarial guarantees , but I can also imagine it not working out . I would like to see a rigorous treatment of this issue .

-- It would be nice to see an end - to - end result that instantiates Theorem 1 ( and / or Theorem 2 ) with a contextual bandit algorithm to see a fully instantiated guarantee .



With regards to the algorithm design itself , I have some confusions :

-- How does one create x in practice ? I believe this is described in Appendix H , but it 's not obvious .

-- What happens if we do n't have a good way to generate x and it must be learned as well ? I 'd imagine one would need larger RNNs in that case .

-- If x is actually learned on - the-fly , how does that impact the theoretical results ?

-- I find it curious that there 's no notion of future reward learning in the learning algorithm . For instance , in Q learning , one directly models the the long - term ( discounted ) rewards during learning . In fact , the theoretical analysis talks about advantage functions as well . It would be nice to comment on this aspect at an intuitive level .



With regards to the experiments :

-- I find it very curious that the results are so negative for using only 1 - dev compared to multi-dev ( Figure 9 in Appendix ) . Given that much of the paper is devoted to 1 - dev , it 's a bit disappointing that this issue is not analyzed in more detail , and furthermore the results are mostly hidden in the appendix .

-- It 's not clear if a reference policy was used in the experiments and what value of beta was used .

-- Can the authors speculate about the difference in performance between the RL and bandit structured prediction settings ? My personal conjecture is that the bandit structured prediction settings are more easily decomposable additively , which leads to a greater advantage of the proposed approach , but I would like to hear the authors ' thoughts .



Finally , the overall presentation of this paper could be substantially improved . In addition to the above uncertainties , some more points are described below . I do n't view these points as " deal breakers " for determining accept / reject .

-- This paper uses too many examples , from part - of -speech tagging to credit assignment in determining paths . I recommend sticking to one running example , which substantially reduces context switching for the reader . In every such example , there are extraneous details are not relevant to making the point , and the reader needs to spend considerable effort figuring that out for each example used .

-- Inconsistent language . For instance , x is sometimes referred to as the " input example " , " context " and " features " .

-- At the end of page 4 , " Internally , ReslopePolicy takes a standard learning to search step . " Two issues : 1 ) Reslope Policy is not defined or referred to anywhere else . 2 ) is the remainder of that paragraph a description of a " standard learning to search step " ?

-- As mentioned before , Regret is not defined in Theorem 1 & 2 .

-- The discussion of the high - level algorithmic concepts is a bit diffuse or lacking . For instance , one key idea in the algorithmic development is that it 's sufficient to make a uniformly random deviation . Is this idea from the learning to search literature ? If so , it would be nice to highlight this in Section 2.2.



How does RESLOPE create x ?

RESLOPE learns a representation for the input x on the fly using a neural network architecture as described in Appendix H . We start off with a simple feature representation in all the problems and the model learns a better representation using a neural network architecture . We ’d appreciate any comments regarding the clarity of this section and we ’ll incorporate any suggestions in the final version .

As a recap : For English POS tagging and dependency parsing we use 300 dimensional word embeddings , 300 dimensional 1 layer LSTM , and 2 layer 300 dimensional RNN policy ; for the Chinese POS tagging : we use 300 dimensional word embeddings , 50 dimensional two layer LSTM , one layer 50 dimensional RNN policy . For reinforcement learning , we chose a two layer RNN policy with 20 dimensional vectors . We start off with a simple initial state representation and learn a better representation using the policy network . The initial state representation is task dependant . For instance , in cartpole , the state is represented by a four dimensional vector : [ position of cart , velocity of cart , angle of pole , rotation rate of pole ] .

What happens if we do n't have a good way to generate x and it must be learned as well ?

This is the case in all our experiments . We start - off with simple features and learn a better representation on the fly using a neural network architecture . For structured prediction tasks , the simple features are just the word indices in the dictionary , we learn word embedding for these words and keep track of the state using an RNN architecture ( as described above ) . For RL tasks we start off by simple features of the current state and feed these features to an RNN network to compute the final input x.

If x is learned on the fly , how does that impact the theoretical results ?

In the single deviation case , one can think of the " x " used at the deviation point as the result of applying a deep ( unrolled ) neural network to the base features ( eg word indices ) . The contextual bandit problem , then , is to learn that neural network well . This basically reduces the question to : are there good CB algorithms for learning neural networks . But the analysis for RESLOPE holds .

In the multi-deviation case , things are much more complicated . In fact , this is one of the things that blocked us from a good analysis in the multi-deviation setting . The problem is that if you deviate at steps 2 and 5 , what might be good for improving the reward prediction at step 2 could be bad for step 5 or vice versa , because these two decisions are tied through the network structure as well as the action sequence . ( This issue also arises in other learning to search algorithms , like CPI and Searn , which effectively use a sufficiently small learning rate the ensure that there 's only one deviation per episode . )


Modeling Notions of future Reward

Rather than modeling the Q-function , RESLOPE aims at modeling the advantage function instead , which could be easier to learn in several cases . Learning either the Q-function or the advantage function is sufficient for extracting a greedy policy . Lemma 1 shows that the difference in total loss between two policies can always be computed exactly as a sum of per-time - step advantages of one over the other . We chose to learn the advantages rather than Q-functions as it might be easier to learn and more local . For example , in POS tagging , learning advantages corresponds to learning whether or not the policy made a prediction mistake at a single word which is much easier to learn than the Q-function which requires keeping track of the number of mistakes made from the beginning of the sequence .

Reference Policy Used & Value for Beta

For the structured prediction experiments , the reference policy is a pre-trained model on supervised data ( Appendix G ) . The roll - out probability β is a hyper - parameter that we tune along all the other hyperparameters as described in Appendix H . We pick the best value for β from the set : { 0.0 , 0.5 , 1.0} .

For the reinforcement learning experiments , we do n’t assume access to a reference policy and the roll - out probability β is always set to zero .

Note , though , that in the multi-deviation algorithm , there is not a separate notion of a " rollout " policy , like there is in the single - deviation setting .

Difference in performance between the RL and Structured Prediction

This is a good question that unfortunately we do n't have a good answer to ; we are particularly confused by the poor performance of RESLOPE on cartpole , which is the only place where its behavior is really subpar to even simple approaches like reinforce with baseline ( reinforce without a baseline fails quite poorly here , much worse than RESLOPE ) . This could partially be because RESLOPE came out of a line of work focusing on structured prediction and so the algorithmic style simply is a better fit there , but that 's not at all a convincing answer . More work is needed here .



It ’s true that the empirical results for the one-step deviation setting is are worse ( particularly in terms of the number of samples needed to learn ) than doing multiple deviations . While we do n’t have a theoretical analysis for the multi-deviation case , empirically , we found this to be crucial empirically . Although the generated samples for the same episode are not independent , this is made - up for by the huge increase in the number of available samples for training . This is a case where there is a gap between what we can prove theoretically and what works best in practice . We can restructure the outline of the paper to promote the display of the 1 - step deviation results on earlier exposure .


The authors propose a new episodic reinforcement learning algorithm based on contextual bandit oracles .
The key specificity of this algorithm is its ability to deal with the credit assignment problem by learning automatically a progressive " reward shaping " ( the residual losses ) from a feedback that is only provided at the end of the epochs .

The paper is dense but well written .

The theoretical grounding is a bit thin or hard to follow .
The authors provide a few regret theoretical results ( that I did not check deeply ) obtained by reduction to " value -aware " contextual bandits .

The experimental section is solid . The method is evaluated on several RL environments against state of the art RL algorithms . It is also evaluated on bandit structured prediction tasks .
An interesting synthetic experiment ( Figure 4 ) is also proposed to study the ability of the algorithm to work on both decomposable and non-decomposable structured prediction tasks .


Question 1 : The credit assignment approach you propose seems way more sophisticated than eligibility traces in TD learning . But sometimes old and simple methods are not that bad . Could you develop a bit on the relation between RESLOPE and eligibility traces ?

Question 2 : RESLOPE is built upon contextual bandits which require a stationary environment . Does RESLOPE inherit from this assumption ?


Typos :
page 1
" scalar loss that output . " -> " scalar loss . "
" , effectively a representation " -> " . By effective we mean effective in term of credit assignment . "
page 5
" and MTR " -> " and DR "
page 6
" in simultaneously . " -> ???
" . In greedy " -> " . In greedy "


Question 1 : RESLOPE and Eligibility Traces

Both RESLOPE and eligibility trace algorithms tackles the problem of credit assignment when learning by interaction with the environment . In eligibility trace algorithms , e.g. TD ( λ ) , a state is eligible for credit assignment if it was recently visited , with the eligibility declining over time [ 1 ] . In our episodic setting , our notion of eligibility decay is " the end of the episode " : any reward from this episode is eligible , and reward from other episodes is not . The " degree " of eligibility is most similar to the probability of the exploration event which created the observation ( the deviation ) . This is particularly important for getting unbiased & convergent estimates .

Question 2 : RESLOPE and Non-stationary Environments

Thank you for raising this point : we were remiss to not include this in the initial draft and have now added a bit of discussion in the last section . The issue pointed out here is that because the policy is changing , the reward decomposition is changing , so the costs that the CB algorithm sees are also changing . While many CB algorithms operate effectively under shifting distributions of x ( e.g. most online CB algorithms ) , many can not work with the " label distribution " shifts . There has been some work on CB in an adversarial environment , but to our knowledge none of these algorithms is efficient . It seems likely that the RESLOPE setting is probably not as bad as full adversarial , and perhaps something could be done in the middle , but this is still an open question .

[ 1 ] Satinder P. Singh and Richard S. Sutton , Reinforcement learning with replacing eligibility traces , pp. 123–158 , Springer US , Boston , MA , 1996 .


The authors present a new RL algorithm for sparse reward tasks . The work is fairly novel in its approach , combining a learned reward estimator with a contextual bandit algorithm for exploration / exploitation . The paper was mostly clear in its exposition , however some additional information of the motivation for why the said reduction is better than simpler alternatives would help .

Pros
1 . The results on bandit structured prediction problems are pretty good
2 . The idea of a learnt credit assignment function , and using that to separate credit assignment from the exploration / exploitation tradeoff is good .

Cons :
1 . The method seems fairly more complicated than PPO / A2C , yet those methods seem to perform equally well on the RL problems ( Figure 2 . ) . It also seems to be designed only for discrete action spaces .
2 . Reslope Boltzmann performs much worse than Reslope Bootstrap , thus having a bag of policies helps . However , in the comparison in Figures 2 and 3 , the policy gradient methods dont have the advantage of using a bag of policies . A fairer comparison would be to compare with methods that use ensembles of Q-functions . ( like this https://arxiv.org/abs/1706.01502 by Chen et al . ) . The Q learning methods in general would also have better sample efficiency than the policy gradient methods .
3 . The method claims to learn an internal representation of a denser reward function for the sparse reward problem , however the experimental analysis of this is pretty limited ( Section 5.3 ) . It would be useful to do a more thorough investigation of whether it learnt a good credit assignment function in the games . One way to do this would be to check the qualitative aspects of the function in a well understood game , like Blackjack .

Suggestions :
1 . What is the advantage of the method over a simple RL method that predicts a reward at every step ( such that the dense rewards add up to match the sparse reward for the episode ) , and uses this predicted dense reward to perform RL ? This , and also a bigger discussion on prior bandit learning methods like LOLS will help under the context for why we ’re performing the reduction stated in the paper .

Significance : While the method is novel and interesting , the experimental analysis and the explanations in the paper leave it unclear as to whether its significant compared to prior work .

Revision : I thank the authors for addressing some of my concerns . The comparison with relative gain of bootstrap wrt ensemble of policies still needs more thorough experimentation , but the approach is novel and as the authors point out , does improve continually with better Contextual Bandit algorithms . I update my review to 6 .

Authors ’ Response to highlighted cons :

RESLOPE is more complicated than PPO / A2C

We suppose this depends on how " complicated " is measured . Given a known , fixed contextual bandit algorithm , RESLOPE becomes quite straightforward to implement , certainly more simple ( in lines of code ) than PPO / A2C would be if you did not have access to , for instance , an autodiff toolkit . Given good lower-level abstractions ( autodiff , CB , etc. ) , both are quote straightforward to implement . Furthermore , RESLOPE comes with significant advantage over PPO / A2C : RESLOPE continually improves as better contextual bandit algorithms become available , a property lacked by PPO / A2C . RESLOPE also fares well empirically , and comes with some nice theoretical guarantees ( which , for instance , A2C lacks ) .


RESLOPE and Continuous Action Spaces

It ’s true that RESLOPE is designed for discrete action spaces . Extension to continuous action spaces remains an open problem . We have updated the discussion section ( section 6 ) to include this extension as a future work .

Comparison with Ensemble Learning Methods

This is a great question , thank you for raising it ! Indeed , the original submission did not adequately separate gains from more complex representation ( bag of policies ) and alternative estimation methods ( bootstrap ) .

To address this , we have done an addition set of experiments to answer the following question empirically : what is the relative gain of bootstrap exploration with respect to using an ensemble of policies .

Ensemble exploration trains a bag of multiple policies simultaneously . Each policy in the bag generates a Boltzmann probability distribution over actions . These probability distributions are aggregated by averaging . An action is sampled from the aggregated distribution . This has the property of identical network representation , but not using the Bootstrap estimation method .

The result is that in the MTR setting , the Ensemble method is worse than Bootstrap by factors of 3.52 , 0.757 and 0.815 respectively on the first three RL tests and , surprisingly , better by a factor of 6.39 on the last . We plan to complete these experiments with more rigor , and extend to the SP setting , in a final version .

Evaluating the learned loss representation for a well - understood RL Environment

We added additional experiments for evaluating the learned loss representation in the grid world reinforcement
learning environment ( Appendix K ) . This experiment is akin to the experimental setting in section 5.3 , however
it ’s performed on the grid world RL environment , where the quantitative aspects of the loss function is well understood . Results are very similar to the structured prediction setting ( section 5.3 ) . Performance is better when the loss is additive vs non-additive .

Authors ’ Response to proposed suggestions :

RESLOPE & Reward Prediction at Every Step

We ’re not aware of a different way for learning the reward in every time step without computing the residual loss as we do in RESLOPE . After estimating the residual losses , RESLOPE reduces the problem to a contextual bandit oracle . This is crucial for accounting for the exploration probability and is necessary for obtaining an unbiased and convergent estimates for the loss . It ’s not clear how standard RL can account for the exploration probability when the estimated rewards is used instead of the true reward values , and thus , we did n’t consider this approach in our experiments . ( But we 're open to suggestions ! )

RESLOPE vs LOLS

Both RESLOPE and the bandit version of LOLS ( Chang et al. , 2015 ) aim to learn from sparse reward signals by building on the bandit learning to search frameworks . As highlighted in the discussion section ( Section 6 ) , they differ significantly in both theory and practice :
The “ bandit ” version of LOLS was analyzed theoretically but not empirically in the original paper ; Sharaf & Daumé ( 2017 ) found that it failed to learn empirically ;
RESLOPE learns a representation for the episodic loss as a decomposition over time - steps , while LOLS learns directly from the episodic loss signal , this is prone to high variance and does n’t work in practice ( Sharaf & Daumé 2017 ) ;
RESLOPE separates the problem of credit assignment from the exploration problem via a reduction to a contextual bandit oracle . This enables the usage of better variance reduction techniques ( e.g. Doubly Robust cost estimation & Multi-task Regression ) as well as different exploration algorithms ( e.g. bootstrap exploration ) . LOLS can only use Inverse Propensity Scoring and greedy exploration .

[ 1 ] Kai- Wei Chang , Akshay Krishnamurthy , Alekh Agarwal , Hal Daume ́ , III , and John Langford . Learning to search better than your teacher . In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37 , ICML ’15 , pp. 2058–2066 . JMLR.org , 2015 . URL http://dl.acm.org/citation.cfm?id=3045118.3045337.
[ 2 ] Amr Sharaf and Hal Daume ́ , III . Structured prediction via learning to search under bandit feedback . In Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing , pp. 17–26 , Copenhagen , Denmark , September 2017 . Association for Computational Linguistics . URL http://www.aclweb.org/anthology/W17-4304.
[ 3 ] Miroslav Dud ́ık , Dumitru Erhan , John Langford , and Lihong Li. Doubly robust policy evaluation and optimization . Statist . Sci. , 29 ( 4 ) :485–511 , 11 2014 . doi : 10.1214/14-STS500 . URL https : //doi.org/10.1214/14-STS500 .



The analyses of this paper ( 1 ) increasing the feature norm of correctly - classified examples induce smaller training loss , ( 2 ) increasing the feature norm of mis-classified examples upweight the contribution from hard examples , are interesting . The reciprocal norm loss seems to be reasonable idea to improve the CNN learning based on the analyses .

However , the presentation of this paper need to be largely improved . For example , Figure 3 seems to be not relevant to Property 2 and may be show the feature norm is lower when the samples is hard example . Therefore , the author used reciprocal norm loss which increases feature norm as shown in Figure 4 . However , both Figures are not explained in the main text , and thus hard to understand the relation of Figure 3 and 4 . The author should refer all Figures and Tables .

Other issues are :
- Large -margin Soft max in Figure 2 is not explained in the introduction section .
- In Eq. ( 7 ) , P_j^ I is not defined .
- In the Property3 , The author wrote “ where r is lower bound of feature norm ” .
However , r is not used .
- In the experimental results , “ RN ” is not defined .
- In the Table3 , the order of \lambda should be increasing or decreasing order .
- Table 5 is not referred in the main text .

== Updated review ==
The presentation has been improved , I have increased the rate from 5 to 6 .
Following are further comments for presentation .

- Fig.2 “ the increasing L2 norm “ seems to “ the order of L2 norm ”
- Pp.4 the first sentence above Eq. ( 7 ) “ According to definition … ” should be improved .
- pp.5 , the first sentence of the second paragraph “ The feature norm can be optimized .. ” is not clear .
- It would be better put Figure 5 under Property 3 .
- D should be defined in Property 3 .
- pp.8 wrote “ However , 259 - misclassfied examples are further introduced ” . However , in Table 5 , it seems to be 261 .
- Section 5. is “ Conclusion and future work ” . However , future work is not mentioned .


Thanks for your comments .

- 1 - The presentation of this paper need to be largely improved .
We have improved the presentation of our paper and updated the pdf files according to your advice .

- 2 - Figure 3 seems to be not relevant to Property 2 and may be show the feature norm is lower when the samples is hard example .
Actually , Figure 3 is relevant to Property 2 . We revised the description and re-plot Figure 3 in the paper to make their relation much clearer and avoid the possible misunderstanding .

We provide a short explanation below . The purpose of Figure 3 is to show that the mis-classified examples ( we can also call them " hard examples " ) tend to be of small feature norm , which has been re-plot based on your advice . Property 2 is proposed to state that we need to increase the feature norm of mis-classified examples ( tend to with small feature norm ) , which makes larger gradient and helps correcting the mis-classified examples .
Especially , the fifth column in Table 5 shows that by increasing the feature norm of mis-classified examples , the " RN + Softmax " correctly classifies 336 examples that are mis-classified by " Softmax " .

- 3 - The author used reciprocal norm loss which increases feature norm as shown in Figure 4 . However , both Figures are not explained in the main text , and thus hard to understand the relation of Figure 3 and 4 .
Thanks for pointing out this problem . We now added the explanations in the main text . Figure 4 is used to show the Reciprocal Norm Loss can result in more intra-class compactness by increasing the small feature norm faster than the large ones . Figure 3 is not related to Figure 4 , and it is about Property 2 .

- 4 - Large -margin Soft max in Figure 2 is not explained in the introduction section .
Thanks for pointing out this problem . We provided the explanation of Large-margin Softmax loss in the first paragraph of Section 2 ( Related work ) . We will put it to the introduction section if it is necessary .

- 5 - In Eq. ( 7 ) , P_j^ I is not defined .
We have added the definition of P_j^i in Eq. ( 7 ) in the updated paper . In fact , we have also defined P_j^i in Property 4 .

- 6 - In the Property 3 , The author wrote “ where r is lower bound of feature norm ” . However , r is not used .
Thanks for pointing out this problem , which is a typo . " r " should be replaced with " alpha " .

- 7 - In the experimental results , “ RN ” is not defined .
" RN " refers to the feature incay with form of Reciprocal Norm . We have added that " ... RN ( Reciprocal Norm loss ) plus the baseline method . e.g. , RN + Softmax means combining the feature incay with Softmax loss . " in the updated paper .

- 8 - In the Table 3 , the order of \lambda should be increasing or decreasing order .
We have resorted it in decreasing order .

- 9 - Table 5 is not referred in the main text .
Thanks for pointing out this problem . We have discussed the results in Table 5 in the first paragraph in Section 4.5 and added the reference to Table 5 in the updated paper .


Pros :
1 . It provided theoretic analysis why larger feature norm is preferred in feature representation learning .

2 . A new regularization method ( feature incay ) is proposed .

Cons :
It seems there is not much comparison between this proposed method and the concurrent work " COCO ( Liu et al . ( 2017 c ) ) " .

Thanks a lot for your positive and constructive comments !

We provide the response to " It seems there is not much comparison between this proposed method and the concurrent work ' COCO ( Liu et al . ( 2017 c ) ) ' . "

( 1 ) Both " COCO " and " Feature Incay " increase the L2 - norm of feature representations , which is the common reason for performance improvement .

( 2 ) There are several clear differences .
a. " COCO " normalizes and rescales all features to have the same L2 - norm while " Feature Incay " adds a new regularizer that prefers features with larger L2 -norm . " COCO " uses the optimal scale value that is fixed during training while " Feature Incay " increases the feature norm without constraining the scale value .
b . “ COCO ” optimizes feature embedding spreading on a hypersphere while “ Feature Incay ” optimizes feature embedding located between two hyperspheres with different radiuses . ( see Property 3 )
c. " COCO " proposes a novel congenerous cosine loss while " Feature Incay " uses the original softmax loss : " Feature Incay " is simpler than " COCO " and it can be easily plugged into almost all the related works that use softmax loss .

( 3 ) We compare the " COCO " with " RN + COCO " on CASIA - WebFace with SphereNet - 20 and find that " Feature Incay " can help improve the performance of " COCO " . e.g. , " RN + COCO " improves " COCO " from 98.90 % to 99.02 % .



The manuscript proposes to increase the norm of the last hidden layer to promote better classification accuracy . However , the motivation is a bit less convincing . Here are a few motivations that are mentioned .
( 1 ) Increasing the feature norm of correctly classified examples helps cross entropy , which is of course correct . However , it only decreases the training loss . How do we know it will not lead to overfitting ?
( 2 ) Increasing the feature norm of mis-classified examples will make gradient larger for self - correction . And the manuscript proves it in property 2 . However , the proof seems not complete . In Eq ( 7 ) , increasing the feature norm would also affect the value of the term in parenthesis . As an example , if a negative example is already mis-classified as a positive , and its current probability is very close to 1 , then further increasing feature norm would make the probability even closer to 1 , leading to saturation and smaller gradient .
( 3 ) Figure 1 shows that examples with larger feature norm tend to be predicted well . However , it is not very convincing since it is only a correlation rather than causality . Let 's use simple linear softmax regression as a sanity check , where features to softmax are real features rather than hidden units . Increasing the feature norm seems to be against the best practice of feature normalization in which each feature after normalization is of variance 1 .

The manuscript states that the feature norm wo n't be infinitely increased since there is an upper bound . However , the proof of property 3 seems to only apply to the certain cases where K < 2D . In addition , alpha is in the formula of upper bound , but what is the upper bound of alpha ?

The manuscript does comprehensive experiments to test the proposed method . The results are good , since the proposed method outperforms other baselines in most datasets . But the results are not impressively strong .

Minor issues :
( 1 ) For proof of property 3 , it seems that alpha and beta are used before defined . Are they the radius of two circles ?

Thanks a lot for your insightful comments .

- 1 - Increasing the feature norm of correctly classified examples helps cross entropy , which is of course correct . However , it only decreases the training loss . How do we know it will not lead to overfitting ?

Good question . In our experiments , we do n't find the feature incay will lead to overfitting . e.g. , by considering feature incay , RN + Softmax decreases the training loss and improves the Softmax from 91.41 % to 92.16 % on the test set of CIFAR10 . It remains an open problem to provide theoretical analysis about whether increasing the feature norm will lead to overfitting currently .


- 2 - Increasing the feature norm of mis-classified examples will make gradient larger for self - correction . And the manuscript proves it in property 2 . However , the proof seems not complete . In Eq ( 7 ) , increasing the feature norm would also affect the value of the term in parenthesis . As an example , if a negative example is already mis-classified as a positive , and its current probability is very close to 1 , then further increasing feature norm would make the probability even closer to 1 , leading to saturation and smaller gradient .

Thanks for pointing this problem . The proof of property 2 is indeed complete . In your described case , increasing the feature norm will not lead to smaller gradients for both the weight vectors of the ground truth category and the wrongly predict category , which instead will have larger gradients .

We give the reasons below . For a mis-classified sample i with ground truth label y_i . It is true that " When the mis-classified f_i has probability of class k( k!=y_i ) close to 1 , then increase the feature norm of f_i will make the probability of class k even closer to 1 " , but this will not cause " saturation and smaller gradient " for all w_k and w_ ( y_i ) . According to Equation ( 7 ) :

( 1 ) the gradient of w_ ( y _i ) : when j=y_i , h( i ) =1 , P_j^i is close to 0 , then ( P_j^i-h ( i ) ) is close to - 1 , so the gradients for the weight vector of ground truth category can be increased by increasing the norm of f_i ;
( 2 ) the gradient of w_ ( k ) : when j=k , h( i ) =0 , as that P_k^i is close to 1 , ( P_k^i-h ( i ) ) is close 1 , the gradients for weight vector of the wrongly predict category can be increased by increasing the norm of f_i .
( 3 ) the gradients of other w_j : when j!=y_i && j!=k , ( P_j^i-h ( i ) ) is close to 0 , thus the gradients is close to zero .



- 3 - Figure 1 shows that examples with larger feature norm tend to be predicted well . However , it is not very convincing since it is only a correlation rather than causality . Let 's use simple linear softmax regression as a sanity check , where features to softmax are real features rather than hidden units . Increasing the feature norm seems to be against the best practice of feature normalization in which each feature after normalization is of variance 1 .

Thanks for pointing out this interesting problem . As we observe that the feature norm and the classification accuracy is positively related , and we investigate whether increasing the feature norm explicitly could improve the performance and find that the classification accuracy is improved with the feature incay . It also remains an open problem to provide theoretical analysis about whether it is correlation or causality currently .
Increasing the feature norm is not against the best practice of feature normalization . In fact , increasing the feature norm before normalization can also help improve the final performance , which is shown in Table 1 and stated in the last sentence of Section 4.2 . ( " feature incay can even promote the A-softmax with normalized features by elongating the features before normalization " )



- 4 - The manuscript states that the feature norm wo n't be infinitely increased since there is an upper bound . However , the proof of property 3 seems to only apply to the certain cases where K < 2D . In addition , alpha is in the formula of upper bound , but what is the upper bound of alpha ?

Thanks for pointing out this issue . Our property essentially is not limited to K < 2D . We updated Property 3 for both K < 2D and K>=2D case :
" ... ( 2 ) to ensure the maximal intra-class distance is smaller than the minimal inter-class distance , the upper bound of feature norm is 3 * alpha , especially when K < 2D , the upper bound in a tighter range of [ ( 1 + sqrt ( 2 ) ) * alpha , 3 * alpha ] " . So 3 * alpha is a general upper bound whether K <2D or K>=2D . Especially , when K < 2D , we can formulate a tighter range for the upper bound .

What 's the upper bound of alpha is an interesting problem , but it is not our current interest . The main point of Property 3 lies in that the ratio of the upper bound beta to the lower bound alpha is bounded : beta / alpha <= 3.

- 5 - For proof of property 3 , it seems that alpha and beta are used before defined . Are they the radius of two circles ?

Yes , they are the radius of the two circles .

The authors propose an approach for zero - shot visual learning . The robot learns inverse and forward models through autonomous exploration . The robot then uses the learned parametric skill functions to reach goal states ( images ) provided by the demonstrator . The “ zero-shot ” refers to the fact that all learning is performed before the human defines the task . The proposed method was evaluated on a mobile indoor navigation task and a knot tying task .

The proposed approach is well founded and the experimental evaluations are promising . The paper is well written and easy to follow .

I was expecting the authors to mention “ goal emulation ” and “ distal teacher learning ” in their related work . These topics seem sufficiently related to the proposed approach that the authors should include them in their related work section , and explain the similarities and differences .

Learning both inverse and forward models is very effective . How well does the framework scale to more complex scenarios , e.g. , multiple types of manipulation together ? Do you have any intuition for what kind of features or information the networks are capturing ? For the mobile robot , is the robot learning some form of traversability affordances , e.g. , recognizing actions for crossings , corners , and obstacles ? The authors should consider a test where the robot remains stationary with a fixed goal , but obstacles are move around it to see how it affects the selected action distributions .

How much can change between the goal images and the environment before the system fails ? In the videos , it seems that the people and chairs are always in the same place . I could imagine a network learning to ignore features of objects that tend to wander over time . The authors should consider exploring and discussing the effects of adding / moving / removing objects on the performance .

I am very happy to see experimental evaluations on real robots , and even in two different application domains . Including videos of failure cases is also appreciated . The evaluation with the sequence of checkpoints was created by using every fifth image . How does the performance change with the number of frames between checkpoints ? In the videos , it seems like the robot could get a slightly better view if it took another couple of steps . I assume this is an artifact of the way the goal recognizer is trained . For the videos , it may be useful to indicate when the goal is detected , and then let it run a couple more steps and stop for a second . It is difficult to compare the goal image and the video otherwise .

We are glad that the reviewer found the proposed approach well founded and the evaluations promising . We thank the reviewer for the constructive feedback and address the concerns in detail below .

R3 : " ... mention goal emulation and distal teacher learning in their related work "
= > We thank the reviewer for bringing this to our notice . We will include these in the related work and highlight the differences .

R3 : " How well does the framework scale ... any intuition for what kind of features or information the networks are capturing ? "
= > One interesting insight from the training process is that forward consistency loss improves the accuracy of inverse model even if the predicted image quality of forward model is not pixel accurate . One explanation is that the forward model is not required to be pixel - accurate till the time gradients are regularized , which leads to better performance .

Regarding what information is being captured -- intuitively inverse models only represent information that is required to predict the action . We will include nearest neighbor visualization in the final revision of the paper to provide further insights into the nature of the learned representations .

As far as complex manipulation tasks are concerned , we have shown results for manipulating a rope into ‘ S’ shape and tying into a knot . For more complex tasks and scaling to high dimensional spaces , we believe that instead of random exploration , more structured intrinsic motivation driven exploration , e.g. pseudo-count ( Bellemare et al , 2016 ) or learning progress ( Schmidhuber , 1991 ) or curiosity ( Pathak et al , 2017 ) will play key role . We discuss this briefly in the Section - 5 of the paper and will elaborate in more detail in the next revision .

R3 : " ... authors should consider a test where the robot remains stationary with a fixed goal , but obstacles are move around ... discussing the effects ... on the performance "
= > Thank you for the suggestion . This is an interesting experiment and we will look into it . In our preliminary experiments , we found that the turtlebot is robust to dynamic obstacles as long as goal image does not change significantly .

R3 : " ... the performance change with the number of frames between checkpoints ? "
= > Generally speaking , as the number of frames between checkpoints increases , the performance deteriorates , but quite gracefully . To quantify this effect , we conducted an experiment where in the navigation setup the robot was only shown the goal image ( ~approximately 15 - 25 steps away from the current image on an optimal route ) . In this scenario , our robot exhibited goal searching behavior until the goal comes in field of view , followed by goal directed navigation . Our method significantly outperformed other baselines as reported in Table - 1 .

R3 : " ... it seems like the robot could get a slightly better view if it took another couple of steps ... useful to indicate when the goal is detected in video "
= > Yes , it is indeed the artifact of goal recognizer training because it was trained with some stochasticity around the arbitrarily selected states from collected exploration data .

Thanks for the suggestions on making the result videos more comprehensible on how to analyze goal recognizer stop model . We will include qualitative analysis of the goal recognizer stop model in the supplementary materials of our final revision .

Summary :
The authors present a paper about imitation of a task presented just during inference , where the learning is performed in a completely self - supervised manner .
During training , the agent explores by itself related ( but different ) tasks , learning a ) how actions affect the world state , b ) which action to perform given the previous action and the world state , and c ) when to stop performing actions . This learning is done without any supervision , with a loss that tries to predict actions which result in the state achieved through self exploration ( forward consistency loss ) .
During testing , the robot is presented with a sequence of goals in a related but different task . Experiments show that the system achieves a better performance than different subparts of the system ( through an ablation study ) , state of the art and common open source systems .

Positive aspects :
The paper is well written and clear to understand . Since this is not my main area of research I can not judge its originality in a completely fair way , but it is original AFAIK . The idea of learning the basic relations between actions and state through self exploration is definitely interesting .
This line of work is specially relevant since it attacks one of the main bottlenecks in learning complex tasks , which is the amount of supervised examples .
The experiments show clearly that a ) the components of the proposed pipeline are important since they outperform ablated versions of it and b) the system is better than previous work in those tasks

Negative aspects :
My main criticism to the paper is that the task learning achieved through self exploration seems relatively shallow . From the navigation task , it seems like the system mainly learns a discover behavior that is better than random motion . It definitely does not seem able to learn higher level concepts like certain scenes being more likely to be close to each other than others ( e.g. it is likely to find an oven in the same room as a kitchen sink but not in a toilet ) . It is not clear whether this is achievable by the current system even with more training data .
Another aspect that worries me about the system is how it can be extended to higher dimensional action spaces . Extending control laws through self - exploration under random disturbances has been studied in character control ( e.g. " Domain of Attraction Expansion for Physics -based Character Control " by Borno et al. ) , but the dimensionality of the problem makes this exploration very expensive ( even for short time frames , and even in simulation ) . I wonder if the presented ideas wo n't suffer from the same curse of dimensionality .
In terms of experiments , it is shown that the system is more effective than others but not so much * how * it achieves this efficiency . It would be good to show whether part of its efficiency comes from effective image - guided navigation : does a partial image match entail with targetted navigation ( e.g. matches in the right side of the image make the robot turn right ) ?
A couple more specific comments :
- I think that dealing with multimodal distributions of actions with the forward consistency loss is effective for achieving the goal , but not necessarily good for modeling multimodality . Is n't it possible that the agent learns only one way of achieving such goal ?
- It is not clear how the authors achieve to avoid the problem of starting from scratch by " pre-train the forward model and PSF separately by blocking gradient flow " . Is n't it still challenging to update them independently , given that at the beginning both components are probably not very accurate ?


Conclusion :
I think the paper presents an interesting idea which should be exposed to the community . The paper is easy to read and its experiments show the effectiveness of the method . The relevance of the method to achieve a deeper sense of learning and performing more complex tasks is however unclear to me .

We thank the reviewer for the constructive feedback and are glad that the reviewer found the idea original and general direction as specially relevant and interesting . We address the concerns in detail below .

R2 : " ... learns a discover behavior that is better than random motion ... not clear whether this is achievable by the current system even with more training data . "
= > In the current setup of navigation , the exploration is random , and the robot learns the skills of avoiding walls , moving in free spaces and turning around to find the target image , etc . Note that we report performance in a setup when the robot is dropped in entirely new environments , showing that the learned skills generalize . In essence , what we have shown is that is possible to distill exploration data into generalizable skills that can be used to reach target images .

The complexity of the skills learned by our robot inevitably depends on the interaction data it collects via its exploration . We agree that random exploration is insufficient for learning more higher - level concepts . There are many works in the literature , such as pseudo-count ( Bellemare et al , 2016 ) or learning progress ( Schmidhuber , 1991 ) or curiosity ( Pathak et al , 2017 ) , that have proposed efficient exploration schemes . In the future , we plan to incorporate these exploration policies to collect data and train PSF ( parameterized skill function ) with the help of this data . The intuition is that with a good exploration policy , the kitchen sink and the oven will be closer to each other as compared to the toilet in robot ’s roll outs . The PSF learned using this roll out data is therefore likely to learn these relationships implicitly . Our goal in this work is not to propose an exploration policy , but a mechanism that can make use of exploration data to learn a skill function to achieve desired goals . Our initial experiments in simulation suggest that performance of goal reaching improves when data is collected using a non-random exploration policy .

R2 : " ... how it can be extended to higher dimensional action spaces ... curse of dimensionality . "
= > This is a great point . In our opinion there are two mechanisms : ( a ) discovering a low - dimensional embedding of the action space ( say using motor babbling ) and controlling in this space ; ( b ) a more general mechanism is to make use of structured exploration mechanisms that has a rich literature . Some works incentivize the agent to visit previously unseen states , e.g. , pseudo-count ( Bellemare et al , 2016 ) , other incentivize the agent to take actions that lead to high - prediction error ( Pathak et al , 2017 ) or measure learning progress ( Schmidhuber , 1991 ) . Ours proposed forward consistent way of learning PSF is agnostic to how exploration data is collected . A PSF learned over the data obtained using any of these structured exploration mechanisms has the potential to scale to high dimensional spaces . We acknowledge this issue in the Section - 5 of paper and will make it clearer in the final revision .

R2 : " ... it is shown that the system is more effective than others but not so much * how * it achieves this efficiency ... partial image match "
= > There are two perspectives : ( a ) the quantitative view that can be used to systematically ablate the model to understand what components of the formulation are most critical and ( b ) a more qualitative and intuitive perspective , as suggested by you , that analyzes whether our robot learns to make a right / left turns more accurately when trained with forward consistency loss .

For ( a ) , as an ablation , we trained the inverse model with the forward loss as a regularizer . In this forward regularizer setup , the forward model and inverse model are trained jointly and share the visual features . The difference from our proposed approach is that inverse model is trained with action prediction loss and does not receive gradients through the forward model . In case of knot tying task , the forward regularizer achieves 44 % accuracy which is above the baseline ( 36 % ) but well below our proposed approach ( 60 % ) . This result shows that the forward model is not merely acting as a regularizer , but optimizing the inverse model through the forward model is potentially critical to addressing the multi-modality issue . We will add these numbers in the final version of the paper .

For ( b ) , when images have little overlap we found that classical solutions based on SIFT failed due to lack of keypoint matches . However , the inverse model and our approach are able to make left and right turns depending on whether the right or left part of the current image is visible in the goal image . We will include quantitative evaluation for this analysis in the final version of the paper .


R2 : " ... forward consistency loss is effective for achieving the goal , but not necessarily good for modeling multimodality ... learns only one way of achieving such goal ? "
= > Yes , you are correct . We do - not directly model the multimodal distribution of the action , but we address the instability issues of gradient based learning due to multimodality . In an attempt to match the multiple ground - truth targets for the same input , the predictions will oscillate , which in turn will make the gradient of the loss function with respect to neural network parameters also oscillate . The purpose of forward consistency loss is to mitigate gradient oscillation , i.e. -- it stabilizes the learning process by ensuring that network is not penalized for outputting a different action than ground truth as long as its predicted action has the same effect as ground truth one .

Learning all possible ways of achieving a goal is slightly different question . In theory , it could be dealt by incorporating a stochastic sampling layer in the neural network in addition to forward consistency and is an interesting direction for future research .

R2 : " ... how the authors avoid the problem of starting from scratch by pre-train the forward model and PSF separately ... Is n't it still challenging "
= > Training the PSF through forward consistency loss is a challenging problem because the learning of inverse model PSF depends on how good the forward model is . This learned forward model will not be very accurate in the beginning of training , thus making the gradients noisy . Therefore , we first pretrained inverse and forward model independently until convergence , and then fine - tune them jointly with consistency loss . Empirically , we found that such a pre-training followed by joint fine - tuning to be more stable than joint fine- tuning from scratch .

One of the main problems with imitation learning in general is the expense of expert demonstration . The authors here propose a method for sidestepping this issue by using the random exploration of an agent to learn generalizable skills which can then be applied without any specific pretraining on any new task .

The proposed method has at its core a method for learning a parametric skill function ( PSF ) that takes as input a description of the initial state , goal state , parameters of the skill and outputs a sequence of actions ( could be of varying length ) which take the agent from initial state to goal state .

The skill function uses a RNN as function approximator and minimizes the sum of two losses i.e. the state mismatch loss over the trajectory ( using an explicitly learnt forward model ) and the action mismatch loss ( using a model - free action prediction module ) . This is hard to do in practice due to jointly learning both the forward model as well as the state mismatches . So first they are separately learnt and then fine- tuned together .

In order to decide when to stop , an independent goal detector is trained which was found to be better than adding a ' goal - reached ' action to the PSF .

Experiments on two domains are presented . 1 . Visual navigation where images of start and goal states are given as input . 2 . Robotic knot -tying with a loose rope where visual input of the initial and final rope states are given as input .

Comments :

- In the visual navigation task no numbers are presented on the comparison to slam - based techniques used as baselines although it is mentioned that it will be revisited .

- In the rope knot -tying task no slam - based or other classical baselines are mentioned .

- My main concern is that I am really trying to place this paper with respect to doing reinforcement learning first ( either in simulation or in the real world itself , on -policy or off - policy ) and then just using the learnt policy on test tasks . Or in other words why should we call this zero-shot imitation instead of simply reinforcement learnt policy being learnt and then used . The nice part of doing RL is that it provides ways of actively controlling the exploration . See this pretty relevant paper which attempts the same task and also claims to have the target state generalization ability .

Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning by Zhu et al .

I am genuinely curious and would love the authors ' comments on this . It should help make it clearer in the paper as well .

Update :

After evaluating the response from the authors and ensuing discussion as well as the other reviews and their corresponding discussion , I am revising my rating for this paper up . This will be an interesting paper to have at the conference and will spur more ideas and follow - on work .

We thank you for the constructive feedback and address the concerns in detail below .

R1 : " In visual navigation ... no numbers for slam - based techniques ... will be revisited . "
= > When the imitator is shown a dense demonstration sequence ( i.e. , every frame ) , it is possible to use SIFT features to estimate the odometry and guide the robot . However , in the more interesting scenario of the imitator being shown a sparser demonstration ( i.e. , every 5th frame ) , SIFT matching fails . The major reason for failure is the wide baseline , which on many occasions leads to little overlap between the current image and the next image in the demonstration . We will add SIFT numbers to the final revision .

We also tried state - of - the- art open source methods : OpenSFM and ORB SLAM2 . These methods were unable to generate the map with every 5th image of a demonstration sequence . The other possibility was to let the robot explore randomly and build a map . However , with random exploration the robot tends to wander off and is not focussed on constructing a map of the part of the environment from where the demonstration sequences were taken . This leads to failure in following the demonstration .

R1 : " In rope knot - tying task , no other classical baselines are mentioned "
= > Thanks for bringing this point up . The analog of SLAM in rope manipulation is to perform non- linear alignment of rope between the current and target image and use this alignment to select the action . TPS - RPM is a well - known method to align deformable objects such as ropes and is described in Section - 4.2 . TPS - RPM based method was compared in Nair et al . in which their inverse model outperformed this classical baseline by a significant margin . Under the same setup and data provided by Nair et al. , our forward consistency based imitator significantly outperforms the model proposed in Nair et al . This directly implies that our method outperforms this classical baseline .

R1 : " ... place this paper with respect to doing reinforcement learning first ... controlling the exploration . "
= > This is a very relevant question . Our overarching aim is to enable robotic systems to perform complex new tasks from raw visual inputs . Instead of training our robot to perform only one task , we would like to provide the goal task description ( i.e. , an image depicting the goal ) as input to the robot ’s policy .

The most general way would be to learn a policy ( say using reinforcement learning ) that takes as input the images showing the current and goal state and outputs a sequence of actions to reach the goal . There are some major concerns with using RL in the real world : ( a ) measuring rewards is non-trivial . For e.g. , in order to train the robot to configure the rope in shape S or knot , one would need to build classifiers that detect these shapes and use the binary output of classifier as the reward . However , these classifiers will inevitably be imperfect , which in turn would lead to noisy reward . More critically , in order for the system to generalize to novel goals , one would have to train the system with many goals . This implies that large amounts of human supervision would be required to train these classifiers . ( b) RL typically requires ~ 10 - 100 million interactions to learn from visual inputs for all but the simplest of tasks , which is simply infeasible in the real world .

In our paper , we propose to learn such a policy using supervised learning . The agent explores its environment and generates interaction data in the form of pair of states and sequence of actions the agent executed . This action sequence provides supervision to learn the policy . One major issue in training such a model through supervised learning is that multiple actions can help the agent transition from current to goal state . We resolve this issue by proposing the forward consistency loss . Our method is sample efficient ( ~60 K interactions for rope manipulation , ~ 200 K for navigation ) and does not rely on environmental rewards .

The number of samples required to learn such a policy grows with the number of actions needed to reach goal . To perform complex tasks , we trade off this difficulty by using subgoals in the form of a sequence of images provided by an expert . In other words , we learn a low - level policy ( i.e. PSF ) which is accurate for predicting actions when current and goal state are not that far apart , and the expert demonstration provides a high - level plan to goto far away goal states .

R1 : " pretty relevant paper which attempts the same task ... by Zhu et al . "
= > This paper uses RL using multiple goals . One major issue with RL based methods in real world is their bad sample efficiency . Adding multiple goals to the same policy usually hurts the sample efficiency even more , making it generally impractical to train on real robots . For e.g. , the above paper trained using RL for 100M samples in simulation for 10 - 20 steps away goals . However , this is a relevant citation and we will include it in the final revision .

The setup in the paper for learning representations is different to many other approaches in the area , using to agents that communicate over descriptions of objects using different modalities . The experimental setup is interesting in that it allows comparing approaches in learning an effective representation . The paper does mention the agents will be available , but leaves open wether the dataset will be also available . For reproducibility and comparisons , this availability would be essential .

I like that the paper gives a bit of context , but presentation of results could be clearer , and I am missing some more explicit information on training and results ( eg how long / how many training examples , how many testing , classification rates , etc ) .
The paper says is the training procedure is described in Appendix A , but as far as I see that contains the table of notations .




Thank you for your thoughtful comments .

> The paper does mention the agents will be available , but leaves open whether the dataset will be also available .

You bring up a great point that in order to reproduce our results it would be necessary to have access to a similar dataset . In addition , even with written details of the implementation , it can be difficult to reproduce experiments . For these reasons , we ’ve prepared to release the code and instructions on how to build the dataset , and will include a link in the de -anonymized version of the paper . We allude to this in section 5.2 under Code .

> missing some more explicit information on training and results

And

> The paper says the training procedure is described in Appendix A

We also thank the reviewer for pointing out the typo in relation to Appendix A . In terms of training and results , a plot of the classification accuracy by epoch is shown in the updated Figure 6 . We added the following details in sections 5.1 and 5.2 that should clear up confusion about the training procedure :

The number of images per class in the out -of-domain test set is 100 images per class ( for 10 classes in total ) .
We use early stopping with a maximum 500 training epochs .
We train on a single GPU ( Nvidia Titan X Pascal ) , and a single experiment takes roughly 8 hours for 500 epochs .

Is the addition of these details sufficient ?


--------------
Summary and Evaluation :
--------------

The paper presents a nice set of experiments on language emergence in a mutli-modal , multi-step setting . The multi-modal reference game provides an interesting setting for communication , with agents learning to map descriptions to images . The receiving agent 's direct control over dialog length is also novel and allows for the interesting analysis presented in later sections .

Overall I think this is an interesting and well - designed work ; however , some details are missing that I think would make for a stronger submission ( see weaknesses ) .


--------------
Strengths :
--------------
- Generally well - written with the Results and Analysis section appearing especially thought - out and nicely presented .

- The proposed reference game provides a number of novel contributions -- giving the agents control over dialog length , providing both agents with the same vocabulary without constraints on how each uses it ( implicit through pretraining or explicit in the structure / loss ) , and introducing an asymmetric multi-modal context for the dialog .

- The analysis is extensive and well - grounded in the three key hypothesis presented at the beginning of Section 6 .

--------------
Weaknesses :
--------------

- There is room to improve the clarity of Sections 3 and 4 and I encourage the authors to revisit these sections . Some specific suggestions that might help :
- numbering all display style equations
- when describing the recurrent receiver , explain the case where it terminates ( s^t =1 ) first such that P ( o_r=1 ) is defined prior to being used in the message generation equation .

- I did not see an argument in support of the accuracy @ K metric . Why is putting the ground truth in the top 10 % the appropriate metric in this setting ? Is it to enable comparison between the in -domain , out -domain , and transfer settings ?

- Unless I missed something , the transfer test set results only comes up once in the context of attention methods and are not mentioned elsewhere . Why is this ? It seems appropriate to include in Figure 5 if no where else in the analysis .

- Do the authors have a sense for how sensitive these results are to different runs of the training process ?

- I did not understand this line from Section 5.1 : " and discarding any image with a category beyond the 398 - th most frequent one , as classified by a pretrained ImageNet classifier ' "

- It is not specified ( or I missed it ) whether the F1 scores from the separate classifier are from training or test set evaluations .

- I would have liked to see analysis on the training process such as a plot of reward ( or baseline adjusted reward ) over training iterations .

- I encourage authors to see the EMNLP 2017 paper " Natural Language Does Not Emerge ‘ Naturally ’ in Multi-Agent Dialog " which also perform multi-round dialogs between two agents . Like this work , the authors also proposed removing memory from one of the agents as a means to avoid learning degenerate ' non - dialog ' protocols .

- Very minor point : the use of fixed - length , non -sequence style utterances is somewhat disappointing given the other steps made in the paper to make the reference game more ' human like ' such as early termination , shared vocabularies , and unconstrained utterance types . I understand however that this is left as future work .


--------------
Curiosities :
--------------
- I think the analysis is Figure 3 b ,c is interesting and wonder if something similar can be computed over all examples . One option would be to plot accuracy@k for different utterance indexes -- essentially forcing the model to make a prediction after each round of dialog ( or simply repeating its prediction if the model has chosen to stop ) .



We would like to thank the reviewer for their thoughtful compliments and criticism . In particular , the detailed list of areas for improvement have lead us to run additional experiments and make edits in the text that we believe have strengthened our work .

Let us address your concerns and questions below .

> analysis on the training process

We ’ve updated Figure 6 in the paper to display Accuracy@1 in addition to Accuracy @6 . We hope this metric plotted over each epoch gives a useful overview of the training process and some insight in how the model ’s performance changes over time .

> Do the authors have a sense for how sensitive these results are to different runs of the training process ?

We ran six experiments with different random seeds and reported the mean and variance on their loss and accuracy in Appendix B , but would be open to include these values in the main text if this seems useful .

> the transfer test set

There was not much to be gleaned from the transfer set besides the effect of the attention mechanism . We ’re more explicit about saying so in Section 6 .

> the accuracy @ K metric

We use this metric since many mammal classes are quite similar to each other , and we do n't want to overpenalize predicting similar classes such as kangaroo and wallaby . As suggested by the reviewer , this metric also enables comparison between the in -domain , out - domain , and transfer test sets .

> the F1 scores from the separate classifier are from training or test set evaluations

The plot in Figure 2a and its associated F1 scores are derived from the in - domain test set .

> discarding any image with a category beyond the 398 - th most frequent one

When we build our dataset , we discard images that are not likely to be an animal , as determined by a pre-trained classifier .

> numbering all display style equations

We appreciate the reviewer ’s suggestion to add equation numbers , but believe that since we have so many equations , it is alright to only number the equations that we reference explicitly in the text .

> when describing the recurrent receiver , explain the case where it terminates ( s^t =1 ) first such that P ( o_r=1 ) is defined prior to being used in the message generation equation

The first message of the receiver is learned as a separate parameter in all cases and we ’ve mentioned this in the “ Recurrent Receiver ” portion of Section 3.

> the analysis is Figure 3 b , c

For Figure 3 b and 3c , we show only the top - 4 predicted classes because the probabilities given to the other classes are negligible in comparison . The observation that we made regarding this figure ( that as the conversation progresses , similar but incorrect categories receive smaller probabilities than the correct one ) held for all other categories , but we limited to these two classes as we felt this sufficiently conveyed the idea .

> I encourage authors to see the EMNLP 2017 paper " Natural Language Does Not Emerge ‘ Naturally ’ in Multi-Agent Dialog " which also perform multi-round dialogs between two agents . Like this work , the authors also proposed removing memory from one of the agents as a means to avoid learning degenerate ' non - dialog ' protocols .

And

> Very minor point : the use of fixed - length , non -sequence style utterances is somewhat disappointing given the other steps made in the paper to make the reference game more ' human like ' such as early termination , shared vocabularies , and unconstrained utterance types . I understand however that this is left as future work .

There are some matters that we will leave for future work . Kottur et al. explain how limiting memory can force consistency over different steps in a dialog . This can be a useful property , but our work was primarily concerned with the distribution over messages and the model ’s prediction confidence . It ’s a natural progression to investigate the meaning of these messages as a follow - up work , and to attempt models that encode meaning not only in individual words , but also the latent structure in sequences of words .


The paper proposes a new multi-modal , multi-step reference game , where the sender has access to visual data and the receiver has access to textual messages , and also the conversation can be terminated by the receiver when proper .

Later , the paper describes their idea and extension in details and reports comprehensive experiment results of a number of hypotheses . The research questions seems straightforward , but it is good to see those experiments review some interesting points . One thing I am bit concerned is that the results are based on a single dataset . Do we have other datasets that can be used ?

The authors also lay out further several research directions . Overall , I think this paper is easy to read and good .



We ’d like to thank the reviewer for their thoughtful feedback . In response to the following comment :

> One thing I am bit concerned is that the results are based on a single dataset .

A distinguishing property of our dataset is that , in addition to images , each class has an informative textual description , and there is a natural hierarchy of properties shared between classes . As there was n’t a similar dataset already available , we had to collect the data ourselves . In section 5.2 of the de -anonymized version of the paper , we ’ll include a link to our codebase which contains instructions to build such a dataset .


This paper deals with improving language models on mobile equipments
based on small portion of text that the user has ever input . For this
purpose , authors employed a linearly interpolated objectives between user
specific text and general English , and investigated which method ( learning
without forgetting and random reheasal ) and which interepolation works better .
Moreover , authors also look into privacy analysis to guarantee some level of
differential privacy is preserved .

Basically the motivation and method is good , the drawback of this paper is
its narrow scope and lack of necessary explanations . Reading the paper ,
many questions arise in mind :

- The paper implicitly assumes that the statistics from all the users must
be collected to improve " general English " . Why is this necessary ? Why not
just using better enough basic English and the text of the target user ?

- To achieve the goal above , huge data ( not the " portion of the general English " ) should be communicated over the network . Is this really worth doing ? If only
" the portion of " general English must be communicated , why is it validated ?

- For measuring performance , authors employ keystroke saving rate . For the
purpose of mobile input , this is ok : but the use of language models will
cover much different situation where keystrokes are not necessarily
available , such as speech recognition or machine translation . Since this
paper is concerned with a general methodology of language modeling ,
perplexity improvement ( or other criteria generally applicable ) is also
important .

- There are huge number of previous work on context dependent language models ,
let alone a mixture of general English and specific models . Are there any
comparison with these previous efforts ?

Finally , this research only relates to ICLR in that the language model employed
is LSTM : in other aspects , it easily and better fit to ordinary NLP conferences , such as EMNLP , NAACL or so . I would like to advise the authors to submit
this work to such conferences where it will be reviewed by more NLP experts .

Minor :
- t of $ G_t $ in page 2 is not defined so far .
- What is " gr " in Section 2.2 ?


Thank you for your review !
I would like to make some clarifications and remarks .

You write :
" - The paper implicitly assumes that the statistics from all the users must be collected to improve " general English " . Why is this necessary ? Why not just using better enough basic English and the text of the target user ? "

There is strong evidence that the language of SMS and / or private messaging is sufficiently different from what we can collect in publicly available resources . Since language changes constantly we need to update the models and we cannot just make a single fine-tuning of basic LM on device . On the other hand the data from a single user is not sufficient for model update so we need data from many different users . The problem is that we cannot ( or at least do n't want to ) collect user data . We 've proposed a method of continuous update of language models without need to collect private data .

" - To achieve the goal above , huge data ( not the " portion of the general English " ) should be communicated over the network . Is this really worth doing ? If only the portion of " general English must be communicated , why is it validated ? "

As we mention in the paper the volume of the user generated data is small . Actually users generate approx . 600 bytes / day . In our experiments we proceeded from the assumption that fine-tuning starts as soon as 10 Kb of text data is accumulated on device . Our experiments showed that random rehearsal with volume of the rehearsal data should be to the volume of the fine-tuning data . So it is 10 Kb. This number is very small compared to the volume of model weights which are communicated in Federated Learning algorithms . We also discussed the communication efficiency in the answers to other reviewers ( see above ) .

" - For measuring performance , authors employ keystroke saving rate . For the purpose of mobile input , this is ok : but the use of language models will cover much different situation where keystrokes are not necessarily available , such as speech recognition or machine translation . Since this paper is concerned with a general methodology of language modeling , perplexity improvement ( or other criteria generally applicable ) is also important . "

Basically , we agree . And perplexity is reported in all our experiments . We just wanted to emphasize that target metrics should also be evaluated in language modeling like it is done in speech recognition ( ASR ) or machine translation ( BLEU ) . Also , in ( McMahan et al. 2017 , https://openreview.net/pdf?id=B1EPYJ-C-) word prediction accuracy is evaluated which is relative to KSS .

" - There are huge number of previous work on context dependent language models , let alone a mixture of general English and specific models . Are there any comparison with these previous efforts ? "


The term context is a bit vague . E.g. in ( Mikolov , Zweig 2014 , http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.258.5120&;rep=rep1&type=pdf) term " context " refers to the longer left context which is unavailable to standard RNN . Anyway longer left contexts are reasonably well catched by LSTM . If " context " refers to the running environment ( e.g. application context ) it is not the exact scope of our work . The standard approach to model adaptaion for the user is either model fine -tuning or interpolation with simpler language model ( e.g. Knesser - Ney smoothed n-gram ) . We tried approaches similar to the proposed in ( Ma et al. 2017 , https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/46439.pdf) but they performed significantly worse .

I also would like to draw your attention to the privacy analysys part of the paper which we included in the list of our contributions . We consider our contribution significant at least for the following reason . To our knowledge deep neural networks have never been checked for differential privacy coming from the randomness of the training algorithm ( combination of SGD , dropout regularization and model averaging in our case ) . Existing approaches ( e.g. Papernot et al. 2017 , https://arxiv.org/pdf/1610.05755.pdf) suggest adding random noise at different stages of training leading to the tradeoff between accuracy and privacy . At the same time our experiments show that the differential privacy can be guaranteed even without special treatment of the neural networks at least in some situations .

my main concern is the relevance of this paper to ICLR .
This paper is much related not to representation learning but to user-interface .
The paper is NOT well organized and so the technical novelty of the method is unclear .
For example , the existing method and proposed method seems to be mixed in Section 2 .
You should clearly divide the existing study and your work .
The experimental setting is also unclear .
KSS seems to need the user study .
But I do not catch the details of the user study , e.g. , the number of users .


Thank you for your review !
Below I will try to answer to your remarks .

1 ) You write
" my main concern is the relevance of this paper to ICLR "
" This paper is much related not to representation learning but to user- interface "
We think that our paper is relevant for ICLR because the papers on the same or adjacent topics were or will be presented :
1 . https://openreview.net/forum?id=SkhQHMW0W ( ICLR 2018 , federated learning )
2 . https://arxiv.org/pdf/1610.05755.pdf ( ICLR 2017 , differential privacy )
3 . https://openreview.net/forum?id=BJ0hF1Z0b&;noteId=Bkg5_kcxG ( ICLR 2018 , differentially private RNN )
We admit that our field of study is not as broad because we work only with language models , but the approach proposed in the paper may be used to different types of data and ML tasks . Our method gives good privacy guarantees and provides low communication cost ( compared to previous results ) .

Let me cite my answer to the previous reviewer .

" In the works on Federated Learning issued so far each node is considered only as a client in distributed learning system for gradient calculation . In our approach we guarantee that the model sent to the aggregation server is at the same time the actual model used for typing and gives the best performance for the end user . It is guaranteed by the forgetting prevention mechanism . It has at least following advantages : 1 ) No need for synchronization after every iteration as in standard Federated learning scheme . Standard Federated learning uses no more than 20 iterations on each device for reduction of the communication cost ( McMahan et al. 2016 , https://arxiv.org/abs/1602.05629) while we send our models only once in an epoch thus significantly reducing the communication cost . ; 2 ) Simpler synchronization scheme on the server ; 3 ) Faster convergence ; 4 ) Only 1 model is stored on the disk . We think that these results may be interesting to many ML practitioners . "

2 ) " KSS seems to need the user study " . - KSS is measured according to the formula ( 5 ) given in the paper . The process of testing automatization seems obvious . No user testing is needed . ( comp. WER calculation for ASR ) . In any case perplexity results are also given for all experiments .

3 ) As far as you did n't discuss the privacy analysis part which was included into the contributions list I 'll cite my previous comment again :

" We consider our contribution significant at least for the following reason . To our knowledge deep neural networks have never been checked for differential privacy coming from the randomness of the training algorithm ( combination of SGD , dropout regularization and model averaging in our case ) . Existing approaches ( e.g. Papernot et al. 2017 , https://arxiv.org/pdf/1610.05755.pdf) suggest adding random noise at different stages of training leading to the tradeoff between accuracy and privacy . At the same time our experiments show that the differential privacy can be guaranteed even without special treatment of the neural networks at least in some situations . "

This paper discusses the application of word prediction for software keyboards . The goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requirements .

The authors propose a simple method of mixing the global model with user specific data . Collecting the user specific models and averaging them to form the next global model .

The proposal is practical . However , I am not convinced that this is novel enough for publication at ICLR .

One major question . The authors assume that the global model will depict general english . However , it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+ 1 might be significantly different from general English . It is not clear to me as how this mechanism guarantees that it will not over - fit or that there will be no catastrophic forgetting .

Thank you for your review !
I would like to make some clarifications and remarks .

1 ) In the review you write
" One major question . The authors assume that the global model will depict general english . However , it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+ 1 might be significantly different from general English . It is not clear to me as how this mechanism guarantees that it will not over - fit or that there will be no catastrophic forgetting . "

In our problem statement we consider general English to be the common language i.e. the commonly used language with statistically insignificant portion of user-specific expressions . It is NOT necessarily the language induced by our in - house corpus . As far as we have a model averaged on many users on the server we can treat this model as general language model . So at each stage T the server - side model represents general language . When the model is sent to the device it is updated according to the user data so there is a risk of catastrophic forgetting . We prevent it by using ( eventually ) random rehearsal on device ( only ! ) .

2 ) I also would like to draw your attention to the original and practically relevant problem statement . In the works on Federated Learning issued so far each node is considered only as a client in distributed learning system for gradient calculation . In our approach we guarantee that the model sent to the aggregation server is at the same time the actual model used for typing and gives the best performance for the end user . It is guaranteed by the forgetting prevention mechanism . It has at least following advantages : 1 ) No need for synchronization after every iteration as in standard Federated learning scheme . Standard Federated learning uses no more than 20 iterations on each device for reduction of the communication cost ( McMahan et al. 2016 , https://arxiv.org/abs/1602.05629) while we send our models only once in an epoch thus significantly reducing the communication cost . ; 2 ) Simpler synchronization scheme on the server ; 3 ) Faster convergence ; 4 ) Only 1 model is stored on the disk . We think that these results may be interesting to many ML practitioners .

3 ) You did n't discuss the privacy analysis part of the paper which we included in the list of our contributions . We consider our contribution significant at least for the following reason . To our knowledge deep neural networks have never been checked for differential privacy coming from the randomness of the training algorithm ( combination of SGD , dropout regularization and model averaging in our case ) . Existing approaches ( e.g. Papernot et al. 2017 , https://arxiv.org/pdf/1610.05755.pdf) suggest adding random noise at different stages of training leading to the tradeoff between accuracy and privacy . At the same time our experiments show that the differential privacy can be guaranteed even without special treatment of the neural networks at least in some situations .

This paper studies the importance of the noise modelling in Gaussian VAE . The original Gaussian VAE proposes to use the inference network for the noise that takes latent variables as inputs and outputs the variances , but most of the existing works on Gaussian VAE just use fixed noise probably because the inference network is hard to train . In this paper , instead of using the fixed noise or inference network for the noise , the authors proposed to train the noise using Empirical - Bayes like fashion . The algorithm to train noise level for the single Gaussian decoder and mixture of Gaussian decoder is presented , and the experiments show that fitting the noise actually improves the ELBO and enhances the ability to disentangle latent factors .

I appreciate the importance of noise modeling , but not sure if the presented algorithm is a right way to do it . The proposed algorithm assumes the Gaussian likelihood with homoscedastic noise , but this is not the case for many real - world data ( MNIST and Color images are usually modelled with Bernoulli likelihood ) . The update equations for noises rely on the simple model structure , and this may not hold for the arbitrary complex likelihood ( or implicit likelihood case ) . In my personal opinion , making the inference network for the noise to be trainable would be more principled way of solving the problem .

The paper is too long ( 30 pages ) and dense , so it is very hard to read and understand the whole stuff . Remember that the ‘ recommended ’ page limit is 8 pages . The proposed algorithm was not compared to the generative models other than the basic VAE or beta-VAE .

We ’d like to thank the reviewer for affirmation of noise modeling and their reviews .

1 . We do agree that using the implicit generative model such as GANs might be a promising way to learn the noise factors . However , our work focus on the basic framework of VAE and its representation learning properties and capabilities theoretically and practically through noise modeling . Personally , other generative models currently might not be as scalable as Gaussian Prior VAE under proper noise modeling in learning disentangled representation although we do not exclude the possibility of GAN and other implicit generative models could succeed in this subfield in the future . For example , though we did not implement the adversarial loss in the pixel reconstruction domain , we did implement the adversarial loss in the latent space to encourage the disentanglement ( they are useful ) , however , we find even under the same hyperparameter setting their performance can be found really unstable . In contrast , several benefits of VAE are its stability of training model under maximum likelihood principle and the natural inference / encoder capability . By the way , our theoretical analysis on VAE could also be transferable and instructive to the analysis of some other generative models but those theoretical studies and relevant comparison might be left for the future work .

2 . We personally believe our noise modeling can be extended to many real - world data and could be better than the Bernoulli likelihood modeling . Admittedly , there could be plenty ways people perceive and imagine the noise and data . We observe that you can view the [ 0 ,1 ] range as the pixel - wise probability but still inherits the " cross-entropy " similarity metric to take place of " E_{q ( z|x ) }p ( x|z ) " for " E_{q ( z|x ) }cross-entropy ( x||x_z ) " . However , this implementation more or less ruins the maximum likelihood principle and the variational procedure , and therefore this implementation disables the usage and intuition of probability knowledge to some extent . In particular , if you want to further discuss the noise on the probability mass , everything could be awkward . As the result , it may need a new theory to enable the knowledge to be accumulated if those assumptions or optimizations are undertaken .

3 . We condense the original paper into 10 pages ( * excluding * references and appendices ) . The appendices are necessary to enable the reader to assess our proofs and details of experiments and algorithms . The discussion on the noise modeling is weakened due to the length limitation and emphasis is put on the theoretical discussion of the VAE properties . However , readers can still access the algorithm of MoG-noise VAE in appendices .

This paper proposes to modify how noise factors are treated when developing VAE models . For example , the original VAE work from ( Kingma and Welling , 2013 ) applies a deep network to learn a diagonal approximation to the covariance on the decoder side . Subsequent follow - up papers have often simplified this covariance to sigma^2 * I , where sigma^2 is assumed to be known or manually tuned . In contrast , this submission suggests either treating sigma^2 as a trainable parameter , or else introducing a more flexible zero-mean mixture - of - Gaussians ( MoG ) model for the decoder noise . These modeling adaptations are then analyzed using various performance indicators and empirical studies .

The primary issues I have with this work are threefold : ( i ) The paper is not suitably organized / condensed for an ICLR submission , ( ii ) the presentation quality is quite low , to the extent that clarity and proper understanding are jeopardized , and ( iii ) the novelty is limited . Consequently my overall impression is that this work is not yet ready for acceptance to ICLR .

First , regarding the organization , this submission is 19 pages long ( * excluding * references and appendices ) , despite the clear suggestion in the call for papers to limit the length to 8 pages : " There is no strict limit on paper length . However , we strongly recommend keeping the paper at 8 pages , plus 1 page for the references and as many pages as needed in an appendix section ( all in a single pdf ) . The appropriateness of using additional pages over the recommended length will be judged by reviewers . " In the present submission , the first 8 + pages contain minimal new material , just various background topics and modified VAE update rules to account for learning noise parameters via basic EM algorithm techniques . There is almost no novelty here . In my mind , this type of well - known content is in no way appropriate justification for such a long paper submission , and it is unreasonable to expect reviewers to wade through it all during a short review cycle .

Secondly , the presentation quality is simply too low for acceptance at a top-tier international conference ( e.g. , it is full of strange sentences like " Such amelioration facilitates the VAE capable of always reducing the artificial intervention due to more proper guiding of noise learning . " While I am sympathetic to the difficulties of technical writing , and realize that at times sufficiently good ideas can transcend local grammatical hiccups , my feeling is that , at least for now , another serious pass of editing is seriously needed . This is especially true given that it can be challenging to digest so many pages of text if the presentation is not relatively smooth .

Third and finally , I do not feel that there is sufficient novelty to overcome the issues already raised above . Simply adapting the VAE decoder noise factors via either a trainable noise parameter or an MoG model represents an incremental contribution as similar techniques are exceedingly common . Of course , the paper also invents some new evaluation metrics and then applies them on benchmark datasets , but this content only appears much later in the paper ( well after the soft 8 page limit ) and I admittedly did not read it all carefully . But on a superficial level , I do not believe these contributions are sufficient to salvage the paper ( although I remain open to hearing arguments to the contrary ) .

We ’d like to thank the reviewer for their making effort to reviewing and providing helpful suggestions although they did n't provide fair assessments of our contribution , especially the important content which appears later that used to reveal some basic facts and behaviors of idealistic VAE as well as our indicators . We have made a number of changes to address them .

A . We condense the original paper into 10 pages . We also try to reduce the number of strange sentences .

B . We weaken our discussion on noise modeling due to the limitation of the paper length and strengthen the theoretical troubleshooting of VAE 's properties and they are listed below

1 . Intrinsic dimension Issue : " Could the VAE learn the intrinsic number of factors underlying the data ?
Our paper : Yes , idealistic VAE learns and only learns the intrinsic factor dimension and the VAE objective induced by the Gaussian prior also encourages the information sparsity in dimension which is contributing to the learn the intrinsic dimension .
Besides , in real implementations , the conclusion is also instructive if the noise is proper modeling and the disentanglement ( clarified in our paper ) is achieved to some extent .

2 . Disentanglement Issue : " What are need and range induced by word disentanglement ? "
We provide the clarification according to information conservation theorem :
the learned the factors are close to being independent .
the factors incline to generate the oracle signal and to be inferred perfectly from the oracle signal through a continuous procedure / mapping .

3 . Real Factor Issue : " Could the VAE learn the real generating factor underlying the data or just some fantasies ? "
We show that idealistic VAE possibly learn any factors set in the equivalence class . Besides , the experiment results also suggest that the VAE 's factor equivalence generally exist .

4 . Indicator Issue : " Could the effectiveness of current disentanglement metric be guaranteed ? "
We show that the current disentanglement introduced by ( beta - VAE ) is based on " simulated factors " while idealistic VAE possibly learns any factor set in equivalence class induced by the " simulated factors " . Hence , that metric may work sometimes and suffer instability among different trials .
We further introduce some indicator regarding the mutual information I ( x ; z ) and Dkl ( q ( z ) || p ( z ) ) which provide the assessment to the determination of ``used factors " and to the disentanglement .

5 . Implementation Issue : " Could the aforementioned analysis be instructive in real implementation ? '
We introduce noise modeling to relax the consideration of the real situation . The experiment results empirically testify the knowledge derived from the idealistic case could be instructive in the real situation . They also demonstrate own characteristic of noise modeling in pursuing the disentanglement .

C. Despite the theoretical discussion on the intrinsic properties of VAE , if we just discuss the novelty of noise modeling of VAE alone , we do n't think it is limited . If you find different noise assumptions / specifications just significantly influence the disentanglement you will believe it .

This paper attempts to improve the beta-VAE ( Higgins et al , 2017 ) by removing the trade - off between the quality of disentanglement in the latent representation and the quality of the reconstruction . The authors suggest doing so by explicitly modelling the noise of the reconstructed image Gaussian p( x|z ) . The authors assume that VAEs typically model the data using a Guassian distribution with a fixed noise . This , however , is not the case . Since the authors are trying to address a problem that does not actually exist , I am not sure what the contributions of the paper are .

Apart from the major issue outlined above , the paper also makes other errors . For example , it suggests using D_KL ( q ( z ) || p ( z ) ) as a measure of disentanglement , with lower values being indicative of better disentanglement . This , however , is incorrect , since one can have tiny D_KL by encoding all the information into a single latent z_i . Such a representation would be highly entangled while still satisfying all of the conditions the authors propose for a disentangled representation .

Given the points outlined above and the fact that the paper is hard to read and is excessively long , I do not believe it should be accepted .

This comment is an illustration of our perspective and a quick response to the reviewers ’ counterexample in their second paragraph comment . We are trying to show that counterexample does n’t exist in the idealistic situation .

Example :
Suppose the input X follows a 2 dimension independent unit Gaussian . Let us say we want to encode X into only one dimension unit Gaussian latent Z and decode it back to X.

Discussion :
To simplify the situation , we could first consider the idealistic encoding and decoding procedure that the q( z|x ) =\delta ( z=f ( x ) ) and p( x|z ) =\delta ( x=g ( z ) ) are the two deterministic procedures . [ correpsonds to no-information - loss channel case ]

Then we have z=f ( g ( z ) ) for all z in R^1 , and x=g ( f ( x ) ) for all x in R^2.

If we further assume , the encoder f and decoder g are both continuous mappings ( It 's innocuous since the continuity of a mapping is a weak condition and mappings induced by the neural network are typically continuous . ) , then f and g are Homeomorphism mappings . However , R^1 and R^2 spaces have different topological structure and there thus is no Homeomorphism mapping between those two spaces . This leads to the contradiction .

Conclusion :
In short , roughly , in the idealistic situation , there is no way to encode 2 dim Gaussian into 1 dim Gaussian and then perfectly decode them back through continuous procedures .

Relation to our paper :
The aforementioned argument can also be found in the information conservation theorem in Intrinsic Dimension Issue ( section 3 ) in the latest paper .


We thanks the reviewer for their work . However , we 're afraid they may have misunderstood the point of our paper and did n't provide fair assessments of our contribution . We hope our responses below and the comments of the other reviewers may help clarify the scope of our work and its significance .

According to your suggestion that paper is too long , we amend the length of our paper from 19 pages to 10 . In order to achieve this and keeping it still comprehensive and informative , we have to weaken the discussion on noise modeling and put more emphasis on the intrinsic properties of VAE . We hope our amendment can increase the information channel capacity between the proposed ideal and our readers and provide better and more friendly reading experience .

A . We understand your first consideration that there exist some implementations under other noise assumptions including Bernoulli distribution for two - point valued data and some other more sophisticated ones with specific oriented domain knowledge . Some of them might already enable the parameter of noise to be learned . However , also in many implementations on real - valued data , many papers just simplified Gaussian assumption to be sigma^ 2 I where the sigma^2 is either assumed to be known or to be manually tuned . In particular , the tutorial on VAE ( https://arxiv.org/abs/1606.05908), which is a really respectable work and is also my first contact with VAE model , is also under this noise assumptions . And we personally believe that we are the first one publicly emphasizes and demonstrates on " noise modeling influences the disentanglement " though we have also shown some other benefit could be induced by noise modeling in our original paper version .

B . We thank for your intuitive counterexample to our clarification on disentanglement . We have proved several theorems especially the information conservation theorem [ e.g. two independent Gaussian and one Gaussian can not be the generating factor set of each other . ] in Intrinsic Dimension issue ( section 3 in the latest version ) to exclude this counterexample in the idealistic case and our experiment results also turn to support the instruction suggested by the theorem in the real implementation . In order to theoretically illustrate our perspective regarding the counterexample that review proposed , more compactly and informatively , we also add an auxiliary deduction in the latter comment . We will be grateful if the reviewer or someone else can further provide some facts and evidence from the theoretical perspective or experimental perspective to show the existence or the probability of the existence of that counterexample in the real situation .

C . You also mention there might be some other theoretical errors . We are grateful if you can list them . We are open to the opinion and argument from the other side and believe those arguments can improve the direction of scientific research and accelerate our mission to the AI . This will be helpful to improve our work but also good for the whole community .

The paper presents three contributions : 1 ) it shows that the proof of convergence Adam is wrong ; 2 ) it presents adversarial and stochastic examples on which Adam converges to the worst possible solution ( i.e. there is no hope to just fix Adam 's proof ) ; 3 ) it proposes a variant of Adam called AMSGrad that fixes the problems in the original proof and seems to have good empirical properties .

The contribution of this paper is very relevant to ICLR and , as far as I know , novel .
The result is clearly very important for the deep learning community .
I also checked most of the proofs and they look correct to me : The arguments are quite standard , even if the proofs are very long .

One note on the generality of the results : the papers states that some of the results could apply to RMSProp too . However , it has been proved that RMSProp with a certain settings of its parameters is nothing else than AdaGrad ( see Section 4 in Mukkamala and Hein , ICML ' 17 ) . Hence , at least for a certain setting of its parameters , RMSProp will converge . Of course , the proof in the ICML paper could be wrong , I did not check that ...

A general note on the learning rate : The fact that most of these algorithms are used with a fixed learning rate while the analysis assume a decaying learning rate should hint to the fact that we are not using the right analysis . Indeed , all these variants of AdaGrad did not really improve the AdaGrad 's regret bound . In this view , none of these algorithms contributed in any meaningful way to our understanding of the optimization of deep networks * nor * they advanced in any way the state - of - the- art for optimizing convex Lipschitz functions .
On the other hand , analysis of SGD - like algorithms with constant step sizes are known . See , for example , Zhang , ICML '04 where linear convergence is proved in a neighbourhood of the optimal solution for strongly convex problems .
So , even if I understand this is not the main objective of this paper , it would be nice to see a discussion on this point and the limitations of regret analysis to analyse SGD algorithms .

Overall , I strongly suggest to accept this paper .


Suggestions / minor things :
- To facilitate the reader , I would state from the beginning what are the common settings of beta_1 and beta_2 in Adam . This makes easier to see that , for example , the condition of Theorem 2 is verified .
- \hat{v}_{0 } is undefined in Algorithm 2 .
- The graphs in figure 2 would gain in readability if the setting of each one of them would be added as their titles .
- McMahan and Streeter ( 2010 ) is missing the title . ( Also , kudos for citing both the independent works on AdaGrad )
- page 11 , last equation , 2C-4=2C - 4 . Same on page 13 .
- Lemma 4 contains x _1 , x_2,z_1 , and z_2 : are x_1 and z_1 the same ? also x_2 and z_2 ?

We thank the reviewer for very helpful and constructive feedback .

About Mukkamala and Hein 2017 [ MH17 ] : Thanks for pointing this paper . As the anonymous reviewer rightly points out , the [ MH17 ] does not look at the standard version of RMSProp but rather a modification and thus , there is no contradiction with our paper . We will make this point clear in the final version of the paper .

Regarding note about learning rate : While it is true that none of these new rates improve upon Adagrad rates , in fact , in the worst case one cannot improve the regret of standard online gradient descent in general convex setting . Adagrad improves this in the special case of sparse gradients ( see for instance , Section 1.3 of Duchi et al. 2011 ) . However , these algorithms , which are designed for specific convex settings , appear to perform reasonably well in the nonconvex settings too ( especially in deep networks ) . Exponential moving average ( EMA ) variants seem to further improve the performance in the ( dense ) nonconvex setting . Understanding the cause for good performance in nonconvex settings is an interesting open problem . Our aim was to take an initial step to develop more principled EMA approaches . We will add a description in the final version of the paper .

Lemma 4 : Thanks for pointing it out and sorry for the confusion . Indeed , x 1 = z1 and x 2 = z 2 . We have corrected this typo .

We have also revised the paper to address the minor typos mentioned in the review .

This work identifies a mistake in the existing proof of convergence of
Adam , which is among the most popular optimization methods in deep
learning . Moreover , it gives a simple 1 - dimensional counterexample with
linear losses on which Adam does not converge . The same issue also
affects RMSprop , which may be viewed as a special case of Adam without
momentum . The problem with Adam is that the " learning rate " matrices
V_t^{1 /2} / alpha_t are not monotonically decreasing . A new method , called
AMSGrad is therefore proposed , which modifies Adam by forcing these
matrices to be decreasing . It is then shown that AMSGrad does satisfy
essentially the same convergence bound as the one previously claimed for
Adam . Experiments and simulations are provided that support the
theoretical analysis .

Apart from some issues with the technical presentation ( see below ) , the
paper is well - written .

Given the popularity of Adam , I consider this paper to make a very
interesting observation . I further believe all issues with the technical
presentation can be readily addressed .



Issues with Technical Presentation :

- All theorems should explicitly state the conditions they require
instead of referring to " all the conditions in ( Kingma & Ba , 2015 ) " .
- Theorem 2 is a repetition of Theorem 1 ( except for additional
conditions ) .
- The proof of Theorem 3 assumes there are no projections , so this
should be stated as part of its conditions . ( The claim in footnote 2
that they can be handled seems highly plausible , but you should be up
front about the limitations of your results . )
- The regret bound Theorem 4 establishes convergence of the optimization
method , so it plays the role of a sanity check . However , it is
strictly worse than the regret bound O ( sqrt{T} ) for online gradient
descent [ Zinkevich , 2003 ] , so it cannot explain why the proposed
AMSgrad method might be adaptive . ( The method may indeed be adaptive
in some sense ; I am just saying the * bound * does not express that .
This is also not a criticism of the current paper ; the same remark
also applies to the previously claimed regret bound for Adam . )
- The discussion following Corollary 1 suggests that sum_i
hat{v}_{ T , i}^{1 / 2 } might be much smaller than d G_infty . This is true ,
but we should always expect it to be at least a constant , because
hat{v}_{t , i} is monotonically increasing by definition of the
algorithm , so the bound does not get better than O( sqrt ( T ) ) .
It is also suggested that sum_i ||g_{ 1:T , i}| | = sqrt{sum_{t=1}^T
g_{t , i}^2 } might be much smaller than dG_infty , but this is very
unlikely , because this term will typically grow like O ( sqrt{T} ) ,
unless the data are extremely sparse , so we should at least expect
some dependence on T.
- In the proof of Theorem 1 , the initial point is taken to be x_1 = 1 ,
which is perfectly fine , but it is not " without loss of generality " ,
as claimed . This should be stated in the statement of the Theorem .
- The proof of Theorem 6 in appendix B only covers epsilon= 1 . If it is
" easy to show " that the same construction also works for other
epsilon , as claimed , then please provide the proof for general
epsilon .


Other remarks :

- Theoretically , nonconvergence of Adam seems a severe problem . Can you
speculate on why this issue has not prevented its widespread adoption ?
Which factors might mitigate the issue in practice ?
- Please define g_t \circ g_t and g_{1:T , i}
- I would recommend sticking with standard linear algebra notation for
the sqrt and the inverse of a matrix and simply using A^{ - 1 } and
A^{1 / 2 } instead of 1 / A and sqrt {A} .
- In theorems 1 , 2,3 , I would recommend stating the dimension ( d=1 ) of
your counterexamples , which makes them very nice !

Minor issues :

- Check accent on Nicol\`o Cesa-Bianchi in bibliography .
- Near the end of the proof of Theorem 6 : I believe you mean Adam
suffers a " regret " instead of a " loss " of at least 2C - 4.
Also 2C-4=2C -4 is trivial in the second but last display .


We deeply appreciate the reviewer for a thorough and constructive feedback .

- Theorem 2 & 3 are much more involved and hence the aim of Theorem 1 was to provide a simplified counter - example for a restrictive setting , thereby providing the key ideas of the paper .
- We will emphasize your point about projections in the final version of the paper .
- We agree that the role of Theorem 4 right now is to provide a sanity check . Indeed , it is not possible to improve upon the of online gradient descent in the worst case convex settings . Algorithms such as Adagrad exploit structure in the problem such as sparsity to provide improved regret bounds . Theorem 4 provides some adaptivity to sparsity of gradients ( but note that these are upper bounds and it is not clear if they are tight ) . Adaptive methods seem to perform well in few non-sparse and nonconvex settings too . It remains open to understand it in the nonconvex settings of our interest .
- Indeed , there is a typo ; we expect ||g { 1:T , i}| | to grow like sqrt ( T ) . The main benefit in adaptive methods comes in terms of sparsity ( and dimension dependence ) . For example see Section 1.3 in Duchi et al. 2011 ) . We have revised the paper to incorporate these changes .
- We can indeed assume that x_1 = 1 ( without loss of generality ) because for any choice of initial point , we can always translate the function so that x_1 = 1 is the initial point in the new coordinate system . We will add a discussion about this in the final version of the paper .
- The last part of Theorem 6 explains the reduction with respect to general epsilon . We will further highlight this in the final version of the paper .

Other remarks :

Regarding widespread adoption of Adam : It is possible that in certain applications the issues we raised in this work are not that severe ( although they can still lead to degradation in generalization performance ) . On the contrary , there exist a large number of real - world applications , for instance training models with large output spaces , which suffer from the issues we have highlighted and non-convergence has been observed to occur more frequently . Often , this non-convergence is attributed to nonconvexity but our paper shows one of the causes that applies even to convex settings .
As stated in the paper , using a problem specific large beta2 seems to help in some applications . Researchers have developed many tricks ( such as gradient clipping ) which might also play a role in mitigating these issues . We propose two different approaches to fix this issue and it will be interesting to investigate these approaches in various applications .

We have addressed all other minor concerns directly in the revision of the paper .


This paper examines the very popular and useful ADAM optimization algorithm , and locates a mistake in its proof of convergence ( for convex problems ) . Not only that , the authors also show a specific toy convex problem on which ADAM fails to converge . Once the problem was identified to be the decrease in v_t ( and increase in learning rate ) , they modified the algorithm to solve that problem . They then show the modified algorithm does indeed converge and show some experimental results comparing it to ADAM .

The paper is well written , interesting and very important given the popularity of ADAM .

Remarks :
- The fact that your algorithm cannot increase the learning rate seems like a possible problem in practice . A large gradient at the first steps due to bad initialization can slow the rest of training . The experimental part is limited , as you state " preliminary " , which is a unfortunate for a work with possibly an important practical implication . Considering how easy it is to run experiments with standard networks using open - source software , this can easily improve the paper . That being said , I understand that the focus of this work is theoretical and well deserves to be accepted based on the theoretical work .

- On page 14 the fourth inequality not is clear to me .

- On page 6 you talk about an alternative algorithm using smoothed gradients which you do not mention anywhere else and this is n't that clear ( more then one way to smooth ) . A simple pseudo- code in the appendix would be welcome .

Minor remarks :
- After the proof of theorem 1 you jump to the proof of theorem 6 ( which is n't in the paper ) and then continue with theorem 2 . It is a bit confusing .
- Page 16 at the bottom v_t= ... sum beta^{t - 1 - i}g_i should be g_i^2
- Page 19 second line , you switch between j&t and it is confusing . Better notation would help .
- The cifarnet uses LRN layer that is n't used anymore .

We thank the reviewer for the helpful and supportive feedback . The focus of the paper is to provide a principled understanding for the exponential moving average ( EMA ) adaptive optimization methods , which are now used as building blocks of many modern deep learning applications . The counter - example for non- convergence we show is very natural and is observed to arise in extremely sparse real - world problems ( e.g. , pertaining to problems with large output spaces ) . We provided two general directions to address the convergence issues in these algorithms ( by either changing the structure of the algorithm or by gradually increasing beta2 as algorithm proceeds ) . We have provided preliminary experiments on a few commonly used networks & datasets but we do agree that a thorough empirical study will be very useful and is part of our future plan .

- Fourth inequality on Page 14 : We revised the paper to explain it further .
- We will be happy to elaborate our comment about smoothed gradients in the final version of the paper .
- We also addressed other minor suggestions .


This paper creates adversarial images by imposing a flow field on an image such that the new spatially transformed image fools the classifier . They minimize a total variation loss in addition to the adversarial loss to create perceptually plausible adversarial images , this is claimed to be better than the normal L2 loss functions .

Experiments were done on MNIST , CIFAR - 10 , and ImageNet , which is very useful to see that the attack works with high dimensional images . However , some numbers on ImageNet would be helpful as the high resolution of it make it potentially different than the low-resolution MNIST and CIFAR .

It is a bit concerning to see some parts of Fig . 2 . Some of Fig. 2 ( especially ( b ) ) became so dotted that it no longer seems an adversarial that a human eye cannot detect . And model B in the appendix looks pretty much like a normal model . It might needs some experiments , either human studies , or to test it against an adversarial detector , to ensure that the resulting adversarials are still indeed adversarials to the human eye . Another good thing to run would be to try the 3x3 average pooling restoration mechanism in the following paper :

Xin Li , Fuxin Li. Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics . ICCV 2017 .

to see whether this new type of adversarial example can still be restored by a 3 x3 average pooling the image ( I suspect that this is harder to restore by such a simple method than the previous FGSM or OPT - type , but we need some numbers ) .

I also do n't think FGSM and OPT are this bad in Fig . 4 . Are the authors sure that if more regularization are used these 2 methods no longer fool the corresponding classifiers ?

I like the experiment showing the attention heat maps for different attacks . This experiment shows that the spatial transforming attack ( stAdv ) changes the attention of the classifier for each target class , and is robust to adversarially trained Inception v3 unlike other attacks like FGSM and CW .

I would likely upgrade to a 7 if those concerns are addressed .

After rebuttal : I am happy with the additional experiments and would like to upgrade to an accept .

We thank the reviewer for the thoughtful comments and suggestions .

Human study :
We have added a human study in Section 4.3 . In particular , we follow the same perceptual study protocol used in prior image synthesis work [ Zhang et al. 2016 ; Isola et al. 2017 ] . In our study , the participants are asked to choose the more visually realistic image between ( 1 ) an adversarial example generated by st Adv and ( 2 ) its original image . The user study shows that the generated adversarial examples can fool human participants 47 % of the time ( perfectly realistic results would achieve 50 % ) . This experiment shows that our adversarial examples are almost indistinguishable from natural images . Please see section 4.3 for more details .

3x3 mean blur defense
We included the suggested related work and added an analysis of the 3x3 average pooling restoration mechanism [ Li et al. 16 ’ ] . See section 4.4 and Table 5 in Appendix B for the discussion and results . In summary , the restoration is not as effective on st Adv examples . The classification accuracy on restored st Adv examples is around 50 % ( Table 5 ) , compared to restored C&W examples ( around 80 % ) and FGSM examples ( around 70 % ) [ Carlini et al. 2017 , Li et al. 2016 ] . In addition , st Adv achieves near 100 % attack success rate in a perfect knowledge adaptive attack [ Carlini et al. 2017 ] .

Comparison with C&W and FGSM ( Figure 4 )
In our revised version , we have updated Figure 4 to show adversarial examples for FGSM and C&W with a strong adversarial budget as : L_infinity perturbation limit of 0.3 on MNIST and 8 on CIFAR - 10 . We apply the same setting for the later evaluations against defenses

References :
[ Carlini et al. 2017 ] Carlini , Nicholas , and David Wagner . " Adversarial Examples Are Not Easily Detected : Bypassing Ten Detection Methods . " arXiv preprint arXiv:1705.07263 ( 2017 ) .
[ Li et al. 2016 ] Li , Xin , and Fuxin Li . " Adversarial examples detection in deep networks with convolutional filter statistics . " arXiv preprint arXiv:1612.07767 ( 2016 ) .
[ Zhang et al. 2016 ] Zhang , Richard , Phillip Isola , and Alexei A. Efros . " Colorful image colorization . " European Conference on Computer Vision . Springer International Publishing , 2016 .
[ Isola et al. 2017 ] Isola , Phillip , et al . " Image - to-image translation with conditional adversarial networks . " arXiv preprint arXiv:1611.07004 ( 2016 ) .


This paper proposes a new way to create adversarial examples . Instead of changing pixel values they perform spatial transformations .

The authors obtain a flow field that is optimized to fool a target classifier . A regularization term controlled by a parameter tau is ensuring very small visual difference between the adversarial and the original image .

The used spatial transformations are differentiable with respect to the flow field ( as was already known from previous work on spatial transformations ) it is easy to perform gradient descent to optimize the flow that fools classifiers for targeted and untargeted attacks .

The obtained adversarial examples seem almost imperceivable ( at least for ImageNet ) .
This is a new direction of attacks that opens a whole new dimension of things to consider .

It is hard to evaluate this paper since it opens a new direction but the authors do a good job using numerous datasets , CAM attention visualization and also additional materials with high -res attacks .

This is a very creative new and important idea in the space of adversarial attacks .

Edit : After reading the other reviews , the replies to the reviews and the revision of the paper with the human study on perception , I increase my score to 9 . This is definitely in the top 15 % of ICLR accepted papers , in my opinion .

Also a remark : As far as I understand , a lot of people writing comments here have a misconception about what this paper is trying to do : This is not about improving attack rates , or comparing with other attacks for different epsilons , etc .
This is a new * dimension * of attacks . It shows that limiting l_inf of l_2 is not sufficient and we have to think of human perception to get the right attack model . Therefore , it is opening a new direction of research and hence it is important scholarship . It is asking a new question , which is frequently more important than improving performance on previous benchmarks .



We thank the reviewer for the helpful comments . To further improve our work , we have added a user study to our updated version to evaluate the perceptual realism for the generated instances . In particular , we follow the same perceptual study protocol used in prior image synthesis work [ Zhang et al. 2016 ; Isola et al. 2017 ] . In our study , the participants are asked to choose the more visually realistic image between ( 1 ) an adversarial example generated by st Adv and ( 2 ) its original image . The user study shows that the generated adversarial examples can fool human participants 47 % of the time ( perfectly realistic results would achieve 50 % ) . This experiment shows that our adversarial examples are almost indistinguishable from natural images . Please see section 4.3 for more details .


This paper explores a new way of generating adversarial examples by slightly morphing the image to get misclassified by the model . Most other adversarial example generation methods tend to rely on generating high frequency noise patterns based by optimizing the perturbation on an individual pixel level . The new approach relies on gently changing the overall image by computing a flow an spatially transforming the image according to that flow . An important advantage of that approach is that the new attack is harder to protect against than to previous attacks according to the pixel based optimization methods .

The paper describes a novel model method that might become a new important line of attack . And the paper clearly demonstrates the advantages of this attack on three different data sets .

A minor nitpick : the " optimization based attack ( Opt ) " was first employed in the original " Intriguing Properties ... " 2013 paper using box -LBFGS as the method of choice predating FGSM .

We thank the reviewer for the constructive suggestions ! We have updated the method Opt to C&W throughout the paper .


I thank the authors for their updates and clarifications . I stand by my original review and score . I think their method and their evaluation has some major weaknesses , but I think that it still provides a good baseline to force work in this space towards tasks which can not be solved by simpler models like this . So while I 'm not super excited about the paper I think it is above the accept threshold .
--------------------------------------------------------------------------
This paper extends an existing thread of neural computation research focused on learning resuable subprocedures ( or options in RL-speak ) . Instead of simply input and output examples , as in most of the work in neural computation , they follow in the vein of the Neural Programmer - Interpreter ( Reed and de Freitas , 2016 ) and Li et. al. , 2017 , where the supervision contains the full sequence of elementary actions in the domain for all samples , and some samples also contain the hierarchy of subprocedure calls .

The main focus of their work is learning from fewer fully annotated samples than prior work . They introduce two new ideas in order to enable this :
1 . They limit the memory state of each level in the program heirarchy to simply a counter indicating the number of elementary actions / subprocedure calls taken so far ( rather than a full RNN embedded hidden / cell state as in prior work ) . They also limit the subprocedures such that they do not accept any arguments .
2 . By considering this very limited set of possible hidden states , they can compute the gradients using a dynamic program that seems to be more accurate than the approximate dynamic program used in Li et. al. , 2017 .

The main limitation of the work is this extremely limited memory state , and the lack of arguments . Without arguments , everything that parameterizes the subprocedures must be in the visible world state . In both of their domains , this is true , but this places a significant limitation on the algorithms which can be modeled with this technique . Furthermore , the limited memory state means that the only way a subprocedure can remember anything about the current observation is to call a different subprocedure . Again , their two evalation tasks fit into this paradigm , but this places very significant limitations on the set of applicable domains . I would have like to see more discussion on how constraining these limitations would be in practice . For example , it seems it would be impossible for this architecture to perform the Nanocraft task if the parameters of the task ( width , height , etc. ) were only provided in the first observation , rather than every observation .

None-the-less I think this work is an important step in our understanding of the learning dynamics for neural programs . In particular , while the RNN hidden state memory used by the prior work enables the learning of more complicted programs * in theory * , this has not been shown in practice . So , it 's possible that all the prior work is doing is learning to approixmate a much simpler architecture of this form . Specifically , I think this work can act as a great base - line by forcing future work to focus on domains which cannot be easily solved by a simpler architecture of this form . This limitation will also force the community to think about which tasks require a more complicated form of memory , and which can be solved with a very simple memory of this form .


I also have the following additional concerns about the paper :

1 . I found the current explanation of the algorithm to be very difficult to understand . It 's extremely difficult to understand the core method without reading the appendix , and even with the appendix I found the explanation of the level - by-level decomposition to be too terse .

2 . It 's not clear how their gradient approximation compares to the technique used by Li et. al . They obviously get better results on the addition and Nanocraft domains , but I would have liked a more clear explanation and / or some experiments providing insights into what enables these improvements ( or at least an admission by the authors that they do n't really understand what enabled the performance improvements ) .


Thank you for this valuable and detailed feedback .

You are correct in pointing out that PHPs impose a constraining memory structure , and we added to Sections 1 and 6 notes on their limitations . In principle , any finite memory structure can be implemented with sufficiently many PHPs , by having a distinct procedure for each memory state . Specifically in NanoCraft , PHPs can remember task parameters by calling a distinct sub - procedure for each building location and size . This lacks generalization , which was also not shown for NanoCraft by Li et al . ( 2017 ) . We expect the generalization achieved by limiting the number of procedures to be further enhanced by allowing them to depend on a program counter .

This paper thus makes an important first step towards neural programming with structural constraints that are both useful as an inductive bias that improves sample complexity , and computationally tractable . We agree that more expressive structures will be needed as the field moves beyond the current simple benchmarks , which we hope this work promotes . We agree that passing arguments to hierarchical procedures is an important extension to explore in future work .

We clarified in Section 4.2 and in the Appendix the explanations of the algorithm and of the level - wise training procedure . Specifically , in Section 4.2 we elaborated on the structure of the full likelihood P( zeta , xi | theta ) as a product of the relevant PHP operations , and how this leads to the given gradient expression ; and clarified the expression for sampling from the posterior P( zeta | xi , theta ) in level - wise training .

We added in Section 2 a short comparison of our method to that of Li et al . ( 2017 ) . The main difference is that their method computes approximate gradients by averaging selectively over computation paths , whereas our method computes exact gradients using dynamic programming , enabled by having small discrete latent variables in each time step .

In the paper titled " Parameterized Hierarchical Procedures for Neural Programming " , the authors proposed " Parametrized Hierarchical Procedure ( PHP ) " , which is a representation of a hierarchical procedure by differentiable parametrization . Each PHP is represented with two multi-layer perceptrons with ReLU activation , one for its operation statement and one for its termination statement . With two benchmark tasks ( NanoCraft and long -hand addition ) , the authors demonstrated that PHPs are able to learn neural programs accurately from smaller amounts of strong / weak supervision .

Overall the paper is well - written with clear logic and accurate narratives . The methodology within the paper appears to be reasonable to me . Because this is not my research area , I can not judge its technical contribution .

Thank you for your time and for your assessment . We are very excited about these results and are making updates to improve the paper .

Summary of paper : The goal of this paper is to be able to construct programs given data consisting of program input and program output pairs . Previous works by Reed & Freitas ( 2016 ) ( using the paper 's references ) and Cai et al. ( 2017 ) used fully supervised trace data . Li et al. ( 2017 ) used a mixture of fully supervised and weakly supervised trace data . The supervision helps with discovering the hierarchical structure in the program which helps generalization to other program inputs . The method is heavily based on the " Discovery of Deep Options " ( DDO ) algorithm by Fox et al . ( 2017 ) .

---

Quality : The experiments are chosen to compare the method that the paper is proposing directly with the method from Li et al . ( 2017 ) .
Clarity : The connection between learning a POMDP policy and program induction could be made more explicit . In particular , section 3 describes the problem statement but in terms of learning a POMDP policy . The only sentence with some connection to learning programs is the first one .
Originality : This line of work is very recent ( as far as I know ) , with Li et al. ( 2017 ) being the other work tackling program learning from a mixture of supervised and weakly supervised program trace data .
Significance : The problem that the paper is solving is significant . The paper makes good progress in demonstrating this on toy tasks .

---

Some questions / comments :
- Is the Expectation - Gradient trick also known as the reinforce / score function trick ?
- This paper could benefit from being rewritten so that it is in one language instead of mixing POMDP language used by Fox et al. ( 2017 ) and program learning language . It is not exactly clear , for example , how are memory states m_t and states s_t related to the program traces .
- It would be nice if the experiments in Figure 2 could compare PHP and NPL on exactly the same total number of demonstrations .

---

Summary : The problem under consideration is important and experiments suggest good progress . However , the clarity of the paper could be made better by making the connection between POMDPs and program learning more explicit or if the algorithm was introduced with one language .

Thank you for these constructive comments .

We added to Section 3 clarification of the connection between the POMDP formulation and program learning . In particular , the state s_t of the POMDP models the configuration of the computer ( e.g. , the tapes and heads of a Turing Machine , or the RAM of a register machine ) , whereas the memory m_t of the agent models the internal state of the machine itself ( e.g. the state of a Turing Machine 's Finite State Machine , or the registers of a register machine ) .

The Expectation – Gradient method is somewhat similar to but distinct from the REINFORCE trick , which uses the so-called “ log -gradient ” identity \nabla_\theta{p_\theta ( x ) } = p( x ) \nabla_\theta{\log p( x ) } to compute \nabla_\theta{ E_p [ f ( x ) ] } . In fact , we use that same identity twice to compute \nabla_\theta{\log P ( \xi | \ theta ) } : once to express the gradient of log P ( xi | theta ) using the gradient of P ( xi | theta ) ; then after introducing the sum over zeta , we use the identity again in the other direction to express this using the gradient of log P( zeta , xi | theta ) .

We added to Section 5.1 clarification that we did use the same total number of demonstrations for PHP as was used for NPL . The results for 64 demonstrations are shown in Figure 2 , and the results for PHP with 128 and 256 demonstrations were essentially the same as with 64 , and were omitted for figure clarity .

The paper considers a problem of adversarial examples applied to the deep neural networks . The authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal ( or noisy ) examples . More precisely , the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points ( see Section 4 ) . Based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal ( and noisy ) ones during the test time . In other words , the paper proposes a particular approach for the adversarial defence .

It turns out that there is a well - studied concept in the literature capturing the desired intrinsic dimensionality : it is called the local intrinsic dimensionality ( LID , Definition 1 ) . Moreover , there is a known empirical estimator of LID , based on the k-nearest neighbours . The authors propose to use this estimator in computing the intrinsic dimensionalities for the test time examples . For every test - time example X the resulting Algorithm 1 computes LID estimates of X activations computed for all intermediate layer of DNN . These values are finally used as features in classifying adversarial examples from normal and noisy ones .

The authors empirically evaluate the proposed technique across multiple state - of - the art adversarial attacks , 3 datasets ( MNIST , CIFAR10 , and SVHN ) and compare their novel adversarial detection technique to 2 other ones recently reported in the literature . The experiments support the conjecture mentioned above and show that the proposed technique * significantly * improves the detection accuracy compared to 2 other methods across all attacks and datasets ( see Table 1 ) .

Interestingly , the authors also test whether adversarial attacks can bypass LID - based detection methods by incorporating LID in their design . Preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the time . In other words , the proposed technique is rather stable and can not be easily exploited .

I really enjoyed reading this paper . All the statements are very clear , the structure is transparent and easy to follow . The writing is excellent . I found only one typo ( page 8 , " We also NOTE that ... " ) , otherwise I do n't actually have any comments on the text .

Unfortunately , I am not an expert in the particular field of adversarial examples , and can not properly assess the conceptual novelty of the proposed method . However , it seems that it is indeed novel and given rather convincing empirical justifications , I would recommend to accept the paper .


We are glad that you like our work and would like to thank you for the summary . The typo has been fixed in the updated version .

This paper tried to analyze the subspaces of the adversarial examples neighborhood . More specifically , the authors used Local Intrinsic Dimensionality to analyze the intrinsic dimensional property of the subspaces . The characteristics and theoretical analysis of the proposed method are discussed and explained . This paper helps others to better understand the vulnerabilities of DNNs .

We appreciate your candor about this research topic . At a high level , although deep neural networks have demonstrated superior performance for many tasks , certain properties which can affect their behavior ( such as subspaces , manifold properties ) are still not well understood . A better understanding of these properties can motivate more robust / efficient / effective deep learning models , which can in turn lead to further improving their performance . Adversarial vulnerability is one such property that jeopardizes the reliability of deep neural network learning models , as very small changes on inputs can sometimes lead to completely incorrect predictions ( such changed inputs are called adversarial inputs ) . In this paper , we investigate the expansion dimensional property of the subspaces surrounding such adversarial inputs and show that it can be used as an effective characteristic for detecting such inputs . We hope our work can provide some new insights into adversarial subspaces and their detection .

The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly . The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility . My main concern is the underlying assumption ( if I understand correctly ) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector . Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique . However , this does not invalidate the contributions of this manuscript .

Specific comments / questions :
- ( Minor ) Page 3 , Eq 1 : I think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the Euclidean setting .
- Section 4 : The different pieces of the problem ( estimation , intuition for adversarial subspaces , efficiency ) are very well described .
- Alg 1 , L3 : Is this where the normal exmaples are converted to adversarial examples using some attack technique ?
- Alg 1 , L12 : Is LID_norm computed using a leave - one - out estimate ? Otherwise , r_1 ( . ) for each point is 0 , leading to a somewhat " under-estimate " of the true LID of the normal points in the training set . I understand that it is not an issue in the test set .
- Section 4 and Alg 1 : S we do not really care about the " labels / targets " of the examples . All examples in the dataset are considered " normal " to start with . Is this assuming that the " initial training set " which is used to obtain the " pre-trained DNN " free of adversarial examples ?
- Section 5 , Experimental Setup : Seems like normal points in the test set would get lesser values if we are not doing the " leave - one - out " version of the estimation .
- Section 5 : The authors have done a great job at evaluating every aspect of the proposed method .


Thank you very much for these comments . We address them in detail below .
Q1 : The adversarial attack technique needs to be available for training .
A1 : Thank you for highlighting this . The ability to detect unseen adversarial attacks is an interesting issue . We have conducted some additional experiments to evaluate the generalizability of our LID based detector , see “ Generalizability Analysis ” , Section 5.3 . The result illustrates that our LID - based detector generalizes well to detect previously unseen adversarial attacks .

Q2 : ( Minor ) Page 3 , Eq 1 : The expansion dimension cares more about the probability mass .
A2 : Yes , we agree . The suggested explanation has been added to Paragraph 1 , Section 3.

Q3 : Alg 1 , L3 : is this where the adversarial attacks are applied ?
A3 : Yes . We have updated the description of the algorithm to clarify this ( see Paragraph 2 in " Using LID to Characterize Adversarial Examples " , Section 4 ) .

Q4 : Alg 1 , L12 & Section 5 , Experimental Setup : leave-one - out estimate ?
A4 : Yes , the query point x is " left out " . We have added extra explanations of how Eq ( 4 ) ( as used in L12 - 14 , Alg 1 ) works in the last paragraph of Section 3.

Q5 : Section 4 and Alg 1 : assuming training data is free of adversarial examples ?
A5 : Yes . This is a reasonable assumption and is the one which has been made in previous work . We have highlighted this in the 2nd paragraph of " Using LID to Characterize Adversarial Examples " , Section 4 .

I thank the authors for the thoughtful response and rebuttal . The authors have substantially updated their manuscript and improved the presentation .

Re : Speed . I brought up this point because this was a bulleted item in the Introduction in the earlier version of the manuscript . In the revised manuscript , this bullet point is now removed . I will take this point to be moot .

Re : High resolution . The authors point to recent GAN literature that provides some first results with high resolution GANs but I do not see quantitative evidence in the high resolution setting for this paper . ( Figure 4 provides qualitative examples from Image Net but no quantitative assessment . )

Because the authors improved the manuscript , I upwardly revised my score to ' Ok but not good enough - rejection ' . I am not able to accept this paper because of the latter point .
==========================

The authors present an interesting new method for generating adversarial examples . Namely , the author train a generative adversarial network ( GAN ) to adversarial examples for a target network . The authors demonstrate that the network works well in the semi-white box and black box settings .

The authors wrote a clear paper with great references and clear descriptions .

My primary concern is that this work has limited practical benefit in a realistic setting . Addressing each and every concern is quite important :

1 ) Speed . The authors suggest that training a GAN provides a speed benefit with respect to other attack techniques . The FGSM method ( Goodfellow et al , 2015 ) is basically 1 inference operation and 1 backward operation . The GAN is 1 forward operation . Granted this results in a small difference in timing 0.06s versus 0.01s , however it would seem that avoiding a backward pass is a somewhat small speed gain .

Furthermore , I would want to question the practical usage of having an ' even faster ' method for generating adversarial examples . What is the reason that we need to run adversarial attacks ' even faster ' ? I am not aware of any use -cases , but if there are some , the authors should describe the rationales at length in their paper .

2 ) High spatial resolution images . Previous methods , e.g. FGSM , may work on arbitrarily sized images . At best , GANs generate reasonable images that are lower resolutions ( e.g. < 128x128 ) . Building GAN 's that operate above - and - beyond moderate spatial resolution is an open research topic . The best GAN models for generating high resolution images are difficult to train and it is not clear if they would work in this setting . Furthermore , images with even higher resolutions , e.g. 512x512 , which is quite common in ImageNet , are difficult to synthesizes using current techniques .

3 ) Controlling the amount of distortion . A feature of previous optimization based methods is that a user may specify the amount of perturbation ( epsilon ) . This is a key feature if not requirement in an adversarial perturbation because a user might want to examine the performance of a given model as a function of epsilon . Performing such an analysis with this model is challenging ( i.e. retraining a GAN ) and it is not clear if a given image generated by a GAN will always achieve a given epsilon perturbation /

On a more minor note , the authors suggest that generating a * diversity * of adversarial images is of practical import . I do not see the utility of being able to generate a diversity of adversarial images . The authors need to provide more justification for this motivation .

We thank the reviewer for the thoughtful comments and suggestions .

Speed . Speed is just one advantage of our method and is not the main motivation of our method . We agree with the reviewer that FGSM is already fast enough for most applications . However , our proposed model is much more effective in the fooling rate against both white - box and blackbox settings than the FGSM method and is still faster than the FGSM method . In wide resnet setting on CIFAR , FGSM takes 0.39s to generate 100 examples ( 1 forward and 1 backward pass through the classifier ) , while AdvGAN takes 0.16s to generate 100 examples ( 1 forward pass through the generator ) . In our experiments , the classifier was a wide resnet with 46.16M parameters , while the generator had 0.24M parameters . The speed difference is even larger with deeper classifiers . Moreover , in AdvGAN , the forward operation does not use classifier ’s network , but uses generator ’s network . Overall , we claim to have developed a faster and more effective alternative method to generating adversarial examples , but improving the speed is just a byproduct for us and generating more photorealistic and effective adversarial examples in both semi white - box and blackbox settings is the main goal .

High spatial resolution images . Early GANs have had this problem . However , we claim that AdvGAN still works at high spatial resolution ( and it is not unique in doing so ) . Here are three techniques we applied in AdvGAN .
( i ) Previous work in high resolution : Our method is built on image - to-image translation and conditional GANs ( e.g. [ pix2pix ] , [ CycleGAN ] ) rather than unconditional GANs ( e.g. [ vanilla GANs ] , [ DCGAN ] ) . Many conditional GAN methods have been shown to be able to produce photorealistic results at relatively high resolution ( 256x256 and 512x512 from [ pix2 pix ] and [ CycleGAN ] ) . The recent pix2pixHD paper on arXiv from NVIDIA [ Wang et al. 2017 ] can even produce 2 k photo-realistic images . Even recent unconditional GANs like progressive GANs [ Karras et al. 2017 ] are able to produce 1 k images .
( ii ) Retaining details from original image : Our goal is to produce the perturbation rather than the final image : output = input + G ( input ) . Details and textures are copied from the input image .
( iii ) Resolution - independent architecture : Our model is fully convolutional and can be applied to input images with arbitrary sizes , similar to [ pix2 pix ] and [ CycleGAN ] .

Controlling the amount of distortion . We added more detailed description in the updated paper about how we control the amount of perturbation . Basically , we use parameter c within the hinge loss as shown in eq . ( 3 ) to allow users to specify the perturbation amount ( epsilon ) . Note that AdvGAN can explicitly control the amount of perturbation since in the MNIST challenge , it is strictly required that the perturbation is bounded within 0.3 in terms of L-infinity . So the competition results also show that we are able to bound the perturbation accurately so as to win the challenge .

Why are we interested in the diversity of adversarial examples ? We have seen that ensemble adversarial training works better than adversarial training against FGS + rand [ Florian et al. 2017 ] . This indicates that more diverse adversarial examples are needed to perform adversarial training as a defense . In addition , exploring other diverse adversarial examples can help us better understand the space of adversarial examples . For these reasons , we are interested in how to produce diverse adversarial examples , but indeed , we have not made it the main goal of AdvGAN .

Reference
[ Pix2pix ] Isola , Phillip , et al . " Image - to-image translation with conditional adversarial networks . " arXiv preprint arXiv:1611.07004 ( 2016 ) .
[ CycleGAN ] Zhu , Jun - Yan , et al . " Unpaired image - to-image translation using cycle-consistent adversarial networks . " arXiv preprint arXiv:1703.10593 ( 2017 ) .
[ Vanilla GAN ] Goodfellow , Ian , et al . " Generative adversarial nets . " Advances in neural information processing systems . 2014 .
[ DCGAN ] Radford , Alec , Luke Metz , and Soumith Chintala . " Unsupervised representation learning with deep convolutional generative adversarial networks . " arXiv preprint arXiv:1511.06434 ( 2015 ) .
[ Wang et al. 2017 ] Wang , Ting - Chun , et al . " High -Resolution Image Synthesis and Semantic Manipulation with Conditional GANs . " arXiv preprint arXiv:1711.11585 ( 2017 ) .
[ Karras et al. 2017 ] Karras , Tero , et al . " Progressive growing of gans for improved quality , stability , and variation . " arXiv preprint arXiv :1710.10196 ( 2017 ) .
[ Florian et al. 2017 ] Tramèr , Florian , et al . " Ensemble Adversarial Training : Attacks and Defenses . " arXiv preprint arXiv:1705.07204 ( 2017 ) .


Thanks for the valuable feedback . Regarding the reviewer ’s request about high resolution images , we have added ( 1 ) a quantitative experiment on attack success rates , ( 2 ) a user study on perceptual realism of the examples , and ( 3 ) additional qualitative examples , which demonstrate that AdvGAN can effectively generate high resolution adversarial examples . The details are as follows .

We generate 100 high resolution ( 299x299 ) adversarial examples under an L_infinity bound of 0.01 ( pixel values are in range [ 0 ,1 ] ) . This competition provided a dataset compatible with ImageNet . We observe that the attack success rate of AdvGAN is 100 % . Section 4.5 details the experiment settings .

In order to evaluate the perceptual realism of high resolution adversarial examples generated by AdvGAN , we have added a human study in Section 4.5 . In our study , participants chose AdvGAN ’s adversarial examples as more realistic over the original images in 49.4 % of the trials ( matching the realism of the original images results in around 50 % ) . This experiment shows that these high resolution AdvGAN adversarial examples are about as realistic as benign images .

In addition to the quantitative experiment and the user study , we include some high resolution adversarial examples in Figure 8.


This paper describes AdvGAN , a conditional GAN plus adversarial loss . AdvGAN is able to generate adversarial samples by running a forward pass on generator . The authors evaluate AdvGAN on semi-white box and black box setting .

AdvGAN is a simple and neat solution to for generating adversary samples . The author also reports state - of - art results .

Comment :

1 . For MNIST samples , we can easily find the generated sample is a mixture of two digitals . Eg , for digital 7 there is a light gray 3 overlap . I am wondering this method is trying to mixture several samples into one to generate adversary samples . For real color samples , it is harder to figure out the mixture .
2 . Based on mixture assumption , I suggest the author add one more comparison to other method , which is relative change from original image , to see whether AdvGAN is the most efficient model to generate the adversary sample ( makes minimal change to original image ) .





We thank the reviewer for the thoughtful comments and suggestions . We plot the perturbation in Figure 3 ( c ) ( d ) and 4 ( b ) in the updated version . From these plots , we can see that AdvGAN ’s perturbations ( amplified by 10 × ) do not resemble images from CIFAR - 10 / ImageNet .
For fair comparison against other attacks , we limit the perturbation to 0.3 L_infinity distance for MNIST and 8 for CIFAR - 10 .

We have compared the attack success rate of adversarial examples by different methods under defenses in Tables 3 and 4 and show that AdvGAN can often achieve high attack success rate under the same perturbation budget compared to other methods .
We have also added another comparison on MNIST and CIFAR - 10 with FGSM and optimization methods , showing the relative change from original image in Table 7 in the appendix .
From the table , we can see that AdvGAN adds comparable perturbation with CW and less perturbation compared with FGSM . As AdvGAN aims to generate photo realistic images with bounded perturbation instead of minimizing the perturbation as CW does , the perturbation added by AdvGAN is slightly higher compared to CW .


The paper proposes a way of generating adversarial examples that fool classification systems .
They formulate it for a blackbox and a semi-blackbox setting ( semi being , needed for training their own network , but not to generate new samples ) .

The model is a residual gan formulation , where the generator generates an image mask M , and ( Input + M ) is the adversarial example .
The paper is generally easy to understand and clear in their results .
I am not awfully familiar with the literature on adversarial examples to know if other GAN variants exist . From this paper 's literature survey , they dont exist .
So this paper is innovative in two parts :
- it applies GANs to adversarial example generation
- the method is a simple feed - forward network , so it is very fast to compute

The experiments are pretty robust , and they show that their method is better than the proposed baselines .
I am not sure if these are complete baselines or if the baselines need to cover other methods ( again , not fully familiar with all literature here ) .


We thank the reviewer for the thoughtful comments and suggestions . As mentioned by the reviewer , for baselines comparison , the proposed AdvGAN is currently the best attack method in Madry et al . ’s MNIST Adversarial Examples Challenge ( https://github.com/MadryLab/mnist_challenge), which includes many state - of - the- art attack methods .
In our updated version , we have also added another comparison on MNIST and CIFAR - 10 with FGSM and optimization methods , showing the perturbation amount in Table 7 in the appendix .

Summary of paper :

The authors present a novel attack for generating adversarial examples , deemed OptMargin , in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations . They compare this optimization method with two baselines in MNIST and CIFAR , and provide an analysis of the decision boundaries by their adversarial examples , the baselines and non-altered examples .

Review summary :

I think this paper is interesting . The novelty of the attack is a bit dim , since it seems it 's just the straightforward attack against the region cls defense . The authors fail to include the most standard baseline attack , namely FSGM . The authors also miss the most standard defense , training with adversarial examples . As well , the considered attacks are in L2 norm , and the distortion is measured in L2 , while the defenses measure distortion in L_\infty ( see detailed comments for the significance of this if considering white - box defenses ) . The provided analysis is insightful , though the authors mostly fail to explain how this analysis could provide further work with means to create new defenses or attacks .

If the authors add FSGM to the batch of experiments ( especially section 4.1 ) and address some of the objections I will consider updating my score .

A more detailed review follows .


Detailed comments :

- I think the novelty of the attack is not very strong . The authors essentially develop an attack targeted to the region cls defense . Designing an attack for a specific defense is very well established in the literature , and the fact that the attack fools this specific defense is not surprising .

- I think the authors should make a claim on whether their proposed attack works only for defenses that are agnostic to the attack ( such as PGD or region based ) , or for defenses that know this is a likely attack ( see the following comment as well ) . If the authors want to make the second claim , training the network with adversarial examples coming from OptMargin is missing .

- The attacks are all based in L2 , in the sense that the look for they measure perturbation in an L2 sense ( as the paper evaluation does ) , while the defenses are all L_\infty based ( since the region classifier method samples from a hypercube , and PGD uses an L_\infty perturbation limit ) . This is very problematic if the authors want to make claims about their attack being effective under defenses that know OptMargin is a possible attack .

- The simplest most standard baseline of all ( FSGM ) is missing . This is important to compare properly with previous work .

- The fact that the attack OptMargin is based in L2 perturbations makes it very susceptible to a defense that backprops through the attack . This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack .

- I think the authors rush to conclude that " a small ball around a given input distance can be misleading " . Wether balls are in L2 or L_\infty , or another norm makes a big difference in defense and attacks , given that they are only equivalent to a multiplicative factor of sqrt ( d ) where d is the dimension of the space , and we are dealing with very high dimensional problems . I find the analysis made by the authors to be very simplistic .

- The analysis of section 4.1 is interesting , it was insightful and to the best of my knowledge novel . Again I would ask the authors to make these plots for FSGM . Since FSGM is known to be robust to small random perturbations , I would be surprised that for a majority of random directions , the adversarial examples are brought back to the original class .

- I think a bit more analysis is needed in section 4.2 . Do the authors think that this distinguishability can lead to a defense that uses these statistics ? If so , how ?

- I think the analysis of section 5 is fairly trivial . Distinguishability in high dimensions is an easy problem ( as any GAN experiment confirms , see for example Arjovsky & Bottou , ICLR 2017 ) , so it 's not surprising or particularly insightful that one can train a classifier to easily recognize the boundaries .

- Will the authors release code to reproduce all their experiments and methods ?

Minor comments :
- The justification of why OptStrong is missing from Table2 ( last three sentences of 3.3 ) should be summarized in the caption of table 2 ( even just pointing to the text ) , otherwise a first reader will mistake this for the omission of a baseline .

- I think it 's important to state in table 1 what is the amount of distortion noticeable by a human .

=========================================

After the rebuttal I 've updated my score , due to the addition of FSGM added as a baseline and a few clarifications . I now understand more the claims of the paper , and their experiments towards them . I still think the novelty , significance of the claims and protocol are still perhaps borderline for publication ( though I 'm leaning towards acceptance ) , but I do n't have a really high amount of experience in the field of adversarial examples in order to make my review with high confidence .

We thank the reviewer for the helpful suggestions and comments .
[ Comparison with FGSM ] In our updated version , we ’ve added the corresponding experiments with FGSM adversarial examples throughout the paper . Thanks for the suggestion . In summary , FGSM was able to create some robust adversarial examples , but it also had higher distortion and lower success rate , especially on adversarially trained models .
[ Standard defense , adversarial training ] The adversarially trained model we used in this paper ( with PGD adversarial training ) is already intended to defend against gradient - based attacks , such as OptMargin and the other attacks we experiment with in this paper . We do not refute Madry et al . ’s claim that PGD adversarial training is effective against attacks bounded in L_inf distortion ( 2017 ) , although we do not find that threat model to be realistic .
[ Attacks in L2 , defenses in L_infinity ] The discrepancy between the attacks ’ focus on L_2 distance and the defenses ’ focus on L_inf distance is definitely not the most elegant thing in this paper . However , our analysis of random orthogonal directions is simplest in a distance metric where all directions are uniform . Cao & Gong ’s defense , in particular , was not specialized for L_inf-bounded attacks , and their paper evaluated it successfully against previous L_0 - and L_2 attacks . Nevertheless , we find it interesting that an adversarial example which satisfies points sampled from a hypersphere would be generally robust enough against a defense that checks in a hypercube .
> Designing an attack for a specific defense is very well established in the literature , and the fact that the attack fools this specific defense is not surprising .
The choice of defenses and attacks used in this paper are meant to cover a variety of scenarios for looking at decision boundaries . The proposal of OptMargin as an attack intends , in part , to demonstrate an adaptive attack , but primarily to create images farther from decision boundaries . The result that OptMargin bypasses region classification at all , we think is interesting because ( i ) research in nondeterministic defenses has picked up recently , and it is expected to have an advantage of unpredictability even in white - box settings ; and ( ii ) it succeeds with less distortion than previously known methods , including OptStrong and Cao & Gong ’s CW-L_*-A.
> If the authors want to make the second claim [ that OptMargin would work against other , specialiazed defenses ] , training the network with adversarial examples coming from OptMargin is missing .
We do not aim to make that claim . We claim that OptMargin creates adversarial examples that are robust to small random perturbations and that have high attack success rate against region classification , a specific defense that examines the decision regions around the input .
> I think the authors rush to conclude that " a small ball around a given input distance can be misleading " . Wether balls are in L2 or L_\infty , or another norm makes a big difference ...
Thanks for bringing this up —we intentionally evaluate classifier boundaries in terms of both metric , and we see evidence for this in our decision boundary distance plots , both for the hypersphere ( Figure 2 , formerly Table 4 ) and the hypercube ( Figure 7 , formerly Table 9 ) . Additionally , for the hypercube , we experimentally validate that the ball is consistent enough to fool a region classifier .
> Do the authors think that this distinguishability can lead to a defense that uses these statistics ?
We ’re not sure whether these , or even the mechanisms of Section 5 would be a good enough defense . The summary statistics in Sections 4.1.2 and 4.2 are not separable with good accuracy in some settings . There is a fraction of high-distortion FGSM attacks that can fool a classifier and have a convincing distribution of surrounding decision boundaries . We have yet to find out if an attacker can achieve this with a high success rate and , ideally , lower distortion .
> Distinguishability in high dimensions is an easy problem
Yes , it was our intention to use more data to make the problem easier . We agree , though , that once we have the decision boundary data and see how different it is across different kinds of examples , it is not as big of a leap to run it through a classifier and to expect it to work . But we definitely wanted to have an application to showcase how such information can be used and improve over previous techniques .
> release code
Yes , we intend to release our code , as well as the random values we used in our experiments upon acceptance .
Thanks for you minor comments as well . We have made changes in our updated draft to address them . We have updated the caption of Table 2 regarding the omission of OptStrong , and we have added an explanation to Table 1 about the visibility of perturbations .


Compared to previous studies , this paper mainly claims that the information from larger neighborhoods ( more directions or larger distances ) will better characterize the relationship between adversarial examples and the DNN model .

The idea of employing ensemble of classifiers is smart and effective . I am curious about the efficiency of the method .

The experimental study is extensive . Results are well discussed with reasonable observations . In addition to examining the effectiveness , authors also performed experiments to explain why OPTMARGIN is superior . Authors are suggested to involve more datasets to validate the effectiveness of the proposed method .

Table 5 is not very clear . Authors are suggested to discuss in more detail .


Thanks for reviewing our paper .

In our updated draft , we ’ve added some small - scale experiments on ImageNet data , in Appendix D . Thanks for the suggestion . The OptMargin attack is effective against region classification in these new experiments too .

We ’ve added some interpretation guidelines to the caption of Table 5 ( the adjacent class purity plots , changed to Figure 3 in the updated version ) .

The paper presents a new approach to generate adversarial attacks to a neural network , and subsequently present a method to defend a neural network from those attacks . I am not familiar with other adversarial attack strategies aside from the ones mentioned in this paper , and therefore I can not properly assess how innovative the method is .

My comments are the following :

1 - I would like to know if benign examples are just regular examples or some short of simple way of computing adversarial attacks .

2 - I think the authors should provide a more detailed and formal description of the OPTMARGIN method . In section 3.2 they explain that " Our attack uses existing optimization attack techniques to ... " , but one should be able to understand the method without reading further references . Specially a formal representation of the method should be included .

3 - Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples ( " it succeeds on 28 % of the samples on MNIST ; 73 % on CIFAR - 10 " ) . What is the meaning of success rate in here ? Is it the % of times that the classifier is confused ?

4 - OPTSTRONG produces images that are notably more distorted than OPTBRITTLE ( by RMS and also visually in the case of MNIST ) . So I actually cannot tell which method is better , at least in the MNIST experiment . One could do a method that completely distort the image and therefore will be classified with as a class . But adversarial images should be visually undistinguishable from original images . Generated CIFAR images seem similar than the originals , although CIFAR images are very low resolution , so judging this is hard .

4 - As a side note , it would be interesting to have an explanation about why region classification is providing a worse accuracy than point classification for CIFAR - 10 benign samples .

As a summary , the authors presented a method that successfully attacks other existing defense methods , and present a method that can successfully defend this attack . I would like to see more formal definitions of the methods presented . Also , just by looking at RMS it is expected that this method works better than OPTBRITTLE , since the images are more distorted . It would be needed to have a way of visually evaluate the similarity between original images and generated images .

Thanks for the comments and questions . Here are our responses and the corresponding changes we ’ve made in our updated draft .

1 . ( What are benign examples ? ) The benign examples are just taken directly from the test set .

2 . ( More detail on how OptMargin works ) Agreed . We ’ve added an overview of the technique to Section 3.2 , which should cover the relevant parts of the cited work .

3 . ( What is the success rate for OptStrong ? ) It ’s the fraction of cases when the attack generates an image that ’s misclassified * with high enough of a confidence margin* ( 40 logits in our experiments ) . Note that the official implementation of Carlini & Wagner ’s high - confidence attack ( referred to as OptStrong in in this paper ) attack outputs a blank black image if the internal optimization procedure does not encounter a satisfactory image , even if it does encounter an image that would fool the classifier but with a lower confidence margin .

4 . ( OptStrong is heavily distorted ) That ’s a good point that an indistinguishable example would be better ( more stealthy , lower cost by some measure , etc . ) . But the defender would n’t have an unmodified version of the image to compare against . The distortion numbers ( Table 1 ) tell part of the story of how much the adversarial examples are changed . Internally , we like to look at sample images ( Appendix A ) and make sure the original class is still clearly visible to humans . Our opinion is that OptStrong images are less recognizable than OptMargin ’s .

4 . ( Why is region classification worse on CIFAR - 10 ? ) Our intuition is that hypercubes , L_inf distances , and the like are especially well suited for MNIST , because the dataset is black and white . Random perturbations that change black to dark gray can be systematically ignored by a model that ’s smart enough . CIFAR - 10 deals with colors that use the whole gamut of pixel values , so it should be more sensitive to small changes bounded by an L_inf distance . Experimentally , we evaluated a model made with PGD adversarial training , which is trained not to be sensitive to these small perturbations , and the result is that the accuracy is lower than that of the model without adversarial training , but there ’s no accuracy drop between point classification and region classification ( Table 2 ) .


This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach , namely that the induced representation is not invariant to monotonic transform of the marginal distributions ( as opposed to the mutual information on which it is based ) . The authors address this shortcoming by applying the DIB to a transformation of the data , obtained by a copula transform . This explicit approach is shown on synthetic experiments to preserve more information about the target , yield better reconstruction and converge faster than the baseline . The authors further develop a sparse extension to this Deep Copula Information Bottleneck ( DCIB ) , which yields improved representations ( in terms of disentangling and sparsity ) on a UCI dataset .

( significance ) This is a promising idea . This paper builds on the information theoretic perspective of representation learning , and makes progress towards characterizing what makes for a good representation . Invariance to transforms of the marginal distributions is clearly a useful property , and the proposed method seems effective in this regard .
Unfortunately , I do not believe the paper is ready for publication as it stands , as it suffers from lack of clarity and the experimentation is limited in scope .

( clarity ) While Section 3.3 clearly defines the explicit form of the algorithm ( where data and labels are essentially pre-processed via a copula transform ) , details regarding the “ implicit form ” are very scarce . From Section 3.4 , it seems as though the authors are optimizing the form of the gaussian information bottleneck I ( x , t ) , in the hopes of recovering an encoder $ f_\beta ( x ) $ which gaussianizes the input ( thus emulating the explicit transform ) ? Could the authors clarify whether this interpretation is correct , or alternatively provide additional clarifying details ? There are also many missing details in the experimental section : how were the number of “ active ” components selected ? Which versions of the algorithm ( explicit / implicit ) were used for which experiments ? I believe explicit was used for Section 4.1 , and implicit for 4.2 but again this needs to be spelled out more clearly . I would also like to see a discussion ( and perhaps experimental comparison ) to standard preprocessing techniques , such as PCA - whitening .

( quality ) The experiments are interesting and seem well executed . Unfortunately , I do not think their scope ( single synthetic , plus a single UCI dataset ) is sufficient . While the gap in performance is significant on the synthetic task , this gap appears to shrink significantly when moving to the UCI dataset . How does this method perform for more realistic data , even e.g. MNIST ? I think it is crucial to highlight that the deficiencies of DIB matter in practice , and are not simply a theoretical consideration . Similarly , the representation analyzed in Figure 7 is promising , but again the authors could have targeted other common datasets for disentangling , e.g. the simple sprites dataset used in the beta - VAE paper . I would have also liked to see a more direct and systemic validation of the claims made in the paper . For example , the shortcomings of DIB identified in Section 3.1 , 3.2 could have been verified more directly by plotting I ( y , t ) for various monotonic transformations of x . A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion .

Pros :
* Theoretically well motivated
* Promising results on synthetic task
* Potential for impact
Cons :
* Paper suffers from lack of clarity ( method and experimental section )
* Lack of ablative / introspective experiments
* Weak empirical results ( small or toy datasets only ) .

We would like to thank the reviewer for the additional review . We respond to the questions and issues raised in the review below .




While Section 3.3 clearly defines the explicit form of the algorithm ( where data and labels are essentially pre-processed via a copula transform ) , details regarding the “ implicit form ” are very scarce . From Section 3.4 , it seems as though the authors are optimizing the form of the gaussian information bottleneck I ( x , t ) , in the hopes of recovering an encoder $ f_\beta ( x ) $ which gaussianizes the input ( thus emulating the explicit transform ) ? Could the authors clarify whether this interpretation is correct , or alternatively provide additional clarifying details ?

This seems to be a misunderstanding . The $ f_\beta$ transformation stands for an abstract , general transformation of the input data . In our model , it is implemented by the copula transformation ( explicit or implicit ) and the encoder network . $ f_\beta$ thus does not emulate the explicit transformation , and is not confined to representing the ( implicit or explicit ) copula transformation . The copula transformation , not necessarily implemented as a neural network , is a part of $ f_\beta$ .
The purpose of introducing $ f_\beta$ is to explain the difference of the model with and without the extra copula transformation and why applying the transformation translates to sparsity not observed in the “ regular ” sparse Gaussian information bottleneck .
We elaborate on the difference between the implicit and explicit copula in the answer to the last question .




There are also many missing details in the experimental section : how were the number of “ active ” components selected ?

The only parameter of our model is $ \lambda$ . As described in Section 3.4 , by continuously increasing $ \lambda$ , one decreases sparsity defined by the number of active neurons . Thus , one can adjust the number of active components by continuously varying $ \lambda$ ( curves in Figures 2 , 4 , 6 with increasing numbers of active components correspond to increasing $ \lambda$ ) .
The number of active components is chosen differently in different experiments . In Experiments 1 , 6 , 7 $ \lambda$ , and thus the number of active components , is varied over a large interval . In Experiment 3 , $ \lambda$ is also varied , and subsequently chosen so that the dimensionality of latent spaces in the two compared models is the same .




Which versions of the algorithm ( explicit / implicit ) were used for which experiments ? I believe explicit was used for Section 4.1 , and implicit for 4.2 but again this needs to be spelled out more clearly

As we mentioned in the rebuttal , throughout the paper as well as for the experiments , the explicit copula transformation defined in Eq. ( 6 ) is used . The explicit transformation is also the default choice of the form of the copula transformation .




I would also like to see a discussion ( and perhaps experimental comparison ) to standard preprocessing techniques , such as PCA - whitening .

PCA whitening , in contrast to the copula transformation , does not disentangle marginal distributions from the dependence structure captured by the copula . It also does not restore the invariance properties of the model we identified as motivation . It does not lead to a boost in information curves such as in Figure 2 ; we can add the appropriate experiment to our manuscript .




I do not think their [ experiments ’ ] scope ( single synthetic , plus a single UCI dataset ) is sufficient . While the gap in performance is significant on the synthetic task , this gap appears to shrink significantly when moving to the UCI dataset . How does this method perform for more realistic data , even e.g. MNIST ? I think it is crucial to highlight that the deficiencies of DIB matter in practice , and are not simply a theoretical consideration .
[ … ]
the representation analyzed in Figure 7 is promising , but again the authors could have targeted other common datasets for disentangling , e.g. the simple sprites dataset used in the beta- VAE paper .

We would like to stress that imposing sparsity on the latent representation is an important aspect of our model . It is in general difficult to quantify latent representations . Our model yields significantly sparser representations even when the information curves are closer .
Our model shows its full strength when a multiview analysis is involved , especially with data where multiple variables have different and rescaled distributions . Datasets constructed such that marginals ( or simply labels , such as in the MNIST dataset ) are uniform distributed do not pose enough challenge , since the output space is too easy to reconstruct even without the copula transformation .
As for dataset size , we would like to point out that finding meaningful sparse representations is more challenging for smaller datasets with higher dimensionality , therefore we think that the datasets we used do show the most relevant properties of the copula DIB .


I would have also liked to see a more direct and systemic validation of the claims made in the paper . For example , the shortcomings of DIB identified in Section 3.1 , 3.2 could have been verified more directly by plotting I ( y , t ) for various monotonic transformations of x.

We verified this for beta transformation in Experiment 1 . We observe that the impact of our method is most pronounced when different variables are transformed in possibly different ways ( i.e. when they are subject to diverse transformations with various scales ) .




A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion .

We mention the implicit copula transformation learned by neural networks in Section 3.3 for completeness as an alternative to the default explicit approach , but we would like to point out that the explicit approach is a preferred choice in practice .
In the same section ( in the revised paper ) , we elaborate on the few situations where the implicit copula might be advantageous , such as when there is a necessity of implicit tie breaking between data points . We also explain why the explicit copula is usually more advantageous . One circumvents the problem of devising an architecture capable of learning the marginal cdf , thus simplifying the neural network . Perhaps more importantly , the implicit approach does not scale well with dimensionality of the data , since the networks used for approximating the marginal cdf have to be trained independently for every dimension .


This paper claims to demystify MMD - GAN , a generative adversarial network with the maximum mean discrepancy ( MMD ) as a critic , by showing that the usual estimator for MMD yields unbiased gradient estimates ( Theorem 1 ) . It was noted by the authors that biased gradient estimate can cause problem when performing stochastic gradient descent , as also noted previously by Bellemare et al . The authors also proposed a kernel inception distance ( KID ) as a quantitative evaluation metric for GAN . The KID is defined to be the squared MMD between inception representation of the distributions . In experiments , the authors compared the quality of samples generated by MMD - GAN with various kernels with the ones generated from WGAN - GP ( Gulrajani et al. , 2017 ) and Cramer GAN ( Bellemare et al. , 2017 ) . The empirical results show the benefits of using the MMD on top of deep convolutional features .

The major flaw of this paper is that its contribution is not really clear . Showing that the expectation and gradient can be interchanged ( Theorem 1 ) does not seem to provide sufficient significance . Unbiasedness of the gradient alone does not guarantee that training will be successful and that the resulting models will better reflect the underlying data distribution , as evident by other successful variants of GANs , e.g. , WGAN , which employ biased estimate . Indeed , since the training process relies on a small mini-batch , a small bias could help counteract the potentially high variance of the gradient estimate . The key is rather a good balance of both bias and variance during the training process and a guarantee that the estimate is asymptotically unbiased wrt the training iterations . Lastly , I do not see how the empirical results would demystify MMD - GANs , as claimed by the paper .

The paper is clearly written .

Some minor comments :

- The proof of the main result , Theorem 1 , should be placed in the main paper .
- Page 7 , 2nd paragraph : later --> layer

Thanks for your comments , and please also see our comments about improvements in the new revision above .

You are certainly correct that an unbiased gradient does not guarantee that training will succeed ; our recent revision also substantially clarifies the bias situation . However , in SGD the bias-variance tradeoff is somewhat different than the situation in e.g. ridge regression , where the regularization procedure adds some bias but also reduces variance enough that it is worthwhile . There does n't seem to be any reason to think that the gradient variance is any higher for MMD GANs than for WGANs , and so a direct analogy does n't quite apply . Also , when performing SGD , the biases of each step might add up over time , and so – as in Bellemare et al . 's example – following biased gradients is worth at least some level of concern .

With regards to the rest of the contribution : the title " demystifying " was intended more for the earlier parts of the paper , which elucidate the relationship of MMD GANs to other models and ( especially in the revision ) clarify the nature of the bias argument of Bellemare et al . The empirical results perhaps do not directly " demystify , " but rather bring another level of understanding of these models .

The quality and clarity of this work are very good . The introduction of the kernel inception metric is well - motivated and novel , to my knowledge . With the mention of a bit more related work ( although this is already quite good ) , I believe that this could be a significant resource for understanding MMD GANs and how they fit into the larger model zoo .

Pros
- best description of MMD GANs that I have encountered
- good contextualization of related work and descriptions of relationships , at least among the works surveyed
- reasonable proposed metric ( KID ) and comparison with other scores
- proof of unbiased gradient estimates is a solid contribution

Cons
- although the review of related work is very good , it does focus on ~ 3 recent papers . As a review , it would be nice to see mention ( even just in a list with citations ) of how other models in the zoo fit in
- connection between IPMs and MMD gets a bit lost ; a figure ( e.g. flow chart ) would help
- wavers a bit between proposing / proving novel things vs . reviewing and lacks some overall structure / storyline
- Figure 1 is a bit confusing ; why is KID tested without replacement , and FID with ? Why 100 vs 10 samples ? The comparison is good to have , but it 's hard to draw any insight with these differences in the subfigures . The figure caption should also explain what we are supposed to get out of looking at this figure .

Specific comments :
- I suggest bolding terms where they are defined ; this makes it easy for people to scan / find ( e.g. Jensen - Shannon divergence , Integral Probability Metrics , witness functions , Wasserstein distance , etc . )
- Although they are common knowledge in the field , because this is a review it could be helpful to provide references or brief explanations of e.g. JSD , KL , Wasserstein distance , RKHS , etc .
- a flow chart ( of GANs , IPMs , MMD , etc. , mentioning a few more models than are discussed in depth here , would be *very * helpful .
- page 2 , middle paragraph , you mention " ... constraints to ensure the kernel distribution embeddings remained injective " ; it would be helpful to add a sentence here to explain why that 's a good thing .


Thanks for your comments . We 've posted a new revision addressing most of them ; see also our separate comment describing other significant improvements .

- Review of related work : thanks for the suggestion . We have added a brief section 2.4 with some more related work ; we would be happy to add more if you have some other suggestions .

- We have attempted to slightly clarify the description of IPMs in this revision , and will further consider better ways to do this .

- KID / FID comparison figure : We agree that this difference is confusing . It was done because the standard KID estimator becomes biased when there are repeated points due to sampling with replacement , but of course when sampling 10,000 / 10,000 points without replacement , it is unsurprising that there is no variance in the estimate , so it made more sense for the point we were trying to make to evaluate FID with replacement . The difference in number of samples was due to the relatively higher computational expense of the FID ( which requires the SVD of a several thousand dimensional - matrix ) , but we have increased that to the same number of samples as well . The figures look essentially identical changing either of these issues ; we have changed to using a variant of the KID estimator which is still unbiased for samples with replacement and clarified the caption .

- We have added a footnote on why injectivity of the distribution embeddings is desirable .

The main contribution of the paper is that authors extend some work of Bellemare : they show that MMD GANs [ which includes the Cramer GAN as a subset ] do possess unbiased gradients . They provide a lot of context for the utility of this claim , and in the experiments section they provide a few different metrics for comparing GANs [ as this is a known tricky problem ] . The authors finally show that an MMD GAN can achieve comparable performance with a much smaller network used in the discriminator .

As previously mentioned , the big contribution of the paper is the proof that MMD GANs permit unbiased gradients . This is a useful result ; however , given the lack of other outstanding theoretical or empirical results , it almost seems like this paper would be better shaped as a theory paper for a journal . I could be swayed to accept this paper however if others feel positive about it .



Thanks for your comments . We do feel that this paper has contributions outside just the proof of unbiased gradients , in particular clarifying the relationship among various slightly - different GAN models , the KID score , and the new experimental results about success with smaller critic networks , which are of interest to the ICLR community .

Please also see our general comments about the new revision above , which includes substantial improvements .


Revised Review :

The authors have largely addressed my concerns with the revised manuscript . I still have some doubts about the C > N setting ( the new settings of C / N of 4 and 2 are n't C >> N , and the associated results are n't detailed clearly in the paper ) , but I think the paper warrants acceptance .

Original Review :

The paper proposes fixing the classification layers of neural networks , replacing the traditional learned affine transformation with a fixed ( e.g. , Hadamard ) matrix . This is motivated by the observation that classification layers frequently constitute a non-trivial fraction of a network 's overall parameter count , compute requirements , and memory usage , and by the observation that removal of pre-classification fully - connected layers has often been found to have minimal impact on performance . Experiments are performed on a range of datasets and network architectures , in both image classification and NLP settings .

First , I 'd like to note that the empirical component of this paper is strong : I was impressed by the breadth of architectures and settings covered , and the experiments left me reasonably convinced that the classification layer can often be fixed , at least for image classification tasks , without significant loss of accuracy .

I have two general concerns . For one , removing the fully connected classification layer is not a novel idea ; All Convolutional Networks ( https://arxiv.org/abs/1412.6806) reported excellent results without an additional fully connected affine transform ( just a global average pooling after the last convolutional layer ) . I think it would be worth at least referencing / discussing differences with this and other all - convolutional architectures . Including a fixed Hadamard matrix for the classification layer is I believe new ( although related to an existing literature on using structured matrices in neural networks ) .

However , I have doubts about the ability of the approach to scale to problems with a larger number of classes , which arguably is a primary motivation of the paper ( " parameters ... grow linearly with the number of classes " ) . Specifically , the idea of using a fixed N x C matrix with C orthogonal columns ( such as Hadamard ) is only possible when N > C . This is a critical point : in the N > C regime , a final hidden representation with N dimensions can be chosen to achieve * any * C-dimensional output , regardless of the projection matrix used ( so long as it is full rank ) . This makes it seem fairly reasonable to me that the network can ( at least approximately , and complicated by the ReLU nonlinearities ) fold the " desired " classification layer into the previous layer , especially with a learned scaling and bias term . In fact it 's not clear to me that the fixed classification layer accomplishes anything here , beyond projecting from N -> C ( i.e. , if N = C , I 'd guess it could be removed entirely similar to all convolutional nets , as long as the learned scaling and bias were retained ) .

On the other hand , when C > N , it is not possible to have mutually orthogonal columns , and in general the output is constrained to lie in an N-dimensional subspace of the overall C-dimensional output space . Picking somewhat randomly a * fixed * N-dimensional subspace seems like a bad idea when N << C , since it is unlikely to select a subspace in which it is possible to adequately capture correlations between the different classes . This makes the proposed technique much less appealing for precisely the family of problems where it would be most effective in reducing compute / memory requirements . It also provides ( in my view ) a clearer explanation for the failure of the approach in the NLP setting . These issues were not discussed anywhere in the text as far as I can tell , and I think it 's necessary to at least acknowledge that mutually orthogonal columns ca n't be chosen when C > N in section 2.2 ( and probably include a longer discussion on the probable implications ) .

Overall , I think the paper provides a useful observation that clearly is n't common knowledge , since classification layers persist in many popular recent architectures . But the notion of fixing or removing the classification layer is n't particularly novel , and I do n't believe the proposed technique would scale well to settings with many classes . As is I think the paper falls slightly short .


We thank the reviewer for his detailed feedback on our paper .
We hope to address the 2 main concerns raised :
1 ) Novelty - " removing the fully connected classification layer is not a novel idea ; All Convolutional Networks ( https://arxiv.org/abs/1412.6806) reported excellent results without an additional fully connected affine transform ( just a global average pooling after the last convolutional layer ) "

We believe there is a slight misunderstanding here : in the " All convolutional networks " paper the fully - connected was not removed , as it just got replaced with a convolutional layer with the same number of parameters . This means there is still a final classifier ( a conv layer ) with number of parameters proportional to the number of classes .
Our work introduces what we believe to be a novel idea - removing the classifier layer altogether making the number of network parameters independent from the number of classes . We added a clarification to this matter in our recent revision .

2 ) Applicability of our method when C > N :

The reviewer is right in his claim that when C > N we can not have mutually orthogonal columns , but this is true even for a fully learned weight matrix .
We empirically verified that for the vision use-cases brought in the paper we achieve good performance for C > N ( e.g. , on imagenet , so C=1000 , with either mobilenet 0.5 where N = 512 or resnet with N reduced to 256 ) .
We do agree with the reviewer that this can be limiting when the classes have strong correlation with one another ( as in the NLP case ) and we add this as another possible explanation . We still , however , feel that this can be useful even for C >> N in other domains such as vision .

The paper proposes to use a fixed weight matrix to replace the final linear projection in a deep neural network .
This fixed classifier is combined with a global scaling and per output shift that are learned .
The authors claim that this can be used as a drop in replacement for standard architectures and does not result in reduced performance .
The key advantage is that it generates a reduction in parameters ( e.g. for resent 50 8 % of parameters are eliminated ) .

The idea is extremely simple and I like it conceptually .
Currently it looks like my reimplementation on resent 50 is working .
I do lose a about 1 % in accuracy compared to my baseline learned projection implementation .
Is the scale and bias regularized ?

I have assigned a score of 6 now . but I will wait for my final rating when I get the actual results .
Overall the evaluation is seems reasonably thorough many tasks were presented and the model was applied to different architectures .

I also think the manuscript could benefit from the following experiments :
- how does the chosen projection matrix affect performance .
- is the scale needed
I assume the authors did these experiments when they developed the method but it is unclear how important these choices are .
Including these experiments would make it a more scientific contribution .

The amount of computation saved seems rather limited ? Especially since the gradient of the scale parameter has to go through the weight vector ?
Therefore my assumption is that only the application of the gradients save a limited amount of time and memory ?
At least in my experiments reproducing these results , the computational benefit is not there / obvious .

While I like the idea , the way the manuscript is written is a bit strange at times .
The introduction appears to be there to be because you need a introduction , not to explain the background .
For this reason some of the cited work seems a bit out of place .
Especially the universal approximation and data memorization references .
What I find interesting is that this work is the complement of the reservoir computing / extreme learning machines approach .
There the final output layer is trained but the network itself uses random weights .

It would be nice if Fig 2 had a better caption . Which dataset , model , … .
Is there an intuition why the training error remains higher but the validation error is identical ? This is difficult to get my head round .
Also , it would be nice if an analysis was provided where the computational cost of not doing the gradient update was computed .


We thank the reviewer for his detailed feedback on our paper and his suggestions . We hope to answer his questions below . We also made adjustments to latest revision accordingly .

1 ) " Are the scale and bias regularized ? " - Yes . We found that regularization can help with the final validation error in the same way it helps with common learned weights . Best results appeared when trained with weight decay for several epochs and removed later .

2 ) " how does the chosen projection matrix affect performance " - We found no significance change in final accuracy when using different projection matrix . We do find slight change in convergence rate when initial scale is changed .

3 ) " is the scale needed " - We added some experiments to show that the scale is not needed as a learned parameter , but this may help convergence .

4 ) " The amount of computation saved seems rather limited ? " - The compute saved is for the gradient of the classifier weights ( which is not needed to get the gradient for the scale ) . This may be limited for the cases shown , but becomes more apparent when number of classes is larger . As we noted , these gradients and weights can now be avoided in communication over several nodes in distributed setting - saving precious bandwidth . Moreover using a Hadamard matrix we can replace all multiplication operations preformed by the classifier with additions which are far more hardware friendly .

5 ) " Is there an intuition why the training error remains higher but the validation error is identical ? " - Our conjecture is that with our new fixed parameterization , the network can no longer increase the norm of a given sample 's representation - thus learning its label requires more effort . As this may happen for specific seen samples - it affects only training error .

Regarding clarity and manuscript structure - we have taken the reviewer 's comments into account and revised our paper accordingly .



This paper proposes replacing the weights of the final classifier layer in a CNN with a fixed projection matrix . In particular a Hadamard matrix can be used , which can be represented implicitly .

I 'd have liked to see some discussion of how to efficiently implement the Hadamard transform when the number of penultimate features does not match the number of classes , since the provided code does not do this .

How does this approach scale as the number of classes grows very large ( as it would in language modeling , for example ) ?

An interesting experiment to do here would be to look this technique interacts with distillation , when used in the teacher or student network or both . Does fixing the features make it more difficult to place dog than on boat when classifying a cat ? Do networks with fixed classifier weights make worse teachers for distillation ?


We thank the reviewer for his feedback and suggestions . We added an explanation as well as extended the supplementary code for the case where number of penultimate features does not match the number of classes .
We also added to the discussion the case where C >> N. Regarding distillation - we found no apparent difference when distilling a network with fixed classifier .

The paper is well written . The proposal is explained clearly .
Although the technical contribution of this work is relevant for network learning , several key aspects are yet to be addressed thoroughly , particularly the experiments .

Will there be any values of alpha , beta and gamma where eq ( 8 ) and eq ( 9 ) are equivalent . In other words , will it be the case that SharedPerturbation ( alpha , beta , gamma , N ) = Flipout ( alpha1 , beta1 , gamma1 , N1 ) for some choices of alpha , alpha1 , beta , beta1 , ...? This needs to be analyzed very thoroughly because some experiments seem to imply that Flip and NoFlip are giving same performance ( Fig 2 ( b ) ) .
It seems like small batch with shared perturbation should be similar to large batch with flipout ?
Will alpha and gamma depend on the depth of the network ? Can we say anything about which networks are better ?
It is clear that the perturbations E1 and E2 are to be uniform +/- 1 . Are there any benefits for choosing non-uniform sampling , and does the computational overhead of sampling them depend on the network depth / size .

The experiments seem to be inconclusive .
Firstly , how would the proposed strategy work on standard vision problems including learning imagenet and cifar datasets ( such experiments would put the proposal into perspective compared to dropout and residual net type procedures ) ?
Secondly , without confidence intervals ( or significance tests of any kind ) , it is difficult to evaluate the goodness of Flipout vs . baselines , specifically in Figures 2 ( b , d ) .
Thirdly , it is known that small batch sizes give better performance guarantees than large ones , and so , what does Figure 1 really imply ? ( Needs more explanation here , relating back to description of alpha , beta and gamma ; see above ) .


Thank you for your careful and insightful feedback .

--> Q : Will there be any values of alpha , beta and gamma where eq ( 8 ) and eq ( 9 ) are equivalent ? Will alpha and gamma depend on the depth of the network ? Can we say anything about which networks are better ?

Mathematically , eqns . ( 8 ) and ( 9 ) are equivalent if alpha = 0 , and they are nearly identical if beta or gamma dominates . However , we did not observe any examples of either case in any of our experiments . ( In fact , beta was indistinguishable from 0 in all of our experiments . ) Based on Figure 1 , the values seem fairly consistent between very different architectures .

--> Q : FlipES does n’t outperform NaiveES in Figure 2 .

Here , NaiveES refers to fully independent perturbations , rather than a single shared perturbation . Hence , it is an upper bound on how well FlipES should perform , and indeed FlipES achieves this with a much faster wall - clock time ( see Fig. 5 in the appendix , cpuES corresponds to noFlip ) . For clarity , we will rename NaiveES to be IdealES in the final version .

--> Q : Shared perturbation with small batch should be similar to large batch with flipout ?

The reason to train with large batches is to take advantage of parallelism . ( If we only cared about the number of arithmetic operations , we ’d all use batch size 1 . ) The size of this benefit depends on the hardware , and hardware trends ( more GPU cores , TPUs ) strongly favor increasing batch sizes . Currently , one may sometimes be able to use small batches to compensate for the inefficiency of shared perturbations , but this is a band - aid which wo n’t remain competitive much longer .

--> Q : Can we use non-uniform E1 , E2 and does the computational overhead of sampling depend on the network depth ?

Yes , Proposition 1 certainly allows for non-uniform E1 and E2 , although the advantage of this is unclear . In principle , sampling E1 and E2 ought not to be very expensive compared to the matrix multiplications . However , the overhead can be significant if the framework implements it inefficiently ; in this case , one can use the trick in Footnote 1 .

--> Q : Will the proposed strategy work on standard vision problems including ImageNet and CIFAR ?

Our experiments include CIFAR - 10 , and we see no reason why flipout should n’t work on ImageNet . Weight perturbations are not currently widely used in vision tasks , but if that changes , flipout ought to be directly applicable . Our experiments focus on Bayesian neural nets and ES , which inherently require weight perturbations .

Additionally , it was shown that DropConnect ( which is a special case of weight perturbation , as we show in Sec. 2.1 ) regularizes LSTM - based word language models and achieves SOTA on several tasks [ 1 ] . Flipout can be directly applied to it , and we show in Appendix E.4 that flipout reduces the stochastic gradient variance compared to [ 1 ] .

--> Q : Small batch sizes give better performance , what does Fig. 1 imply ?

We ’re not sure what you mean by this . Due to the variance reduction effects of large batches , one typically uses as large a batch as will fit on the GPU , and sometimes resorts to distributed training in order to use even larger batches . ( A batch size of 1 is optimal if you only count the number of iterations , but this is n’t a realistic model , even on a single CPU . )

[ 1 ] Merity , Stephen , Keskar , Nitish S. , and Socher , Richard . " Regularizing and optimizing LSTM language models . " arXiv preprint arXiv :1708.02182 ( 2017 ) .

Typical weight perturbation algorithms ( as used for e.g. Regularization , Bayesian NN , Evolution
Strategies ) suffer from a high variance of the gradient estimates . This is caused
by sharing a weight perturbation by all training examples in a minibatch . More specifically
sharing perturbed weights over samples in a minibtach induces correlations between gradients of each sample , which can
not be resolved by standard averaging . The paper introduces a simple idea , flipout , to
perturb the weights quasi-independently within a minibatch : a base perturbation ( shared
by all sample in a minibatch ) is multiplied by a random rank - one sign matrix ( different
for every sample ) . Due to its special structure it is possible to vectorize this
per-sample -operation such that only matrix - matrix products ( as in the default forward
propagation ) are involved . The incurred computational cost is roughly twice as much
as a standard forward propagation path . The paper also proves that this approach
reduces the variance of the gradient estimates ( and in practice , flipout should
obtain the ideal variance reduction ) . In a set of experiments it is demonstrated
that a significant reduction in gradient variance is achieved , resulting
in speedups for training time . Additionally , it is demonstrated that
flipout allows evolution strategies utilizing GPUs .

Overall this is a very nice paper . It clearly lays out the problem , describes
one solution to it and shows both theoretically as well as empirically
that the proposed solution is a feasable one . Given the increasing importance
of Bayesian NN and Evolution Strategies , flipout is an important contribution .

Quality : Overall very well written . Relevant literature is covered and an important
problem of current research in ML is tackled .

Clarity : Ideas / Reasons are clearly presented .

Significance : The presented work is highly significant for practical applicability
of Bayesian NN and Evolution Strategies .

Thank you for your positive comments and for recognizing the work !

In this article , the authors offer a way to decrease the variance of the gradient estimation in the training of neural networks .
They start in the Introduction and Section 2 by explaining the multiple uses of random connection weights in deep learning and how the computational cost often restricts their use to a single randomly sampled set of weights per minibatch , which results to higher - variance gradient estimatos than could be achieved otherwise . In Section 3 the authors offer to get the benefits of multiple weights without most of the cost , when the distribution of the weights is symmetric and fully factorized , by multiplying sampled - once random perturbations of the weights by a rank - 1 random sign matrix . This efficient mechanism is only twice as costly as a single random perturbation , and the authors show how to efficiently parallelize it on GPUs , thereby also allowing GPU - ization of evolution strategies ( something so far difficult toachieve ) . Of note , they provide a theoretical analysis in Section 3.2 , proving the actual variance reduction of their efficient pseudo-sampling scheme . In Section 4 they provide quite varied empirical analysis : they confirm their theoretical results on four architectures ; they show its use it to regularise on language models ; they apply it on large minibatch settings where high variance is a main problem ; and on evolution strategies .

While it is a rather simple idea which could be summarised much earlier in the single equation ( 3 ) , I really like the thoroughness and the clarity of the exposure of the idea . Too many papers in our community skimp on details and on formalism , and it is a delight to see things exposed so clearly -- even accompanied with a proof .

However , the painful part : while I am convinced by the idea and love its detailed exposure , and the gradient variance reduction is made very clear , the experimental impact in terms of accuracy ( or perplexity ) is , sadly , not very convincing . Nowhere in the text did I find a clear rationale of why it is beneficial to reduce the variance of the gradient . The numerical results in Table 1 and Table 2 also do not show a clear improvement : Flipout does not provide the best accuracy . The gain in wall clock could be a factor , but would need to be measured on the figures more clearly . And the validation errors in Figure 2 for Evolution strategies seem to be worse than backprop . The main text itself also only claims performance “ comparable to the other methods ” . The only visible gain is on the lower part Figure 2.a on a ConvNet .

This makes me wonder if the authors could do a better job of putting forward the actual advantages of their methods on the end-results : could wall clock measure be put more forward , to justify the extra work ? This would , in my mind , strongly improve the case for publication of this article .


A few improvement suggestions :
* Could put earlier more emphasis of superiority to Local Reparameterization Trick in terms of architecture , not wait until Section 2.2 and section 4.1
* Should also put more emphasis on limitations , not wait until 3.1.
* Proposition 1 is quite straightforward , not sure it deserves a proposition , but it ’s elegant to put it forward .
* Footnote 1 on re-using the matrices is indeed practical , but also somewhat surprising in terms of bias risks . Could it be explained in more depth , maybe by the random permutations of the minibatches making the bias non systematic and cancelling out ?
* Theorem 1 : For readability could merge the expectations on the joint distribution as E_{x , \hat \delta W} , rather than separate expectations with the conditional distributions .
* Theorem 1 : could the authors provide a clearer intuitive explanation of the \beta term alone , not only as part of \alpha + \beta , especially as it plays such a key role , being the only one that does not disappear ? And how do they explain their empirical observation that \beta is close to 0 ? Any intuition on that ?
* Experiments : I salute the authors for providing all the details in exhaustive manner in the Appendix . Very commendable .
* Experiments : I like the empirical verification of the theory . Very neat to see .

Minor typo :
* page 2 last paragraph , “ evolution strategies ” is plural but the verbs are used in singular ( “ is black box ” , “ It does n’t ” , “ generates ” )


Thank you for your careful and insightful feedback .

-> Q : Why is it beneficial to reduce the variance of the gradient , flipout does n’t provide the best accuracy , wall - clock time advantage ?

The importance of variance reduction in SGD is well - established . Variance reduction is the whole reason for using batch sizes larger than 1 , and some well - known works [ 1 ] have found that with careful implementation , the variance - reducing effect of large batches translates into a linear optimization speedup . Whether this relationship holds in a particular case depends on a whole host of configuration details which are orthogonal to our paper ; e.g. the aforementioned paper had to choose careful initializations and learning rate schedules in order to achieve it . Still , we can confidently say there is no hope for unlocking the optimization benefits of large batches unless one uses some scheme ( such as flipout ) that enables variance reduction .

Note that we do in fact observe significant optimization benefits ( flipout converges 3X faster in terms of iterations ) , as shown in Figure 2 ( a ) . Additionally , although flipout is 2X more computationally expensive in theory , it can be implemented more efficiently in practice . For example , we can send each of the two matmul calls to a separate TPU chip , so they are done in parallel . Communication will add overhead but it should n’t be much , as it is only two chips communicating and the messages are matrices of size [ batch_size , hidden_size ] rather than the set of full weights .

With respect to the advantages offered by flipout for training LSTMs , we have conducted new experiments to compare regularization methods , and have updated Section 4.2 and the results in Table 2 . We found that using flipout to implement DropConnect for recurrent regularization yields strong results , and significantly outperforms the other methods in both validation and test perplexity . For our original word - level LSTM experiments , we used the setup of [ 2 ] , with a fixed learning schedule that decays the learning rate by a factor of 1.2 each epoch starting after epoch 6 . In our new experiments , we decay the learning rate by a factor of 4 based on the nonmonotonic criterion introduced in [ 3 ] ; the perplexities of all methods except the unregularized LSTM are reduced compared to the previous experiments . Using flipout to implement DropConnect allows us to use a different DropConnect mask per example in a batch efficiently ( compared to [ 3 ] , which shares the weights between all examples ) .

We also added Appendix E.4 , which shows that using flipout with DropConnect yields significant variance reduction and faster training compared to using a shared DropConnect mask for all examples ( as is done in [ 3 ] ) .

-> Q : ES seems to be worse than backprop .

We ’re not advocating for ES to replace backprop . The main comparison in this section is between NaiveES and FlipES ; we show that FlipES behaves like NaiveES , but is more efficient due to parallelism . Our reason for including the backprop comparison is to show that this is an interesting regime to investigate . One might have thought that ES would hopelessly underperform backprop ( since the latter uses gradients ) , but in fact FlipES turns out to be competitive . The reason this result is interesting is that unlike backprop , ES can also be applied to non-differentiable models .

-> Q : Footnote 1 with bias risk .

The trick in Footnote 1 does not introduce any bias . Proposition 1 implies that the gradients are unbiased for any distribution over E which is independent of Delta W. This applies in particular to deterministic E ( which is trivially independent ) , so E can be fixed throughout training . Such a scheme may not achieve the full variance reduction , but it is at least unbiased . Note that we do not use this trick in our experiments .

-> Q : Why is it close to 0 in practice , intuitive explanation of beta term ?

Beta is the estimation variance when E is marginalized out . We ’d expect this term to be much smaller than the full variance because it ’s marginalizing over a symmetric perturbation distribution , so the perturbations in opposite directions should cancel . The finding that it was so close to zero was a pleasant surprise .

We also thank the reviewer for the suggestions for improvement . We will revise the final version to take them into account .


[ 1 ] Goyal , Priya , et al . " Accurate , large minibatch SGD : Training ImageNet in 1 hour . " arXiv preprint arXiv:1706.02677 ( 2017 )
[ 2 ] Yarin Gal and Zoubin Ghahramani . A theoretically grounded application of dropout in recurrent
neural networks . In Advances in Neural Information Processing Systems ( NIPS ) , pp. 1019–1027 , ( 2016 ) .
[ 3 ] Merity , Stephen , Keskar , Nitish S. , and Socher , Richard . " Regularizing and optimizing LSTM language models . " arXiv preprint arXiv :1708.02182 ( 2017 ) .

This paper presents a tree- to- tree neural network for translating programs
written in one Programming language to another . The model uses soft attention
mechanism to locate relevant sub -trees in the source program tree when
decoding to generate the desired target program tree . The model is evaluated
on two sets of datasets and the tree - to- tree model outperforms seq2tree and
seq2seq models significantly for the program translation problem .

This paper is the first to suggest the tree - to- tree network and an interesting
application of the network for the program translation problem . The evaluation
results demonstrate the benefits of having both tree - based encoder and decoder .
The tree encoder , however , is based on the standard Tree -LSTM and the application
in this case is synthetic as the datasets are generated using a manual rule - based
translation .

Questions / Comments for authors :

The current examples are generated using a manually developed rule - based system .
As the authors also mention it might be challenging to obtain the aligned examples
for training the model in practice . What is the intended use case then of
training the model when the perfect rule - based system is already available ?

How complex are the rules for translating the programs for the two datasets and what
type of logic is needed to write such rules ? It would be great if the authors can
provide the rules used to generate the dataset to better understand the complexity
of the translation task .

There are several important details missing regarding the baselines . For the
seq2seq and seq2tree baseline models , are bidirectional LSTMs used for the encoder ?
What type of attention mechanisms are used ? Are the hyper- parameters presented in
Table 1 based on best training performance ?

In section 4.3 , it is mentioned that the current models are trained and tested on
programs of length 20 and 50 . Does the dataset contain programs of length upto
20/50 or exactly of length 20/50 ? How is program length defined -- in terms of
tree nodes or the number of lines in the program ?

What happens if the models trained with programs upto length 20 are evaluated on
programs of larger length say 40 ? It would be interesting to observe the
generalization capabilities of all the different models .

There are two benefits of using the tree2 tree model : i ) use the grammar of the
language , and ii ) use the structure of the tree for locating relevant sub- trees
( using attention ) . From the current evaluation results , the empirical benefit
of using the attention is not clear . How would the accuracies look when using
the tree2 tree model without attention or when attention vector e_t is set to the
hidden state h of the expanding node ?


Thank you for your valuable comments ! We respond to the questions below .

The reviewer asks about the meaning of the two program translation tasks studied in our work . We have explained that this is a first step to understand the problem of using a deep neural network approach to solve the program translation problem , and we consider the more challenging task without aligned input- output pairs as an important future direction . Also , although we do not evaluate it in our work , we believe the study of tree - to - tree translation model may have applications to other tree - to - tree translation tasks .

The CoffeeScript-to-JavaScript compiler is available online , which is too complex to explain in the paper . We have added the code of our translator between two synthetic languages in the appendix .

To clarify some implementation details , our seq2seq model and seq2tree model faithfully implement [ 1 ] and [ 2 ] . In particular , we only use uni-directional LSTMs for the encoder , and the attention mechanism is the same as described in the original papers as well . We did grid search for hyper - parameters , and chose the best one based on their performance on the validation set .

A program ’s length is defined to be the total number of tokens in the program , and is guaranteed to be equal to 20/50 .

When we train the model on shorter programs ( e.g. , programs of length 20 ) , then evaluate on longer programs ( e.g. , programs of length 50 ) , the test accuracy is 0 for all models , including our proposed tree - to - tree model and the baseline models . We consider solving the generalization issue as the next important problem that we want to address in the future .

We have clarified all above details in our revised version as well .

We are running more experiments to provide a complete ablation study to understand the effectiveness of attention . In some preliminary results , we observe that the performance decreases dramatically when attention is not used . We will update the results once we finish the experiments .

[ 1 ] Oriol Vinyals , Lukasz Kaiser , Terry Koo , Slav Petrov , Ilya Sutskever , Geoffrey Hinton . Grammar as a foreign language . NIPS 2015 .
[ 2 ] Li Dong and Mirella Lapata . Language to logical form with neural attention . ACL 2016 .


We have updated the revision to include results of our tree2 tree model without the attention mechanism , and observe that the performance decreases significantly . In particular , the program accuracy drops to nearly 0 % . More details can be found in the paper .

This paper aims to translate source code from one programming language to another using
a neural network architecture that maps trees to trees . The encoder uses an upward pass of
a Tree LSTM to compute embeddings for each subtree of the input , and then the decoder
constructs a tree top-down . As nodes are created in the decoder , a hidden state is passed
from parents to children via an LSTM ( one for left children , one for right children ) , and
an attention mechanism allows nodes in the decoder to attend to subtrees in the encoder .

Experimentally , the model is applied to two synthetic datasets , where programs in the
source domain are sampled from a PCFG and then translated to the target domain with a
hand - coded translator . The model is then trained on these pairs . Results show that the
nproposed approach outperforms sequence representations or serialized tree representations
of inputs and outputs .

Pros :

- Nice model which seems to perform well .

- Reasonably clear explanation .

A couple questions about the model :

- the encoder uses only bottom - up information to determine embeddings of subtrees . I wonder
if top -down information would create embeddings with more useful information for the attention
in the decoder to pick up on .

- I would be interested to know more details about how the hand - coded translator works . Does
it work in a context - free , bottom - up fashion ? That is , recursively translate two children nodes
and then compute the translation of the parent as a function of the parent node and
translations of the two children ? If so , I wonder what is missing from the proposed model
that makes it unable to perfectly solve the first task ?

Cons :

- Only evaluated on synthetic programs , and PCFGs are known to generate unrealistic programs ,
so we can only draw limited conclusions from the results .

- The paper overstates its novelty and does n't properly deal with related work ( see below )

The paper overstates its novelty and has done a poor job researching related work .
Statements like " We are the first to consider employing neural network approaches
towards tackling the problem [ of translating between programming languages ] " are
obviously not true ( surely many people have * considered * it ) , and they 're particularly
grating when the treatment of related work is poor , as it is in this paper . For example ,
there are several papers that frame the code migration problem as one of statistical
machine translation ( see Sec 4.4 of [ 1 ] for a review and citations ) , but this paper
makes no reference to them . Further , [ 2 ] uses distributed representations for the purpose
of code migration , which I would call a " neural network approach , " so there 's not any
sense that I can see in which this statement is true . The paper further says , " To the best
of our knowledge , this is the first tree - to - tree neural network architecture in the
literature . " This is worded better , but it 's definitely not the first tree - to- tree
neural network . See , e.g. , [ 3 , 4 , 5 ] , one of which is cited , so I 'm confused about
this claim .

In total , the model seems clean and somewhat novel , but it has only been tested on
unrealistic synthetic data , the framing with respect to related work is poor , and the
contributions are overstated .


[ 1 ] https://arxiv.org/abs/1709.06182
[ 2 ] Trong Duc Nguyen , Anh Tuan Nguyen , and Tien N Nguyen. 2016b . Mapping API elements for code migration with
vector representations . In Proceedings of the International Conference on Software Engineering ( ICSE ) .
[ 3 ] Socher , Richard , et al . " Semi-supervised recursive autoencoders for predicting sentiment distributions . " Proceedings of the conference on empirical methods in natural language processing . Association for Computational Linguistics , 2011 .
[ 4 ] https://arxiv.org/abs/1703.01925
[ 5 ] Parisotto , Emilio , et al . " Neuro-symbolic program synthesis . " arXiv preprint arXiv:1611.01855 ( 2016 ) .


Thank you for the valuable comments !

Thank you for pointing out these related work ! We have revised our paper to carefully compare with more prior work . From a high level , the papers cited in Section 4.4 of [ 1 ] are not neural network models ; [ 2 ] uses word2vec , which is simply to learn a lookup table . Therefore , we also do not consider [ 2 ] as a deep neural network approach ; [ 3 , 4 ] propose tree -structured autoencoder , which is a generative model , rather than a translation model . The key difference is that a translation model has access to the source tree , while a generative model does not . Therefore , we think it is fair to claim our work as “ the first deep neural network approach for the tree - to - tree translation problem . ”

In addition , the reviewer mentions [ 5 ] as a tree - to - tree model , which is definitely not true . In fact , [ 5 ] is a sequence - to - tree model : the input of the model proposed in [ 5 ] is a sequence rather than a tree .

We clarify the questions below .

We use the bottom - up fashion to aggregate the information so that each tree node contains all information of its descendants . Propagating information from top to bottom does not match our intuition that the attention is allocated based on the sub-trees of the source tree .

The hand - coded translator is in a bottom - up fashion , but not context - free . To construct some parents , the translator may need to manipulate its two children . We have added the code of our translator for the synthetic task in the appendix .


Authors proposed a neural network based machine translation method between two programming languages . The model is based on both source / target syntax trees and performs an attentional encoder - decoder style network over the tree structure .

The new things in the paper are the task definition and using the tree-style network in both encoder and decoder . Although each structure of encoder / decoder / attention network is based on the application of some well - known components , unfortunately , the paper pays much space to describe them . On the other hand , the whole model structure looks to be easily generalized to other tree - to - tree tasks and might have some potential to contribute this kind of problems .

In experimental settings , there are many shortages of the description . First , it is unclear that what the linearization method of the syntax tree is , which could affect the final model accuracy . Second , it is also unclear what the method to generate train / dev / test data is . Are those generated completely randomly ? If so , there could be many meaningless ( e.g. , inexecutable ) programs in each dataset . What is the reasonableness of training such kind of data , or are they already avoided from the data ? Third , the evaluation metrics " token / program accuracy " looks insufficient about measuring the correctness of the program because it has sensitivity about meaningless differences between identifier names and some local coding styles .

Authors also said that CoffeeScript has a succinct syntax and Javascript has a verbose one without any agreement about what the syntax complexity is . Since any CoffeeScript programs can be compiled into the corresponding Javascript programs , we should assume that CoffeeScript is the only subset of Javascript ( without physical difference of syntax ) , and this translation task may never capture the whole tendency of Javascript . In addition , authors had generated the source CoffeeScript codes , which seems that this task is only one of " synthetic " task and no longer capture any real world 's programs .
If authors were interested in the tendency of real program translation task , they should arrange the experiment by collecting parallel corpora between some unrelated programming languages using resources in the real world .

Global attention mechanism looks somewhat not suitable for this task . Probably we can suppress the range of each attention by introducing some prior knowledge about syntax trees ( e.g. , only paying attention to the descendants in a specific subtree ) .

Suggestion :
After capturing the motivation of the task , I suspect that the traditional tree - to- tree ( also X - to- tree ) " statistical " machine translation methods still can also work correctly in this task . The traditional methods are basically based on the rule matching , which constructs a target tree by selecting source / target subtree pairs and arranging them according to the actual connections between each subtree in the source tree . This behavior might be suitable to transform syntax trees while keeping their whole structure , and also be able to treat the OOV ( e.g. , identifier names ) problem by a trivial modification . Although it is not necessary , it would like to apply those methods to this task as another baseline if authors are interested in .

Thank you for your valuable comments ! We clarify some confusions below , and we would greatly appreciate it if the reviewer could provide more feedbacks based on our response .

We have updated our paper to provide more details about our experimental setup . We employ the S-expression to serialize the tree . For example , the parse tree of source program in Figure 1 ( i.e. , the parse tree of x = 1 if y == 0 )
is represented as

( Block ( If ( Op=== ( Value ( Identifier Literal ( y ) ) Value ( Number Literal ( 0 ) ) ) Block ( Assign ( Value ( Identifier Literal ( x ) ) Value ( Number Literal ( 1 ) ) ) ) ) )

This is the de facto standard approach used in the literature such as [ 1 ] and [ 2 ] . To the best of our knowledge , we are not aware of more effective ways to encode a tree . We would greatly appreciate it if the reviewer could provide alternatives that have been examined in the literature , and we would be happy to try them out .

As described in Section 4.1 , we use a pCFG to generate train / dev / test programs , while guaranteeing their lengths are equal to the value we specify . We think testing on randomly generated cases can effectively examine the correctness of the learned translator . Different from natural language translation , program translation task requires to handle all corner cases that may not be frequently seen in practice . Thus , using random test cases can effectively reach to all such corner cases , and we cannot agree with the reviewer that doing so is meaningless .

Our program accuracy is an under - approximation of semantic equivalence , while token accuracy can provide a detailed measurement to understand an approach when the program accuracy is low . In this sense , these two metrics can capture some meaningful information about approaches ’ effectiveness . Note that verifying if two programs are semantic-equivalent definition is a turing - complete problem , thus all metrics have to be an approximation to some degree . We consider proposing a better metric as future work .

The reviewer comments on the syntax of CoffeeScript and JavaScript , and argues that CoffeeScript is a subset of JavaScript , on which we do not agree . The syntactical grammars of two languages do not imply their complexity class . Both of these two languages are Turing -complete , meaning any program in one language has a correspondence in another . Also , for both comments on the syntax and the complexity class , we do not see the direct implication on the later comments on that our synthetic task does not capture the real world programs . For the later comment , as we have explained above , our task is designed to capture different corner cases of a program translator , and we consider handling longer real - world examples as an important future direction .

We are happy to try some traditional statistical machine translation baselines . Thanks for the suggestion !

[ 1 ] Oriol Vinyals , Lukasz Kaiser , Terry Koo , Slav Petrov , Ilya Sutskever , Geoffrey Hinton . Grammar as a foreign language . NIPS 2015 .
[ 2 ] Li Dong and Mirella Lapata . Language to logical form with neural attention . ACL 2016 .


Summary :
I thank the authors for their update and clarifications . They have addressed my concerns , and I will keep my score as it is .

-----------------------------------------------------------------------
The authors present a system that parses DSL expressions into syntax trees when trained using input-output examples . Their approach is based around LSTMs predicting a sequence of program - like instructions / arguments , and they argue that their work is an illustration of how we should approach synthesis of complex algorithms using neural techniques .

Overall I liked this paper :
The authors provide a frank view on the current state of neural program synthesis , which I am inclined to agree with : ( 1 ) existing neural program synthesis has only ever worked on ‘ trivial ’ problems , and ( 2 ) training program synthesizers is hard , but providing execution traces in the training data is not a practical solution . I am somewhat convinced that the task considered in this paper is not trivial ( so the authors do not obviously fall into trap ( 1 ) ) , and I am convinced that the authors ’ two - phase reinforcement learning solution to ( 2 ) is an interesting approach .
My score reflects the fact that this paper seems like a solid piece of work : The task is difficult , the solution interesting and the results are favourable .

However , I would like the authors to clarify the following points :
1 . In the inner loop of Algorithm 1 , it looks like Net_in is updated M1 times , and a candidate trace is only stored if arguments that generate an exact match with the ground truth tree are found . Since M1 = 20 , I am surprised that an exact match can be generated with so few samples / updates . Similarly , I am surprised that the appendix mentions that only 1000 samples in the outer loop are required to find an exact match with the instruction trace . Could you clarify that I am reading this correctly and perhaps suggest intuition for why this method is so efficient . What is the size of the search space of programs that your LL machine can run ? Should I conclude that the parsing task is actually not as difficult as it seems , or am I missing something ?
2 . The example traces in the appendix ( fig 3 , 6 ) only require 9 instructions . I ’m guessing that these short programs are just for illustration – could you provide an example execution trace for one of the programs in the larger test sets ? I assume that these require many more instructions & justify your claims of difficulty .
3 . As a technique for solving the parsing problem , this method seems impressive . However , the authors present the technique as a general approach to synthesizing complex programs . I feel that the authors need to either justify this bold assertion with least one additional example task or tone down their claims . In particular , I would like to see performance on a standard benchmarking task e.g. the Robust Fill tasks . I want to know whether ( 1 ) the method works across different tasks and ( 2 ) the baselines reproduce the expected performance on these benchmark tasks .
4 . Related to the point above , the method seems to perform almost too well on the task it was designed for – we miss out on a chance to discuss where the model fails to work .

The paper is reasonably clear . It took a couple of considered passes to get to my current understanding of Algorithm 1 , and I found it essential to refer to the appendix to understand LL machines and the proposed method . In places , the paper is somewhat verbose , but since many ideas are presented , I did not feel too annoyed by the fact that it ( significantly ) overshoots the recommended 8 page limit .



Thank you for your review ! We will upload a revision by next week . We will update our paper to explain more about our training method . In particular , we will include some running examples to further explain our training algorithm and demonstrate the complexity of our parsing problem . About your questions :

1 . We will add more explanation about what will happen during training . To give you a teaser , consider an example input of 3 tokens , x+y , there are 72810 valid traces --- which seems not so many . But consider that we need to find one valid trace for each example , then there could be 72810 ^n combinations for a training set of n examples of length 3 . In this sense , even n>=2 will render an exhaustive approach impractical . When the input length increases to 5 tokens , the number of valid traces increases to 50549580 , and 33871565610 for 7 . We can observe that the search space grows exponentially as input length increases . We hope this can give you an intuitive idea of the difficulty of the problem that we are working on . We will provide a more detailed explanation of the problem complexity in our revision .

2 . The challenge does not come from the length of the execution trace --- it is mostly linear to the length of the input and output . The main difficulty comes from the volume of the search space , which can grow exponentially large as the length of the execution trace increases .

3 . We believe our technique can be generalized to other tasks , but the evaluation on other tasks will not be easy . The main challenge is that we need to design a new domain-specific machine , a neural program architecture , so that we can test our RL - based strategy . This could result in a workload of an entirely new research . Notice that it is also not trivial to adapt Robust Fill to other tasks , as we mentioned in our paper . Meanwhile , since Microsoft will not release the FlashFill test data , it can be hard to make a fair comparison on their task . Thus , we will choose to tone down our claim and make our contribution more specific to the parsing problem .

4 . We will add a section to include some running examples and failure modes in our revision , either in the main body or in the appendix depending on what reviewers prefer .


This paper presents a reinforcement learning based approach to learn context - free
parsers from pairs of input programs and their corresponding parse trees . The main
idea of the approach is to learn a neural controller that operates over a discrete
space of programmatic actions such that the controller is able to produce the
desired parse trees for the input programs . The neural controller is trained using
a two - phase reinforcement learning approach where the first phase is used to find
a set of candidate traces for each input- output example and the second phase is
used to find a satisfiable specification comprising of 1 unique trace per example
such that there exists a program that is consistent with all the traces . The
approach is evaluated on two datasets comprising of learning parsers for an
imperative WHILE language and a functional LAMBDA language . The results show that
the proposed approach is able to achieve 100 % generalization on test sets with
programs upto 100 x longer than the training programs , while baseline approaches
such as seq2seq and stack LSTM do not generalize at all .

The idea to decompose the synthesis task into two sub-tasks of first learning
a set of individual traces for each example , and then learning a program consistent
with a satisfiable subset of traces is quite interesting and novel . The use of
reinforcement learning in the two phases of finding candidate trace sets with
different reward functions for different operators and searching for a satisfiable
subset of traces is also interesting . Finally , the results leading to perfect
generalization on parsing 100 x longer input programs is also quite impressive .

While the presented results are impressive , a lot of design decisions such as
designing specific operators ( Call , Reduce , .. ) and their specific semantics seem
to be quite domain -specific for the parsing task . The comparison with general
approaches such as seq2seq and stack LSTM might not be that fair as they are
not restricted to only those operators and this possibly also explains the low
generalization accuracies . Can the authors comment on the generality of the
presented approach to some other program synthesis tasks ?

For comparison with the baseline networks such as seq2seq and stack - LSTM , what
happens if the number of training examples is 1M ( say programs upto size 100 ) ?
10 k might be too small a number of training examples and these networks can
easily overfit such a small dataset .

The paper mentions that developing a parser can take upto 2x / 3 x more time than
developing the training set . How large were the 150 examples that were used for
training the models and were they hand - designed or automatically generated by a
parsing algorithm ? Hand generating parse trees for complex expressions seems to
be more tedious and error- prone that writing a modular parser .

The reason there are only 3 to 5 candidate traces per example is because the training
examples are small ? For longer programs , I can imagine there can be thousands of bad
traces as it only needs one small mistake to propagate to full traces . Related to this
question , what happens to the proposed approach if it is trained with 1000 length programs ?

What is the intuition behind keeping M1 , M2 and M3 constants ? Should n’t they be adaptive
values with respect to the number of candidate traces found so far ?

For phase - 1 of learning candidate traces , what happens if the algorithm was only using the
outside loop ( M2 ) and performing REINFORCE without the inside loop ?

The current paper presentation is a bit too dense to clearly understand the LL machine
model and the two - phase algorithm . A lot of important details are currently in the
appendix section with several forward references . I would suggest moving Figure 3
from appendix to the main paper , and also add a concrete example in section 4 to
better explain the two - phase strategy .



Thank you for your review !

About the generality of our approach , we would like to mention that CALL , RETURN and FINAL instructions are general and used in the design of non-differentiable machines for a wide range of different programs , e.g. , in Neural Programmer - Interpreter ( NPI ) . The unique instructions of the LL machine are SHIFT and REDUCE , which are two fundamental operations to build a parser , and they are also used in Shift -Reduce machines proposed in NLP field . Although these two instructions are for parser per se , we would like to emphasize that the LL machine is generic enough to handle a wide spectrum of grammars . We will revise our paper to make this point clearer .

For baseline models , we will train them on datasets with 1M samples including longer inputs . We may not be able to finish training on inputs of length 100 due to our hardware limitation , but will at least train on inputs of length 20 , and we will update the results once we finish our experiments . However , we would like to point out that for some baseline models , it is already hard to fit to current training set with 10 K samples . For example , the training accuracies of Stack -LSTM and DeQue-LSTM on LAMBDA dataset are below 3 % , and the training accuracies of seq2seq on the two standard training sets are below 95 % . We will open -source our code for replication after the double - blind review period . The code can also replicate the experiments in the original papers of the baseline models .

For the training curriculum , the average lengths of training samples are 5.6 ( Lambda ) and 9.3 ( WHILE ) , which are not long . Training samples in the curriculum are manually generated , which are used to test our manually written parser . The reason why generating a small set of training samples is faster than writing a parser is twofold . First , debugging the parser takes a long time , and it could take longer without the help of the training curriculum . Second , there are only a few samples in the curriculum , and these samples are short , thus it does not take long to generate them .

Meanwhile , as we mentioned in our paper , our model relies heavily on the training curriculum to find the correct traces . In order to fit to longer samples , we need to train the model to fit to all samples in previous lessons with shorter samples first . At the beginning , the model can only find traces for the simplest input samples , e.g. , x + y. Then the model gradually learns to fit to samples of length 5 , 7 , etc. , in the training curriculum . If we randomly initialize the model and then train it directly on samples of length 1000 , then our model will completely fail to find any trace that leads to the correct output parse tree . We will update our paper to explain more about the curriculum as well .

The choice of hyper - parameters M1 , M2 and M3 is based on our empirical results . Their values are not adaptively tuned to make sure that the training algorithm can search for candidate traces for enough time . For example , we can not simply stop the algorithm after finding 3 candidate traces , because we are not sure whether it can still find the 4th trace or more .

For the training algorithm , without the inner loop in phase 1 , the model will trap into a local minimum without finding the whole set of traces . Most likely , the traces found in this case are wrong ones .

Thank you for your advice on writing ! We defer these details to the Appendix to shorten the main body of our paper . Following your suggestions , we will add a section to include some running examples , and provide more descriptions of our training algorithm . Also , we will move some important details in the Appendix to the main body of the paper .

We have added a set of experiments to train baseline models on the dataset with 1M samples of length 50 , and included the results in our paper . We observe that for seq2seq and seq2tree models , training with 1M samples mitigates the overfitting issue ; however , for Stack LSTM , Queue LSTM and DeQue LSTM , both training and test accuracies drop to 0 .

This paper proposes a method for learning parsers for context - free languages . They demonstrate that this achieves perfect accuracy on training and held - out examples of input / output pairs for two synthetic grammars . In comparison , existing approaches appear to achieve little to no generalization , especially when tested on longer examples than seen during training .

The approach is presented very thoroughly . Details about the grammars , the architecture , the learning algorithm , and the hyperparameters are clearly discussed , which is much appreciated . Despite the thoroughness of the task and model descriptions , the proposed method is not well motivated . The description of the relatively complex two - phase reinforcement learning algorithm is largely procedural , and it is not obvious how necessary the individual pieces of the algorithm are . This is particularly problematic because the only empirical result reported is that it achieves 100 % accuracy . Quite a few natural questions left unanswered , limiting what readers can learn from this paper , e.g.
- How quickly does the model learn ? Is there a smooth progression that leads to perfect generalization ?
- Presumably the policy learned in Phase 1 is a decent model by itself , since it can reliably find candidate traces . How accurate is it ? What are the drawbacks of using that instead of the model from the second phase ? Are there systematic problems , such as overfitting , that necessitate a second phase ?
- How robust is the method to hyperparameters and multiple initializations ? Why choose F = 10 and K = 3 ? Presumably , there exists some hyperparameters where the model does not achieve 100 % test accuracy , in which case , what are the failure modes ?

Other misc . points :
- The paper mentions that " the training curriculum is very important to regularize the reinforcement learning process . " Unless I am misunderstanding the experimental setup , this is not supported by the result , correct ? The proposed method achieves perfect accuracy in every condition .
- The reimplementations of the methods from Grefenstette et al . 2015 have surprisingly low training accuracy ( in some cases 0 % for Stack LSTM and 2.23 % for DeQueue LSTM ) . Have you evaluated these reimplementations on their reported tasks to tease apart differences due to varying tasks and differences due to varying implementations ?

Thank you for your review ! We are working on a revision , and we will upload the new version no later than next week .

We will update our paper to explain more about our training method , including the curriculum learning and two - phase training algorithm . In particular , we will add a section to include some running examples , and describe what would happen if not following our proposed strategy , e.g. , removing phase 2 in our algorithm . In general , using an alternative method , the model could overfit to a subset of the training examples , and thus fails to generalize .

For the choice of hyper - parameters , F ( the number of different function IDs ) and K ( the maximal number of elements in each stack frame ’s list ) are parameters of the LL machine , not the training algorithm . These parameters ensure that the LL machine is expressive enough to serve as a parser . If the values of them are too small , then there exists no program that can run the machine to simulate the parser for a complex grammar .

For the curriculum learning , we want to point out that we only report our approach employing the curriculum training . If curriculum training is not employed , then our model cannot even fit to the training data , and thus will fail completely on the test data . This point has been explained in Section 4.1 and Section 5 . We will make it more explicit in our next revision .

Our re-implementation of the methods from Grefenstette et al . 2015 can replicate their results on their tasks . We will open-source our code for replication of both Grefenstette et al . 2015 and our experiments after the double - blind review period .


This paper presents two complementary models for unsupervised domain adaptation ( classification task ) : 1 ) the Virtual Adversarial Domain Adaptation ( VADA ) and 2 ) the Decision -boundary Iterative Refinement Training with a Teacher ( DIRT - T ) . The authors make use of the so-called cluster assumption , i.e. , decision boundaries should not cross high - density data regions . VADA extends the standard Domain - Adversarial training by introducing an additional objective L_t that measures the target - side cluster assumption violation , namely , the conditional entropy w.r.t. the target distribution . Since the empirical estimate of the conditional entropy breaks down for non- locally - Lipschitz classifiers , the authors also propose to incorporate virtual adversarial training in order to make the classifier well - behaved . The paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of L_t using natural gradient descent ( DIRT - T ) which ensures that the decision boundary changes incrementally and slowly .

Pros :
+ The paper is written clearly and easy to read
+ The idea to keep the decision boundary in the low - density region of the target domain makes sense
+ The both proposed methods seem to be quite easy to implement and incorporate into existing DATNN - based frameworks
+ The combination of VADA and DIRT - T performs better than existing DA algorithms on a range of visual DA benchmarks

Cons :
- Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers
- The paper deals exclusively with visual domains ; applying the proposed methods to other modalities would make this submission stronger

Overall , I think it is a good paper and deserves to be accepted to the conference . I ’m especially appealed by the fact that the ideas presented in this work , despite being simple , demonstrate excellent performance .

Post-rebuttal revision :
After reading the authors ' response to my review , I decided to leave the score as is .

Thank you for the review ! To improve the quality of the paper , we have made several adjustments to our paper in accordance with your review .

“ Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers ”

The purpose of Table 1 is to offer a holistic evaluation of the entire set-up . As such , it demonstrates that there exists a training objective + architecture + optimization configuration such significant improvements over previous methods / implementations are possible . We provided such a table in part because doing so seems to be standard practice in semi-supervised learning and domain adaptation papers ( Miyato et al. , 2017 ; Laine & Aila , 2016 ; Tarvainen & Valpola , 2017 ; Saito et al. , 2017 ; French et al. , 2017 ) .

To offer a fairer comparison , we made the following modifications to the paper :

1 . We explicitly mention Table 2 in the main body of the paper , in the section Model Evaluation / Overall

2 . We added an ablation study to Section 6.3.1 to demonstrate the relative contribution of virtual adversarial training in comparison to our base implementation of domain adversarial training .

“ The paper deals exclusively with visual domains ; applying the proposed methods to other modalities would make this submission stronger ”

We agree that the submission would be stronger by performing experiments in other modalities . To that end , we added an example of applying VADA and DIRT - T to a non-visual data in Section 6.2 . We chose to apply our model to a Wi- Fi activity recognition dataset . Our results show that VADA significantly improves upon DANN . Unfortunately , due to the small target domain training set size , DIRT - T does not improve upon VADA . We provide additional experiments in Appendix F which suggest that VADA already achieves strong clustering on the Wi-Fi dataset , and therefore DIRT - T is not expected to improve performance in this situation .

We leave as future work the study of applying VADA / DIRT - T ( and the general application of the cluster assumption ) to text classification domain adaptation tasks . Given the success of VAT on text classification ( Miyato et al. , 2016 ) , we are optimistic that this line of work is promising .

References
Takeru Miyato , Shin-ichi Maeda , Masanori Koyama , and Shin Ishii . Virtual adversarial train - ing : a regularization method for supervised and semi-supervised learning . arXiv preprint arXiv:1704.03976 , 2017 .

Takeru Miyato , Andrew M Dai , and Ian Goodfellow . Virtual adversarial training for semi-supervised text classification . stat , 1050:25 , 2016 .

Samuli Laine and Timo Aila . Temporal ensembling for semi-supervised learning . arXiv preprint arXiv:1610.02242 , 2016 .

Antti Tarvainen and Harri Valpola . Mean teachers are better role models : Weight - averaged consis - tency targets improve semi-supervised deep learning results . 2017 .

Geoffrey French , Michal Mackiewicz , and Mark Fisher . Self -ensembling for domain adaptation . arXiv preprint arXiv:1706.05208 , 2017 .

Kuniaki Saito , Yoshitaka Ushiku , and Tatsuya Harada . Asymmetric tri-training for unsupervised domain adaptation . arXiv preprint arXiv:1702.08400 , 2017 .

As there are many kinds of domain adaptation problems , the need to mix several learning strategies to improve the existing approaches is obvious . However , this task is not necessarily easy to succeed . The authors proposed a sound approach to learn a proper representation ( in an adversarial way ) and comply the cluster assumption .

The experiments show that this Virtual Adversarial Domain Adaptation network ( VADA ) achieves great results when compared to existing learning algorithms . Moreover , we also see the learned model is consistently improved using the proposed " Decision - boundary Iterative Refinement Training with a Teacher " ( DIRT - T ) approach .

The proposed methodology relies on multiple choices that could sometimes be better studied and / or explained . Namely , I would like to empirically see which role of the locally - Lipschitz regularization term ( Equation 7 ) . Also , I wonder why this term is tuned by an hyperparameter ( lamda_s ) for the source , while a single hyperparamer ( lambda_t ) is used for the sum of the two target quantity .

On the theoretical side , the discussion could be improved . Namely , Section 3 about " limitation of domain adversarial training " correctly explained that " domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high - capacity " . It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper ( due to Ben-David et al. , 2010 ) , on which several domain adversarial approaches are based . The need to consider supplementary assumptions ( such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David 's work , e.g. Ben- David and Urner ( 2014 ) . In the latter , the notion of " Probabilistic Lipschitzness " , which is a relaxation of the " cluster assumption " seems very related to the actual work .

Reference :
Ben-David and Urner . Domain adaptation - can quantity compensate for quality ? , Ann. Math. Artif. Intell. , 2014

Pros :
- Propose a sound approach to mix two complementary strategies for domain adaptation .
- Great empirical results .

Cons :
- Some choices leading to the optimization problem are not sufficiently explained .
- The theoretical discussion could be improved .

Typos :
- Equation 14 : In the first term ( target loss ) , theta should have an index t ( I think ) .
- Bottom of page 6 : " ... and that as our validation set " ( missing word ) .


Thank you for the review ! To improve the quality of the paper , we have made several adjustments to our paper in accordance with your review .

“ Namely , I would like to empirically see which role of the locally - Lipschitz regularization term ( Equation 7 ) . ”

Thank you for the suggestion . We have included an extensive ablation study of the role of the locally - Lipschitz regularization term in Section 6.3.1 . Our results show that while conditional entropy minimization alone is sufficient to instantiate the cluster assumption and improve over DANN , the additional incorporation of the locally - Lipschitz regularization term does indeed offer additional performance improvement .

“ Also , I wonder why this term is tuned by an hyperparameter ( λ_s ) for the source , while a single hyperparamer ( λ_t ) is used for the sum of the two target quantity ”

The choice to use λ_t for the sum of the two target quantities is purely for simplicity . Since the official implementation of VAT ( https://github.com/takerum/vat_tf) used the same weighting for the conditional entropy and virtual adversarial training , we opted to do that as well in the target domain .

“ It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper ( due to Ben-David et al. , 2010 ) ”

Thank you for the suggestion . We have added the connection in Appendix E . In particular , we can show that , if the embedding function has infinite - capacity , the H\DeltaH - divergence achieves the maximum value of 2 even when the feature distribution matching constraint is satisfied . This results in Theorem 1 becoming a vacuous upper bound .

“ The notion of ‘ Probabilistic Lipschitzness ’ , which is a relaxation of the ‘ cluster assumption ’ seems very related to the actual work . ”

Thank you for this insight . We have incorporated a brief mention of probabilistic Lipschitzness in Section 2 . It seems that a stronger connection can be made and we appreciate any additional suggestions you may have on how to better address probabilistic Lipschitzness in our paper .

“ Equation 14 : In the first term ( target loss ) , theta should have an index t ( I think ) . ” and “ Bottom of page 6 : ‘... and that as our validation set ’ ( missing word ) . ”

Fixed . Thanks !

The paper was a good contribution to domain adaptation . It provided a new way of looking at the problem by using the cluster assumption . The experimental evaluation was very thorough and shows that VADA and DIRT - T performs really well .

I found the math to be a bit problematic . For example , L_d in ( 4 ) involves a max operator . Although I understand what the authors mean , I do n't think this is the correct way to write this . ( 5 ) should discuss the min-max objective . This will probably involve an explanation of the gradient reversal etc . Speaking of GRL , it 's mentioned on p.6 that they replaced GRL with the traditional GAN objective . This is actually pretty important to discuss in detail : did that change the symmetric nature of domain - adversarial training to the asymmetric nature of traditional GAN training ? Why was that important to the authors ?

The literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017 . The latter also had MNIST / MNIST - M experiments .

Thank you for the review ! To improve the quality of the paper , we have made several adjustments to our paper in accordance with your review .

“ The experimental evaluation was very thorough and shows that VADA and DIRT - T performs really well . ”

We have added additional experiments that may be of interest to you . In Section 6.3.1 , we perform an extensive ablation study demonstrate the relative contribution of virtual adversarial training . In Section 6.2 , we apply VADA / DIRT - T to a non-visual domain adaptation task .

“ For example , L_d in ( 4 ) involves a max operator . Although I understand what the authors mean , I do n’t think this is the correct way to write this . ”

We agree the the use of the max operator is informal . To account for the possibility that the maximum is not achievable , using the supremum is more appropriate . We have updated the paper to reflect this . Our choice of presentation is now in keeping with that in GAIL ( Eq. ( 14 ) , Ho & Ermon ( 2016 ) ) and WGAN ( Eq . ( 2 ) , Arjovsky et al . ( 2017 ) ) .

“ ( 5 ) should discuss the min-max objective . This will probably involve an explanation of the gradient reversal etc . Speaking of GRL , it ’s mentioned on p.6 that they replaced GRL with the traditional GAN objective . This is actually pretty important to discuss in detail : did that change the symmetric nature of domain - adversarial training to the asymmetric nature of traditional GAN training ? Why was that important to the authors ? ”

Thank you for pointing this out . We have added a footnote next to ( 5 ) and modified Appendix C to reflect the following opinion :

We believe that , at a high level , it is not of particular importance which optimization procedure is used to approximate the mini-max optimization problem . Our decision to switch from the symmetric to asymmetric training is motivated by

1 . The extensive GAN literature which advocates the asymmetric optimization approach .

2 . Our initial experiments on MNIST → MNIST - M which suggest that the asymmetric optimization approach is more stable .

We are not committed to either optimization strategy and encourage practitioners to try both when applying VADA . In case the reviewer is interested in the performance of pure domain adversarial training using the asymmetric optimization approach , we have included it in Section 6.3.1 .

“ The literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017 . The latter also had MNIST / MNIST - M experiments . ”

Thank you for the suggestion . We have incorporated Bousmalis ’s paper into our comparison .

References
Jonathan Ho and Stefano Ermon . Generative adversarial imitation learning . In Advances in Neural Information Processing Systems , pp. 4565–4573 , 2016 .

Martin Arjovsky , Soumith Chintala , and Le ́on Bottou . Wasserstein gan . arXiv preprint arXiv:1701.07875 , 2017 .


This paper proposes NerveNet to represent and learn structured policy for continuous control tasks . Instead of using the widely adopted fully connected MLP , this paper uses Graph Neural Networks to learn a structured controller for various MuJoco environments . It shows that this structured controller can be easily transferred to different tasks or dramatically speed up the fine-tuning of transfer .

The idea to build structured policy is novel for continuous control tasks . It is an exciting direction since there are inherent structures that should be exploited in many control tasks , especially for locomotion . This paper explores this less -studied area and demonstrates promising results .

The presentation is mostly clear . Here are some questions and a list of minor suggestions :
1 ) In the Output Model section , I am not sure how the controller is shared . It first says that " Nodes with the same node type should share the instance of MLP " , which means all the " joint " nodes should share the same controller . But later it says " Two LeftHip should have a shared controller . " What about RightHip ? or Ankle ? They all belongs to the same node type " joint " . Am I missing something here ? It seems that in this paper , weights sharing is an essential part of the structured policy , it would be great if it can be described in more details .

2 ) In States Update of Propagation Model Section , it is not clear how the aggregated message is used in eq . ( 4 ) .

3 ) Typo in Caption of Table 1 : Centipede Four not CentipedeSix .

4 ) If we just use MLP but share weights among joints ( e.g. the weights from observation to action of all the LeftHips are constrained to be same ) , how would it compare to the method proposed in this paper ?

In summary , I think that it is worthwhile to develop structured representation of policies for control tasks . It is analogue to use CNN that share weights between kernels for computer vision tasks . I believe that this paper could inspire many follow - up work . For this reason , I would recommend accepting this paper .

We thank the reviewer for the great suggestions regarding the quality of the paper , and we would like to bring your attention to the general comment above . We added new experiments in the latest revision .

Q1 : How does MLP with share weights among joints perform ?
A1 : We name the variant proposed by the reviewer as MLP - Bind . Note that MLP - Bind and TreeNet are equivalent for the Snake agents , since the snakes only have one type of joint . We ran MLP - Bind for the zero-shot and fine-tuning experiments on centipedes . We summarize the results here :

1 . Zero - shot performances of MLP - Bind and MLPAA are very similar . Both models have limited performance in the zero-shot scenario . Attached below is a sample table for several transfer tasks in centipedes ( full results in the appendix of the revised draft )

2 . For fine -tuning on ordinary centipedes from pretrained models , the performance is only slightly worse than when using MLP . In our experiment , in the two curves of transferring from Centipede Four to CentipedeEight as well as CentipedeSix to CentipedeEight , MLP - Bind ’s reward is 100- 500 worse than MLPAA during fine-tuning .

3 . For the Crippled agents , the MLP - Bind agent is stuck at around 800 reward . This might be due to MLP - Bind not being able to efficiently exploit the information of crippled and well - functioning legs .

For the Average Reward :
----------------------------------------------------------------------------
Task | MLPAA | MLP - Bind | NerveNet
----------------------------------------------------------------------------
4to6 | 109.4 | 62.13 | 139.6
4to8 | 18.2 | 24.62 | 44.3
----------------------------------------------------------------------------
6to8 | 21.1 | 235.97 | 1674.9
6to10 | - 42.4 | 18.65 | 940.0
----------------------------------------------------------------------------
4toCp06 | - 5.1 | 11.47 | 47.6
4toCp08 | 5.1 | 7.34 | 40.0
----------------------------------------------------------------------------
6toCp08 | 36.5 | 29.09 | 523.6
6toCp10 | 12.8 | 8.32 | 504.0
----------------------------------------------------------------------------

For the average distance the agents could run in one episode ( see updated version for the details of average distance , which is another metrics to evaluate how well the agents perform . )
----------------------------------------------------------------------------
Task | MLPAA | MLP - Bind | NerveNet
----------------------------------------------------------------------------
4to6 | 545.3 | 62.13 | 577.3
4to8 | 62.0 | 24.62 | 146.9
----------------------------------------------------------------------------
6to8 | 87.8 | 235.97 | 10612.6
6to10 | - 17.0 | 18.65 | 6343.6
----------------------------------------------------------------------------
4toCp06 | - 22.5 | 11.47 | 91.1
4toCp08 | - 26.9 | 7.34 | 80.1
----------------------------------------------------------------------------
6toCp08 | 138.3 | 29.09 | 3117.3
6toCp10 | 13.6 | 8.32 | 3230.3
----------------------------------------------------------------------------

The details of experiments we performed are updated in the appendix of the latest version .


Q2 : How the controller is shared ? ( “ In the Output Model section , I am not sure how the controller is shared . It first says that " Nodes with the same node type should share the instance of MLP " , which means all the " joint " nodes should share the same controller . ” )
A2 : We clarified the Output Model section in the latest version .
Nervnet : nodes of the same type ( joint , root , body ) share the same state update function , e.g. , GRU weights .
Every node , regardless of its node type , shares the same output MLP instance .


Q3 : Typos and presentation regarding to Eq . ( 4 ) .
A3 : We improved the clarity as per suggestions .


The submission proposes incorporation of additional structure into reinforcement learning problems . In particular , the structure of the agent 's morphology . The policy is represented as a graph neural network over the agent 's morphology graph and message passing is used to update individual actions per joint .

The exposition is fairly clear and the method is well - motivated . I see no issues with the mathematical correctness of the claims made in the paper . However , the paper could benefit from being shorter by moving some details to the appendix ( such as much of section 2.1 and PPO description ) .

Related work section could consider the following papers :

" Discrete Sequential Prediction of Continuous Actions for Deep RL "
Another approach that outputs actions per joint , although in a general manner that does not require morphology structure

" Generalized Biped Walking Control "
Considers the task of interactively changing limb lengths ( your size transfer task ) in a zero -shot manner , albeit with a non-neural network controller

The experimental results investigate the effects of various algorithm parameters , which is appreciated . However , a wider range of experiments would have been helpful to judge the usefulness of the proposed policy representation . In addition to robustness to limb length and disability perturbations , it would have been very nice to see multi-task learning that takes advantage of body structure ( such as learning to reach for target with arms while walking with legs and being able to learn those independently , for example ) .

However , I do think using agent morphology is an under - explored idea and one that is general , since we tend to have access to this structure in continuous control tasks for the time being . As a result , I believe this submission would be of interest to ICLR community .

We thank the reviewer for the great suggestion regarding multi-task learning .

Q1 : “ However , a wider range of experiments would have been helpful to judge the usefulness of the proposed policy representation … it would have been very nice to see multi-task learning that takes advantage of body structure ”
A1 : We added the multi-task learning experiment . Please see the general comment above with additional experiments .


Q2 : Moving some details to the appendix .
A2 : We revised the paper and shortened the model ’s section .


Q3 : Adding references .
A3 : Thanks for pointing out these two references . We included them in the latest version .

We believe that the focus of “ Discrete Sequential Prediction of Continuous Actions for Deep RL ” paper ( action space discretization ) and the focus of our paper ( using structure information ) are different . Combining these ideas might further boost the performance of the agents .

For the second paper , we agree that model - based control has been well studied , and this paper should be cited .

The authors present an interesting application of Graph Neural Networks to learning policies for controlling " centipede " robots of different lengths . They leverage the non-parametric nature of graph neural networks to show that their approach is capable of transferring policies to different robots more quickly than other approaches . The significance of this work is in its application of GNNs to a potentially practical problem in the robotics domain . The paper suffers from some clarity / presentation issues that will need to be improved . Ultimately , the contribution of this paper is rather specific , yet the authors show the clear advantage of their technique for improved performance and transfer learning on some agent types within this domain .

Some comments :
- Significant : A brief statement of the paper 's " contributions " is also needed ; it is unclear at first glance what portions of the work are the authors ' own contributions versus prior work , particularly in the section describing the GNN theory .
- Abstract : I take issue with the phrase " are significantly better than policies learned by other models " , since this is not universally true . While there is a clear benefit to their technique for the centipede and snake models , the performance on the other agents is mostly comparable , rather than " significantly better " ; this should be reflected in the abstract .
- Figure 1 is instructive , but another figure is needed to better illustrate the algorithm ( including how the state of the world is mapped to the graph state h , how these " message " are passed between nodes , and how the final graph states are used to develop a policy ) . This would greatly help clarity , particularly for those who have not seen GNNs before , and would make the paper more self - contained and easier to follow . The figure could also include some annotated examples of the input spaces of the different joints , etc . Relatedly , Sec. 2.2.2 is rather difficult to follow because of the lack of a figure or concrete example ( an example might help the reader understand the procedure without having to develop an intuition for GNNs ) .
- There is almost certainly a typo in Eq. ( 4 ) , since it does not contain the aggregated message \bar{m}_u^t .

Smaller issues / typos :
- Abstract : please spell out spell out multi-layer perceptrons ( MLP ) .
- Sec 2.2 : " servers " should be " serves "
- " performance By " on page 4 is missing a " . "

Pros :
- The paper presents an interesting application of GNNs to the space of reinforcement learning and clearly show the benefits of their approach for the specific task of transfer learning .
- To the best of my knowledge , the paper presents an original result and presents a good - faith effort to compare to existing , alternative systems ( showing that they outperform on the tasks of interest ) .

Cons :
- The contributions of the paper should be more clearly stated ( see comment above ) .
- The section describing their approach is not " self contained " and is difficult for an unlearned reader to follow .
- The problem the authors have chosen to tackle is perhaps a bit " specific " , since the performance of their approach is only really shown to exceed the performance on agents , like centipedes or snakes , which have this " modular " quality .

I certainly hope the authors improve the quality of the theory section ; the poor presentation here brings down the rest of the paper , which is otherwise an easy read .

We thank the reviewer for the careful reading of our paper and suggestions .

Q1 : The problem the authors have chosen to tackle is perhaps a bit specific
A1 : Please see the general comment above with additional experiments .


Q2 : A brief statement of the paper 's " contributions " is also needed .
A2 : We made the statement more clear in the latest version . Specifically , our main contribution is in exploring graph neural networks in reinforcement learning and investigating their ability to transfer structure . To the best of our knowledge , we are the first to address transfer learning for continuous control tasks . We also make small contributions on the model side , i. e. GNNs . In particular , we introduce node type and associate an instance of an update function with each type . This fits the RL setting very well and also increases the model ’s capacity .


Q3 : Abstract : I take issue with the phrase " are significantly better than policies learned by other models .
A3 : We agree and will modify the wording in the abstract . Our main claims were with respect to transferability , since our model has significant improvement in the zero-shot and transfer learning tasks .


Q4 : Another figure is needed to better illustrate the algorithm . Relatedly , Sec. 2.2.2 is rather difficult to follow because of the lack of a figure or concrete example
A4 : We added a new figure ( Fig. 2 in the newest revision ) to illustrate how the input state is constructed , how messages are passed between the nodes and how the final policy is being output .


Q5 : Typo in Eq. ( 4 ) and other minor issues .
A5 : Thanks for pointing this out . We corrected them in the latest version .


The paper proposes a novel architecture for capsule networks . Each capsule has a logistic unit representing the presence of an entity plus a 4 x4 pose matrix representing the entity / viewer relationship . This new representation comes with a novel iterative routing scheme , based on the EM algorithm .
Evaluated on the SmallNORB dataset , the approach proves to be more accurate than previous work ( beating also the recently proposed " routing - by-agreement " approach for capsule networks by Sabour et al . ) . It also generalizes well to new , unseen viewpoints and proves to be more robust to adversarial examples than traditional CNNs .

Capsule networks have recently gained attention from the community . The paper addresses important shortcomings exhibited by previous work ( Sabour et al. ) , introducing a series of valuable technical novelties .
There are , however , some weaknesses . The proposed routing scheme is quite complex ( involving an EM - based step at each layer ) ; it 's not fully clear how efficiently it can be performed / how scalable it is . Evaluation is performed on a small dataset for shape recognition ; as noted in Sec. 6 , the approach will need to be tested on larger , more challenging datasets . Clarity could be improved in some parts of the paper ( e.g. : Sec. 1.1 may not be fully clear if the reader is not already familiar with ( Sabour et al. , 2017 ) ; the authors could give a better intuition about what is kept and what is discarded , and why , from that approach . Sec. 2 : the sentence " this is incorrect because the transformation matrix ... " could be elaborated more . V_{ih} in eq . 1 is defined only a few lines below ; perhaps , defining the variables before the equations could improve clarity . Sec. 2.1 could be accompanied by mathematical formulation ) .
All in all , the paper brings an original contribution and will encourage further research / discussion on an important research question ( how to effectively leverage knowledge about the part - whole relationships ) .

Other notes :
- There are a few typos ( e.g. Sec. 1.2 " ( Jaderberg et al. ( 2015 ) " , Sec. 2 " the the transformation " , Sec. 4 " cetral crop " etc . ) .
- The authors could discuss in more detail why the approach does not show significant improvement on NORB with respect to the state of the art .
- The authors could provide more insights about why capsule gradients are smaller than CNN ones .
- It would be interesting to discuss how the network could potentially be adapted , in the future , to : 1 . be more efficient 2 . take into account other changes produced by viewpoint changes ( pixel intensities , as noted in Sec. 1 ) .
- In Sec , 4 , the authors could provide more details about the network training .
- In Procedure 1 , for indexing tensors and matrices it might be better to use a comma to separate dimensions ( e.g. V_{ : , c , : } instead of V_{:c : } ) .

Thank you for your detailed reading of the paper and suggestions !
As per your comments on the EM routing , we agree that it was not presented as best it could have been , and have added an appendix to present a gentle and thorough introduction to the free energy view of EM and the objective function which our routing operation minimizes . In response to the question about efficiency , we would like to draw your attention to the total number of arithmetic operations required for the routing procedure - each iteration of routing represents fewer arithmetic operations than a single layer feed forward pass , but due to architectural optimization decisions in tensorflow , our current capsule implementation is not as fast as it could be .

We agree that larger scale testing would ideal , but due to the aforementioned efficiency limitations were not able to include it in this paper .

In regards to your other comments we have done the following :
- To increase the clarity of the paper , we have made several changes to the language used , and improved the mathematical notation .
- We have added section 2 which provides an intuitive explanation of capsules and makes clear when the routing occurs . We feel that improves the readers ' ability to engage with the rest of the presented content . We also defined the variables and notation used in the rest of the paper more explicitly .
- We have expanded on the sentence " this is incorrect because the transformation matrix ... " you mentioned which is now in the appendix .
- We have also made several changes to the nation and language throughout the paper to make it more comprehensible .
thank you for your feedback , and hope that we have addressed your comments to your satisfaction .

This paper proposes a new kind of capsules for CNN . The capsule contains a 4x4 pose matrix motivated by 3D geometric transformations describing the relationship between the viewer and the object ( parts ) . An EM - type of algorithm is used to compute the routing .

The authors use the small NORB dataset as an example . Since the scenes are simulated from different viewer angles , the pose matrix quite fits the motivation . It would be more beneficial to know if this kind of capsules is limited to the motivation or is general . For example , the authors may consider reporting the results of the aff NIST dataset where the digits undergo 2D affine transformations ( in which case perhaps 3 x3 pose matrices are enough ? ) .

Minor : The arguments in line 5 of the procedure RM Routing ( a , V ) do not match those in line 1 of the procedure E-Step .

Section 2.1 ( objective of EM ) is unclear . The authors may want to explicitly write down the free energy function .

The section about robustness against adversarial attacks is interesting .

Overall the idea appears to be useful but needs more empirical validation ( affNIST , ImageNet , etc ) .


thank you for the feedback ! To address your comments we have done the following :
- To clarify the EM objective we have added an extended and thorough appendix which presents a gentle and intuitive explanation of the free energy view of EM , and explicit free energy function , and how our routing algorithm makes use of it .
- We believe that the benefit of capsules is not limited to small NORB and will generalize . As suggested , we replicated the affNIST generalization experiment reported in the previous Capsule paper ( Sabour et al. 2017 ) . We found that our EM capsule model ( the exact architecture used for small NORB and MNIST in the paper ) , when trained to 0.8 % test error on expanded MNIST ( 40x40 pixel MNIST images , created by padding and shifting MNIST ) , achieved 6.9 % test error on affNIST . We trained a baseline CNN ( with AlexNet architecture , without pooling ) to 0.8 % test error and it was only able to achieve 14.1 % test error on affNIST . Our capsule model was able to half the test error of a CNN when trained on MNIST and tested on affNIST . Due to time and space constraints these results are not reported in the paper as it is now .
- finally we address the minor issue raised in line 5 of the routing procedure .
we hope this has addressed your concerns , and thank you for your suggestions .

The paper describes another instantiation of " capsules " which attempt to learn part - whole relationships and the geometric pose transformations between them . Results are presented on the small NORB test set obtaining impressive performance .

Although I like very much this overall approach , this particular paper is so opaquely written that it is difficult to understand exactly what was done and how the network works . It sounds like the main innovation here is using a 4 x4 matrix for the pose parameters , and an iterative EM algorithm to find the correspondence between capsules ( routing by agreement ) . But what exactly the pose matrix represents , and how they get transformed from one layer to the next , is left almost entirely to the reader 's imagination . In addition , how EM factors in , what the probabilities P_ih represent , etc. is not clear . I think the authors could do a much better job explaining this model , the rationale behind it , and how it works .

Perhaps the most interesting and compelling result is Figure 2 , which shows how ambiguity in object class assignment is resolved with each iteration . This is very intriguing , but it would be great to understand what is going on and how this is happening .

Although the results are impressive , if one ca n't understand how this was achieved it is hard to know what to make of it .



Thank you for your comments . upon reflection we agree that the paper was confusing and we have taken several steps to reduce the opacity of our work to the reader . To that end we have done the following :
- We have added section 2 which gives a general and intuitive explanation of the mechanism of capsule networks , paying close attention to how pose matrices get transformed from one layer to the next .
- Having identified the EM objective as another source of confusion , we added an extended appendix in which we provide a gentle and approachable explanation for the free energy view of EM and how our routing algorithm builds upon it .
- We have also added a paragraph to further explain figure 2 in the experiments section .
- Finally we have made several changes to the language of the paper , focusing in particular on the notation .
We believe that the comprehensibility of the paper has thus improved and appreciate your criticism .

This paper discusses several gradient based attribution methods , which have been popular for the fast computation of saliency maps for interpreting deep neural networks . The paper provides several advances :
- \epsilon - LRP and DeepLIFT are formulated in a way that can be calculated using the same back - propagation as training .
- This gives a more unified way of understanding , and implementing the methods .
- The paper points out situations when the methods are equivalent
- The paper analyses the methods ' sensitivity to identifying single and joint regions of sensitivity
- The paper proposes a new objective function to measure joint sensitivity

Overall , I believe this paper to be a useful contribution to the literature . It both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing methods . Especially the latter will be appreciated .

We thank the reviewer for his / her feedbacks .

The paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input space .

The main contributions are the introduction of a unified framework that expresses 4 common attribution techniques ( Gradient * Input , Integrated Gradient , eps -LRP and DeepLIFT ) in a similar way as modified gradient functions and the definition of a new evaluation measure ( ' sensitivity n' ) that generalizes the earlier defined properties of ' completeness ' and ' summation to delta ' .

The unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps -LRP and DeepLIFT substantially more easy on modern frameworks . However , as correctly stated by the authors some of the unification ( e.g. relation between LRP and Gradient * Input ) has been already mentioned in prior work .

Sensitivity - n as a measure tries to tackle the difficulty of estimating the importance of features that can be seen either separately or in combination . While the measure shows interesting trends towards a linear behaviour for simpler methods , it does not persuade me as a measure of how well the relevance attribution method mimics the decision making process and does not really point out substantial differences between the different methods . Furthermore , The authors could comment on the relation between sensitivity - n and region perturbation techniques ( Samek et al. , IEEE TNNLS , 2017 ) . Sensitivtiy -n seems to be an extension of the region perturbation idea to me .

It would be interesting to see the relation between the " unified " gradient - based explanation methods and approaches ( e.g. Saliency maps , alpha-beta LRP , Deep Taylor , Deconvolution Networks , Grad-CAM , Guided Backprop ... ) which do not fit into the unification framework . It 's good that the author mention these works , still it would be great to see more discussion on the advantages / disadvantages , because these methods may have some nice theoretically properties ( see e.g. the discussion on gradient vs . decompositiion techniques in Montavon et al. , Digital Signal Processing , 2017 ) which can not be incorporated into the unified framework .

Thanks for your extensive review and useful feedbacks .

While we agree that it would be interesting to compare with other mentioned methods , the reasons we decided not to do so are various . Saliency maps and Deep Taylor Decomposition only produce positive attribution maps and and this would penalize these methods in the sensitivity - n metric given that our task inputs do contain some negative evidence ( as shown in Figure 3c ) . Similarly , alpha-beta LRP also adds some bias towards positive attributions with the parameters suggested by the authors . Grad- CAM , Deconvolutional Networks and Guided Backpropagation can only be applied to specific network architectures and do not fit our goal to compare methods across tasks and architectures , while we believe it is important for attribution methods to be as general as possible . We reported the same arguments at the end of section 2.2.
We also agree with your statement that other methods have been shown to have interesting theoretical properties . We actually did not intend to claim the superiority of gradient - based methods and added a note to clarify this in Section 3.1 in our last revision .

About the connection with the region perturbation technique ( Samek et al. 2017 ) , this is similar to what we use ( and now mention explicitly ) to produce Figure 3c , with the difference that we occlude one pixel at the time , we produce the curves for the negative ranking as well as for the positive and we plot directly the output variation on the y-axis instead of the AOPC . This technique evaluates methods based on i ) how " fast " the target activation drops or increase and ii ) how " much " the target activation changes . However , we show in Figure 3c that these two criteria often collide if the curves for different methods intersect . This is in fact what motivated sensitivity - n.
With sensitivity - n , we fix a value n and remove random subsets of n features from the input ( without following the ranking given by the attribution maps ) and measure the Pearson correlation with the output variation . This shows that different methods are better at producing different explanations ( influence of single features vs influence of regions ) and therefore the question itself of which is the best attribution method does not make sense if a task is not further specified .

The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results . The authors show that these techniques can all be seen as a product of input activations and a modified gradient , where the local derivative of the activation function at each neuron is replaced by some fixed function .

A second part of the paper looks at whether explanations are global or local . The authors propose a metric called sensitivity - n for that purpose , and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case . The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real - world datasets . Results further outline the resemblance between the compared methods .

In the appendix , the last step of the proof below Eq. 7 is unclear . As far as I can see , the variable g_i^LRP was n’t defined , and the use of Eq. 5 to achieve this last could be better explained . There also seems to be some issues with the ordering i , j , where these indices alternatively describe the lower / higher layers , or the higher / lower layers .

Thanks for your your review .
We reworked the last part of the proof A.1 in our last revision . In particular , we removed the variable g_i that was not defined and better explained the last step .
We did not find issues with the ordering of the subscripts i , j in the proof itself but we did notice that it was inconsistent with the convention we used in Section 2 . We have now fixed it such that , when two subscripts are present , the first one always refers to the layer closer to the output .

This paper studies the impact of angle bias on learning deep neural networks , where angle bias is defined to be the expected value of the inner product of a random vectors ( e.g. , an activation vector ) and a given vector ( e.g. , a weight vector ) . The angle bias is non-zero as long as the random vector is non- zero in expectation and the given vector is non -zero . This suggests that the some of the units in a deep neural network have large values ( either positive or negative ) regardless of the input , which in turn suggests vanishing gradient . The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero . Although this does not rule out angle bias in general , it does so for the very special case where the expected value of the random vector is a vector consisting of a common value . Nevertheless , numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR - 10 task . Test accuracy is not improved , however .

Overall , this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks , but the results are rather preliminary both on theory and experiments .

On the theoretical side , the linearly constrained weights are only shown to work for a very special case . There can be many other approaches to mitigate the impact of angle bias . For example , how about scaling each variable in a way that the mean becomes zero , instead of scaling it into [ - 1 , + 1 ] as is done in the experiments ? When the mean of input is zero , there is no angle bias in the first layer . Also , what about if we include the bias term so that b + w a is the preactivation value ?

On the experimental side , it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error , but the test error is unfortunately increased for the particular task with the particular dataset in the experiments . It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines . It is intuitively expected that the proposed approach has some merit in some domains , but it is unclear exactly when and where it is .

Minor comments :

In Section 2.2 , is Layer 1 the input layer or the next ?

We thank the reviewer for the insightful comments on our paper .

--

Comment 1 : How about scaling each variable in a way that the mean becomes zero , instead of
scaling it into [ - 1 , + 1 ] as is done in the experiments ? When the mean of input is zero ,
there is no angle bias in the first layer .

Response 1 : We did experiments with CIFAR - 10 , in which each variable was scaled to have
zero mean . As the reviewer pointed out , we have no angle bias in the first layer ( the layer
after the input layer ) in this case .
However , the training of MLPs then got harder and the test accuracy was very row , even if
we applied either LCW or batch - normalization . We think this is because normalizing each pixel
of images in CIFAR - 10 ruined the relationship between pixels .

--

Comment 2 : What about if we include the bias term so that b + w a is the preactivation value ?

Response 2 : We have already included the bias term in our original experiment , although
it was omitted in Equation 2 for simplicity . We have modified Equation 2 to include
the bias term for clarity in the revised manuscript .

--

Comment 3 : It would be desirable to identify specific tasks and datasets for which
the proposed approach outperforms baselines . It is intuitively expected that the proposed
approach has some merit in some domains , but it is unclear exactly when and where it is .

Response 3 : We did additional experiments with the SVHN dataset and the CIFAR - 100 dataset ,
which are reported in the appendix B of the revised manuscript . The peak value of the test
accuracy of the proposed method was comparable to that of batch - normalization when the MLP
has 5 layers or 50 layers , as shown in Figure 12 ( f ) and ( i ) , Figure 14 ( f ) and ( i ) , and
Figure 15 ( f ) and ( i ) .
An interesting point is that the peak of the test accuracy is around 20 epochs in the
proposed method . However , we have no clear explanation for this finding . We have added
a description on this point in the third paragraph of Section 5.1 in the revised manuscript .

--

Comment 4 : In Section 2.2 , is Layer 1 the input layer or the next ?

Response 4 : Layer 1 is the layer next to the input layer . We have added an explanation of
these points to the first paragraph of Section 2.2.1 in the revised version .


The authors introduce the concept of angle bias ( angle between a weight vector w and input vector x ) by which the resultant pre-activation ( wx ) is biased if ||x| | is non - zero or ||w| | is non- zero ( theorm 2 from the article ) . The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction . Authors chose to add an additional optimization constraint LCW ( |w|=0 ) to achieve zero-mean pre-activation while , as mentioned in the article , other methods like batch normalization BN tend to push for |x|=0 and unit std to do the same .

Clearly , because of lack of scaling factor incase of LCW , like that in BN , it does not perform well when used with ReLU . When using with sigmoid the activation being bouded ( 0 ,1 ) seems to compensate for the lack of scaling in input . While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features . Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN .

We thank the reviewer for taking the time to evaluate our paper .

--

Comment 1 : The authors introduce the concept of angle bias ( angle between a weight vector w
and input vector x ) by which the resultant pre-activation ( wx ) is biased if ||x| | is non- zero
or ||w| | is non-zero ( theorem 2 from the article ) . The angle bias results in almost constant
activation independent of input sample resulting in no weight updates for error reduction .
Authors chose to add an additional optimization constraint LCW ( |w|=0 ) to achieve zero- mean
pre-activation while , as mentioned in the article .

Response 1 : We did not intend to indicate that the proposed method ( LCW ) adds additional
constraint ||w||=0 on weight vectors , and we have added an explanation to clearly state that
it is assumed that ||w| | > 0 in our paper to the first paragraph of Section 2.1 in the
revised manuscript .
The proposed method adds constraints ' w _1 + .. + w_m = 0 ' on weight vectors w , where
w = ( w_1 , ... , w_m ) ^\top in R^m , to force w perpendicular to 1 _m = ( 1 , ... , 1 ) in R^m ,
which is assumed to be the mean vector of the activation vector in the previous layer .

--

Comment 2 : Clearly , because of lack of scaling factor in case of LCW , like that in BN ,
it does not perform well when used with ReLU . When using with sigmoid the activation being
bounded ( 0 ,1 ) seems to compensate for the lack of scaling in input .

Response 2 : As the reviewer pointed out , the lack of scaling factor in LCW is a cause
for not performing well with ReLU . We tried ReLU6 ( = min ( max ( x , 0 ) , 6 ) ) instead
of ReLU with LCW , but it was still hard to train a deep MLP , in which the exploding gradient
still occurred . We are now developing methods to make LCW applicable to ReLU nets .

--

Comment 3 : While BN explicitly makes the activation zero-mean LCW seems to achieve it through
constraint on the weight features . Though it is shown to be computationally less expensive
LCW seems to work in only specific cases unlike BN .

Response 3 : We agree that LCW has limitation compared to BN as of now . However , it is also
very important to understand why batch normalization works so well in many situations .
We believe that reducing angle bias is a crucial role of batch normalization , and such
interpretation helps us to determine in which part of the network we should apply methods
like batch normalization .

Pros :
The paper is easy to read . Logic flows naturally within the paper .

Cons :

1 . Experimental results are neither enough nor convincing .

Only one set of data is used throughout the paper : the Cifar10 dataset , and the architecture used is only a 100 layered MLP . Even though LCW performs better than others in this circumstance , it does not prove its effectiveness in general or its elimination of the gradient vanishing problem . For the 100 layer MLP , it 's very hard to train a simple MLP and the training / testing accuracy is very low for all the methods . More experiments with different number of layers and different architecture like ResNet should be tried to show better results .

In Figure ( 7 ) , LCW seems to avoid gradient vanishing but introduces gradient exploding problem .

The proposed concept is only analyzed in MLP with Sigmoid activation function . In the experimental parts , the authors claim they use both ReLU and Sigmoid function , but no comparisons are reflected in the figures .

2 . The whole standpoint of the paper is quite vague and not very convincing .
In section 2 , the authors introduce angle bias and suggest its effect in MLPs that with random weights , showing that different samples may result in similar output in the second and deeper layers . However , the connection between angle bias and the issue of gradient vanishing lacks a clear analytical connection . The whole analysis of the connection is built solely on this one sentence " At the same time , the output does not change if we adjust the weight vectors in Layer 1 " , which is nowhere verified .

Further , the phenomenon is only tested on random initialization . When the network is trained for several iterations and becomes more settled , it is not clear how " angle affect " affects gradient vanishing problem .


Minors :
1 . Theorem 1,2,3 are direct conclusions from the definitions and are mis-stated as Theorems .

2 . ' patters ' -> ' patterns '

3 . In section 2.3 , reasons 1 and 2 state the similar thing that output of MLP has relatively small change with different input data when angle bias occurs . Only reason 1 mentions the gradient vanishing problem , even though the title of this section is " Relation to Vanishing Gradient Problem " .


We thank the reviewer for the insightful comments on our paper .

--

Comment 1 : Only one set of data is used throughout the paper : the Cifar10 dataset , and
the architecture used is only a 100 layered MLP .

Response 1 : We did additional experiments with the SVHN dataset and the CIFAR - 100 dataset
for each of which we trained 5 layered , 50 layered , and 100 layered MLPs .
Results are shown in Figure 12 , Figure 14 , and Figure 15 in the revised manuscript .

--

Comment 2 : For the 100 layer MLP , it 's very hard to train a simple MLP and the
training / testing accuracy is very low for all the methods .

Response 2 : We do not agree to the comment . The training accuracy for CIFAR - 10 or SVHN dataset
is high for the 100 layer MLP , if we apply LCW ( proposed method ) or batch normalization ,
as shown Figure 12 ( a ) and Figure 14 ( a ) in the revised manuscript .

--

Comment 3 : More experiments with different number of layers and different architecture
like ResNet should be tried to show better results .

Response 3 : As mentioned in Response 1 , we did experiments with several sizes of MLPs .
We also tried ResNet , but it was unable to train ResNet with LCW . This is mainly because
ReLU is used in ResNet , and the gradient explosion explained in Section 5.2 occurs .
We are now developing methods that make LCW applicable to ReLU nets , including ResNet .

--

Comment 4 : In Figure ( 7 ) , LCW seems to avoid gradient vanishing but introduces gradient exploding problem .

Response 4 : We agree to the comment . We have added an explanation on these points to the
second paragraph of Section 6 in the revised manuscript .


--

Comment 5 : The proposed concept is only analyzed in MLP with Sigmoid activation function .
In the experimental parts , the authors claim they use both ReLU and Sigmoid function ,
but no comparisons are reflected in the figures .

Response 5 : We omitted results with ReLU in the figures , because MLPs with ReLU were not
trainable at all when LCW is applied , as mentioned in Section 5.2.

--

Comment 6 : In section 2 , the authors introduce angle bias and suggest its effect in MLPs that
with random weights , showing that different samples may result in similar output in the second
and deeper layers . However , the connection between angle bias and the issue of gradient
vanishing lacks a clear analytical connection . The whole analysis of the connection is built
solely on this one sentence " At the same time , the output does not change if we adjust the
weight vectors in Layer 1 " , which is nowhere verified .

Response 6 : We have enriched the explanation in Section 2.1 in the revised manuscript ,
denoting that the shrinking of the distribution of the angle between the weight vector and the
activation vector is a reason for why the activation becomes almost constant in deep layers .
Moreover , we have added analytical results in Section 2.3 that examine the relationship
between the constant activation in deeper layers and the vanishing gradient of weights .

--

Comment 7 : The phenomenon is only tested on random initialization . When the network is trained
for several iterations and becomes more settled , it is not clear how " angle affect " affects
gradient vanishing problem .

Response 7 : We have added Figures 8 and 9 , which show the activation and the distribution of
angles in a MLP with sigmoid activation , respectively , after 10 epochs training .
We have also added discussions on these figures to the third paragraph of Section 3.1.1 in
the revised manuscript .

--

Comment 8 : Theorem 1,2,3 are direct conclusions from the definitions and are mis-stated as Theorems .

Response 8 : We have modified the manuscript to refer to these statements as propositions instead of theorems .

--

Comment 9 : ' patters ' -> ' patterns '

Response 9 : In accordance with the comment , we have modified the expression .

--

Comment 10 : In section 2.3 , reasons 1 and 2 state the similar thing that output of MLP has relatively
small change with different input data when angle bias occurs . Only reason 1 mentions the gradient
vanishing problem , even though the title of this section is " Relation to Vanishing Gradient Problem " .

Response 10 : In accordance with the comment , we have deleted the second reason from the manuscript .
Also , we have enriched the explanation related to reason 1 , as mentioned in Response 6 .


The paper discusses the problems of meta optimization with small look - ahead : do small runs bias the results of tuning ? The result is yes and the authors show how differently the tuning can be compared to tuning the full run . The Greedy schedules are far inferior to hand - tuned schedules as they focus on optimizing the large eigenvalues while the small eigenvalues can not be " seen " with a small lookahead . The authors show that this effect is caused by the noise in the obective function .

pro :
- Thorough discussion of the issue with theoretical understanding on small benchmark functions as well as theoretical work
- Easy to read and follow

cons :
- Small issues in presentation :
* Figure 2 " optimal learning rate " -> " optimal greedy learning rate " , also reference to Theorem 2 for increased clarity .
* The optimized learning rate in 2.3 is not described . This reduces reproducibility .
* Figure 4 misses the red trajectories , also it would be easier to have colors on the same ( log ? ) - scale .
The text unfortunately does not explain why the loss function looks so vastly different
with different look - ahead . I would assume from the description that the colors are based
on the final loss values obtaine dby choosing a fixed pair of decay exponent and effective LR .

Typos and notation :
page 7 last paragraph : " We train the all " -> We train all
notation page 5 : i find \nabla_{\theta_i} confusing when \theta_i is a scalar , i would propose \frac{\partial}{\partial \theta_i}
page 2 : " But this would come at the expense of long - term optimization process " : at this point of the paper it is not clear how or why this should happen . Maybe add a sentence regarding the large / Small eigenvalues ?

Q1 : The optimized learning rate in 2.3 is not described . This reduces reproducibility .
Sorry for such confusion . We use the losses formed by the forward dynamics given in Theorem 1 as training objective , and use Adam to find the learning rate and momentum at each time steps that minimize that training objective . The meta training learning rate is 0.003 , and it is trained for 500 meta training steps . We added this description in our revised version .

Q2 : Figure 4 misses the red trajectories , also it would be easier to have colors on the same ( log ? ) - scale .
Meta-descent on 20 k is time - consuming to run . Since the visualized hyper - surface is smooth , we expect that meta-descent will behave as expected to converge to the local minimum . We will add the red trajectory in the next version of the paper .

Q3 : Why the loss function looks so vastly different with different look - ahead ?
More number of look - ahead means more optimization steps . The loss will go lower as one trains longer .

Q4 . page 2 : " But this would come at the expense of long - term optimization process " : at this point of the paper it is not clear how or why this should happen . Maybe add a sentence regarding the large / Small eigenvalues ?
Thanks for your suggestion . We modified the entire paragraph as you and reviewer 3 suggested . We believe the current version is clearer .


This paper proposes a simple problem to demonstrate the short -horizon bias of the learning rate meta-optimization .

- The idealized case of quadratic function the analytical solution offers a good way to understand how T-step look ahead can benefit the meta-algorithm .
- The second part of the paper seems to be a bit disconnected to the quadratic function analysis . It would be helpful to understand if there is gap between gradient based meta-optimization and the best effort ( given by the analytical solution )
- Unfortunately , no guideline or solution is offered in the paper .

In summary , the idealized model gives a good demonstration of the problem itself . I think it might be of interest to some audiences in ICLR .

Q1 : The second part of the paper seems to be a bit disconnected to the quadratic function analysis . It would be helpful to understand if there is gap between gradient based meta-optimization and the best effort ( given by the analytical solution )
Ans : The second part of the paper experimentally verified the theory in the first part while generalizing to general neural networks with non- convex problems . It shows that quadratic analysis is a valid model for hyper - parameter optimization . We will work on the flow of the paper with more connection between the two parts of the paper .

Q2 : Unfortunately , no guideline or solution is offered in the paper .
Ans : We agree with the reviewer that in the current version of the paper there ’s no solution provided to the problem . We only offered one potential solution to the problem , following theorem 3 . Theorem 3 states that the greedy solution is optimal when the curvature is spherical and the noise is codiagonalizable with the curvature . This implies stochastic meta descent could work well with a good enough natural gradient method , such as Kronecker - factored approximate curvature ( K - FAC ) . We will show more experimental results on this subject in a later version .


This paper studies the issue of truncated backpropagation for meta-optimization . Backpropagation through an optimization process requires unrolling the optimization , which due to computational and memory constraints , is typically restricted or truncated to a smaller number of unrolled steps than we would like .

This paper highlights this problem as a fundamental issue limiting meta-optimization approaches . The authors perform a number of experiments on a toy problem ( stochastic quadratics ) which is amenable to some theoretical analysis as well as a small fully connected network trained on MNIST .

( side note : I was assigned this paper quite late in the review process , and have not carefully gone through the derivations -- specifically Theorems 1 and 2 ) .

The paper is generally clear and well written .

Major comments
-------------------------
I was a bit confused why 1000 SGD + mom steps pre-training steps were needed . As far as I can tell , pre-training is not typically done in the other meta-optimization literature ? The authors suggest this is needed because " the dynamics of training are different at the very start compared to later stages " , which is a bit vague . Perhaps the authors can expand upon this point ?

The conclusion suggests that the difference in greedy vs . fully optimized schedule is due to the curvature ( poor scaling ) of the objective -- but Fig 2 . and earlier discussion talked about the noise in the objective as introducing the bias ( e.g. from earlier in the paper , " The noise in the problem adds uncertainty to the objective , resulting in failures of greedy schedule " ) . Which is the real issue , noise or curvature ? Would running the problem on quadratics with different condition numbers be insightful ?

Minor comments
-------------------------
The stochastic gradient equation in Sec 2.2.2 is missing a subscript : " h_i " instead of " h "

It would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in Figure 2 , just to get a sense of how that compares with the greedy and optimized curves .

It looks like there was an upper bound constraint placed on the optimized learning rate in Figure 2 - - is that correct ? I could n't find a mention of the constraint in the paper . ( the optimized learning rate remains at 0.2 for the first ~ 60 steps ) ?

Figure 2 ( and elsewhere ) : I would change ' optimal ' to ' optimized ' to distinguish it from an optimal curve that might result from an analytic derivation . ' Optimized ' makes it more clear that the curve was obtained using an optimization process .

Figure 2 : can you change the line style or thickness so that we can see both the red and blue curves for the deterministic case ? I assume the red curve is hiding beneath the blue one -- but it would be good to see this explicitly .

Figure 4 is fantastic - - it succinctly and clearly demonstrates the problem of truncated unrolls . I would add a note in the caption to make it clear that the SMD trajectories are the red curves , e.g. : " SMD trajectories ( red ) during meta-optimization of initial effective ... " . I would also change the caption to use " meta-training losses " instead of " training losses " ( I believe those numbers are for the meta-loss , correct ? ) . Finally , I would add a colorbar to indicate numerical values for the different grayscale values .

Some recent references that warrant a mention in the text :
- both of these learn optimizers using longer numbers of unrolled steps :
Learning gradient descent : better generalization and longer horizons , Lv et al , ICML 2017
Learned optimizers that scale and generalize , Wichrowska et al , ICML 2017
- another application of unrolled optimization :
Unrolled generative adversarial networks , Metz et al , ICLR 2017

In the text discussing Figure 4 ( middle of pg. 8 ) , " which is obtained by using ... " should be " which are obtained by using ... "

In the conclusion , " optimal for deterministic objective " should be " deterministic objectives "

I think you could make a figure that much more clearly demonstrates the issue to replace or add to the current Figure 1 .

Compute the meta-loss for learning the learning rate for some small problem ( e.g. stochastic quadratics ) . This meta-loss is a 1D function over the learning rate . For a small number of unrolled steps , this function should have minima at low values of the learning rate . You can plot this meta-loss for different numbers of unrolls on the same graph , which should show that the minima of the meta-loss shifts to higher learning rates as you unroll for more steps . This is related to Figure 4 , but I think would be a nice way to introduce the problem in an easily digestible picture .

Q1 : Why 1000 SGD + mom steps pre-training steps were needed ?
We want to choose a setting that our observation is less sensitive to which part of training . If we always start looking ahead at zeroth step , then there is a higher chance that the optimal hyperparameter is only fitted to the beginning ; whereas if we start at some pre-trained steps , e.g. 1000 , then the optimal hyperparameter is more likely to generalize to , say 500 or 5000 steps .

Q2 : Which is the real issue , noise or curvature ?
The problem will arise if you have both noise in the objective and different curvature directions . We showed that in a deterministic problem , the greedy optimal learning rate and momentum is optimal as it is essentially doing conjugate gradient , regardless of how many different curvature directions you have . We also showed in theorem 3 that the greedy learning rate is optimal if the curvature is spherical . On the other hand , if there ’s noise in the objective , and there are many different curvature directions the problem will arise . This is because , the noise in the objective forbids one to completely get rid of the loss on a particular direction . Hence , one should always first remove the loss on low curvature directions and then move onto high curvature directions . But short -horizon objective encourages the opposite because high curvature directions gives most rapid decrease in loss . Therefore , both noise in the objective and different curvature directions cause the problem .

Q3 : Figure 2 : 1 . Show fixed learning rate . 2 . Thickness of the red curve . 3 . Upper bound .
Figure 2 is edited as reviewer suggests . Also the reviewer is correct that we upper bounded the learning rate to avoid the loss on any curvature direction becoming larger than its initial value , so as to assure the quadratic assumption . We added the description in the revised version .

Q4 : Figure 4 : 1 . Add a color bar to indicate numerical values for the different grayscale values .
Thanks for the suggestion . We will add it in the next version of our paper .

Q5 : Citations :
We added those citations reviewer mentioned .

In summary , the paper introduces a memory module to the GANs to address two existing problems : ( 1 ) no discrete latent structures and ( 2 ) the forgetting problem . The memory provides extra information for both the generation and the discrimination , compared with vanilla GANs . Based on my knowledge , the idea is novel and the Inception Score results are excellent . However , there are several major comments should be addressed , detailed as follows :

1 . The probabilistic interpretation seems not correct .

According to Eqn ( 1 ) , the authors define the likelihood of a sample x given a slot index c as p( x|c=i ) = N ( q ; K_i , sigma^2 ) , where q is the normalized output of a network mu given x . It seems that this is not a well defined probability distribution because the Gaussian distribution is defined over the whole space while the support of q is restricted within a simplex due to the normalization . Then , the integral over x should be not equal to 1 and hence all of the probabilistic interpretation including the equations in the Section 3 . and results in the Section 4.1. are not reliable . I 'm not sure whether there is anything misunderstood because the writing of the Section 3 is not so clear .

2 . The writing of the Section 3 should be improved .

Currently , the Section 3 is not easy to follow for me due to the following reasons . First , there lacks a coherent description of the notations . For instance , what 's the difference between x and x ' , used in Section 3.1.1 and 3.1.2 respectively ? According to the paper , both denote a sample . Second , the setting is somewhat unclear . For example , it is not natural to discuss the posterior without the clear definition of the likelihood in Eqn ( 1 ) . Third , a lot of details and comparison with other methods should be moved to other parts and the summary of the each part should be stated explicitly and clearly before going into details .

3 . Does the large memory hurt the generalization ability of the GANs ?

First of all , I notice that the random noise is much lower dimensional than the memory , e.g. 2 v.s. 256 on affine-MNIST . Does such large memory hurt the generalization ability of GANs ? I suspect that most of the information are stored in the memory and only small change of the training data is allowed . I found that the samples in Figure 1 and Figure 5 are very similar and the interpolation only shows a very small local subspace near by a training data , which cannot show the generalization ability . Also note that the high Inception Score can not show the generalization ability as well because memorizing the training data will obtain the highest score . I know it 's hard to evaluate a GAN model but I think the authors can at least show the nearest neighbors in the training dataset and the training data that maximizes the activation of the corresponding memory slot together with the generated samples to see the difference .

Besides , personally speaking , Figure 1 is not so fair because a Memory GAN only shows a very small local subspace near by a training data while the vanilla GAN shows a large subspace , making the quality of the generation different . The Memory GAN also has failure samples in the whole latent space as shown in Figure 4 .

Overall , I think this paper is interesting but currently it does not reach the acceptance threshold .

I change the rating to 6 based on the revised version , in which most of the issues are addressed .

We thank Reviewer 1 for positive and constructive reviews . Below , we respond each comment in details . Please see blue fonts in the newly uploaded draft to check how our paper is updated .

1 . The probabilistic interpretation .
Thanks for pointing out the unclearness of our formulation . First of all , the normalizing constant does not affect the model formulation , because it is a common denominator in the posterior . However , as Reviewer 1 pointed out , we use distributions on a unit sphere , and thus they should be Von Mises - Fisher ( vMF ) distributions with a concentration constant k=1 , instead of Gaussian distributions . Without changing any fundamentals of Memory GAN , we change the Gaussian mixtures to Von Mises - Fisher Mixtures in the draft . We appreciate Reviewer 1 for the correction .

2 . Writing improvement of Section 3.
( 1 ) The difference between x and x ' in Section 3.1.1-3.1.2.
We used x to denote samples for updating discriminator parameters , and x’ for updating the memory module . Since every training sample goes through these two updating operations , there is no need to use both , and we unify them to x.
( 2 ) Discuss the posterior without the clear definition of the likelihood in Eq . ( 1 ) .
The likelihood for Eq. ( 1 ) is identical to that of the standard vMF mixture model . Thus , we omitted it and directly introduced the posterior equation . We will clarify them .
( 3 ) Overall organization
We will re-organize the draft so that key ideas in each part are explicitly summarized before the details .

3 . Generalization ability .
As Reviewer 1 suggested , we add an additional result to the Figure 5 , where for each sample produced by MemoryGAN ( in the left - most column ) , the seven nearest images in the training set are shown in the following columns . Apparently , our Memory GAN generates novel images rather than merely memorizing and retrieving the images in the training set .
The memory is used to represent not only positive samples but also possible fake samples . Thus , the memory size is rather large ( n=16384 ) , for the CIFAR10 dataset . That is , the more diverse the dataset is , the larger memory size is required to represent both variability . In our experiments , we set the memory size based on the performance on the validation set .

4 . Fig.1.
Initially , we fixed the discrete latent variable c for Memory GAN , because it is a memory index , and thus it is meaningless to interpolate over c. However , we follow Reviewer ’s rationale and update Fig.1 in the new draft . Please check it .
In new Fig.1 . ( b , d ) , we first randomly sample both ( z , c ) shown at the four corners in blue boxes . We then generate 64 images by interpolating both ( z , c ) . However , since the interpolation over c is meaningless , we take key values K_c of the four randomly sampled c’s , and then perform interpolation over their K_c ’s . Then , for each interpolated K_c’ , we find the memory slot c = argmax p( c| K_c’ ) , i.e. the memory index whose posterior is the highest with respect to K_c ’.
As shown in Fig.1 . ( b , d ) , different classes are shown at the four corners , and other samples gradually change , but no structural discontinuity occurs . We hope the modified Fig.1 delivers the merits of Memory GAN more intuitively .

5 . Failure cases of Fig.4.
As Reviewer 1 pointed out , Memory GAN also has failure samples in the whole latent space as shown in Figure 4 . Since our approach is completely unsupervised , sometimes a single memory slot may include similar images from different classes . It causes failure cases . Nevertheless , significant proportion of memory slots of Memory GAN contain similar shaped single class , which leads much better performance than existing unsupervised GAN models .


MemoryGAN is proposed to handle structural discontinuity ( avoid unrealistic samples ) for the generator , and the forgetting behavior of the discriminator . The idea to incorporate memory mechanism into GAN is interesting , and the authors make nice interpretation why this needed , and clearly demonstrate which component helps ( including the connections to previous methods ) .

My major concerns :

Figure 1 is questionable in demonstrating the advantage of proposed Memory GAN . My understanding is that four z's used in DCGAN and Memory GAN are " randomly sampled " and fixed , interpolation is done in latent space , and propagate to x to show the samples . Take MNIST for example , It can be seen that the DCGAN has to ( 1 ) transit among digits in different classes , while MemoryGAN only ( 2 ) transit among digits in the same class . Task 1 is significantly harder than task 2 , it is not surprise that DCGAN generate unrealistic images . A better experiment is to fix four digits from different class at first , find their corresponding latent codes , do interpolation , and propagate back to sample space to visualize results . If the proposed technique can truly handle structural discontinuity , it will " jump " over the sample manifold from one class to another , and thus avoid unrealistic samples . Also , the current illustration also indicates that the generated samples by Memory GAN is not diverse .

It seems the memory mechanism can bring major computational overhead , is it possible to provide the comparison on running time ?

To what degree the Memory GAN can handle structural discontinuity ? It can be seen from Table 2 that larger improvement is observed when tested on a more diverse dataset . For example , the improvement gap from MNIST to CIFAR is larger . If the Memory GAN can truly deal with structural discontinuity , the results on generating a wide range of different images for ImageNet may endow the paper with higher impact .

The authors should consider to make their code reproducible and public .


Minor comments :

In Section 4.3 , Please fix " Results in 2 " as " Results in Table 2 " .




We thank Reviewer 2 for positive and constructive reviews . Below , we respond each comment in details . Please see blue fonts in the newly uploaded draft to check how our paper is updated .

1 . Fig.1.
Initially , we fixed the discrete latent variable c for Memory GAN , because it is a memory index , and thus it is meaningless to interpolate over c. However , we follow Reviewer ’s rationale and update Fig.1 in the new draft . Please check it .
In new Fig.1 . ( b , d ) , we first randomly sample both ( z , c ) shown at the four corners in blue boxes . We then generate 64 images by interpolating both ( z , c ) . However , since the interpolation over c is meaningless , we take key values K_c of the four randomly sampled c’s , and then perform interpolation over their K_c ’s . Then , for each interpolated K_c’ , we find the memory slot c = argmax p( c| K_c’ ) , i.e. the memory index whose posterior is the highest with respect to K_c ’.
As shown in Fig.1 . ( b , d ) , different classes are shown at the four corners , and other samples gradually change , but no structural discontinuity occurs . We hope the modified Fig.1 delivers the merits of Memory GAN more intuitively .

2 . Computation overhead .
As we replied to Reviewer 2 , we measure the training time per epoch for MemoryGAN ( 4,128 K parameters ) and DCGAN ( 2,522 K parameters ) , which are 135 sec and 124 sec , respectively .
It means MemoryGAN is only 8.9 % slower than DCGAN for training , even with a scalable memory module . At test time , since only generator is used , there is no time difference between MemoryGAN and DCGAN .

3 . Image Net experiments .
We observed that the memory module significantly helps improve the performance when using highly diverse datasets . For example , inception scores are higher for CIFAR10 than for FashionMNIST . Thus , as Reviewer 2 suggested , we can easily expect that the our Memory GAN works better for the ImageNet dataset . We did not test with ImageNet , mainly because of too long training time ( more than two weeks by our estimation ) . However , we will do it as a future work .

4 . Source code and typos .
We plan to make public the source code .
Thank you for correct typos !


[ Overview ]

In this paper , the authors proposed a novel model called MemoryGAN , which integrates memory network with GAN . As claimed by the authors , Memory GAN is aimed at addressing two problems of GAN training : 1 ) difficult to model the structural discontinuity between disparate classes in the latent space ; 2 ) catastrophic forgetting problem during the training of discriminator about the past synthesized samples by the generator . It exploits the life - long memory network and adapts it to GAN . It consists of two parts , discriminative memory network ( DMN ) and Memory Conditional Generative Network ( MCGN ) . DMN is used for discriminating input samples by integrating the memory learnt in the memory network , and MCGN is used for generating images based on random vector and the sampled memory from the memory network . In the experiments , the authors evaluated memory GAN on three datasets , CIFAR - 10 , affine -MNIST and Fashion -MNIST , and demonstrated the superiority to previous models . Through ablation study , the authors further showed the effects of separate components in memory GAN .

[ Strengths ]

1 . This paper is well - written . All modules in the proposed model and the experiments were explained clearly . I enjoyed much to read the paper .

2 . The paper presents a novel method called MemoryGAN for GAN training . To address the two infamous problems mentioned in the paper , the authors proposed to integrate a memory network into GAN . Through memory network , Memory GAN can explicitly learn the data distribution of real images and fake images . I think this is a very promising and meaningful extension to the original GAN .

3 . With Memory GAN , the authors achieved best Inception Score on CIFAR - 10 . By ablation study , the authors demonstrated each part of the model helps to improve the final performance .

[ Comments ]

My comments are mainly about the experiment part :

1 . In Table 2 , the authors show the Inception Score of images generated by DCGAN at the last row . On CIFAR - 10 , it is ~ 5.35 . As the authors mentioned , removing EM , MCGCN and Memory will result in a conventional DCGAN . However , as far as I know , DCGAN could achieve > 6.5 Inception Score in general . I am wondering what makes such a big difference between the reported numbers in this paper and other papers ?

2 . In the experiments , the authors set N = 16,384 , and M = 512 , and z is with dimension 16 . I did not understand why the memory size is such large . Take CIFAR - 10 as the example , its training set contains 50 k images . Using such a large memory size , each memory slot will merely count for several samples . Is a large memory size necessary to make Memory GAN work ? If not , the authors should also show ablated study on the effect of different memory size ; If it is true , please explain why is that . Also , the authors should mention the training time compared with DCGAN . Updating memory with such a large size seems very time - consuming .

3 . Still on the memory size in this model . I am curious about the results if the size is decreased to the same or comparable number of image categories in the training set . As the author claimed , if the memory network could learn to cluster training data into different category , we should be able to see some interesting results by sampling the keys and generate categoric images .

4 . The paper should be compared with InfoGAN ( Chen et al. 2016 ) , and the authors should explain the differences between two models in the related work . Similar to Memory GAN , InfoGAN also did not need any data annotations , but could learn the latent code flexibly .

[ Summary ]

This paper proposed a new model called Memory GAN for image generation . It combined memory network with GAN , and achieved state - of - art performance on CIFAR - 10 . The arguments that Memory GAN could solve the two infamous problem make sense . As I mentioned above , I did not understand why the authors used such large memory size . More explanations and experiments should be conducted to justify this setting . Overall , I think Memory GAN opened a new direction of GAN and worth to further explore .



We thank Reviewer 3 for positive and constructive reviews . Below , we respond each comment in details . Please see blue fonts in the newly uploaded draft to check how our paper is updated .

1 . DCGAN inception scores .
Thanks for a correction . As R3 pointed out , the DCGAN inception score of the original paper is 6.54+-0.67 . The value 5.35 that we reported previously was the score of “ MemoryGAN without memory ” , which is identical to the DCGAN in terms of model structure . That was the reason why we named it as DCGAN . However , the “ MemoryGAN without memory ” had different details from the DCGAN , including the ELU activation ( instead of ReLU and Leaky ReLU ) and layer - normalization ( instead of batch normalization ) . To resolve the confusion , we change the values of Table 1 to 6.54+-0.67 ( the numbers reported in the original DCGAN paper ) .

2 . The memory size of MemoryGAN .
In our experiments , we set the memory size based on the performance on the validation set . The memory is used to represent not only positive samples but also possible fake samples . Thus , the memory size is rather large ( n=16384 ) , for the CIFAR10 dataset whose size is 50,000 . That is , the more diverse the dataset is , the larger memory size is required to represent both variability . When we used a half - size memory ( n=8192 ) , the inception score for CIFAR10 decreased from 8.04 to 6.71 .
As Reviewer 3 suggested , we test with decreasing the memory size to n=16 , which is similar to the number of classes , on the Fashion-MNIST and CIFAR10 datasets . We obtain the inception score 6.14 for Fashion-MNIST with n=16 , which is slightly lower than the reported score 6.39 with n=4096 . On the other hand , for CIFAR10 , the inception score significantly decreases from 8.04 with n= 16384 to 3.06 with n= 16 . These results indicate that intra-class variability of Fashion-MNIST is small , while that of CIFAR10 is very high .

3 . Training / Test time .
The training time per epoch for MemoryGAN ( 4,128 K parameters ) and DCGAN ( 2,522 K parameters ) are 135 sec and 124 sec , respectively . It means MemoryGAN is only 8.9 % slower than DCGAN for training , even with a scalable memory module . At test time , since only the generator is used , there is no time difference between MemoryGAN and DCGAN .

4 . Comparison with InfoGAN .
There are two key differences between InfoGAN and Memory GAN . First , InfoGAN implicitly learns the latent cluster information of data into model parameters , while Memory GAN explicitly maintains the information about the whole training set using a life - long memory network . Thus , Memory GAN keeps track of current cluster information stably and flexibly without suffering from forgetting old samples . Second , MemoryGAN explicitly offers various distributions like prior distribution p( c ) , conditional likelihood p( x|c ) and marginal likelihood p( x ) , unlike InfoGAN . Such interpretability is useful for designing or training the models .


The authors has addressed my concerns , so I raised my rating .

The paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting .

There are no results on large corpora such as 1 billion tokens benchmark corpus , or at least medium level corpus with 50 million tokens . The corpora the authors choose are quite small , the variance of the estimates are high , and similar conclusions might not be valid on a large corpus .

[ 1 ] provides the results of character level language models on Enwik8 dataset , which shows regularization does n't have much effect and needs less tuning . Results on this data might be more convincing .

The results of MOS is very good , but the computation complexity is much higher than other baselines . In the experiments , the embedding dimension of MOS is slightly smaller , but the number of mixture is 15 . This will make it less usable , I think it 's necessary to provide the training time comparison .

Finally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER .

[ 1 ] Melis , Gábor , Chris Dyer , and Phil Blunsom . " On the state of the art of evaluation in neural language models . " arXiv preprint arXiv:1707.05589 ( 2017 ) .

[ 2 ] Joris Pelemans , Noam Shazeer , Ciprian Chelba , Sparse Non-negative Matrix Language Modeling , Transactions of the Association for Computational Linguistics , vol. 4 ( 2016 ) , pp. 329-342

[ 3 ] Shazeer et al. ( 2017 ) . Outrageously Large Neural Networks : The Sparsely - Gated Mixture-of-Experts Layer . ICLR 2017


Thanks for your valuable comments .

[ [ Large-scale experiment ] ] : We ’ve added a “ large-scale language modeling experiment ” using the 1B Word Dataset ( section 3.1 ) , where MoS significantly outperforms the baseline model by a large margin . This indicates that MoS consistently outperforms Softmax , regardless of the scale of the dataset . Also , note that PTB and WT2 are two de - facto benchmarks widely used in previous work on language modeling . None of the following papers had experiments on datasets larger than WT2 : Zoph & Le ICLR 2017 , Zilly et al ICML 2017 , Inan et al ICLR 2017 , Grave et al ICLR 2017 , Merity et al ICLR 2017 .

[ [ Character - level LM ] ] : Firstly , note that the largest possible rank of the log-probability matrix is upper bounded by the vocabulary size . In character - level LM , the vocabulary size is usually much smaller than the embedding size . In this case , Softmax does not suffer from the rank bottleneck problem , and we expect MoS and Softmax to achieve similar performance in practice . To verify our expectation , we perform character - level LM experiment on the text8 dataset , where MoS and Softmax indeed achieve almost the same performance ( section 3.2 & appendix C.2 in the updated version ) .

[ [ Training time ] ] : We have added the training time analysis for MoS and provided empirical numbers in the updated versions of the paper ( section 3.3 & Appendix C.3 ) . In general , computational wall time of MoS is actually sub-linear w.r.t. the number of mixture components . In most settings , we observe a two to three times slowdown compared to Softmax when using up to 15 components for MoS . We believe such additional computational cost is acceptable for the following reasons :
- MoS is highly parallelizable , meaning that using more machines can always speed up the computation almost linearly .
- The field of deep learning systems ( both hardware and software ) is making rapid progress . It might be possible to further optimize MoS on GPUs for fast computation . More developed hardware systems would also further reduce the computational cost .
- Historically , important techniques sometimes come with an additional computational cost , e.g. , LSTM , attention , deep ResNets . We believe that with MoS , the extra cost is reasonable and the gain is substantial .

[ [ Application to MT / ASR ] ] : We believe this is best left to future research , as performing rigorous experiments and careful comparison for such a real - world applications is non-trivial . And we believe language modeling is of its own importance already .


The authors argue in this paper that due to the limited rank of the context - to- vocabulary logit matrix in the currently used version of the softmax output layer , it is not able to capture the full complexity of language . As a result , they propose to use a mixture of softmax output layers instead where the mixing probabilities are context - dependent , which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components ( here 15 ) . This leads to improvements in the word - level perplexities of the PTB and wikitext2 data sets , and Switchboard BLEU scores .

The question of the expressiveness of the softmax layer , as well as its suitability for word - level prediction , is indeed an important one which has received too little attention . This makes a lot of the questions asked in this paper extremely relevant to the field . However , it is unclear that the rank of the logit matrix is the right quantity to consider . For example , it is easy to describe a rank D NxM matrix where up to 2 ^ D lines have max values at different indices . Further , the first two " observations " in Section 2.2 would be more accurately described as " intuitions " of the authors . As they write themselves " there is no evidence showing that semantic meanings are fully linearly correlated . " Why then try to link " meanings " to basis vectors for the rows of A ?

To be clear , the proposed model is undoubtedly more expressive than a regular softmax , and although it does come at a substantial computational cost ( a back - of - the envelope calculation tells us that computing 15 components of 280d MoS takes the same number of operations as one with dimension 1084 = sqrt ( 280*280 * 15 ) ) , it apparently manages not to drastically increase overfitting , which is significant .

Unfortunately , this is only tested on relatively small data sets , up to 2M tokens and a vocabulary of size 30 K for language modeling . They do constitute a good starting place to test a model , but given the importance of regularization on those specific tasks , it is difficult to predict how the MoS would behave if more training data were available , and if one could e.g. simply try a 1084 dimension embedding for the softmax without having to worry about overfitting .

Another important missing experiment would consist in varying the number of mixture components ( this could very well be done on WikiText2 ) . This could help validate the hypothesis : how does the estimated rank vary with the number of components ? How about the performance and pairwise KL divergence ?

This paper offers a promising direction for language modeling research , but would require more justification , or at least a more developed experimental section .

Pros :
- Important starting question
- Thought - provoking approach
- Experimental gains on small data sets

Cons :
- The link between the intuition and reality of the gains is not obvious
- Experiments limited to small data sets , some obvious questions remain

Thank you for the valuable feedback .

[ [ Rank and meanings ] ] It could be possible that A is low-rank for a natural language as it is hard to rule out this possibility rigorously , but we hypothesize that A is high -rank . Our hypothesis is supported by our intuitive reasoning and empirical experiments . Empirically , we give three more pieces of evidences supporting our hypothesis that the rank is the key bottleneck of Softmax and MoS improves the performance by solving the rank bottleneck ( section 3.2 ) :
- Before the rank saturates to the full rank , using more mixture components in MoS continues to increase the rank of the log-probability matrix . Further , when the rank increases , the perplexity also decreases .
- MoS has a similar generalization gap compared to Softmax , which rules out the concern that the improvement actually comes from some unexpected regularization effects of MoS.
- In character - level language modeling , since the largest possible rank is upper bounded by the limited vocabulary size , Softmax does not suffer from the rank bottleneck . In this case , Softmax and MoS have almost the same performance , which matches our analysis .
We agree that linking semantic meanings to bases lacks rigor and this would be better described as intuitions . We have made corresponding changes in the paper .

[ [ Computation vs. Capacity ] ] : It is true that MoS involves a larger amount of computation compared to the standard Softmax . However , [ Collins et al ] suggests the capacity of neural language models is mostly related to the number of parameters , rather than computation . Moreover , powerful models often require a larger amount of computation . For example , the attention based seq2seq model involves much more computation compared to the vanilla seq2seq .

[ [ Large-scale experiment ] ] : We have added a “ large-scale language modeling experiment ” using the 1B Word Dataset ( section 3.1 ) , where MoS significantly outperforms the baseline model with a large margin . This indicates that MoS consistently outperforms Softmax , regardless of the scale of the dataset . Also , note that PTB and WT2 are two de - facto benchmarks widely used in previous work on language modeling . None of the following papers had experiments on datasets larger than WT2 : Zoph & Le ICLR 2017 , Zilly et al ICML 2017 , Inan et al ICLR 2017 , Grave et al ICLR 2017 , Merity et al ICLR 2017 .

[ [ Varying the number of mixtures ] ] : Thanks for the suggestion . We performed this experiment , whose result is summarized in the second bullet point of section 3.2 ( updated version ) . As expected , the number of mixture components is positively correlated with the empirical rank . More importantly , before the rank saturates to the full rank , MoS with a higher rank leads to a better performance ( lower perplexity ) .

------------------------------------------------------------------------------------------------------------------------
[ Collins et al ] Capacity and Trainability in Recurrent Neural Networks


Language models are important components to many NLP tasks . The current state - of- the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN 's hidden state . This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes . The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization .

Pros :
-- The paper is very well written and easy to follow . The ideas build up on each other in an intuitive way .
-- The idea behind the paper is novel : translating language modeling into a matrix factorization problem is new as far as I know .
-- The maths is very rigorous .
-- The experiment section is thorough .

Cons :
-- To claim SOTA all models need to be given the same capacity ( same number of parameters ) . In Table 2 the baselines have a lower capacity . This is an unfair comparison
-- I suspect the proposed approach is slower than the baselines . There is no mention of computational cost . Reporting that would help interpret the numbers .

The SOTA claim might not hold if baselines are given the same capacity . But regardless of this , the paper has very strong contributions and deserves acceptance at ICLR .

Thank you for the valuable comments .

[ [ Claim of SOTA ] ] : We believe the 2M difference in the number of parameters is negligible compared to the number of parameters we use ( i.e. , 35M ) . In fact , we ran MoS in another setting with 31M parameters and got 63.59 on WT2 without finetuning , compared to 63.33 obtained by our best - performing model .

[ [ Training time ] ] : Thanks for the suggestion and we have added the training time analysis for MoS and provided empirical numbers in the updated versions of the paper ( section 3.3 & Appendix C.3 ) . In general , computational wall time of MoS is actually sub-linear w.r.t. the number of mixture components . In most settings , we observe a two to three times slowdown compared to Softmax when using up to 15 components for MoS . We believe such additional computational cost is acceptable for the following reasons :
- MoS is highly parallelizable , meaning that using more machines can always speed up the computation almost linearly .
- The field of deep learning systems ( both hardware and software ) is making rapid progress . It might be possible to further optimize MoS on GPUs for fast computation . More developed hardware systems would also further reduce the computational cost .
- Historically , important techniques sometimes come with an additional computational cost , e.g. , LSTM , attention , deep ResNets . We believe that with MoS , the extra cost is reasonable and the gain is substantial .


This paper is an extension of the “ prototypical network ” which will be published in NIPS 2017 . The classical few - shot learning has been limited to using the unlabeled data , while this paper considers employing the unlabeled examples available to help train each episode . The paper solves a new semi-supervised situation , which is more close to the setting of the real world , with an extension of the prototype network . Sufficient implementation detail and analysis on results .

However , this is definitely not the first work on semi-supervised formed few - shot learning . There are plenty of works on this topic [ R1 , R2 , R3 ] . The authors are advised to do a thorough survey of the relevant works in Multimedia and computer vision community .

Another concern is that the novelty . This work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data .

The experiments are also not enough . Not only some other works such as [ R1 , R2 , R3 ] ; but also the other naïve baselines should also be compared , such as directly nearest neighbor classifier , logistic regression , and neural network in traditional supervised learning . Additionally , in the 5 - shot non-distractor setting on tiered ImageNet , only the soft kmeans method gets a little bit advantage against the semi-supervised baseline , does it mean that these methods are not always powerful under different dataset ?

[ R1 ] “ Videostory : A new multimedia embedding for few - example recognition and translation of events , ” in ACM MM , 2014

[ R2 ] “ Transductive Multi- View Zero - Shot Learning ” , IEEE TPAMI 2015

[ R3 ] “ Video2vec embeddings recognize events when examples are scarce , ” IEEE TPAMI 2014


“ There are plenty of works on this topic … ”
We also thank the reviewer for pointing out related zero - shot learning literature and we will study them and add those references to the next version of the paper . Based on our preliminary reading , [ 1 ] is a journal version that builds on top of [ 2 ] , with both papers presenting very similar approaches for the application of event recognition in videos . Transductive Multi-View Zero - Shot Learning [ 3 ] uses a similar label propagation procedure as ours . However , while [ 3 ] uses standalone deep feature extractors , we show that our semi-supervised prototypical network can be trained completely end - to-end . One of the non-trivial results of our paper is that we show that end - to - end meta-learning significantly improves the performance ( see Semi-supervised Inference vs. Soft K-means ) . We would like to emphasize that end - to - end semi-supervised learning in a meta-learning framework is , to the best of our knowledge , a novel contribution .

“ ... other naïve baselines should also be compared ... ”
The recent literature on few - shot learning has established that meta-learning - based approaches outperform kNN and standard neural network based approaches . For the Omniglot dataset , Mann et al . [ 4 ] has previously studied baselines such as KNN either in pixel space or deep features , and feedforward NNs . They found these baselines all lag behind their method by quite a lot , and meanwhile Prototypical Networks outperform Mann et al. by another significant margin . For example , Table 1 summarizes the performance for 5 - shot , 5 - way classification . Therefore , we will provide supervised nearest neighbor , logistic regression , and neural network baselines for completeness ; however , we believe that our work is built on top of state - of - the- art methods , and should beat these simple baselines .

Table 1 - Omniglot dataset baselines
Method Accuracy
KNN pixel 48 %
KNN deep 69 %
Mann et al . [ 4 ] 88 %
ProtoNet 99.7 %

“ ... not always powerful under different dataset ? ”
For completeness we ran both 1 - shot and 5 - shot settings and found that our method consistently outperforms the baselines . While in 5 - shot the improvement is less , this is reasonable since the number of labeled items is larger and the benefit brought by unlabeled items is considerably smaller than in 1 - shot settings . We disagree with the comment that our model is not robust under different datasets , since the best settings we found is consistent across all three , quite diverse , datasets , including the novel and much larger tiered ImageNet .

References :
[ 1 ] “ Video2vec embeddings recognize events when examples are scarce , ” IEEE TPAMI 2014
[ 2 ] “ Videostory : A new multimedia embedding for few - example recognition and translation of events , ” in ACM MM , 2014 .
[ 3 ] : Transductive Multi-View Zero - Shot Learning , IEEE TPAMI 2015 .
[ 4 ] : One-shot learning with Memory -Augmented Neural Networks . ICML 2016 .

In this paper , the authors studied the problem of semi-supervised few - shot classification , by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes . The studied problem is interesting , and the paper is well - written . Extensive experiments are performed to demonstrate the effectiveness of the proposed methods . While the proposed method is a natural extension of the existing works ( i.e. , soft k-means and meta-learning ) . On top of that , It seems the authors have over - claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outliers . Overall , I would like to vote for a weakly acceptance regarding this paper .

Thank you for the comments . We ’d like to clarify our setup here : The problem as we have defined it is to correctly perform the given N- way classification in each episode ( similarly as in the previous work ) . Distractors are introduced to make the problem harder in a more realistic way , but the goal is not to be able to classify them . Specifically , our model needs to understand which points are irrelevant for the given classification task ( “ distractors ” ) in order to not take them into account , but actually classifying these distractors into separate categories is not required in order to perform the given classification task , so our models make no effort to do this .

Further , we would like to emphasize that adding distractor examples in few - shot classification settings is a novel and more realistic learning environment compared to previous approaches in supervised few - shot learning and as well as concurrent approaches in semi-supervised few - shot learning [ 1 , 2 ] . It is non-trivial to show that various versions of semi-supervised clustering can be trained end - to - end from scratch as another layer on top of prototypical networks , with the presence of distractor clusters ( note that each distractor class has the same number of images as a non-distractor class ) .

References :
[ 1 ] : Few - Shot Learning with Graph Neural Networks . Anonymous . Submitted to ICLR , 2017 .
[ 2 ] : Semi- Supervised Few - Shot Learning with Prototypical Networks . Rinu Boney and Alexander Ilin. CoRR , abs /1711.10856 , 2017 .

This paper proposes to extend the Prototypical Network ( NIPS17 ) to the semi-supervised setting with three possible
strategies . One consists in self - labeling the unlabeled data and then updating the prototypes on the basis of the
assigned pseudo-labels . Another is able to deal with the case of distractors i.e. unlabeled samples not beloning to
any of the known categories . In practice this second solution is analogous to the first , but a general ' distractor ' class
is added . Finally the third technique learns to weight the samples according to their distance to the original prototypes .

These strategies are evaluated in a particular semi-supervised transfer learning setting : the models are first trained
on some source categories with few labeled data and large unlabeled samples ( this setting is derived by subselecting
multiple times a large dataset ) , then they are used on a final target task with again few labeled data and large
unlabeled samples but beloning to a different set of categories .

+ the paper is well written , well organized and overall easy to read
+ / - this work builds largely on previous work . It introduces only some small technical novelty inspired by soft -k- means
clustering that anyway seems to be effective .
+ different aspect of the problem are analyzed by varying the number of disctractors and varying the level of
semantic relatedness between the source and the target sets

Few notes and questions
1 ) why for the omniglot experiment the table reports the error results ? It would be better to present accuracy as for the other tables / experiments
2 ) I would suggest to use source and target instead of train and test -- these two last terms are confusing because
actually there is a training phase also at test time .
3 ) although the paper indicate that there are different other few - shot methods that could be applicable here ,
no other approach is considered besides the prothotipical network and its variants . An further external reference
could be used to give an idea of what would be the experimental result at least in the supervised case .






We appreciate the constructive comments from reviewer 2 and we are delighted to learn that the reviewer feels that our paper is well written and organized .

“ builds largely on previous work … only some small technical novelty … ”
We would like to emphasize that we introduce a new task for few - shot classification , incorporating unlabeled items . This is impactful as follow - up work can use our dataset as a public benchmark . In fact , there are several concurrent ICLR submissions and arxiv pre-prints [ 1 , 2 ] that also introduce semi-supervised few - shot learning . However compared to these concurrent papers , our benchmark extends beyond this work into more realistic and generic settings , with hierarchical class splits and unlabeled distractor classes , which we believe will make positive contributions to the community .

The fact that our semi-supervised prototypical network can be trained end - to - end from scratch is non-trivial , especially under many distractor clusters ( note that each distractor class has the same number of images as a non-distractor class ) . We argue that our extension is simple yet effective , serving as another layer on top of the regular prototypical network layer , and provides consistent improvement in the presence of unlabeled examples . Further , to our knowledge , our best - performing method , the masked soft k-means , is novel .

“ It would be better to present accuracy … ”
Thank you for the suggestion . We will revise it in our next version .

“ no other approach is considered besides the prototypical network and its variants . ”
ProtoNets is one of the top performing methods for few - shot learning and our proposed extensions each naturally forms another layer on top of the Prototypical layer . To address the concern , we are currently running other variants of the models such as a nearest neighbor baseline , and will report results before the ICLR discussion period ends . In the Omniglot dataset literature , many simple baselines has been extensively explored , and Prototypical Networks are so far the state - of - the-art . Table 1 summarizes the performance for a 5 - way 5 - shot benchmark ( results reported by [ 3 ] )

Table 1 - Omniglot dataset baselines
Method Accuracy
KNN pixel 48 %
KNN deep 69 %
Mann et al . [ 3 ] 88 %
ProtoNet 99.7 %

References :
[ 1 ] : Few - Shot Learning with Graph Neural Networks . Anonymous . Submitted to ICLR , 2017 .
[ 2 ] : Semi- Supervised Few - Shot Learning with Prototypical Networks . Rinu Boney and Alexander Ilin. CoRR , abs /1711.10856 , 2017 .
[ 3 ] : One-shot learning with Memory -Augmented Neural Networks . ICML 2016 .


In this paper , the authors interpret the training of GAN by potential field and inspired from which to provide new training procedure for GAN . They claim that under the condition that global optima are achieved for discriminator and generator in each iteration , the Coulomb GAN converges to the global solution .

I think there are several points need to be addressed .

1 , I agree that the " model collapsing " is due to converging to a local Nash Equilibrium . However , there are more reasons besides the drawback of the loss function , which is emphasized in the paper . Leave the stochastic gradient descent optimization algorithm apart ( since most of the neural networks are trained in this way ) , the parametrization and the richness of discriminator family play a vital role in the model collapsing issue . In fact , even with KL - divergence in which log operation is involved , if one can select reasonable parametrization , e.g. , directly handling in function space , the saddle point optimization is convex - concave , which means under the same assumption made in the paper , there is only one global Nash Equilibrium . On the other hand , the richness of the discriminator also important in the training of GAN . I did not get the point about the drawback of III . If indeed as the paper considered in the ideal case , the discriminator is rich enough , III cannot happen .

The model collapsing is not just because loss function in training GAN . It is caused by the twist of these three issues listed above . Modifying the loss can avoid partially model collapsing , however , it is not appropriate to claim that the proposed algorithm is ' provable ' . The assumption in this paper is too restricted , and the discussion is unfair to the existing variants of GAN , e.g. , GMMN or Wasserstein GAN , which under some assumptions , there is also only one global Nash Equilibrium .

2 , In the training procedure , the discriminator family is important as we discussed . The paper claims that the reason to introduce the extra discriminator is reducing variance . However , such parametrization will introduce bias too . The bias and variance tradeoff should be explicitly discussed here . Ideally , it should contain all the functions formed with Plummer kernel , but not too large ( otherwise , it will increase the sample complexity . ) . Which function family used in the paper is not clear .


3 , As the authors already realized , the GMMN is one closely related model . It will be more convincing to add the comparison with GMMN .

In sum , this paper provides an interesting perspective modeling GAN from the potential field , however , there are several issues need to be addressed . I expect to see the reply of the authors regarding the mentioned issues .

We 'd like to thank the reviewer for this in - depth and constructive review , it was a great help in improving our manuscript . The major changes in the new version of the text are :

* Clarified notion of Nash Equilibria : reformulated Theorem 2 in function space ( your point 1 )
* Clarified bias / variance issues ( your point 2 )
* Added comparison to MMD GAN ( your point 3 )

In hindsight , we did n't outline the contribution that Coulomb GANs make as clear as we could have . As a result of this review , we have rewritten large portions of Section 2 and strongly clarified some of our main statements . We were able to reformulate Theorem 2 in a much more precise way . We think that the new version of the text is much improved , and we hope it clears up the items you mentioned .

To your specific points :

1 . Thank you , this comment was very helpful , and lead us to reformulate some of our claims in a clearer way . We agree that loss functions are not the only culprit for unsuccessful GAN learning , and that all practical learning approaches - where generator / discriminator are parametrized models and learned by SGD - introduce a whole lot of convergence and local optimality problems in GANs . But more fundamentally the choice of the loss function might introduce bad local Nash equlibria already in the " theoretical " function space . This fundamental issue is - to the best of our knowledge - not explored in the current literature , neither in the context of Wasserstein GANs nor in GMMN losses and we are not sure if the absence of local Nash equilibria in function space could be proven for those cases . This issue has fundamental implications for all GAN architectures . Therefore our work aims to be more than " just another cool GAN “ , but hopefully furthers the theoretical understanding of GANs in general .
We think that the main contribution of Coulomb GANs is to provide a loss function for GAN learning with the mathematically rigorous guarantee that no such local Nash Equlibria * IN FUNCTION SPACE * exist . We think this is a crucial issue that has not received proper attention yet in order to put scientific GAN research on a solid rigorous ground . We are not aware of any other paper that provides such a strong claim as our Theorem 2 . Neither WGAN nor MMD - based approaches have made this claim and we are not sure that a corresponding claim for them would be provable at all .
We hope you will appreciate our newly written section 2.1 , where we discuss in more depth and mathematical precision what we mean by " local nash equlibrium in function space " , and how it differs from looking at things in probability - distribution space or parameter space .
With that said , we fully agree that for all practical purposes the choice of rich discriminators ( and the parametrization in general ) is highly important for good empirical performance . However , that topic is not the main point we are trying to investigate .

2 . You are right , thank you for this head 's up ! There are two kinds of approximation here : First , we approximate the potential Phi using a mini-batch specific \hat { Phi} . The newer version of the paper discusses the properties of this approximation . Concretely , we show in the appendix that the estimate is unbiased , and explicitly mention the drawback of its high variance in the main text ( Section 2.4 ) .
Secondly , as you correctly stated , we learn Phi with a neural network ( the discriminator ) to reduce the high variance of the mini-batch \hat { Phi} . With this , we run into the usual the bias / variance tradeoff of Machine Learning : trading overfitting against underfitting . And we absolutely agree that finding a good discriminator ( that is able to learn the potential field correctly ) is vital . Thankfully , in GAN learning we can always sample new generator data in each mini-batch , so overfitting on those is not too much of an issue , but we could still overfit on the real - world training data . This could lead to local Nash equilibria in parameter space . Therefore , we tried to be more explicit in the new version of the text that our analysis focuses on the space of functions , and we explicitly mention that neural network learning is vulnerable to issues such as over / underfitting ( again in Section 2.4 ) .

3 . Thank you for the suggestion , we have added this comparison : The original GMMN approach is computationally very expensive to run on the typical GAN datasets , and was recently improved upon by the MMD - GAN model [ Li et al , NIPS 2017 ] . Most importantly , MMD GAN extends the GMMN approach to a learnable discriminator , which makes the approach better and feasible for larger datasets ( & very similar to Coulomb GAN 's discriminator , presumably with the same advantage of reducing variance ) . In their paper , Li et al . show that MMD GAN outperforms GMMN on all tested datasets . We thus added a comparison to MMD - GAN to the current revision of the manuscript .


The authors draw from electrical field dynamics and propose to formulate the GAN learning problem in a way such that generated samples are attracted to training set samples , but repel each other . Optimizing this formulation using gradient descent can be proven to yield only one optimal global Nash equilibrium , which the authors claim allows Coulomb GANs to overcome the " mode collapse " issue . Experimental results are reported on image and language modeling tasks , and show that the model can produce very diverse samples , although some samples can consist of somewhat nonsensical interpolations .

This is a good , well - written paper . It is technically rigorous and empirically convincing . Overall , it presents an interesting approach to overcome the mode collapse problem with GANs .

The image samples presented -- although of high variability -- are not of very high quality , though , and I somewhat disagree with the claim that " Coulomb GAN was able to efficiently learn the whole distribution " ( Sec 3.1 ) . At best , it seems to me that the new objective does in fact force the generator to concentrate efforts on learning over the full support of the data distribution , but the lower quality samples and sometimes somewhat bad interpolations seem to suggest to me that it is * not * yet doing so very " efficiently " .

Nonetheless , I think this is an important step forward in improving GANs , and should be accepted for publication .

Note : I did not check all the proofs in the appendix .

Thank you for your review , and thanks for the „important step forward in improving GANs “ . We appreciate your positive feedback . We agree with your assessment that our objective forces the generator to concentrate efforts on learning over the full support at the cost of somewhat bad interpolations , and toned down the statement about learning " efficiently " in the new update of the paper .
To see how Coulomb GANs perform when contrasted with similar approaches that aim to learn the full support of the distribution , we added a new comparison with MMD approaches . It turns out that Coulomb GANs are more efficient than MMD GANs .

The paper takes an interesting approach to solve the existing problems of GAN training , using Coulomb potential for addressing the learning problem . It is also well written with a clear presentation of the motivation of the problems it is trying to address , the background and proves the optimality of the suggested solution . My understanding and validity of the proof is still an educated guess . I have been through section A.2 , but I 'm unfamiliar with the earlier literature on the similar topics so I would not be able to comment on it .

Overall , I think this is a good paper that provides a novel way of looking at and solving problems in GANs . I just had a couple of points in the paper that I would like some clarification on :

* In section 2.2.1 : The notion of the generated a_i not disappearing is something I did not follow . What does it mean for a generated sample to " not disappear " ? and this directly extends to the continuity equation in ( 2 ) .

* In section 1 : in the explanation of the 3rd problem that GANs exhibit i.e. the generator not being able to generalize the distribution of the input samples , I was hoping if you could give a bit more motivation as to why this happens . I do n't think this needs to be included in the paper , but would like to have it for a personal clarification .

We thank the reviewer for their encouraging review , appreciate the positive feedback . For your questions :

* Thanks for pointing out that our explanation was not clear enough . An a_i is associated with a particular random variable z_i of the generator which is mapped by the generator to a_i . If the generator changes , then the same random variable z_i is mapped to another a_i ' . That is a_i moved to a_i ' . We have explained this more clearly in the current revision .

* We have tried to explain this better in the new version of the text ( even if you said it was n't necessary ) . Informally speaking , we meant the following : in typical GAN learning ( e.g. Goodfellow 's original formulation ) the discriminator is able to say " in this region of space A , the probability of a sample being fake is x % " . Which provides the generator with the information of how well it does in said region . However , the discriminator has usually no way of telling the generator " you should move probability mass over to this region B which is far , far away from A , because there is a lack of generated density there " . Thus , the discriminator cannot tell the generator how to globally move its mass ( it just gets local gradient information at the points where it currently generates ) . In particular , the generator cannot move samples across regions where the real world data has no support . As soon as generator samples appear at the border of such regions , they are penalized and move back to regions with real world support where they come from . Moving again means that samples " a " are associated with random variables " z " , and small changes in the generator lead to small changes of " a " ( " z " is mapped to a slightly different " a " ) . Thus , it is impossible to move samples from one island of real world support to another island of real world support .


The authors propose to compare three different memory architecture for recurrent neural network language models :
vanilla LSTM , random access based on attention and continuous stack . The second main contribution of the paper is to propose an extension of continuous stacks , which allows to perform multiple pop operations at a single time step .
The way to do that is to use a similar mechanism as the adaptive computation time from Graves ( 2016 ) : all the pop operations are performed , and the final state of the continuous stack is weighted average of all the intermediate states . The different memory models are evaluated on two standard language modeling tasks : PTB and WikiText - 2 , as well as on the verb number prediction dataset from Linzen et al ( 2016 ) . On the language modeling tasks , the stack model performs slightly better than the attention models ( 0 - 2 ppl points ) which performs slightly better than the plain LSTM ( 2 - 3 ppl ) . On the verb number prediction tasks , the stack model tends to outperforms the two other models ( which get similar results ) for hard examples ( 2 or more attractors ) .

Overall , I enjoy reading this paper : it is clearly written , and contains interesting analysis of different memory architecture for recurrent neural networks . As far as I know , it is the first thorough comparison of the different memory architecture for recurrent neural network applied to language modeling . The experiments on the Linzen et al. ( 2016 ) dataset is also interesting , as it shows that for hard examples , the different models do have different behavior ( even when the difference are not noticeable on the whole test set ) .

One small negative aspect of the paper is that the substance might be a bit limited . The only technical contribution is to merge the ideas from the continuous stack with the adaptive computation time to obtain the " multi- pop " model . In the experimental section , which I believe is the main contribution of the paper , I would have liked to see more " in - depth " analysis of the different models . I found the experiments performed on the Linzen et al. ( 2016 ) dataset ( Table 2 ) to be quite interesting , and would have liked more analysis like that . On the other hand , I found Figures 2 or 3 not very informative , as it is ( would like to see more ) . For example , from Fig. 2 , it would be interesting to get a better understanding of what errors are made by the different models ( instead of just the distribution ) .

Finally , I have a few questions for the authors :
- In Figure 1 . should n't there be an arrow from h_{t - 1} to m_t instead of x_{t - 1} to m_t ?
- What are the equations to update the stack ? I assume something similar to Joulin & Mikolov ( 2015 ) ?
- Do you have any ideas why there is a sharp jump between 4 and 5 attractors ( Table 2 ) ?
- Why no " pop " operations in Figure 3 and 4 ?

pros / cons :
+ clear and easy to read
+ interesting analysis
- not very original

Overall , while not groundbreaking , this is a serious paper with interesting analysis . Hence , I am weakly recommending to accept this paper .

Thank you for your thoughtful review . Based on your suggestion , we have added examples of mistakes made by competing models in the Linzen experiment instead of just the Venn diagram ( Table 3 ) .

Answers to your specific questions :
- In Figure 1 . should n't there be an arrow from h_{t - 1} to m_t instead of x_{t - 1} to m_t ?
Thanks for pointing this out . You are correct , we have updated the figure to fix the arrow .

- What are the equations to update the stack ? I assume something similar to Joulin & Mikolov ( 2015 ) ?
The equation to update the stack is given in Equation 1 ( page 4 ) .

- Do you have any ideas why there is a sharp jump between 4 and 5 attractors ( Table 2 ) ?
We think that there are two main reasons that could explain the sharp jump .
The first one is because there are much fewer test examples in the dataset with 5 attractors ( ~ 150 ) compared to 4 attractors and above ( 400 , 1100 , 3800 , ... ) , so the standard error on the reported accuracy is also higher ( e.g. , 91.6 +/ - 1.2 and 88.0 +- 2.6 for the stack model with 4 and 5 attractors respectively ) .
Another reason could be that sentences with more attractors are much longer than sentences with fewer attractors , so the difficulty increases non linearly as the number of attractors increases .

- Why no " pop " operations in Figure 3 and 4 ?
The " pop " operations are shown in the x axis ( number of pops ) . Each pair of red and blue bars represents a single pop number .


The authors propose a new stack augmented recurrent neural network , which supports continuous push , stay and a variable number of pop operations at each time step . They thoroughly compare several typical neural language models ( LSTM , LSTM + attention mechanism , etc. ) , and demonstrate the power of the stack baed recurrent neural network language model in the similar parameter scale with other models , and especially show the superiority when the long - range dependencies are more complex in NLP area .

However the corpora they choose to test the ideas , are PTB and Wikitext - 2 , they 're quite small , so the variance of the estimate is high , similar conclusions might not be valid on large corpora such as 1B token benchmark corpus .

Table 1 only gives results with the same level of parameters , the ppls are worse than some other models . Another angle might be the proposed model use the similar size of hidden layer 1500 plus the stack , and see how much ppl reductions it could get .

Finally the authors should do some experiments on machine translation or speech recognition and see whether the model could get performance improvement .




We would like to note that the main goal of the paper is to compare different memory architectures for RNN language models and analyze what kind of dependencies these models fail to learn .

Perplexity is one metric to evaluate such models , so we use PTB and Wikitext - 2 --- the two most commonly used language modeling datasets --- to both compare these models and show that the memory models we implemented perform reasonably well compared to other work on these datasets .

However , as noted in our paper , the overall perplexity on these datasets is strongly dominated by words that have few if any long term dependencies , making it difficult to assess when memory helps using perplexity alone .
Instead of running these models on 1B corpus , which would have the same problem , we chose to include experiments on the Linzen dataset to be able to analyze these memory models further and get a better understanding of their strengths and limitations .
We think this set of experiments adds more value and offers a more useful insight into memory augmented RNN LM than another opaque perplexity result on a larger corpus .

Applications to machine translation and speech recognition are beyond the scope of this paper .


The authors propose a simple defense against adversarial attacks , which is to add randomization in the input of the CNNs . They experiment with different CNNs and published adversarial training techniques and show that randomized inputs mitigate adversarial attacks .

Pros :
( + ) The idea introduced is simple and flexible to be used for any CNN architecture
( + ) Experiments on ImageNet1 k prove demonstrate its effectiveness
Cons :
( -) Experiments are not thorougly explained
( -) Novelty is extremely limited
( -) Some baselines missing


The experimental section of the paper was rather confusing . The authors should explain the experiments and the settings in the table , as those are not very clear . In particular , it was not clear whether the defense model was trained with the input randomization layers ? Also , in Tables 1 - 6 , how was the target model trained ? How do the training procedures of target vs . defense model differ ? In those tables , what is the testing procedure for the target model and how does it compare to the defense model ?

The gap between the target and defense model in Table 4 ( ensemble pattern attack scenario ) shrinks for single step attack methods . This means that when the attacker is aware of the randomization parameters , the effect of randomization might diminish . A baseline that reports the performance when the attacker is fully aware of the randomization of the defender ( parameters , patterns etc. ) is missing but is very useful .

While the experiments show that the randomization layers mitigate the effect of randomization attacks , it 's not clear whether the effectiveness of this very simple approach is heavily biased towards the published ways of generating adversarial attacks and the particular problem ( i.e. classification ) . The form of attacks studied in the paper is that of additive noise . But there is many types of attacks that could be closely related to the randomization procedure of the input and that could lead to very different results .

Thank you very much for the comments . We have updated our paper , especially the experiment section . Below are the detailed answers to your concerns .

“ experiment confusing ” : Sorry for the confusion , and we have made this clearer in the updated paper . The defense model is simply adding two randomization layers to the beginning of the original classification networks . There is no re-training and fine-tuning needed . This is an advantage of our method . We choose Inception - v3 , ResNet - v2 , Inception - ResNet - v2 and ens-adv-Inception - ResNet - v2 as the original CNN models , and these models are public available under Tensorflow github repo . The target models are the models used by attackers to generate adversarial examples . The target models differ under different attack scenarios : ( 1 ) vanilla attack : the target model is the original CNN model , e.g. Inception - v3 ; ( 2 ) single - pattern attack : target model is the original CNN model + randomization layers with only one predefined pattern ; ( 3 ) ensemble - pattern attack : the target model is the original CNN model + randomization layers with an ensemble of predefined patterns . Note that the structure and weights of the classification network in target model and defense model are exactly the same . In tables 2 - 6 , the attackers first use target model to generate adversarial examples , and then tests the top - 1 accuracy on target model and defense model . Specifically , ( 1 ) for target model , a lower accuracy indicates a more successful attack ; ( 2 ) for defense model , a higher accuracy indicates a more successful defense .

“ stronger baseline when the attacker is fully aware the patterns ” : We agree that the performance gap between the target and defense model will shrink as more randomization patterns are considered in the attack process . This is expected . Here we want to emphasize that during defense , the padding and resizing are done randomly , so there is no way for both the attacker and the defender to know the exact instantiated patterns . The strongest possible attack would be that the attackers consider ALL possible patterns when generating the adversarial examples . However , this is not possible . Failing all patterns takes extremely long time , and may not even converge . For example , under our randomization setting , the total number of patterns ( resizing + padding ) is 12528 . Thus , instead of choosing such a large number , we choose 21 representative patterns in our ensemble attack scenario , which becomes computationally manageable . Increasing the number of ensembled patterns means : ( 1 ) more computation time ( take C&W for example , it takes around 0.56 min to generate an adversarial example under vanilla attack , but takes around 8 min to generate an adversarial example under ensemble attack ) ; ( 2 ) more memory consumption ( at most an ensemble of 30 different patterns can be utilized as one batch to generated adversarial examples for one 12 GB GPU , more patterns indicates more GPUs or the GPU with larger memory ) ; ( 3 ) larger magnitude of adversarial perturbation .

“ biased towards the published adversarial attacks ” : Our defense method is not trained using any adversarial examples , so we do n’t think it is biased towards any attacks . We extensively test our method on the most popular attacks ( one single - step attack FGSM , and two representative iterative attacks DeepFool and C&W ) , with various network structures , and using large - scale Image Net datasets . Moreover , we submit this method to a public adversarial defense challenge . Our method is evaluated against 156 different attacks and we are ranked Top 2 , which indicates the effectiveness of our method .

“ particular problem ( e.g. classification ) and additive noise ” : Currently most works on this topic focus on classification problem and assume additive noise as adversarial perturbation . We follow this setting in this paper . We have two future directions to explore : 1 ) apply randomization to other vision tasks , 2 ) apply randomization to other types of attack instead of additive noise . Thanks for the comments .


This paper proposes an extremely simple methodology to improve the network 's performance by adding extra random perturbations ( resizing / padding ) at evaluation time .

Although the paper is very basic , it creates a good baseline for defending about various types of attacks and got good results in kaggle competition .

The main merit of the paper is to study this simple but efficient baseline method extensively and shows how adversarial attacks can be mitigated by some extent .

Cons of the paper : there is not much novel insight or really exciting new ideas presented .

Pros : It gives a convincing very simple baseline and the evaluation of all subsequent results on defending against adversaries will need to incorporate this simple defense method in addition to any future proposed defenses , since it is very easy to implement and evaluate and seems to improve the defense capabilities of the network to a significant degree . So I assume that this paper will be influential in the future just by the virtue of its easy applicability and effectiveness .



Thank you very much for the appreciation of our work . The method is indeed simple and effective . Although the randomization idea is not new , we in this paper apply it to mitigate adversarial effects at test time systematically . And we demonstrate the effectiveness on large - scale Image Net dataset , which is very challenging . Very few defense papers worked on ImageNet before . We hope our method could be served as a simple new baseline for adversarial example defense in the future works .

The paper basically propose keep using the typical data-augmentation transformations done during training also in evaluation time , to prevent adversarial attacks . In the paper they analyze only 2 random resizing and random padding , but I suppose others like random contrast , random relighting , random colorization , ... could be applicable .

Some of the pros of the proposed tricks is that it does n't require re-training existing models , although as the authors pointed out re-training for adversarial images is necessary to obtain good results .


Typically images have different sizes , however in the Dataset are described as having 299x299x3 size , are all the test images resized before hand ? How would this method work with variable size images ?

The proposed defense requires increasing the size of the input images , have you analyzed the impact in performance ? Also it would be good to know how robust is the method for smaller sizes .

Section 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit , please provide an analysis of how results improve as the padding or size increase .

In section 5 for the challenge authors used a lot more evaluations per image , could you provide how much extra computation is needed for that model ?



Thank you very much for the comments , which significantly improve the quality of our paper . We have conducted additional experiments to answer the concerns . These experiments results are included as appendix in the updated paper .

“ Other operations ” : Yes , other random operations also apply . We tried four operations separately : random brightness , random contrast , random saturation , and random hue . For each individual operation , we add it to the beginning of the original classification network . We found that these operations nearly have no hurts on the performance of clean images ( shown in table 7 ) , but they are not as effective as the proposed randomization layers on defending adversarial examples ( shown in table 8 - 11 ) . By combining these random operations with the proposed randomization layers , the performance on defending adversarial examples can be slightly improved . We have updated these new results in the Appendix A.

“ resized beforehand ” : Yes , the test images are resized beforehand . There are two reasons : ( 1 ) easy to form a batch ( e.g. , one batch contains 100 images ) for classification ; ( 2 ) stay aligned with the format of the public competition , where the test dataset are all of the size 299x299x3 . For the images with variable sizes , we can first resize them to 299x299x3 , and then applied the proposed method to defend adversarial examples .

“ impact of size in performance ” : Adding two randomization layers ( increasing size from 299 to 331 ) slightly downgrades the performance on clean images , as shown in Table 1 . This decrease becomes negligible for stronger models . In addition , we also tried applying randomization to smaller- sized images . Specifically , we first resize the images to a size randomly sampled from the range [ 267 , 299 ) , and then randomly pad it to 299x299x3 . We evaluate the performance on both the 5000 clean images and the adversarial examples generated under the vanilla attack scenario ( shown in table 12 ) . We see that the randomization method works well with smaller sizes , but using larger sizes produces slightly better results . We hypothesize that this is because resizing an image to smaller sizes may lose some information . We have updated the new results in the Appendix B.

“ padding or resizing increase ” : As the padding size or resizing size increase , there will be a lot more random patterns . So it becomes much harder for the attackers to generate the adversarial example that can fail all the patterns at the same time . Thus , larger size and more paddings will significantly increase the robustness . Notice that the motivation for the experiments in Sec 4.6 is to decouple the effect of padding and resizing . We want to show that ( 1 ) adversarial example generated on one padding pattern is hard to transfer to another padding pattern ; ( 2 ) adversarial example generated on one size is hard to transfer to another size . Using 1 - pixel padding and resizing provide a controllable way to verify these two points .

“ multiple iterations per image ” : The computation time increases linearly with number of iteration per image ( e.g. , 30 x time in our challenge submission ) . We argue that one iteration is enough to get the most benefits , and additional evaluations only provide marginal gain ( as shown in figures 3 - 5 ) , which is good for the challenge . The experiments that show the relationship between the classification performance and iteration number is included in Appendix C.


The paper proposes an architecture for internal model learning of a robotic system and applies it to a simulated and a real robotic hand . The model allows making relatively long - term predictions with uncertainties . The models are used to perform model predictive control to achieve informative actions . It is shown that the hidden state of the learned models contains relevant information about the objects the hand was interacting with .

The paper reads well . The method is sufficiently well explained and the results are presented in an illustrative and informative way .
update : See critique in my comment below .
I have a few minor points :

- Sec 2 : you may consider to cite the work on maximising predictive information as intrinsic motivation :
G. Martius , R. Der , and N. Ay. Information driven self -organization of complex robotic behaviors . PLoS ONE , 8 ( 5 ) :e63400 , 2013 .
- Fig 2 : bottom : add labels to axis , and maybe mention that same color code as above
- Sec 4 par 3 : .... intentionally not autoregressive : w.r.t. to what ? to the observations ?
- Sec 7.1 : how is the optimization for the MPC performed ? Which algorithm did you use and long does the optimization take ?
in first Eq : should f not be sampled from GMMpdf , so replace = with \sim

Typos :
- Sec1 par2 : This pattern has has ...
- Sec 2 par2 : statistics ofthe
- Sec 4 line2 : prefix of an episode , where ( space before , )



" - Sec 2 : you may consider to cite the work on maximising predictive information as intrinsic motivation :
G. Martius , R. Der , and N. Ay. Information driven self -organization of complex robotic behaviors . PLoS ONE , 8 ( 5 ) :e63400 , 2013 . "

Thank you for the relevant reference .

" - Sec 4 par 3 : .... intentionally not autoregressive : w.r.t. to what ? to the observations ? "

Intentionally not autoregressive with respect to time . We have clarified this in the paper .

" - Sec 7.1 : how is the optimization for the MPC performed ? Which algorithm did you use and long does the optimization take ? "

We take the very naive approach of optimizing the MPC objective for a fixed number of steps by differentiating the cost function with respect to the actions and taking ( projected ) gradient steps with Adam . We project the actions into the constraint set at each step . We initialize the nominal action sequence with a burn in of 1000 Adam steps ( which is fairly time consuming ) and we take 10 additional optimization steps after executing each action and observing a response , warm started from the previous solution . This is acceptably fast for experimentation ( 2 - 5 steps / second after burn in ) but is substantially slower than real time , primarily due to the cost of evaluating the model .


" in first Eq : should f not be sampled from GMMpdf , so replace = with \sim "

f is the Gaussian PDF defined by the parameters , not a sample from the PDF . We do n't need to sample from the predictive distribution in order to compute the control objective . One of the reasons we chose the Renyi entropy as the objective is that it is easy to compute analytically for the Mixture of Gaussians predictions that we make at each step .


Summary :
The paper describes a system which creates an internal representation of the scene given observations , being this internal representation advantageous over raw sensory input for object classification and control . The internal representation comes from a recurrent network ( more specifically , a sequence2 sequence net ) trained to maximize the likelihood of the observations from training

Positive aspects :
The authors suggest an interesting hypothesis : an internal representation of the world which is useful for control could be obtained just by forcing the agent to be able to predict the outcome of its actions in the world . This hypothesis would enable robots to train it in a self - supervised manner , which would be extremely valuable .

Negative aspects :
Although the premise of the paper is interesting , its execution is not ideal . The formulation of the problem is unclear and difficult to follow , with a number of important terms left undefined . Moreover , the experiment task is too simplistic ; from the results , it 's not clear whether the representation is anything more than trivial accumulation of sensory input

- Lack of clarity :
-- what exactly is the " generic cost " C in section 7.1 ?
-- why are both f and z parameters of C ? f is directly a function of z . Given that the form of C is not explained , seems like f could be directly computing as part of C.
-- what is the relation between actions a in section 7.1 and u in section 4 ?
-- How is the minimization problem of u_ { 1 : T} solved ?
-- Are the authors sure that they perform gathering information through " maximizing uncertainty " ( section 7.1 ) ? This sounds profoundly counterintuitive . Maximizing the uncertainty in the world state should result in minimum information about the worlds state . I would assume this is a serious typo , but cannot confirm given that the relation between the minimize cost C and the Renyi entropy H is not explicitely stated .
-- When the authors state that " The learner trains the model by maximum likelihood " in section 7.1 , do they refer to the prediction model or the control model ? It would seem that it is the control model , but the objective being " the same as in section 6 " points in the opposite direction
-- What is the method for classifying and / or regressing given the features and internal representation ? This is important because , if the method was a recurrent net with memory , the differences between the two representations would probably be minimal .

- Simplistic experimental task :
My main intake from the experiments is that having a recurrent network processing the sensory input provides some " memory " to the system which reduces uncertainty when sensory data is ambiguous . This is visible from the fact that the performance from both systems is comparable at the beginning , but degrades for sensory input when the hand is open . This could be achievable in many simple ways , like modeling the classification / regression problem directly with an LSTM for example . Simpler modalities of providing a memory to the system should be used as a baseline .


Conclusion :
Although the idea of learning an internal representation of the world by being able to predict its state from observations is interesting , the presented paper is a ) too simplistic in its experimental evaluation and b ) too unclear about its implementation . Consequently , I believe the authors should improve these aspects before the article is valuable to the community



" Although the premise of the paper is interesting , its execution is not ideal . ... it 's not clear whether the representation is anything more than trivial accumulation of sensory input "

We hope the general reply and videos help with this .

" what exactly is the " generic cost " C in section 7.1 ? "
" why are both f and z parameters of C ? f is directly a function of z . Given that the form of C is not explained , seems like f could be directly computing as part of C . "

In the MPC presentation in section 7.1 we intentionally left the objective generically as C and agree that our current presentation is confusing . For example , to maximize the entropy in the nominal trajectory for exploration , the objective C is the ( negated ) Renyi entropy of the model predictions , which directly uses the GMM PDF f.

You are correct that the PDF f and hidden state z need not necessarily be parameters of C. However in some cases it adds notational convenience ; the PDF f is useful for when the goal of the objective is to reach a desired state ( such as the maximum entropy , or to maximize the fingertip pressure ) , and the hidden state z is useful for when the goal of the objective extracts other information from the hidden state ( such as the type of object , or the height of the object )

We have clarified this portion in the paper .

" what is the relation between actions a in section 7.1 and u in section 4 ? "

Minimizing the objective in 7.1 over a obtains u . We agree this is probably not the best notation and we 've updated the description in 7.1 to use u and u^star instead .

" How is the minimization problem of u_ { 1 : T} solved ? "

This problem is solved using Adam with warm starts from the previous step , see our response to R1 for a full description .

" Are the authors sure that they perform gathering information through " maximizing uncertainty " ... "

We perform information gathering through maximizing ( and not minimizing ) uncertainty . To understand why this is the case it is important to be clear about exactly which uncertainty is being maximized , and what we are trying to gather information about .

It is true that in some settings ( e.g. A Bayesian exploration - exploitation approach for optimal online sensing and planning with a visually guided mobile robot by R Martinez - Cantin , N de Freitas , E Brochu , J Castellanos , and A Doucet in Autonomous Robots 27 ( 2 ) , 93 - 103 , and the many references therein ) one might choose to minimize uncertainty of the internal belief state , so as to gather information . This also arises naturally in Bayesian experimental design ( eg the highly cited work of K Chaloner ) .

However , here we are concerned with the uncertainty in the predictions of the hand signals ( touch sensors , vestibular info , and proprioception ) . By seeking to maximize this uncertainty , the hand is driven to try behaviours where it is highly uncertain about what sensations it will experience ( eg what its pressure sensors will feel ) .

To further emphasize this consistency , we show the outcome of instead acting to minimize the predicted entropy in Video 8 ( Figure 3 ( c ) in the paper ) . Under this objective the hand pulls itself away from the block .

" When the authors state that " The learner trains the model by maximum likelihood " in section 7.1 , do they refer to the prediction model or the control model ? "

The only model in this paper is the predictive model . Control is implemented as planning in the predictive model using MPC . The learner trains the predictive model , and the actors plan trajectories using an objective that depends on the predictive model , but there is no separate learned control model involved .

" What is the method for classifying and / or regressing given the features and internal representation ? "

The diagnostic models are MLPs that look at a single time step of the predictive model state . Your supposition is correct that if we use a recurrent net as the diagnostic model then there is no advantage to including the predictive model features . It would be quite surprising if this were not the case .

The purpose of the diagnostic model ties back to information partitioning . The role of the diagnostic is to show that although we do not have an explicit representation of any external state in the predictive model , we nonetheless obtain such a representation implicitly ( since if this were not the case the diagnostic task would fail ) .

" Simplistic experimental task :... "

The motivation behind our setup is that collecting the data to train the " direct " model can be difficult or impossible in a real life setting . The most complex part of the apparatus for the shadow hand experiment ( apart from the hand itself ) is the mechanism for measuring the angle of the object being grasped .



The authors explore how sequence models that look at proprioceptive signals from a simulated or real - world robotic hand can be used to decode properties of objects ( which are not directly observed ) , or produce entropy maximizing or minimizing motions .

The overall idea presented in the paper is quite nice : proprioception - based models that inject actions and encoder / pressure observations can be used to measure physical properties of objects that are not directly observed , and can also be used to create information gathering ( or avoiding ) behaviors . There is some related work that the authors do not cite that is highly relevant here . A few in particular come to mind :

Yu , Tan , Liu , Turk . Preparing for the Unknown : uses a sequence model to estimate physical properties of a robot ( rather than unobserved objects )

Fu , Levine , Abbeel . One - Shot Learning of Manipulation Skills : trains a similar proprioception - only model and uses it for object manipulation , similar idea that object properties can be induced from proprioception

But in general the citations to relevant robotic manipulation work are pretty sparse .

The biggest issue with the paper though is with the results . There are no comparisons or reasonable baselines of any kind , and the reported results are a bit hard to judge . As far as I can understand , there are no quantitative results in simulation at all , and the real - world results are not good , indicating something like 15 degrees of error in predicting the pose of a single object . That does n't seem especially good , though it 's also very hard to tell without a baseline .

Overall , this seems like a good workshop paper , but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method . I would however strongly encourage the authors to pursue this research further : it seems very promising , and I think that , with more rigorous evaluation and comparisons , it could be quite a nice paper !

One point about style : I found the somewhat lofty claims in the introduction a bit off-putting . It 's great to discuss the greater " vision " behind the work , but this paper suffers from a bit too much high - level vision and not enough effort put into explaining what the method actually does .

" Yu , Tan , Liu , Turk . Preparing for the Unknown ... "
" Fu , Levine , Abbeel . One - Shot Learning of Manipulation Skills ... "

Thank you for these very relevant references . There are similarities , but important differences . The paper of Yu and colleagues uses labels of the world properties ( mu in their notation ) to pre-learn the model in a supervised way . We on the other hand aim to show that properties of the world come to be represented in a model that is only trained with body labels . While relevant the two papers are markedly different . The paper of Lu and colleagues , and in fact may of their subsequent works including guided policy search , use classical control ideas and iterate between fitting trajectories and improving control policies . We have shown that we can learn more complex models and exploration policies jointly . The works are related , but with many differences , and we feel it will be promising to explore this connection further . Thank you for pointing out this connection .

" But in general the citations to relevant robotic manipulation work are pretty sparse . "

We agree . We will address this .

" The biggest issue with the paper though is with the results . "

To demonstrate what we are referring to as “ awareness ” in this paper , we do include baselines for all of our experiments that perform the same tasks using the raw sensor state instead of the hidden state . Please see also the general response and videos , which include plots and comparisons for reference .

In the pose - prediction experiment on the Shadow hand , our baseline model ( in Figure 5 ) is called “ sensors ” and tries to predict the pose of the object from only sensory information ( without the hidden state ) . This baseline also does not surpass the ~ 15 degrees of error , indicating that the sensor readings only contain coarse - grained information about the object and that anything better than ~ 15 degrees is not possible with the current setup . Our approach outperforms this baseline in most cases and shows that the hidden state of the predictive models have learned a useful representation . We will revise our paper and make the baselines more clearly labeled .

" I found the somewhat lofty claims in the introduction a bit off-putting . "

We agree . We 've modified the intro to remove some of the more lofty claims .

This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page . The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self - attention mechanisms like local and memory compressed attention .

In general , the paper is well - written and the main ideas are clear . However , my main concern is the evaluation . It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches . Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input / output size ratios are smaller , experiments could have been performed to show their impact . Furthermore , I really expected to see a comparison with Sauper & Barzilay ( 2009 ) 's non - neural extractive approach of Wikipedia article generation , which could certainly strengthen the technical merit of the paper .

More importantly , it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content . For example , Figure 5 - 7 show variable sizes of the generated outputs . With a fixed reference / target Wikipedia article , if different models generate variable sizes of output , ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference .

It would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures . Did you run any statistical significance test on the evaluation results ?

Authors claim that the proposed model can generate " fluent , coherent " output , however , no evaluation has been conducted to justify this claim . The human evaluation only compares two alternative models for preference , which is not enough to support this claim . I would suggest to carry out a DUC - style user evaluation ( http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt ) methodology to really show that the proposed method works well for abstractive summarization .

Does Figure 8 show an example input after the extractive stage or before ? Please clarify .

---------------
I have updated my scores as authors clarified most of my concerns .

“ It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches . Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input / output size ratios are smaller , experiments could have been performed to show their impact . ”

- In addition to our proposed neural architectures , we compare to very strong existing baselines , seq2seq with attention , which gets state - of - the-art on the Gigaword summarization task as well as the Transformer encoder - decoder , which gets state - of - the-art on translation , a related task from which most summarization techniques arise . We show our models significantly outperform those competitive abstractive methods .


“ Furthermore , I really expected to see a comparison with Sauper & Barzilay ( 2009 ) 's non - neural extractive approach of Wikipedia article generation , which could certainly strengthen the technical merit of the paper . ”

- This is a fair point and we added a section comparing with Sauper & Barzilay in Experiments .


“ More importantly , it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content . For example , Figure 5 - 7 show variable sizes of the generated outputs . With a fixed reference / target Wikipedia article , if different models generate variable sizes of output , ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference . ”

- The models are not constrained to output a certain length . Instead we generate until an end -of - sequence token is encountered . There is a length - penalty , alpha , that we tune based on performance of the validation set . In our case , the ROUGE F1 evaluation is fair because it is the harmonic mean of ROUGE - Recall ( favors long summaries ) and ROUGE - Precision ( favors short summaries ) . As a result , longer output is penalized if it is not useful and related to the target . We tried to clarify this in the Experiments section .


“ It would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures . ”

- We do n’t claim for this task that T-D does better than T-ED for short input lengths . However , we hope Figure 3 makes it clear that the T-ED architecture begins to fail and is no longer competitive for longer inputs . In particular , our architecture improvements allow us to consider much larger input lengths , which results in significantly higher ROUGE and and human evaluation scores .


“ Authors claim that the proposed model can generate " fluent , coherent " output , however , no evaluation has been conducted to justify this claim . The human evaluation only compares two alternative models for preference , which is not enough to support this claim . I would suggest to carry out a DUC - style user evaluation ( http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt ) methodology to really show that the proposed method works well for abstractive summarization . ”

- This is a good point and we followed your suggestion and added a DUC - style human evaluation of linguistic quality . We hope we make it clear that the best abstractive model proposed is significanlty much more fluent / coherent than the best extractive method we tried and another baseline abstractive method ( seq2seq ) . The quality scores are also high in the absolute sense .


“ Does Figure 8 show an example input after the extractive stage or before ? Please clarify . ”

- We clarified in the paper ( now Figure 10 ) that it is the output of the extractive stage , before the abstractive stage .


This paper proposes an approach to generating the first section of Wikipedia articles ( and potentially entire articles ) .
First relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF - based ranking . Then abstractive summarization is performed using a modification of Transformer networks ( Vasvani et al 2017 ) . A mixture of experts layer further improves performance .
The proposed transformer decoder defines a distribution over both the input and output sequences using the same self - attention - based network . On its own this modification improves perplexity ( on longer sequences ) but not the Rouge score ; however the architecture enables memory - compressed attention which is more scalable to long input sequences . It is claimed that the transformer decoder makes optimization easier but no complete explanation or justification of this is given . Computing self - attention and softmaxes over entire input sequences will significantly increase the computational cost of training .

In the task setup the information retrieval - based extractive stage is crucial to performance , but this contribution might be less important to the ICLR community . It willl also be hard to reproduce without significant computational resources , even if the URLs of the dataset are made available . The training data is significantly larger than the CNN / Daily Mail single - document summarization dataset .

The paper presents strong quantitative results and qualitative examples . Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments , especially with regards to the quality of the generated output in comparison to the output of the extractive stage .
In some of the examples the system output seems to be significantly shorter than the reference , so it would be helpful to quantify this , as well how much the quality degrades when the model is forced to generate outputs of a given minimum length . While the proposed approach is more scalable , it is hard to judge the extend of this .

So while the performance of the overall system is impressive , it is hard to judge the significance of the technical contribution made by the paper .

---
The additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper , and I would like to see the paper accepted .


Thank you for the detailed review with actionable feedback . We found common feedback from the three reviewers to augment the evaluation section of the paper and believe we have significantly improved it . In particular , please see responses below in - line as well as rebuttals for other reviews and the summary of changes above .


“ In the task setup the information retrieval - based extractive stage is crucial to performance , but this contribution might be less important to the ICLR community . ”

- We added additional analysis to demonstrate that the extractive stage while important , is far from sufficient to produce good wikipedia articles . We show that ROUGE and human evaluation are greatly improved by the abstractive stage .


“ It willl also be hard to reproduce without significant computational resources , even if the URLs of the dataset are made available . ”

- We will be providing a script for generating the dataset from the CommonCrawl dataset ( which is freely available for download ) . It will run locally instead of downloading over the Internet , and so will be relatively much faster .


“ Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments , especially with regards to the quality of the generated output in comparison to the output of the extractive stage . ”

- We added analysis of the incremental performance of the abstractive model over the extractive output in terms of ROUGE and human evaluation ( DUC - style linguistic quality evaluation ) . We believe we make a strong case that the abstractive model is doing something highly non-trivial and significant .


“ In some of the examples the system output seems to be significantly shorter than the reference , so it would be helpful to quantify this , as well how much the quality degrades when the model is forced to generate outputs of a given minimum length . ”
- We clarified in the paper that the models are not constrained to output a certain length . Instead we generate until an end -of - sequence token is encountered . There is a length - penalty , alpha , that we tune based on performance of the validation set .


“ So while the performance of the overall system is impressive , it is hard to judge the significance of the technical contribution made by the paper . ”
- In addition to the proposed task / dataset , we believe the technical significance is demonstrating how very long text - to - text sequence transduction tasks can be done . Previous related work in translation or summarization focused on much shorter sequences . We had to introduce new model architectures to solve this new problem and believe it would be of great interest to the ICLR community . We hope our added evaluations make this claim more convincing .


The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem . Linked articles as well as the results of an external web search query are used as input documents , from which the Wikipedia lead section must be generated . Further preprocessing of the input articles is required , using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer . A number of variants of attention mechanisms are compared , including the transofer - decoder , and a variant with memory - compressed attention in order to handle longer sequences . The outputs are evaluated by ROUGE - L and test perplexity . There is also a A-B testing setup by human evaluators to show that ROUGE - L rankings correspond to human preferences of systems , at least for large ROUGE differences .

This paper is quite original and clearly written . The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles . The main weakness is that I would have liked to see more analysis and comparisons in the evaluation .

Evaluation :
Currently , only neural abstractive methods are compared . I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods , as well as some simple multi-document selection algorithms such as SumBasic . Do redundancy cues which work for multi-document news summarization still work for this task ?

Extractiveness analysis :
I would also have liked to see more analysis of how extractive the Wikipedia articles actually are , as well as how extractive the system outputs are . Does higher extractiveness correspond to higher or lower system ROUGE scores ? This would help us understand the difficulty of the problem , and how much abstractive methods could be expected to help .

A further analysis which would be nice to do ( though I have less clear ideas how to do it ) , would be to have some way to figure out which article types or which section types are amenable to this setup , and which are not .

I have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition - like entries ( e.g. , Baidu , Wiktionary ) which is not caught by clone detection . In this case , the problem could become less interesting , as no real analysis is required to do well here .

Overall , I quite like this line of work , but I think the paper would be a lot stronger and more convincing with some additional work .

----
After reading the authors ' response and the updated submission , I am satisfied that my concerns above have been adequately addressed in the new version of the paper . This is a very nice contribution .


Thank you for the detailed review with actionable feedback . We found common feedback from the three reviewers to augment the evaluation section of the paper and believe we have significantly improved it . In particular , please see responses below in - line where we address all of your feedback .

“ This paper is quite original and clearly written . The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles . “

- In addition to the task setup , we believe we ’ve demonstrated how to do very long ( much longer than previously attempted ) text - to - text sequence transduction and introduced a new model architecture to do it . We believe this is of great interest to the ICLR community .

“ Currently , only neural abstractive methods are compared . I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods , as well as some simple multi-document selection algorithms such as SumBasic . Do redundancy cues which work for multi-document news summarization still work for this task ? ”

- We implemented SumBasic and TextRank ( along with tf -idf ) to evaluate extractive methods on their own and evaluated them on this task . I believe we show convincingly in the results ( e.g. extractive bar-plot ) that the abstractive stage indeed adds a lot to the extractive output in terms of ROUGE and human evaluation of linguistic quality and that redundancy cues are not enough .

“ I would also have liked to see more analysis of how extractive the Wikipedia articles actually are , as well as how extractive the system outputs are . Does higher extractiveness correspond to higher or lower system ROUGE scores ? This would help us understand the difficulty of the problem , and how much abstractive methods could be expected to help . ”

- In Section 2.1 we computed the proportion of unigrams / words in the output co-occurring in the input for our task and for the Gigaword and CNN / Daily Mail datasets and showed that by this measure WikiSum is much less extractive . In particular , the presence of wiki-clones in the input would give a score of 100 % , whereas we report 59.2 % .

“ A further analysis which would be nice to do ( though I have less clear ideas how to do it ) , would be to have some way to figure out which article types or which section types are amenable to this setup , and which are not . ”

- We added a comparison in the paper with Sauper & Barzilay on two Wiki categories . It turns out we do worse on Diseases compared to Actors . We think this is because we use a single model for all categories and the training data is heavily biased toward people .

“ I have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition - like entries ( e.g. , Baidu , Wiktionary ) which is not caught by clone detection . In this case , the problem could become less interesting , as no real analysis is required to do well here . ”
- We hope our added analysis in Section 2.1 mentioned above should address this concern .


Generating high -quality sentences / paragraphs is an open research problem that is receiving a lot of attention . This text generation task is traditionally done using recurrent neural networks . This paper proposes to generate text using GANs . GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature . This paper 's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator . The whole network is trained on the " fill - in - the -blank " task using the sequence - to- sequence architecture for both the generator and the discriminator . At training time , the generator 's encoder computes a context representation using the masked sequence . This context is conditioned upon to generate missing words . The discriminator is similar and conditions on the generator 's output and the masked sequence to output the probability of a word in the generator 's output being fake or real . With this approach , one can generate text at test time by setting all inputs to blanks .

Pros and positive remarks :
-- I liked the idea behind this paper . I find it nice how they benefited from context ( left context and right context ) by solving a " fill - in - the -blank " task at training time and translating this into text generation at test time .
-- The experiments were well carried through and very thorough .
-- I second the decision of passing the masked sequence to the generator 's encoder instead of the unmasked sequence . I first thought that performance would be better when the generator 's encoder uses the unmasked sequence . Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time .

Cons and negative remarks :
-- There is a lot of pre-training required for the proposed architecture . There is too much pre-training . I find this less elegant .
-- There were some unanswered questions :
( 1 ) was pre-training done for the baseline as well ?
( 2 ) how was the masking done ? how did you decide on the words to mask ? was this at random ?
( 3 ) it was not made very clear whether the discriminator also conditions on the unmasked sequence . It needs to but
that was not explicit in the paper .
-- Very minor : although it is similar to the generator , it would have been nice to see the architecture of the discriminator with example input and output as well .


Suggestion : for the IMDB dataset , it would be interesting to see if you generate better sentences by conditioning on the sentiment as well .


Thank you for your review !

* Pretraining *
We found evidence that this architecture could replicate simple data distributions without pretraining and found it could perform reasonably on larger data sets , however , in the interest of computational efficiency , we relied on pretraining procedures , similar to other work in this field . All our baselines also included pre-training .

To test whether all the pretraining steps were necessary , we experimented with training MaskMLE and MaskGAN on PTB without initializing from a pretrained language model . The perplexity of the generated samples were 117 without pretraining and 126 with pretraining , showing that at least for PTB language model pretraining does not appear to be necessary .

Models trained from scratch were found to more computationally intense . By building off near state - of - the- art language models , we were able to rapidly iterate over architectures thanks to faster convergence . Additionally , we were working at a word - level representation where our softmax is producing a distribution over O ( 10K ) - tokens . Attempting reinforcement learning methods from scratch on an ‘action space ’ of this magnitude is prone to extreme variance . The likelihood of producing a correct token and receiving a positive reward is exceedingly rare ; therefore , the model spends a long time exploring the space with almost always negative rewards . As a related and budding research avenue , one could consider the properties and characteristics of exclusively GAN - trained language models .

* Masking Strategy *
We predominantly evaluated two masking strategies at training time . One was a completely random mask and the other was a contiguous mask , where blocks of adjacent words are masked . Though we were able to train with both strategies , we found that the random mask was more difficult to train . However , and more significantly , the random mask does n’t share the primary benefit of GAN autoregressive text generation ( termed free-running mode in the literature ) . One can see this because for a given percentage of words to omit , a Generator given the random mask will fill - in shorter sequences autoregressively than the contiguous mask will . GAN - training allows our training and inference procedure to be the same , in contrast to teacher - forcing in maximum likelihood training . Therefore , we generally found it beneficial to allow the model to produce long sequences , conditioned on what it had produced before , rather than filling in short disjoint sequences or or even single tokens .

We additionally added results comparing MaskGAN and MaskMLE samples against those from a baseline LSTM language model . Tables 7 and 8 have been updated to include these results .

Thanks again ! Before the review process concludes , do you have any outstanding questions regarding our rebuttal which includes the additional experiments on pretraining and our chosen masking strategy ? In particular , we 'd be interested in your opinion on the MaskGAN algorithm in light of evidence that it functions with less pretraining . Finally , our paper revision seeks to further strengthen our result by comparing against LSTM baselines .

Quality : The work focuses on a novel problem of generating text sample using GAN and a novel in - filling mechanism of words . Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues . As a remedy to these problems an in - filling - task conditioning on the surrounding text has been proposed . But , the use of the rewards at every time step ( RL mechanism ) to employ the actor-critic training procedure could be challenging computationally challenging .

Clarity : The mechanism of generating the text samples using the proposed methodology has been described clearly . However the description of the reinforcement learning step could have been made a bit more clear .

Originality : The work indeed use a novel mechanism of in - filling via a conditioning approach to overcome the difficulties of GAN training in text settings . There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers . How this current work compares with the existing such literature ?

Significance : The research problem is indeed significant since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings . Also , the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing .

pros :
( a ) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples .
( b ) Using a novel in - filling procedure to overcome the complexities in GAN training .
( c ) generation of high quality samples even with higher perplexity on ground truth set .

cons :
( a ) Use of rewards at every time step to the actor-critic training procure could be computationally expensive .
( b ) How to overcome the situation where in - filling might introduce implausible text sequences with respect to the surrounding words ?
( c ) Depending on the Mask quality GAN can produce low quality samples . Any practical way of choosing the mask ?

Thank you for your review !

* Importance and Computational Cost of Actor-Critic *
We ’d like to address your concern about the importance and the computational challenges of the actor-critic method . We believe that this was a crucial component to get the results we did and it was achieved with no significant additional computational cost .

In building architectures for this novel task , we were contending with both reinforcement learning challenges as well as GAN - mode collapse issues . Specifically , variance in the gradients to the Generator was a major issue . To remedy this , we simply added a value estimator as an additional head on the Discriminator . The critic estimates the expected value of the current state , conditioned on everything produced before . This is very lightweight in terms of additional parameters since we ’re sharing almost all parameters with the Discriminator . We found that using this reduced the advantage to the Generator by over an order of magnitude . This was a critical piece of efficiently training our algorithm . We compared the performance of this actor-critic approach against a standard exponential moving average baseline and found there to be no significant difference in training step time .

* Clarity *
Thanks and we updated the writing to more clearly delineate the reinforcement learning training .

* Originality *
As far as we are aware , no work has considered this conditional task where a per-time - step reward is architected in . Additionally , our use of an actor-critic methodology in GAN - training is a minimally explored avenue . Finally , the existing literature on textual adversarial examples focus on classifier accuracy and generally do n't do human evaluations on the quality of the generated examples as we do .

* Masking Strategy *
We predominantly evaluated two masking strategies at training time . One was a completely random mask and the other were contiguous masks , where blocks of adjacent words are masked . Though we were able to train with both strategies , we found that the random mask was more difficult to train . However , and more significantly , the random mask does n’t share the primary benefit of GAN autoregressive text generation ( termed free-running mode in the literature ) . One can see this because for a given percentage of words to omit , a Generator given the random mask will fill - in shorter sequences autoregressively than the contiguous mask . GAN - training allows our training and inference procedure to be the same , in contrast to teacher - forcing in the maximum likelihood training . Therefore , we generally found it beneficial to allow the model to produce long sequences , conditioned on what it had produced before , rather than filling in short disjoint sequences or or even single tokens .

We additionally added results comparing MaskGAN and MaskMLE samples against those from a baseline LSTM language model . Tables 7 and 8 have been updated to include these results .

This paper proposes MaskGAN , a GAN - based generative model of text based on
the idea of recovery from masked text .
For this purpose , authors employed a reinceforcement learning approach to
optize a prediction from masked text . Moreover , authors argue that the
quality of generated texts is not appropriately measured by perplexities ,
thus using another criterion of a diversity of generated n-grams as well as
qualitative evaluations by examples and by humans .

While basically the approach seems plausible , the issue is that the result is
not compared to ordinary LSTM - based baselines . While it is better than a
conterpart of MLE ( MaskedMLE ) , whether the result is qualitatively better than
ordinary LSTM is still in question .

In fact , this is already appearent both from the model architectures and the
generated examples : because the model aims to fill - in blanks from the text
around ( up to that time ) , generated texts are generally locally valid but not
always valid globally . This issue is also pointed out by authors in Appendix
A.2.
While the idea of using mask is interesting and important , I think if this
idea could be implemented in another way , because it resembles Gibbs sampling
where each token is sampled from its sorrounding context , while its objective
is still global , sentence - wise . As argued in Section 1 , the ability of
obtaining signals token - wise looks beneficial at first , but it will actually
break a global validity of syntax and other sentence - wise phenoma .

Based on the arguments above , I think this paper is valuable at least
conceptually , but doubt if it is actually usable in place of ordinary LSTM
( or RNN ) - based generation .
More arguments are desirable for the advantage of this paper , i.e. quantitative
evaluation of diversity of generated text as opposed to LSTM - based methods .

* Based on the rebuttals and thorough experimental results , I modified the global rating .

Thank you for your review and comments !

We reiterate your two primary concerns as the following :
1 . A standard LSTM - baseline of a non-masked task should be included .
2 . The MaskGAN algorithm is enforcing only local consistency within text , but does not aid with global consistency .

* Standard Baselines *
To address your first concern , we added a thorough human evaluation of a language model ( LM ) LSTM baseline . We use the samples produced from our Variational Dropout -LSTM language model and evaluate the resulting sample quality for both the PTB and IMDB datasets using Amazon Mechanical Turk . You can see these results updated in our paper in Table 7 and Table 8 . We demonstrate that the MaskGAN training algorithm results in improvements over both the language model and the MaskMLE benchmarks on all three metrics : grammaticality , topicality and overall quality . In particular , MaskGAN samples are preferred over LM LSTM baseline samples , 58.0 % vs 15.7 % of the time for IMDB reviews .

* Local vs. Global Consistency *
In regards to your comment on Gibbs sampling , we do agree that this would likely be a valid and helpful technique for inference . In our paper , we in - fill our samples autoregressively from left to right , as is conventional in language modeling . ( This approach allows for fast unconditional generation as with the LM baseline and is what our human evaluation is targeted at ) . This autoregressive process relies on the attention module of our decoder in order to provide full context during the sampling process . For instance , when the decoder is producing the probability distribution over token x_t , it is attending over the future context to create this distribution . If the subject of the sentence is known to be a female leader and the model is generating a pronoun , the model has the ability to attend to the future context and select the correct gender - matched pronoun . If the model fails to do this , a well - trained discriminator will ascribe a low reward to this pronoun selection which in turn will generate useful gradients through the attention mechanism . We have observed this behaviour during preliminary experiments . We argue that global consistency is built into this architecture but to solve the boundary problems in appendix C.2 , allowing the autoregressive model decide when to stop instead of forcing it to output a fix number of words may resolve some of the syntactic issues .

We also expand table 6 to show the diversity of the generated samples compared to a standard LM-LSTM .

This paper presents a method to identify high - order interactions from the weights of feedforward neural networks .

The main benefits of the method are :
1 ) Can detect high order interactions and there ’s no need to specify the order ( unlike , for example , in lasso - based methods ) .
2 ) Can detect interactions appearing inside of non-linear function ( e.g. sin ( x 1 * x 2 ) )

The method is interesting , in particular if benefit # 2 holds experimentally . Unfortunately , there are too many gaps in the experimental evaluation of this paper to warrant this claim right now .

Major :

1 ) Arguably , point 1 is not a particularly interesting setting . The order of the interactions tested is mainly driven by the sample size of the dataset considered , so in some sense the inability to restrict the order of the interaction found can actually be a problem in real settings .
Because of this , it would be very helpful to separate the evaluation of benefit 1 and 2 at least in the simulation setting . For example , simulate a synthetic function with no interactions appearing in non-linearities ( e.g. x1+x2x3x4+x4x6 ) and evaluate the different methods at different sample sizes ( e.g. 100 samples to 1e5 samples ) . The proposed method might show high type - 1 error under this setting . Do the same for the synthetic functions already in the paper . By the way , what is the sample size of the current set of synthetic experiments ?
2 ) The authors claim that the proposed method identifies interactions “ without searching an exponential solution space of possible interactions ” . This is misleading , because the search of the exponential space of interactions happens during training by moving around in the latent space identified by the intermediate layers . It could perhaps be rephrased as “ efficiently ” .
3 ) It ’s not clear from the text whether ANOVA and HierLasso are only looking for second order interactions . If so , why not include a lasso with n-order interactions as a baseline ?
4 ) Why are n’t the baselines evaluated on the real datasets and heatmaps similar to figure 5 are produced ?
5 ) Is it possible to include the ROC curves corresponding to table 2 ?


Minor :

1 ) Have the authors thought about statistical testing in this framework ? The proposed method only gives a ranking of possible interactions , but does not give p-values or similar ( e.g. FDRs ) .
2 ) 12 pages of text . Text is often repetitive and can be shortened without loss of understanding or reproducibility .


Thank you for your comments and suggestions . We conducted some experiments based on your suggestions and provide our responses to your major points below . We have also included additional results in Appendices E and F.

1 ) Our proposed method strongly relies on the assumption that the neural network fit well to data , since we are extracting interactions from the learned network weights . The number of data points available plays a critical role here because a small amount of data can cause the neural network to overfit , causing our method to miss true interactions and find spurious ones instead . To avoid this scenario , we employed modern tricks to help our neural network fit well to the data ( e.g. early stopping , regularization ) . That being said , we advise against using our framework when the number of data samples , n , is too small for normal neural networks , e.g. , when n < p , where p is the number of features . Under such scenarios , one might need to impose much stronger assumptions on the data , which goes against our proposal of a general interaction detection algorithm .

For assurance , we conducted the experiments that you requested and confirmed that our approach does well on the multiplicative synthetic function x1+x2x3x4+x4x6 for datasets of sizes [ 1e2 , 1e3 , 1e4 , 1e5 ] , obtaining average interaction ranking AUCs of [ 0.99 , 1.0 , 1.0 , 1.0 ] , respectively . The average AUCs for our 10 synthetic functions with nonlinearities ( combined ) are [ 0.57,0.83,0.92,0.94 ] respectively . The baseline methods are specified to find multiplicative interactions , so their AUC is 1 for the multiplicative synthetic function . Note that we can only obtain interactions accurately when there is enough data to train the model , as seen in the improving scores with more data samples . In our synthetic experiments , we used 10 k training samples ( and 10 k valid / 10 k test ) , and this has been updated in our paper . We also updated our paper with a large -p experiment on multiplicative interactions in Appendix F , where we obtained an AUC of 0.98 .

2 ) While the neural network is technically searching interactions during training , the cost of this implicit exponential search is faster than an explicit exponential search of the space of interaction candidates . Our work avoids this explicit search .

3 ) We originally did not include higher - order detection experiments with ANOVA and Lasso because they are mis-specified to handle detecting the general non-additive form of interactions . For assurance , we ran experiments on ANOVA and Lasso and got average top-rank recall scores of 0.47 and 0.44 respectively , which are much lower than the 0.65 average obtained by our approach .



This paper develops a novel method to use a neural network to infer statistical interactions between input variables without assuming any explicit interaction form or order . First the paper describes that an ' interaction strength ' would be captured through a simple multiplication of the aggregated weight and the weights of the first hidden layers . Then , two simple networks for the main and interaction effects are modeled separately , and learned jointly with posing L1 - regularization only on the interaction part to cancel out the main effect as much as possible . The automatic cutoff determination is also proposed by using a GAM fitting based on these two networks . A nice series of experimental validations demonstrate the various types of interactions can be detected , while it also fairly clarifies the limitations .

In addition to the related work mentioned in the manuscript , interaction detection is also originated from so-called AID , literally intended for ' automatic interaction detector ' ( Morgan & Sonquist , 1963 ) , which is also the origin of CHAID and CART , thus the tree-based methods like Additive Groves would be the one of main methods for this . But given the flexibility of function representations , the use of neural networks would be worth rethinking , and this work would give one clear example .

I liked the overall ideas which is clean and simple , but also found several points still confusing and unclear .

1 ) One of the keys behind this method is the architecture described in 4.1 . But this part sounds quite heuristic , and it is unclear to me how this can affect to the facts such as Theorem 4 and Algorithm 1 . Absorbing the main effect is not critical to these facts ? In a standard sense of statistics , interaction would be something like residuals after removing the main ( additive ) effect . ( like a standard test by a likelihood ratio test for models with vs without interactions )

2 ) the description about the neural network for the main effect is a bit unclear . For example , what does exactly mean the ' networks with univariate inputs for each input variable ' ? Is my guessing that it is a 1 - 10 - 10 - 10 - 1 network ( in the experiments ) correct ...? Also , do g_i and g_i ' in the GAM model ( sec 4.3 ) correspond to the two networks for the main and interaction effects respectively ?

3 ) mu is finally fixed at min function , and I 'm not sure why this is abstracted throughout the manuscript . Is it for considering the requirements for any possible criteria ?

Pros :
- detecting ( any order / any form of ) statistical interactions by neural networks is provided .
- nice experimental setup and evaluations with comparisons to relevant baselines by ANOVA , HierLasso , and Additive Groves .

Cons :
- some parts of explanations to support the idea has unclear relationship to what was actually done , in particular , for how to cancel out the main effect .
- the neural network architecture with L1 regularization is a bit heuristic , and I 'm not surely confident that this architecture can capture only the interaction effect by cancelling out the main effect .



Thank you for your comments and suggestions .

>> “ it is unclear to me how this can affect to the facts such as Theorem 4 and Algorithm 1 . ”
The existence of the univariate networks does not affect Algorithm 1 or Theorem 4 . The univariate networks are meant to reduce the modeling of spurious interactions in the main fully - connected network to improve interaction detection performance .

>> On the architecture of the univariate and GAM networks
Your understanding is correct .

>> Regarding the abstraction of mu ,
We did not define mu until later in the paper because mu was determined by experiments

Based on a hierarchical hereditary assumption , this paper identifies pairwise and high - order feature interactions by re-interpreting neural network weights , assuming higher - order interactions exist only if all its induced lower - order interactions exist . Using a multiplication of the absolute values of all neural network weight matrices on top of the first hidden layer , this paper defines the aggregated strength z_r of each hidden unit r contributing to the final target output y. Multiplying z_r by some statistics of weights connecting a subset of input features to r and summing over r results in final interaction strength of each feature interaction subsets , with feature interaction order equal to the size of each feature subset .

Main issues :

1 . Aggregating neural network weights to identify feature interactions is very interesting . However , completely ignoring
activation functions makes the method quite crude .

2 . High -order interacting features must share some common hidden unit somewhere in a hidden layer within a deep neural network . Restricting to the first hidden layer in Algorithm 1 inevitably misses some important feature interactions .

3 . The neural network weights heavily depends on the l1 - regularized neural network training , but a group lasso penalty makes much more sense . See Group Sparse Regularization for Deep Neural Networks ( https://arxiv.org/pdf/1607.00485.pdf).

4 . The experiments are only conducted on some synthetic datasets with very small feature dimensionality p. Large - scale experiments are needed .

5 . There are some important references missing . For example , RuleFit is a good baseline method for identifying feature interactions based on random forest and l1 - logistic regression ( Friedman and Popescu , 2005 , Predictive learning via rule ensembles ) ; Relaxing strict hierarchical hereditary constraints , high -order l1 - logistic regression based on tree-structured feature expansion identifies pairwise and high - order multiplicative feature interactions ( Min et al. 2014 , Interpretable Sparse High - Order Boltzmann Machines ) ; Without any hereditary constraint , feature interaction matrix factorization with l1 regularization identifies pairwise feature interactions on datasets with high - dimensional features ( Purushotham et al. 2014 , Factorized Sparse Learning Models with Interpretable High Order Feature Interactions ) .

6 . At least , RuleFit ( Random Forest regression for getting rules + l1 - regularized regression ) should be used as a baseline in the experiments .

Minor issues :

Ranking of feature interactions in Algorithm 1 should be explained in more details .

On page 3 : b^{ ( l ) } \in R^{p_l} , l should be from 1 , .. , L. You have b^y .


In summary , the idea of using neural networks for screening pairwise and high - order feature interactions is novel , significant , and interesting . However , I strongly encourage the authors to perform additional experiments with careful experiment design to address some common concerns in the reviews / comments for the acceptance of this paper .

========
The additional experimental results are convincing , so I updated my rating score .


Thank you for your comments and suggestions to improve the paper . Below are our responses to the main points of your comments :

1 . >> “ However , completely ignoring activation functions makes the method quite crude . ”
Our approach to aggregating weight matrices depends on the activation functions in two ways : 1 ) the use of matrix multiplications is based on Lemma 3 , which depends on the activation functions being 1 - Lipschitz , and 2 ) the averaging of weights is empirically determined from neural networks with ReLU activation .

2 . >> “ Restricting to the first hidden layer in Algorithm 1 inevitably misses some important feature interactions . ”
This is an interesting point . We did consider it before . However , it is not straightforward how to incorporate the idea of common hidden units at intermediate layers to get better interaction detection performance . Our previous studies show that naively using the intermediate hidden layers to suggest new interactions have resulted in worse performance in interaction detection because the connections between input features and intermediate layers are not direct .

3 . >> “ a group lasso penalty makes much more sense ”
In general , group lasso requires specifying groupings a priori . It is unclear how to tailor the group lasso penalty to discover interactions , but group lasso might offer an alternative way of finding a cutoff on interaction rankings .

4 . >> “ Large - scale experiments are needed . ”
We have conducted experiments with large scale p ( p=1000 , 950 pairwise interactions ) as you suggested and obtained a pairwise interaction strength AUC of 0.984 . The full experimental setting can be found in our updated paper in Appendix F , which follows Purushotham et al . 2014 on how to generate large p noisy data .

5 & 6 . >> “ , Rule Fit should be used as a baseline in the experiments . ”
We have added experiments with RuleFit into Table 2 as you suggested . Our approach outperforms RuleFit . This is consistent with previous work by Lou et al. 2013 , “ Accurate and Intelligible Models with Pairwise Interactions ” , which found that RuleFit did not perform better than Additive Groves , our main baseline .


5 & 6 . >> On the remaining baseline references

We have added references for the remaining baselines as you suggested ( on Page 2 and in Appendix H ) . The references are Interpretable Sparse High - Order Boltzmann Machines , Min et al. 2014 and Factorized Sparse Learning Models with Interpretable High Order Feature Interactions , Purushotham et al. 2014 .

We performed experiments comparing our method to the " Shooter " and " FHIM " baselines from the references . The details and results of our experiments are shown in Appendix H . We found that indeed , the Shooter baseline benefits from relaxing strict hierarchical hereditary constraints

This paper proposes to apply the obverter technique of Batali ( 1998 ) to a multi-agent communication game . The main novelty with respect to Batali 's orginal work is that the agents in this paper have to communicate about images represented at the raw pixel level . The paper presents an extensive analysis of the patterns learnt by the agents , in particular in terms of how compositional they are .

I greatly enjoyed reading the paper : while the obverter idea is not new , it 's nice to see it applied in the context of modern deep learning architectures , and the analysis of the results is very interesting .

The writeup is somewhat confusing , and in particular the reader has to constantly refer to the supplementary materials to make sense of the models and experiments . At least the model architecture could be discussed in more detail in the main text .

It 's a pity that there is no direct quantitative comparison between the obverter technique and a RL architecture .

Some more detailed comments :

It would be good if a native speaker could edit the paper for language and style ( I only annotated English problems in the abstract , since there were just too many of them ) .

Abstract :

distinguishing natures -> aspects

describe the complex environment -> describe complex environments

with the accuracy -> with accuracy

Introduction :

have shown great progress -> has ...

The claim that language learning in humans is entirely based on communication needs hedging . See , e.g. , cultures where children must acquire quite a bit of their language skills from passive listening to adult conversations ( Schieffelin & Ochs 1983 ; Shore 1997 ) .

Also , while it 's certainly the case that humans do not learn from the neatly hand - crafted features favored in early language emergence studies , their input is most definitely not pixels ( and , arguably , it is something more abstract than raw visual stimuli , see , e.g. , the work on " object biases " in language acquisition ) .

Method :

The description of the experiment sometimes refers to the listener having to determine whether it is seeing the same image as the speaker , whereas the game consists in the listener telling whether it sees the same object as the speaker . This is important , since , crucially , the images can present the same object in different locations .

Section 2.2 is very confusing , since the obverter technique has not been introduced , yet , so I kept wondering why you were only describing the listener architecture . Perhaps , current section 2.3 should be moved before 2.2.

Also in 2.2 : what is the " meaning vector " that BAtali 's RNN hidden vector has to be close to ?

It 's confusing to refer to the binary 0 and 1 vectors as outputs of the sigmoid : they are rather the binary categories you want the agents to approximate with the sigmoid function .

The obverter technique should be described in more detail in the main text . In particular , with max message length 20 and a vocabulary of 5 symbols , you 'll have an astronomical number of possible inputs to evaluate at each step : is this really what you 're doing ?

Experiments :

distinctness : better call it distinctiveness

Is training accuracy in Fig 4 averaged across both agents ? And what were the values the Jaccard similarity was computed over ?

It 's nice that the agents discovered some non-strictly agglutinative morphology , consisting in removing symbols from the prefix . Also , it looks like they are developing some notion of " inflectional class " ( Aronoff 1994 ) ( the color groups ) , which is also intriguing . However , I did not understand rules such as : " remove two as and add one a " ... is n't that the same as : " add one a " ?

Discussion and future work :

I did n't follow the discussion about partial truth and using negations .

There is another ICLR submission that proposes a quantitative measure of compositionality you might find interesting : https://openreview.net/pdf?id=HJGv1Z-AW


We thank you for your review and constructive comments . We address your concerns as follows .

1 . The claim that language learning in humans is entirely based on communication needs hedging :
We appreciate the suggested reading . We revised our paper to be more appropriate in our claims .

2 . Human language acquisition is based on somewhere between hand - engineered features and pixels :
Your point is true , and it led us re-check the contribution of our paper . The agents in our work learns to develop both the linguistic ability and visual cognition simultaneously . The CNN and the RNN of the agents are initialized with random parameters . Through training , CNN is updated to disentangle shapes and colors , while RNN is updated to describe the shapes and colors . Therefore , similar to your point , after the CNN is updated to a certain level , RNN no longer deals with completely raw signals . However , at the early training rounds , the RNN does deal with seemingly random values ( since the CNN has not learned much yet ) and maybe that is why it takes several thousand training rounds for the agents to show some performance gain . We appreciate this insightful comment , and we revised the paper to incorporate this comment .

3 . The description of the game confusingly uses the word “ image ” and “ object ” :
We revised the paper to be more consistent in using those two words .

4 . Maybe section 2.3 should come before section 2.2:
In the revised paper , we mention in section 2.2 that the obverter strategy will be described in detail in section 2.3.

5 . What is the meaning vector in Batali ’s work ?
We revised the paper so that there is a brief description of the meaning vector , and we also inform the readers to refer to the supplementary material for detailed information .

6 . The output of the agents are not vectors but sigmoid values :
We revised the paper to remove the confusion .

7 . The obverter technique should be described in more detail :
We revised the paper to provide more detailed description of the obverter strategy . Also , we explain that the characters are chosen greedily at each timestep . We also discuss the deterministic nature of the obverter strategy in the revised paper ( please see our second explanation for Reviewer 3 for more detail )

8 . Is training accuracy in Fig 4 averaged across both agents ?
In a single training round , only one agent ( i.e. the learner ) makes the prediction . We aggregated those predictions to calculate the accuracy . Therefore , in round 0 , agent 0 ’s prediction accuracy is taken . In round 1 , agent 1 ’s prediction accuracy is taken . And we repeat this process . This is described this in line 146 by saying “ Training accuracy was calculated by rounding the learner ’s sigmoid output by 0.5 ” .

9 . What were the values the Jaccard similarity was computed over ?
If agent0 generates “ aaa ” , “ aba ” , “ abb ” to describe red box , and agent1 generates “ aaa ” , “ aba ” , “ abc ” to describe red box , then we calculate the Jaccard similarity by number_of_duplicate_messages/number_of_unique_messages , which is 2/4=0.5 . We do this for all object types ( total 40 ) and average the Jaccard similarity values to obtain the final value .

10 . However , I did not understand rules such as : " remove two as and add one a " ... is n't that the same as : " add one a " ?
The suffix used to describe the color gray is “ a_hat , a_hat , a ” The two ‘ a_hat ’s indicate removing two ‘a ’s from the prefix ( which is responsible for describing shapes ) . For example , gray box uses “ aaaa ” as the prefix and “ a_hat a_hat a ” as the suffix . Therefore putting them together gives us “ aaa ” , which is the message that describes gray box as shown in the bottom of Table 1 . This is different from adding a single ‘ a ’ to the prefix “ aaaa ” , which gives us “ aaaaa ” .
In the revised paper , we made it clear that a_hat is for deleting the prefix .

11 . I did n't follow the discussion about partial truth and using negations :
Let ’s assume I am aware of red circle , blue square and green triangle . If I came upon a blue circle for the first time and had to describe it to the listener , I could say “ blue circle ” . But I could also say “ blue not_square not_triangle ” . If the listener had a similar knowledge as I did , then we would have a successful communication . However , it is debatable whether saying “ blue not_square not_triangle ” is as compositional as “ blue circle ” .
We provide this explanation in the supplementary material in the revised paper .

12 . Another ICLR submission a quantitative measure of compositionality :
Thank you for the pointer , we will look into this .

This paper presents a technique for training a two - agent system to play a simple reference game involving recognition of synthetic images of a single object . Each agent is represented by an RNN that consumes an image representation and sequence of tokens as input , and generates a binary decision as output . The two agents are initialized independently and randomly . In each round of training , one agent is selected to be the speaker and the other to be the listener . The speaker generates outputs by greedily selecting a sequence of tokens to maximize the probability of a correct recognition w/ r/t the speaker 's model . The listener then consumes these tokens , makes a classification decision , incurs a loss , and updates its parameters . Experiments find that after training , the two agents converge to approximately the same language , that this language contains some regularities , and that agents are able to successfully generalize to novel combinations of properties not observed during training .

While Table 3 is suggestive , this paper has many serious problems . There is n't an engineering contribution --- despite the motivation at the beginning , there 's no attempt to demonstrate that this technique could be used either to help comprehension of natural language or to improve over the numerous existing techniques for automatically learning communication protocols . But this also is n't science : " generalization " is not the same thing as compositionality , and there 's no testable hypothesis articulated about what it would mean for a language to be compositional --- just the post-hoc analysis offered in Tables 2 & 3 . I also have some concerns about the experiment in Section 3.3 and the overall positioning of the paper .

I want to emphasize that these results are cool , and something interesting might be going on here ! But the paper is not ready to be published . I apologize in advance for the length of this review ; I hope it provides some useful feedback about how future versions of this work might be made more rigorous .

WHAT IS COMPOSITIONALITY ?

The title , introduction , and first three sections of this paper emphasize heavily the extent to which this work focuses on discovering " compositional " language . However , the paper does n't even attempt to define what is meant by compositionality until the penultimate page , where it asserts that the ability to " accurately describe an object [ ...] not seen before " is " one of the marks of compositional language " . Various qualitative claims are made that model outputs " seem to be " compositional or " have the strong flavor of " compositionality . Finally , the conclusion notes that " the exact definition of compositional language is somewhat debatable , and , to the best of our knowledge , there was no reliable way to check for the compositionality of an arbitrary language . "

This is very bad .

It is true that there is not a universally agreed - upon definition of compositionality . In my experience , however , most people who study these issues do not ( contra the citation - less 5th sentence of section 4 ) think it is simply an unstructured capacity for generalization . And the implication that nobody else has ever attempted to provide a falsifiable criterion , or that this paper is exempt from itself articulating such a criterion , is totally unacceptable . ( You can not put " giving the reader the tools to evaluate your current claims " in future work ! )

If this paper wishes to make any claims about compositionality , it must _at a minimum_ :

1 . Describe a test for compositionality .

2 . Describe in detail the relationship between the proposed test and other definitions of compositionality that exist in the literature .

3 . If this compositionality is claimed to be " language - like " , extend and evaluate the definition of compositionality to more complex concepts than conjunctions of two predicates .

Some thoughts to get you started : when talking about string - valued things , compositionality almost certainly needs to say something about _syntax_ . Any definition you choose will be maximally convincing if it can predict _without running the model_ what strings will appear in the gray boxes in Figure 3 . Similarly if it can consistently generate analyses across multiple restarts of the training run . The fact that analysis relies on seemingly arbitrary decisions to ignore certain tokens is a warning sign . The phenomenon where every color has 2 - - 3 different names depending on the shape it 's paired with would generally be called " non-compositional " if it appeared in a natural language .

This SEP article has a nice overview and bibliography : https://plato.stanford.edu/entries/compositionality/. But seriously , please , talk to a linguist .

MODEL

The fact that the interpretation model greedily chooses symbols until it reaches a certain confidence threshold would seem to strongly bias the model towards learning a specific communication strategy . At the same time , it 's not actually possible to guarantee that there is a greedily - discoverable sequence that ever reaches the threshold ! This fact does n't seem to be addressed .

This approach also completely rules out normal natural language phenomena ( consider " I know Mary " vs " I know Mary will be happy to see you " ) . It is at least worth discussing these limitations , and would be even more helpful to show results for other architectures ( e.g. fixed - length codes or loss functions with an insertion penalty ) as well .

There 's some language in the appendix ( " We noticed that a larger vocabulary and a longer message length helped the agents achieve a high communication accuracy more easily . But the resulting messages were challenging to analyze for compositional patterns . " ) that suggests that even the vague structure observed is hard to elicit , and that the high - level claim made in this paper is less robust than the body suggests . It 's _really_ not OK to bury this kind of information in the supplementary material , since it bears directly on your core claim that compositional structure does arise in practice . If the emergence of compositionality is sensitive to vocab size & message length , experiments demonstrating this sensitivity belong front - and - center in the paper .

EVALUATION

The obvious null hypothesis here is that unseen concepts are associated with an arbitrary ( i.e. non -compositional ) description , and that to succeed here it 's enough to recognize this description as _different_ without understanding anything about its structure . So while this evaluation is obviously necessary , I do n't think it 's powerful enough to answer the question that you 've posed . It would be helpful to provide some baselines for reference : if I understand correctly , guessing " 0 " identically gives 88 % accuracy for the first two columns of Table 4 , and guessing based on only one attribute gives 94 % , which makes some of the numbers a little less impressive .

Perhaps more problematically , these experiments do n't rule out the possibility that the model always guesses " 1 " for unseen objects . It would be most informative to hold out multiple attributes for each held - out color ( & vice - versa ) , and evaluate only with speakers / listeners shown different objects from the held - out set .

POSITIONING AND MOTIVATION

The first sentence of this paper asserts that artificial general intelligence requires the ability to communicate with humans using natural language . This paper has nothing to do with AGI , humans , or human language ; to be blunt , this kind of positioning is at best inappropriate and at worst irresponsible . It must be removed . For the assertion that " natural language processing has shown great progress " , the paper provides a list of citations employing neural networks exclusively and beginning in 2014 ( ! ) . I would gently remind the authors that NLP research did not begin with deep learning , and that there might be slightly earlier evidence for their claim .

The attempt to cite relevant work in philosophy and psychology is commendable ! However , many of these citations are problematic , and some psycho- / historico- linguistic claims are still missing citations . A few examples : Ludwig Wittgenstein died in 1951 , so it is somewhat surprising to see him cited for a 2010 publication ( PI appeared in 1953 ) ; similarly Zipf ( 2016 ) . The application of this Zipf citation is dubious ; the sentence preceded by footnote 7 is false and has nothing to do with the processes underlying homophony in natural languages . I would encourage you to consult with colleagues in the relevant fields .


- Greedily choosing the characters has limits
Please see our second explanation for Reviewer 3.

- Hyperparameters ( vocabulary size , maximum length of the message ) should not be in the supplementary material .
Thank you for pointing this out . We agree that they are more appropriately discussed in section 2.4 where we discuss the environmental pressure . This is reflected in the revised paper .

- Table 4 is not sufficient to confirm compositionality :
We described this in line 244 ( section 4 ) . We agree that currently many works in this field rely on evaluations that check the necessary conditions of compositional language , and we lack ones that check the sufficient conditions of compositional language . Although , we are not sure if there are such measures . It is another line of future work for all of us .

- Table 4 . Always guessing 0 gives 88 % accuracy :
To be precise , always guessing 0 gives 97.5 % accuracy . Because out of 40 object pairs , you would only be wrong once ( when the same object type is paired ) . However , as the numbers in Table 4 shows , the agents do not always guess 0 .

- Table 4 . Guessing based on only one attribute gives 94 % accuracy :
If the agents only focus on colors ( which is the dominant attribute , since there are 8 colors and 5 shapes ) , then out of 40 object pairs , they would be wrong 4 times ( when the object pair consists of the same color but different shapes ) . So that gives us 90 % accuracy . However , Table 4 shows that the agents are doing better than 90 % most of the time , telling us that they are not just focusing on one attribute .

- Maybe the agents always guess 1 for unseen objects .
If they did , then the last column of Table 4 should be all 1.0.

- Saying artificial intelligence requires the ability to communicate with humans is problematic :
It ’s hard to understand why that statement is problematic . We already have simple AI agents around us that try to communicate with us , such as Google Home , Siri , and Alexa . It is hard to imagine that , in the future , we will not verbally communicate with AI .

- NLP has longer history than deep learning :
Yes , it is true . We agree that our citations were biased due to the venue we are submitting this work to , ICLR , which was born recently after the renaissance of neural networks . We incorporated this point in the revised paper .

- The sentence preceded by footnote 7 is false and has nothing to do with the processes underlying homophony in natural languages :
We can generally observe that homonyms are seldom used in the same context ( e.g. mean1 ( definition : being rude ) and mean2 ( definition : average ) ) , as that will create confusion , leading to inefficient communication . The same could be observed in the emergent language as described in the second paragraph of section G.
However , we do agree that our logic for connecting the principle of least effort with homonyms might be a bit of a stretch . We removed the sentence in the revised paper .

We thank you for your detailed review and helpful comments . We address your concerns as follows .

- Lack of engineering contribution :
Please see our third explanation for Reviewer3 .

- The paper does n't even attempt to define what is meant by compositionality until the penultimate page :
Although we provided the definition of compositionality in line 33 - 34 , we do agree that the definition could be more explicit . In the revised paper , we explicitly provide the definition of compositionality ( or at least what people generally agree to be the definition ) , and connect that definition to the experiment section to claim that our resulting language does meet the requirement of the compositional language qualitatively .
And it was never our intention to equate “ compositionality ” and “ being able to do zero - shot test ” , since we did define compositionality as “ we express our intentions using words as building blocks to compose a complex meaning ” . In the revised paper , we changed any sentence that might confuse the readers to think that we equate “ compositionality ” and “ being able to do zero - shot test ” , because we certainly do not .

- Citation - less fifth sentence of section 4 :
Thank you for pointing this out . We added proper citations for this .

- If this paper wishes to make any claims about compositionality , it must at a minimum :
We would like to point out that ICLR is not a linguistics conference ( nor is it a language - oriented one such as ACL ) and our intention was not to develop a rigorous , quantifiable definition of compositionality and its evaluation strategy , for which even linguists have not been successful yet . The recent papers demonstrating the emergence of grounded / compositional communication using neural agents use a relatively relaxed definition of compositionality . But that does not make their work less interesting or meaningful . In the context of learning representations ( which is the main theme of ICLR ) , we care about certain aspects more ( e.g. pushing the limits of neural agents ) and other aspects less ( e.g. coming up with a definition of compositionality that is both linguistically and computationally meaningful ) . In this spirit , we believe our work has shown sufficient contributions in several aspects as described in the introduction ( section 1 ) .
Lastly , the researchers in this field ( neural agents and communication ) have just begun to discuss the need and strategy for quantitative measure of how much the artificial language resembles human language ( in aspects such as compositionality ) , and we believe our discussion in the last section raises relevant issues in accordance with the movement . Over time , such discussion will result in a practical , quantifiable measure of compositionality that most researchers can agree upon .

- Claim for compositionality would be convincing if we can predict the string to be appeared in the gray boxes in Figure 3.
We would like to point out that the messages in the gray boxes of Figure 3 do actually follow the patterns nicely , except for yellow ellipsoid . After seeing all blue objects and all box objects except blue box , we can infer that the blue box will be very likely be described by “ eeeeee ee ” . Granted , the compositional patterns in Table 3 are weaker than Table 2 , but it is undeniable that there are patterns .

Pros :
1 . Extend the input from disentangled feature to raw image pixels
2 . Employ “ obverter ” technique , showing that it can be an alternative approach comparing to RL
3 . The authors provided various experiments to showcase their approach

Cons :
1 . Comparing to previous work ( Mordatch & Abbeel , 2018 ) , the task is relatively simple , only requiring the agent to perform binary prediction .
2 . Sharing the RNN for speaking and consuming by picking the token that maximizes the probability might decrease the diversity .
3 . This paper lack original technical contribution from themselves .

The paper is well - written , clearly illustrating the goal of this work and the corresponding approach . The “ obverter ” technique is quite interesting since it incorporates the concept from the theory of mind which is similar to human alignment or AGI approach . Authors provided complete experiments to prove their concept ; however , the task is relatively easy compared to Mordatch & Abbeel ( 2018 ) . Readers would be curious how this approach scales to a more complex problem .

While the author sharing the RNN for speaking and consuming by picking the token that maximizes the probability , the model loses its diversity since it discards the sampling process . The RL approach in previous works can efficiently explore sentence space by sampling from policy distribution . However , I can not see how the author tackles this issue . One of the possible reason is that the task is relatively easy , therefore , the agent does not need explicitly exploration to tackle this task . Otherwise , some simple technique like “ beam search ” or perform MC rollout at each time could further improve the performance .

In conclusion , this paper does not have a major flaw . The “ obverter ” approach is interesting ; however , it is originally proposed in Batali ( 1998 ) . Generating language based only on raw image pixels is not difficult . The only thing you need to do is replacing FC layers with CNN layers . Though this paper employs an interesting method , it lacks some technical contribution from themselves .


[ 1 ] Igor Mordatch and Pieter Abbeel . Emergence of grounded compositional language in multi-agent populations . AAAI 2018

[ 2 ] John Batali . Computational simulations of the emergence of grammar . Approaches to the evolution of language : Social and cognitive bases , 405:426 , 1998 .

We thank you for your thoughtful review . We address each of your concerns as follows .

1 . The task is relatively simple :
To the best of our knowledge , this is the first attempt to observe the emergence of compositional communication based on pure pixels . Our aim was to test the possibility of the emergence of compositionality based on a straightforward task so that we can perform extensive analysis on the outcome confidently , based on full control of the experiment . Especially , since our experiment maps a sequence of symbols to each factor ( color and shape ) , instead of mapping one symbol to each factor ( such as Mordatch & Abbeel 2017 [ 1 ] or Kottur 2017 [ 2 ] ) , rashly taking on complex tasks would have made the qualitative analysis of the emergent language exponentially difficult . As mentioned in our paper , a systematic method to evaluate the compositionality of a given language is an on-going effort in the emergent communication community , and as the evaluation strategy advances we will be more prepared to take on interesting , complex tasks to encourage the emergence of human-like language . We clarified our motivation for choosing a relatively simple task in the revised paper .

2 . Using a single RNN for both speaking and listening hinders the agents ’ chance to explore :
While it is true that our deterministic approach in the paper might not match the typical exploratory behavior of RL , there are many ways to encourage the agents to explore the message space while using the obverter strategy . In fact , since only the listener ’s parameters are updated and the speaker ’s parameters are fixed during training , we can be quite creative with the message generation process and still be able to use gradient descent . One of the straightforward ways to increase the message diversity is to sample the character from a multinomial distribution at each timestep , instead of deterministically selecting the one that maximizes the output ( y_hat ) . We can go a little further and adjust the temperature of the softmax function , so that the agents explore the message space a little more actively during the early training rounds and gradually convert to the deterministic behavior . As you suggested , this might help the agents discover a more optimal language when facing complex tasks . We reflected this comment in our revised paper .

3 . This paper lacks original technical contribution :
This may seem so based on our relatively light - weight model structure , and the fact that we employed the obverter strategy . The aim of the paper was to guide the agents to develop compositional communication , and we have shown strong evidence that it could be done with our approach , which combines the recent , advanced neural network techniques and a philosophy well motivated by psychology and linguistics . We believe proposing a method that solves an interesting problem ( although , in our case , how successfully we solved the emergence of compositionality is hard to quantify ) is itself a technical contribution , although the amount of contribution is subject to opinion . Also , the obverter strategy is not a specific algorithm like ADAM , but more like a general philosophy such as GAN , which has been used in various works besides Batali 1998 [ 3 ] . And we believe that introducing a successful philosophy from a different , yet relevant field to the machine learning community is a valuable contribution . We added a small description in the revised paper regarding how obverter strategy was originated in Hurford 1989 [ 4 ] and has been used in many works including Kirby and Hurford 2002 [ 5 ] .

[ 1 ] Igor Mordatch and Pieter Abbeel . Emergence of grounded compositional language in multi-agent populations . AAAI 2018
[ 2 ] Satwik Kottur , Jose MF Moura , Stefan Lee , and Dhruv Batra . Natural language does not emerge ’ naturally ’ in multi-agent dialog . EMNLP 2017
[ 3 ] John Batali . Computational simulations of the emergence of grammar . Approaches to the evolution of language : Social and cognitive bases , 405:426 , 1998
[ 4 ] James R Hurford . Biological evolution of the saussurean sign as a component of the language acquisition device . Lingua , 77 ( 2 ) :187–222 , 1989
[ 5 ] Simon Kirby and James R Hurford . The emergence of linguistic structure : An overview of the iterated learning model . In Simulating the evolution of language , pp. 121–147 . Springer , 2002

Through evaluation of current popular GAN variants .
* useful AIS figure
* useful example of failure mode of inception scores
* interesting to see that using a metric based on a model ’s distance does not make the model better at that distance
the main criticism that can be given to the paper is that the proposed metrics are based on trained models which do not have an independent clear evaluation metrics ( as classifiers do for inception scores ) . However , the authors do show that the results are consistent when changing the critic architecture . Would be nice to see if this also holds for changes in learning rates .
* nice to see an evaluation on how models scale with the increase in training data .

Using an Independent critic for evaluation has been proposed and used in practice before , see “ Comparison of Maximum Likelihood and GAN - based training of Real NVPs ” , Danihelka et all , as well as Variational Approaches for Auto-Encoding Generative Adversarial Networks , Rosca at all .

Improvements to be added to the paper :
* How about overfitting ? Would be nice to mention whether the proposed metrics are useful at detecting overfitting . From algorithm 1 one can see that the critic is trained on training data , but at evaluation time test data is used . However , if the generator completely memorizes the training set , the critic will not be able to learn anything useful . In that case , the test measure will not provide any information either . A way to go around this is to use validation data to train the critic , not training data . In that case , the critic can learn the difference between training and validation data and at test time the test set can be used .
* Using the WGAN with weight clipping is not a good baseline . The improved WGAN method is more robust to hyper parameters and is the one currently used by the community . The WGAN with weight clipping is quite sensitive to the clipping hyperparameter , but the authors do not report having changed it from the original paper , both for the critic or for the discriminator used during training .
* Is there a guidance for which metric should be used ?

Figure 3 needs to be made a bit larger , it is quite hard to read in the current set up .

Thank you for your review and thank you for directing us to “ Comparison of Maximum Likelihood and GAN - based training of Real NVPs ” by Danihelka et al . We have included references to this work in the revised paper .

Regarding the comment on WGAN with weight clipping :
We agree with the reviewer . We will include the clipping hyperparameter , which was 0.1 . Additionally , we added experiments with the ( improved ) WGAN with gradient penalty as shown above . The results can be also found in the updated version of the paper .



Regarding the comment on overfitting :
We agree with the reviewer ’s comments . As such , we experimented with the scenario proposed by the reviewer . We trained two critics on training data and validation data respectively and evaluated on test data from both critics .

We trained six GANs ( GAN , LSGAN , WGAN_GP , DRAGAN , BEGAN , EBGAN ) on MNIST and Fashion MNIST . We trained these GANs with 50 K training examples . At test time , we used 10 k training and 10 k validation examples for training the critics , and evaluated on 10 k test examples . Here , we present the test scores from the critics trained on training and validation data :

Fashion-MNIST
LS score ( trained on training data ) LS score ( trained on validation data )
LSGAN 0.135 +- 0.0046 0.136 +- 0.0074
GAN 0.1638 +- 0.010 0.1635 +- 0.0006
DRAGAN 0.1638 +- 0.015 0.1645 +- 0.0151
BEGAN 0.1133 +- 0.042 0.0893 +- 0.0095
EBGAN 0.0037 +- 0.0009 0.0048 +- 0.0023
WGAN_GP 0.000175 +- 0.0000876 0.000448 +- 0.0000862

MNIST
LS score ( trained on training data ) LS score ( trained on validation data )
LSGAN 0.323 +- 0.0104 0.352 +- 0.0143
GAN 0.312 +- 0.010 0.4408 +- 0.0201
DRAGAN 0.318 +- 0.012 0.384 +- 0.0139
BEGAN 0.081 +- 0.016 0.140 +- 0.0329
EBGAN 3.38e -6 +- 0.1.86e-7 3.82e-6 +- 2.82e-7
WGAN_GP 0.196 +- 0.006 0.307 +- 0.0381

Note that we also have the IW and FID evaluation on these models in the paper . For Fashion -MNIST , we find that test scores with critic trained on training and validation data are very close . Hence , we do n’t see any indication of overfitting . On the other hand , there are gaps between the scores for the MNIST dataset and the test scores from critics trained on validation set gives better performance than the ones that are trained on the training set .



Regarding the comment on guidance towards a metric :

In terms of guidance on which metric to use , we recommend using the metric that is the closest to the human perceptual score ! Thus , we added a comparison between the evaluation metrics to human perceptual scores . Please see the response ( *** ) to Reviewer 1 .


Thank you !!


This paper proposes using divergence and distance functions typically used for generative model training to evaluate the performance of various types of GANs . Through numerical evaluation , the authors observed that the behavior is consistent across various proposed metrics and the test - time metrics do not favor networks that use the same training - time criterion .

More specifically , the evaluation metric used in the paper are : 1 ) Jensen - Shannon divergence , 2 ) Constrained Pearson chi-squared , 3 ) Maximum Mean Discrepancy , 4 ) Wasserstein Distance , and 5 ) Inception Score . They applied those metrics to compare three different GANs : the standard DCGAN , Wasserstein DCGAN , and LS-DCGAN on MNIST and CIFAR - 10 datasets .

Summary :
— —
In summary , it is an interesting topic , but I think that the paper does not have sufficient novelty . Some empirical results are still preliminary . It is hard to judge the effectiveness of the proposed metrics for model selection and is not clear that those metrics are better qualitative descriptors to replace visual assessment . In addition , the writing should be improved . See comments below for details and other points .

Comments :
— —
1 . In Section 3 , the evaluation metrics are existing metrics and some of them have already been used in comparing GAN models . Maximum mean discrepancy has been used before in work by Yujia Li et al. ( 2016 , 2017 )

2 . In the experiments , the proposed metrics were only tested on small scale datasets ; the authors should evaluate on larger datasets such as CIFAR - 100 , Toronto Faces , LSUN bedrooms or CelebA .

3 . In the experiments , the authors noted that “ Gaussian observable model might not be the ideal assumption for GANs . Moreover , we observe a high log-likelihood at the beginning of training , followed by a drop in likelihood , which then returns to the high value , and we are unable to explain why this happens . ” Could the authors give explanation to this phenomenon ? The authors should look into this more carefully .

4 . In algorithm 1 , it seems that the distance is computed via gradient decent . Is it possible to show that the optimization always converges ? Is it meaningful to compare the metrics if some of them can not be properly computed ?

5 . With many different metrics for assessing GANs , how should people choose ? How do we trust the scores ? Recently , Fréchet Inception Distance ( FID ) was proposed to evaluate the samples generated from GANs ( Heusel et al. 2017 ) , how are the above scores compared with FID ?

Minor Comments :
— —
1 . Writing should be fixed : “ It seems that the common failure case of MMD is when the mean pixel intensities are a better match than texture matches ( see Figure 5 ) , and the common failure cases of IS happens to be when the samples are recognizable textures , but the intensity of the samples are either brighter or darker ( see Figure 2 ) . ”


Thank you for your review !

Regarding comment 1 :
The reviewer noted that MMD has been used in previous works by Yujia Li et al . [ 1 ,2 ] . Indeed , MMD has been proposed as a training objective in many previous works . Nevertheless , the goal of this paper was to consider different evaluation metrics for scoring GANs and test whether one type of GANs is statistically better than the other one under different metrics . Our claim of novelty is not in proposing a new metric but in evaluating GANs under many different metrics . We made an observation that many metrics have been used to train GANs but surprisingly have not been used to evaluate GANs at test time . Hence , we used those metrics with a critic network to evaluate GANs . Li et al . [ 1 , 2 ] have not employed MMD as a GAN evaluation metric at test time . In the paper , we systematically compared and ranked GANs under different metrics .


Regarding comment 2 : Please see our response to Reviewer 3.


Regarding comment 3 : The mentioned phenomenon for log-likelihood using AIS by [ 3 ] is interesting . However , we do not know why it has that behaviour and we believe that this is not within the scope of our paper to find out . It would be best to directly ask the authors of [ 3 ] .

Regarding comment 4 : The reviewer asked about convergence guarantees using gradient descent . Note that gradient descent is employed widely in deep learning and optimizing the critic ’s objective ( the distance ) is exactly same as training a deep feedforward network with gradient descent . The scope of this comment falls under all deep learning methods and it is not specific to our paper .

Note : We include the training curve of critics to show that at least the training curve converges ( see Figure 26 ) .

Regarding comment 5 : Please see our response to the review ( Fréchet Inception Distance for evaluating GANs ) from Nov. 20th 2017 . We have included substantial experimental results in the updated version of the paper .

( ***)
Moreover , we added a comparison between the evaluation metrics and human perceptual scores . We showed which metrics are more statistically correlated with human perceptual scores . This was done based on the Wilcoxon rank sum test & two - sided Fisher ’s test . The fraction of pairs on which each metric agrees with humans ( higher the better ) :

Metric Fraction Agreed agreed # samples / # total samples
Inception score 0.862595 ( 113 / 131 )
IW Distance 0.977099 ( 128 / 131 )
LS score 0.931298 ( 122 / 131 )
MMD 0.83261 ( 109 / 131 )


It shows that IW distance agreed the most with human perceptual scores , and then followed by LS , Inception score , and MMD .

Also , here are the results of a two -sided Fisher ’s test ( no multiple comparison correlation ) that these fractions are the same or not :

IS equals IW : p-value = 0.000956
IS equals LS : p-value = 0.102621
IS equals MMD : p-value = 0.606762
IW equals LS : p-value = 0.136684
IW equals MMD : p-value = 0.000075
LS equals MMD : p-value = 0.020512

Over all , it demonstrates that IW , LS are significantly more aligned with the perception scores than the Inception Score and MMD , p < 0.05 . ( See the Section 4.2.1 for details ) .

We have improved the quality of the writing in the revised paper .
Thank you !!


[ 1 ] Yujia Li , Kevin Swersky and Richard Zemel . Generative Moment Matching Networks . International Conference on Machine Learning ( ICML ) , 2015
[ 2 ] Yujia Li , Alexander Schwing , Kuan - Chieh Wang , Richard Zemel . Dualing GANs . https://arxiv.org/abs/1706.06216
[ 3 ] Yuhuai Wu , Yuri Burda , Ruslan Salakhutdinov and Roger Grosse . On the Quantitative Analysis of Decoder - Based Generative Models . ICLR , 2017 .


the paper proposes an evaluation method for training GANs using four standard distribution distances in literature namely :
- JSD
- Pearson-chi-square
- MMD
- Wasserstein- 1

For each distance , a critic is initialized with parameters p . The critic is a neural network with the same architecture as the discriminator .
The critic then takes samples from the trained generator model , and samples from the groundtruth dataset . It trains itself to maximize the distance measure between these two distributions ( trained via gradient descent ) .

These critics after convergence will then give a measure of the quality of the generator ( lower the better ) .

The paper is easy to read , experiments are well thought out .
Figure 3 is missing ( e ) and ( f ) sub-figures .

When proposing a distance measure for GANs ( which is a high standard , because everyone is looking forward to a robust measure ) , one has to have enough convincing to do . The paper only does experiments on two small datasets MNIST and CIFAR . If the paper has to convince me that this metric is good and should be used , I need to see experiments on one large - scale datset , such as Imagenet or LSUN . If one can clearly identify the good generators from bad generators using a weighted - sum of these 4 distances on Imagenet or LSUN , this is a metric that is going to stand well .
If such experiments are constructed in the rebuttal period , I shall raise my rating .

Thank you for your review .

At the reviewer ’s suggestion , we conducted the same experiments on the LSUN bedroom dataset . We used 64 x 64 of 90,000 images to train GAN , LSGAN , WGAN and tested on another 90,000 unseen images . We evaluated using the LS score , IW distance , and MMD . We omitted the Inception score , because LSUN bedroom dataset contains just a single class and there is no pre-trained convolutional network available ( inception score needs a pre-trained convolutional network ) . Samples from each model are also added in the appendix of the paper . Here is a summary of the results :

LS ( higher the better ) IW ( lower the better ) MMD ( higher the better )
GAN : 0.14614 3.79097 0.00708
LSGAN : 0.173077 3.36779 0.00973
WGAN : 0.205725 2.91787 0.00584

All three metrics agrees that WGAN has the best score . LSGAN is ranked second according to the LS score and IW distance , in contrast , MMD puts GAN in second place and LSGAN on third place . Nevertheless , in our more recent experiments added to the revised version of the paper , we showed that MMD score often disagrees with human perceptual score .

In summary , we applied our evaluation methods to larger images and the performance of IW and LS are consistent with what we observed on MNIST and CIFAR10 .

We added these results to the paper .


Additionally , we would like to note that we added a comparison between the evaluation metrics to human perceptual scores . Please see the response ( *** ) to Reviewer 1 .

Thank you !!


The authors investigate knowledge distillation as a way to learn low precision networks . They propose three training schemes to train a low precision student network from a teacher network . They conduct experiments on ImageNet - 1 k with variants of ResNets and multiple low precision regimes and compare performance with previous works

Pros :
( + ) The paper is well written , the schemes are well explained
( + ) Ablations are thorough and comparisons are fair
Cons :
( -) The gap with full precision models is still large
( -) Transferability of the learned low precision models to other tasks is not discussed

The authors tackle a very important problem , the one of learning low precision models without comprosiming performance . For scheme -A , the authors show the performance of the student network under many low precision regimes and different depths of teacher networks . One observation not discussed by the authors is that the performance of the student network under each low precision regime does n't improve with deeper teacher networks ( see Table 1 , 2 & 3 ) . As a matter of fact , under some scenarios performance even decreases .

The authors do not discuss the gains of their best low - precision regime in terms of computation and memory .

Finally , the true applications for models with a low memory footprint are not necessarily related to image classification models ( e.g. ImageNet - 1 k ) . How good are the low - precision models trained by the authors at transferring to other tasks ? Is it possible to transfer student - teacher training practices to other tasks ?

Thank you for the thorough reviews .

We defend the " Cons " reviews below :

Gap from full precision : Agreed . However , compared to prior works the improvement is significant . For example , now with 8 - bits activations and 4 - bits weight , ResNet - 34 without any change in network architecture is only 0.5 % off from baseline full precision . This is currently the best Top - 1 figure at this precision knob -- the best figure with prior techniques gave 3.3 % degradation ( so 2.8 % improvement with our scheme ) . We believe that by making the models slightly larger ( by 10 % or so ) we can close the gap between low - precision and full - precision networks -- this is our future work .

Transferability : We believe , this aspect should not change with our scheme for low - precision settings -- we simply better the accuracy of a given network at low - precision ( compared to prior proposals ) . However much the network was useful in transfer learning scenarios with low - precision tensors before , the network right now with our scheme would be similarly useful ( if at all better when compared to prior works since we achieve a better accuracy with low - precision ) .

Your question : Is it possible to transfer student - teacher training practices to other tasks ?
Although we did not focus on this aspect in this paper , we found the following 3 works ( not an exhaustive list ) that use the student - teacher training procedure for other deep - learning domains :

1 . Transferring Knowledge from a RNN to a DNN , William Chan et al. , ArXiv pre-print , 2015 .
2 . Recurrent Neural Network Training With Dark Knowledge Transfer , Zhiyuan Tang et al. , ArXiv pre-print , 2016 .
3 . Simultaneous Deep Transfer Across Domains and Tasks , Eric Tzeng et al. , ICCV 2015 .


The observation that accuracy does not improve when using bigger teacher network ( s ) : We allude to this in Discussion part of Section 5.2 ( page - 8 ) . We mention that the accuracy improvement saturates at some point . We will elaborate on this aspect in the final version of the paper .

We will also discuss the merits of low precision on savings in compute and memory . We briefly discuss these aspects in Section 2 where we mention about simplification in hardware support required for inference . We will elaborate on these aspects and provide quantification in compute and memory footprint savings vs . accuracy in our final version of the paper .

The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full - precision network . Instead of distillation from a pre-trained network , the paper proposes to train both teacher and student network jointly . The paper shows an interesting result that the distilled low precision network actually performs better than high precision network .

I found the paper interesting but the contribution seems quite limited .

Pros :
1 . The paper is well written and easy to read .
2 . The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network , and that training jointly outperforms the traditional distillation method ( fixing the teacher network ) marginally .

Cons :
1 . The name Apprentice seems a bit confusing with apprenticeship learning .
2 . The experiments might be further improved by providing a systematic study about the effect of precisions in this work ( e.g. , producing more samples of precisions on activations and weights ) .
3 . It is unclear how the proposed method outperforms other methods based on fine-tuning . It is also quite possible that after fine -tuning the compressed model usually performs quite similarly to the original model .

Thank you for the reviews . They are useful .

We defend our contribution aspect below :

Contributions : Distillation along with lowering precision has not been studied before . We show benefits of this combined approach - both distillation and low precision target model compression aspect - but when combined the benefits are significant . We also show how one can use the combined distillation and lowering precision approach to training as well as fine-tuning .

Our approach achieves state - of - the-art in accuracy over prior proposals and using our approach we significantly close the gap between full - precision and low-precision model accuracy . We demonstrate the benefits on ImageNet with large networks ( ResNet ) . For example , with ResNet - 50 on ImageNet , prior work showed 4.7 % accuracy degradation with 8 - bits activation and 4 - bits weight . We lower this gap to less than 1.5 % .

We believe ours to be the first work that targets both model compression ( using knowledge distillation ) and low-precision .


Response to the Cons aspects :
1 . We probably did not do a good job describing why we call our approach Apprentice . We will fix this and disambiguate from apprenticeship- based learning schemes .

2 . The reason we focus on sub 8 - bit precision is that model inference with 8 - bits is becoming mainstream and we seek to target next- gen hardware architectures . Also , from a hardware point- of-view 8 - bits , 4 - bits and 2 - bits simplify design ( e.g. alignment across cache line boundaries and memory accesses vs . 3 - bits or 5 - bits precision for example ) .

3 . We had tried the scheme you mention in ( 3 ) but the results were not ( as ) good compared to the schemes we mention in the paper , hence we omitted this scheme from our paper .

We experimented with ResNet - 18 with ( a ) first , compressing using distillation scheme ( used ResNet - 34 as the teacher network ) and then ( b ) lowered the precision to ternary mode ( fine-tuning for 35 epochs with low learning rate ) . This experiment was done for ImageNet-1K dataset . This experiment is a variation of scheme - C in our paper where we start with full - precision networks and jointly fine - tune ( use distillation scheme with warm start - up ) .
Activation precision was 8 - bits for this experiment . The ResNet - 18 network converged to 33.13 % Top - 1 error rate . Comparing this with " jointly " compressing and lowering precision while training from scratch , we get 32.0 % Top - 1 error rate ( Table - 1 , 4th row and 2nd column ) . So , our Apprentice scheme for this network and configuration is 1.13 % better .

Your point is well taken and we will include results where first we use knowledge distillation scheme to generate a smaller ResNet model and then lower the precision and fine- tune this small model . Currently , we have results of this scheme with ResNet - 18 and few precision knobs and will collect results with this scheme for ResNet - 34 and ResNet - 50 for the final paper version .
As mentioned above , the conclusions of our paper would not change and the new results will show the benefits of joint training with distillation ( Apprentice scheme ) . Many works proposing low-precision knobs advocate for training from scratch or training with warm - startup ( from weights at full - precision numerics ) -- our work is in line with these observations .



Summary :
The paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation .
Scheme A consists of training a high precision teacher jointly with a low precision student . Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode .

Review :
The paper is well written . The experiments are clear and the three different schemes provide good analytical insights .
Using scheme B and C student model with low precision could achieve accuracy close to teacher while compressing the model .

Comments :
Tensorflow citation is missing .
Conclusion is short and a few directions for future research would have been useful .

Thank you for the reviews and comments .

Missing citation : this is an oversight . We will fix this .

Future directions for research :
1 . We are currently pursuing the extension of the ideas in this paper to RNNs . Our preliminary studies on a language model for PTB dataset showed promise and based on this we are evaluating a larger data set and model like Deep Speech - 2 .
2 . Some works proposing low -precision networks advocate for making the layers wider ( or the model larger ) to recover accuracy at low -precision . These works propose making the layers wider by 2 x or 3x . While these works show the benefits of low - precision , making the model larger increases the number of raw computations . Future work could investigate low -precision and less layer widening factor ( say 1.10 x or 1.25 x or ... ) . This would help inference latency while maintaining accuracy at - par with baseline full - precision .
3 . Another interesting line of investigation for future work is looking into sparsifying networks at low - precision while maintaining baseline level accuracy and using knowledge distillation scheme during this process . As mentioned in Sec 5.5 in our paper , sparsifying a model more than a certain percentage leads to accuracy loss . Investigating hyper-sparse network models without accuracy loss using distillation based schemes is an interesting avenue of further research .


*** Revision : based on the author 's work , we have switched the score to accept ( 7 ) ***

Clever ideas but not end - to - end navigation .

This paper presents a hybrid architecture that mixes parametric ( neural ) and non-parametric ( Dijkstra 's path planning on a graph of image embeddings ) elements and applies it to navigation in unseen 3D environments ( Doom ) . The path planning in unseen environments is done in the following way : first a human operator traverses the entire environment by controlling the agent and collecting a long episode of 10 k frames that are put into a chain graph . Then loop closures are automatically detected using image similarity in feature space , using a localization feed - forward ResNet ( trained using a DrLIM - like triplet loss on time - similar images ) , resulting in a topological graph where edges correspond to similar viewpoints or similar time points . For a given target position image and agent start position image , a nearest neighbor search - powered Dijkstra path planning is done on the graph to create a list of waypoint images . The pairs of ( current image , next waypoint ) images are then fed to a feed - forward locomotion ( policy ) network , trained in a supervised manner .

The paper does not discuss at all the problems arising when the images are ambiguous : since the localisation network is feed - forward , surely there must be images that are ambiguously mapped to different graph areas and are closing loops erroneously ? The problem is mitigated by the fact that a human operator controls the agent , making sure that the agent 's viewpoint is clear , but the method will probably fail if the agent is learning to explore the maze autonomously , bumping into walls and facing walls . The screenshots on Figure 3 suggest that the walls have a large variety of textures and decorations , making each viewpoint potentially unique , unlike the environments in ( Mirowski et al , 2017 ) , ( Jaderberg et al , 2017 ) and ( Mnih et al , 2016 ) .

Most importantly , the navigation is not based on RL at all , and ignores the problem of exploration altogether . A human operator labels 10 k frames by playing the game and controlling the agent , to show it how the maze looks like and what are the paths to be taken . As a consequence , comparison to end - to - end RL navigation methods is unclear , and should be stressed upon in the manuscript . This is NOT a proper navigation agent .

Additional baselines should be evaluated : 1 ) a fully Dijkstra - based baseline where the direction of motion of the agent along the edge is retrieved and used to guide the agent ( i.e. , the policy becomes a lookup table on image pairs ) and 2 ) the same but the localization network replaced by image similarities in pixel space or some image descriptor space ( e.g. , SURF , ORB , etc … ) . It seems to me that those baselines would be very strong .

Another baseline is missing : ( Oh et al , 2016 , “ Control of Memory , Active Perception , and Action in Minecraft ” ) .

The paper is not without merit : the idea of storing experiences in a graph and in using landmark similarity rather than metric embeddings is interesting . Unfortunately , that episodic memory is not learned ( e.g. , Neural Turing Machines or Memory Networks ) .

In summary , just like the early paper released in 2010 about Kinect - based RGBD SLAM : lots of excitement but potential disappointment when the method is applied on an actual mobile robot , navigating in normal environments with visual ambiguity and white walls . The paper should ultimately be accepted to this conference to provide a baseline for the community ( once the claims are revised ) , but I street that the claims of learning to navigate in unseen environments are unsubstantiated , as the method is neither end - to - end learned ( as it relies on human input and heuristic path planning ) nor capable of exploring unseen environments with visual ambiguity .

We thank the reviewer for the thoughtful review and useful comments . We will update the paper accordingly .

> feed - forward locomotion ( policy ) network , trained in a supervised manner .

We would like to highlight that the locomotion policy is trained in self - supervised fashion , without any human labels or demonstration .

> the problems arising when the images are ambiguous

This issue , known as perceptual aliasing , is a fundamental problem for all mapping and navigation algorithms . The basic version of our approach , presented in the paper , is indeed unable to disambiguate two identical observations . We implemented a simple modification of the approach : when localizing itself in the graph , the agent first searches in a neighborhood of the previous location , and only resorts for localizing itself in the whole graph if no confident match is found in this first step . This introduces temporal smoothness to localization of the agent , partially addresses the perceptual aliasing problem , and improves the performance of the method , especially in complex mazes . We will add a comparison of the naive and the advanced versions of the method to the paper . More principled treatment of perceptual aliasing is an important direction of future work .

> The problem is mitigated by the fact that a human operator controls the agent , making sure that the agent 's viewpoint is clear , but the method will probably fail if the agent is learning to explore the maze autonomously , bumping into walls and facing walls

It is true that the method can be sensitive to the exploration sequence , and there is room for improvement . However , we would like to point out that 1 ) in this paper we focus on the memory architecture , 2 ) any mapping method would be sensitive to the properties of the walkthrough sequence , 3 ) availability of a short human exploration sequence is a reasonable assumption in many practical settings , for instance for a household robot , and 4 ) the baselines get access to exactly the same walkthrough sequence .

To analyze the robustness of the method to the properties of the walkthrough sequence , we will perform experiments with non-human exploration and include the results in the paper .

> The screenshots on Figure 3 suggest that the walls have a large variety of textures and decorations , making each viewpoint potentially unique , unlike the environments in ( Mirowski et al , 2017 ) , ( Jaderberg et al , 2017 ) and ( Mnih et al , 2016 ) .

It is true that our environments are different from the aforementioned DM Lab ones in terms of the texture distribution . This video https://youtu.be/4QxO8mdOf3M shows a walkthrough sequence of the Test -Difficult maze ( other mazes are similar in texture distribution ) . Indeed the textures are diverse , but they are also repetitive , so the same seemingly discriminative texture appears in multiple locations in the maze . Moreover , the floor and the ceiling textures are uniform . This makes localization challenging . In the DM Lab environments , on the other hand , the floor textures vary , and walls are populated with unique “ markers ” .

The textures we used were procedurally and randomly placed on the walls . We agree that additional evaluation on DM Lab environments or qualitatively similar ones in ViZDoom would be interesting and will work on adding these results to the paper .




> Most importantly , the navigation is not based on RL at all , and ignores the problem of exploration altogether .

In this work we focus on memory , not exploration . In fact , exploration of a previously unseen environment is a very difficult problem of its own , which itself needs memory : in order to know which parts of the environment to explore , the agent needs to know where it is currently located and which parts had already been explored . In this work , we make a useful contribution by designing the memory module . Using it for exploration ( potentially in combination with RL ) is a very exciting avenue for future work .

The proposed method can be seen as an advanced version of visual imitation learning ( the agent does not have access to the expert ’s actions ! ) combined with mapping . Interestingly , a concurrent submission to ICLR proposes to use a method very similar to our locomotion network for visual imitation learning : https://openreview.net/forum?id=BkisuzWRW .

> A human operator labels 10 k frames by playing the game and controlling the agent , to show it how the maze looks like and what are the paths to be taken . As a consequence , comparison to end - to - end RL navigation methods is unclear , and should be stressed upon in the manuscript .

There seems to be some misunderstanding here . The operator does not “ label ” the images : the actions taken by the human are not available to the agent , only the observed image sequence . This image sequence is provided both to our agent and the baseline agents at test time . At training time , none of the agents have access to human demonstrations . Therefore , the comparison is fair .

> a fully Dijkstra -based baseline where the direction of motion of the agent along the edge is retrieved and used to guide the agent

If we understand the suggestion correctly , it requires recording the expert ’s actions during the walkthrough and then repeating them at test time . As noted above , in our setup the actions from the walkthrough sequence * are not available * to the agents . Therefore this baseline has access to more information than the methods evaluated in the paper . Nevertheless , we will work on evaluating this baseline and including it in the paper .

> the localization network replaced by image similarities in pixel space or some image descriptor space ( e.g. , SURF , ORB , etc … ) .

Thanks for this suggestion ! These are indeed very meaningful baselines . We will evaluate and add the results to the paper .

> Oh et al , 2016 , “ Control of Memory , Active Perception , and Action in Minecraft ” ) .

Thanks for pointing this out , the work of Oh et al. is definitely very relevant . We will add a discussion to the paper . However , note that experiments by Oh et al . are performed in gridworld - like environments , in contrast with large mazes with continuous state space in our work . We expect that scaling the method of Oh et al. to these environments will be challenging . We will compare to the work of Oh et al . and will include the results in the paper .

> Unfortunately , that episodic memory is not learned ( e.g. , Neural Turing Machines or Memory Networks ) .

Indeed the memory is not learned end - to - end via RL , but both the embedding and the locomotion network are learned in self - supervised fashion . We strongly believe that both approaches have value , since RL alone provides only a relatively weak training signal . Combining the two is an exciting avenue for future work .

> potential disappointment when the method is applied on an actual mobile robot , navigating in normal environments with visual ambiguity and white walls .

We agree that deployment on a real robot would be challenging , which almost certainly would also be the case for any other navigation algorithm developed in a simulated environment . However , we are happy to see that the independent concurrent approach mentioned earlier ( https://openreview.net/forum?id=BkisuzWRW), which is similar to our locomotion network , can be deployed on a real robot .

> but I street that the claims of learning to navigate in unseen environments are unsubstantiated , as the method is neither end - to - end learned ( as it relies on human input and heuristic path planning ) nor capable of exploring unseen environments with visual ambiguity .

We never aimed to state that the proposed method learns navigation and exploration end - to-end . In fact , the availability of a walkthrough video , as well as our focus on the memory module , are mentioned both in the abstract and in the introduction . We will carefully review the paper to make sure no inappropriate claims are made . If there are any specific parts the reviewer would like us to revise , it would be very helpful to point those out .

The paper introduces a graph based memory for navigation agents . The memory graph is constructed using nearest neighbor heuristics based on temporal adjacency and visual similarity . The agent uses Dijkstra 's algorithm to plan a a path through the graph in order to solve the navigation task .

There are several major problems with this paper . My overall impression is that the the proposed agent is a nearly hard - coded solution ( which I think might be the correct approach to such problems ) , but a poorly implemented one . Specific points : 1 - There are only 5 test mazes , and the proposed agent does n't even solve all of them . 2 - The way in which the maze is traversed in the exploration phase determines the accuracy of the graph that is constructed ( i.e. traversing each location exactly once using a space - filling curve ) . 3 - Of the two heuristics used in Equation 1 how many edges are actually constructed using the visual similarity heuristic ? 4 - How does the visual similarity heuristic handle visually similar map elements that correspond to distinct locations ? 5 - The success criteria of solving a maze is arbitrarily defined -- why exactly 2.4 min ?

> My overall impression is that the the proposed agent is a nearly hard - coded solution ( which I think might be the correct approach to such problems ) ,

The method uses more hand design than most end - to - end RL methods , but also includes crucial learned components . We believe that exploring the tradeoff between design and learning is important for solving complex problems such as mapping and navigation .

> but a poorly implemented one .

It would be helpful if the reviewer could be more specific here .

> There are only 5 test mazes ,

This number of environments is similar to previous works on navigation , such as ( Mirowski et al. 2017 ) . Nevertheless , we will evaluate the methods in more environments .

> the proposed agent does n't even solve all of them .

Typically a method does not have to “ solve ” all test cases it is applied to in order to be useful , but rather has to outperform relevant baselines . This is what we demonstrate . We are personally skeptical when a method works perfectly on all examples shown in a paper , as this typically indicates that either the problem was easy ( not the case in our setting ) or the examples are cherry - picked ( likewise ) .

> The way in which the maze is traversed in the exploration phase determines the accuracy of the graph that is constructed ( i.e. traversing each location exactly once using a space - filling curve ) .

Indeed the properties of the traversal affect the algorithm performance . This is , to our knowledge , the case for all mapping methods . We are not using space - filling curves , but rather simple human traversals .

> Of the two heuristics used in Equation 1 how many edges are actually constructed using the visual similarity heuristic ?

Approximately 4000 shortcuts were made based on visual similarity . After introducing these shortcut connections , the average length of the shortest path to the goal , computed over all nodes in the graph , drops from 2500 to 772 steps . This indicates that the created shortcuts contribute significantly to the success of navigation . We will evaluate the method without these shortcuts and include the results in the paper .

> How does the visual similarity heuristic handle visually similar map elements that correspond to distinct locations ?

This issue , known as perceptual aliasing , is a fundamental problem for all mapping and navigation algorithms . The basic version of our approach , presented in the paper , is indeed unable to disambiguate two identical observations . We implemented a simple modification of the approach : when localizing itself in the graph , the agent first searches in a neighborhood of the previous location , and only resorts for localizing itself in the whole graph if no confident match is found in this first step . This introduces temporal smoothness to localization of the agent , partially addresses the perceptual aliasing problem , and improves the performance of the method , especially in complex mazes . We will add a comparison of the naive and the advanced versions of the method to the paper . More principled treatment of perceptual aliasing is an important direction of future work .

> The success criteria of solving a maze is arbitrarily defined -- why exactly 2.4 min ?

There has to be some upper limit on the duration of a navigation trial , and we chose 5000 simulation steps in this work . Plots in Figure 5 show the success rate as a function of episode duration for durations less than this maximum threshold .


The paper presents for learning to visually navigate using a topological map is presented . The method combines an algorithmic approach to generate topological graphs , which is indexed by observations with Dijkstra 's algorithm to determine a global path from which a waypoint observation is selected . The waypoint along with the current observation is fed to a learned local planner that transitions the agent to the target waypoint . A learned observation similarity mapping localizes the agent and indexes targets in the graph .

The novelty of the approach in context of prior visual navigation and landmark - based robotics research is the hybrid algorithmic and learned approach that builds a graph purely from observations without ego motion or direct state estimation . Most of the individual components have previously appeared in the literature including graph search ( also roadmap methods ) , learned similarity metrics , and learned observation - based local planners . However , the combination of these approaches along with some of the presented nuanced enhancements including the connectivity shortcuts ( a simple form of loop closure ) to visual topological navigation and are compelling contributions . The results while not thorough do support the ability of the method effectively plan on novel and unseen environments .

The approach has potential limitations including the linear growth in the size of the memory , and it is unclear how the method handles degenerate observations ( e.g. , similar looking hallways on opposite sides of the environment ) . The authors should consider including examples or analysis illustrating the method ’s performance in such scenarios .

The impact of the proposed approach would be better supported if compared against stronger baselines including recent research that address issues with learning long sequences in RNNs . Furthermore , additional experiments over a greater number and more diverse set of environments along with additional results showing the performance of the method based on variation environment parameters including number of exploration steps and heuristic values would help the reader understand the sensitivity and stability of the method .

The work in “ Control of Memory , Active Perception , and Action in Minecraft ” by Oh et al. have a learned memory that is used recall previous observations for planning . This method ’s memory architecture can be considered to be nonparametric , and , while different from the proposed method , this similarity merits additional discussion and consideration for empirical comparison .

Some of the the details for the baseline methods are unclear . The reviewer assumes that when the authors state they use the same architecture as Mnih et al . and Mirowski et al. that this also includes all hyper parameters including the size of each layer and training method . However , Mirowski et al. use RGB while the stated baseline is grayscale .

The reviewer wonders whether the baselines may be disadvantaged compared to the proposed method . The input for the baselines are restricted to a 84x84 input image in addition to being grayscale vs the proposed methods 160x120 RGB image . It appears the proposed method is endowed with a much greater capacity with a ResNet - 18 in the retrieval network compared to the visual feature layers ( two layers of CNNs ) of the baseline networks . Finally the proposed method is provided with demonstrations ( the human exploration ) that effectively explore the environment . The authors should consider bootstrapping the baseline methods with similar experience .

We thank the reviewer for the thoughtful review and the useful comments . We will update the paper accordingly .

> the proposed method is provided with demonstrations ( the human exploration ) that effectively explore the environment . The authors should consider bootstrapping the baseline methods with similar experience .

We believe there might be a misunderstanding of our experimental setup here . When testing the baselines , we feed them the walkthrough sequence ( no expert actions , only the observations ! ) before each navigation trial . LSTM could remember the environment layout based on this sequence . Both the proposed method and the baselines have access to the walkthrough sequence at test time , and none of the methods have access to them at training time . Therefore , the comparison is fair .

> the linear growth in the size of the memory

This is indeed an undesirable property , and future work will have to address this . As a first step , we will include in the paper experiments with memory sub -sampled by a constant factor .

> handling degenerate observations ( e.g. , similar looking hallways on opposite sides of the environment )

This issue , known as perceptual aliasing , is a fundamental problem for all mapping and navigation algorithms . The basic version of our approach , presented in the paper , is indeed unable to disambiguate two identical observations . We implemented a simple modification of the approach : when localizing itself in the graph , the agent first searches in a neighborhood of the previous location , and only resorts for localizing itself in the whole graph if no confident match is found in this first step . This introduces temporal smoothness to localization of the agent , partially addresses the perceptual aliasing problem , and improves the performance of the method , especially in complex mazes . We will add a comparison of the naive and the advanced versions of the method to the paper . More principled treatment of perceptual aliasing is an important direction for future work .

> Discuss and evaluate “ Control of Memory , Active Perception , and Action in Minecraft ” by Oh et al .

Thanks for pointing this out , the work of Oh et al. is definitely very relevant . We will add a discussion to the paper . However , note that experiments by Oh et al . are performed in gridworld - like environments , in contrast with large mazes with continuous state space in our work . We expect that scaling the method of Oh et al. to these environments will be challenging . We will compare to the work of Oh et al . and will include the results in the paper .

> compare against stronger baselines including recent research that address issues with learning long sequences in RNNs

We would gladly compare to more elaborate memory architectures , if the reviewer could point at existing implementations that use these memory architectures for navigation in environments with continuous state spaces . Implementing and tuning such a navigation system from scratch would be a complex undertaking of its own .

> Furthermore , additional experiments over a greater number and more diverse set of environments

We will evaluate the methods in additional environments and will include the results in the paper .

> results showing the performance of the method based on variation environment parameters and heuristic values

We agree that such an analysis would be very useful . We will run corresponding experiments and include the results in the paper .

> Hyperparameters of baselines

For the baseline evaluation , we kept the hyperparameters as similar as possible to Mirowski et al .

> Grayscale for the baselines , RGB for the proposed method

Indeed , we fed grayscale images to the baselines instead of RGB . According to our previous experience , RL navigation methods typically do not benefit from RGB images . Still , we agree that for fair evaluation RGB images should be used , and we will re-evaluate the baselines with RGB images .

> 84x84 input image and small network for baselines , 160x120 image and ResNet for the proposed method

The proposed method does make use of a higher resolution image and a larger network . However , we are not aware of any existing works demonstrating training of ResNet-size networks with large input images from scratch using RL . Moreover , such experiments could get extremely slow . 84x84 images and small networks are the absolute standard in the deep RL literature , and therefore we assumed it is fair to use these standard settings for the baselines . We will add baseline experiments with 160x120 input images .


Update :

I have read the rebuttal and the revised manuscript . Paper reads better and comparison to Auto-regression was added . This work presents a novel way of utilizing GCN and I believe it would be interesting to the community . In this regard , I have updated my rating .

On the downside , I still remain uncertain about the practical impact of this work . Results in Table 1 show that proposed method is capable of forecasting next hour temperature with about 0.45 C mean absolute error . As no reference to any state of the art temperature forecasting method is given ( i.e. what is the MAE of a weather app on a modern smartphone ) , I can not judge whether 0.45 C is good or bad . Additionally , it would be interesting to see how well proposed method can deal with next day temperature forecasting .

---------------------------------------------
In this paper authors develop a notion of data quality as the function of local variation of the graph nodes . The concept of local variation only utilizes the signals of the neighboring vertices and GCN is used to take into account broader neighborhoods of the nodes . Data quality then used to weight the loss terms for training of the LSTM network to forecast temperatures at weather stations .

I liked the idea of using local variations of the graph signals as quality of the signal . It was new to me , but I am not very familiar with some of the related literature . I have one methodological and few experimental questions .

Methodology :
Why did you decide to use GCN to capture the higher order neighborhoods ? GCN does so intuitively , but it is not clear what exactly is happening due to non-linearities . What if you use graph polynomial filter instead [ 1 ] ( i.e. linear combination of powers of the adjacency ) ? It can more evidently capture the K-hop neighborhood of a vertex .

Experiments :
- Could you please formalize the forecasting problem more rigorously . It is not easy to follow what information is used for training and testing . I 'm not quite certain what " Temperature is used as a target measurement , i.e. , output of LSTM , and others including Temperature are used as input signals . " means . I would expect that forecasting of temperature tomorrow is solely performed based on today 's and past information about temperature and other measurements .
- What are the measurement units in Table 1 ?
- I would like to see comparison to some classical time series forecasting techniques , e.g. Gaussian Process regression and Auto-regressive models . Also some references and comparisons are needed to state - of - the- art weather forecasting techniques . These comparisons are crucial to see if the method is indeed practical .

Please consider proofreading the draft . There are occasional typos and excessively long wordings .

[ 1 ] Aliaksei Sandryhaila and José MF Moura . Discrete signal processing on graphs . IEEE transactions
on signal processing , 61 ( 7 ) :1644–1656 , 2013 .

Thank you for your comments and suggestions to improve the paper . Below are our responses to the main points of your comments :

Methodology
>> It is commonly believed that the earth climate system is a complex one involving many nonlinear interactions between climate factors . Neural networks have proven to be an effective solution to capture nonlinear dependencies in many applications . To capture nonlinearity , we use a nonlinear activation function ( ReLU ) . Furthermore , multiple layers are more effective to learn features at various levels of abstraction .
Graph polynomial filter ( Equation ( 7 ) in [ 1 ] ) has the similar form as GCN filter operations , but it does not consider nonlinearity . GCN is also based on the polynomial of the adjacent matrix , it equivalently handles the K-hop neighborhood of a vertex .

Experiments
>> Could you please formalize the forecasting problem more rigorously .
Thanks for pointing out the confusion . For the sentence you quoted , we did intend to describe as your suggested - In other words , future temperatures are forecasted by looking at past temperatures as well as other meteorological observations . We have updated the expression more clearly .

>> What are the measurement units in Table 1 ?
We have added the details of the climate dataset in the appendix in the updated draft .

>> I would like to see comparison to some classical time series forecasting techniques , e.g. Gaussian Process regression and Auto-regressive models .
Thanks for the baseline suggestions . We have added the results by auto-regressive for robust comparison in the updated draft .

Thanks for your comments , and we proofread the draft and make it more clear .

The paper is an application of neural nets to data quality assessment . The authors introduce a new definition of data quality that relies on the notion of local variation defined in ( Zhou and Schölkopf , 2004 ) , and they extend it to multiple heterogenous data sources . The data quality function is learned using a GCN as defined in ( Kipf and Welling , 2016 ) .

1 ) How many neighbors are used in the experiments ? Is this fixed or is defined purely by the Gaussian kernel weights as mentioned in 4.2 ? Setting all weights less than 0.9 to zero seems quite abrupt . Could you provide a reason for this ? How many neighbors fit in this range ?
2 ) How many data points ? What is the temporal resolution of your data ( every day / hour / minute /etc . ) ? What is the value of N , T?
3 ) The bandwidth of the Gaussian kernel ( \gamma ) is quite different ( 0.2 and 0.6 ) for the two datasets from Weather Underground ( WU ) and Weather Bug ( WB ) ( sect . 4.2 ) . The kernel is computed on the features ( e.g. , latitude , longitude , vegetation fraction , etc . ) . Location , longitude , distance from coast , etc. are the same no matter the data source ( WU or WB ) . Maybe the way they compute other features ( e.g. , vegetation fraction ) vary slightly , but I would guess the \gamma should be ( roughly ) the same .
4 ) Are the features ( e.g. , latitude , longitude , vegetation fraction , etc. ) normalized in the kernel ? If they are given equal weight ( which is in itself questionable ) they should be normalized , otherwise some of them will always dominate the distances . If they were indeed normalized , that should be made clear in the paper .
5 ) Why do you choose the summer months ? How does the framework perform on other months and other meteorological signals except temperature ? The application is very nice and complex , but I find that the experiments are a little bit too limited .
6 ) The notations w and W are used for different things , and that is slightly confusing . Usually one is used as a matrix notation of the other .
7 ) I would tend to associate data quality with how noisy observations are at a certain node , and not heterogeneity . It would be good to add some discussion on noise in the paper . How do you define an anomaly in 5.2 ? Is it a spatial or temporal anomaly ? Not sure I understand the difference between an anomaly and a bridge node .
8 ) “ A bridge node highly likely has a lower data quality level due to the heterogeneity ” . I would think a bridge node is very relevant , and I do n't necessarily see it as having a lower data quality . This approach seems to give more weight to very similar data , while discarding the transitions , which in a meteorological setting , could be the most relevant ?!
9 ) Update the references , some papers have updated information ( e.g. , Bruna et al . - ICLR 2014 , Kipf and Welling – ICLR 2017 , etc . ) .

Quality – The experiments need more work and editing as mentioned in the comments .
Clarity – The framework is fairly clearly presented , however the English needs significant improvement .
Originality – The paper is a nice application of machine learning and neural nets to data quality assessment , and the forecasting application is relevant and challenging . However the paper mostly provides a framework that relies on existing work .
Significance – While the paper could be relevant to data quality applications by introducing advanced machine learning techniques , it has limited reach outside the field . Maybe publish it in a data quality conference / journal .

We sincerely thank the reviewer for the insightful comments and suggestions . We would like to stress that assessing and mitigating heterogeneity of data quality is an extremely important but less - studied topic in machine learning . Our work aims at positioning the task and providing one possible solution to address this important problem . The novelty of the paper lies in this important contribution rather than purely the novelty of the proposed model .

Below is our response to the questions .
1 ) and 3 ) >>
Thanks for pointing out this potentially confusing issues . To build a symmetric adjacency matrix , we did not set a fixed number of neighbors . Instead , we defined the neighbors by the Gaussian kernel weights with the weight threshold . Since the numbers of the weather stations in WeatherBug ( WB ) and Weather Underground ( WU ) are different ( # WU:42 , # WB:159 ) , the average numbers of neighbors in the datasets are too different under the same bandwidth ( \ gamma ) . We aim at evaluating our model under similar degree distribution ( similar topology ) and therefore , we adjust the bandwidth to make the average numbers of adjacent neighbors in two dataset be similar under the 0.9 threshold . The average degrees are 6.0 and 7.5 for WU and WB , respectively .

2 ) >>
- Each weather station in the data sources ( WU and WB ) has a different temporal resolution . Some weather stations have observed measurements in every 5 minutes but other weather stations have operated sensors in every 30 minutes . So , we fix the temporal granularity as 1 hour and aggregate observations in each hour by averaging them .
- N is the total number of weather stations and # WU:42 , # WB:159 .
- T is the total length of signals and it is 744 ( hours ) ( 24 ( hours / day ) * 31 ( day ) ) for July and August .

4 ) >>
This is a very good point . Yes , we are aware that features represented in a large number ( e.g. , latitude ~ 34 degree ) can dominate features represented in a small number ( e.g. , vegetation fraction < 1.0 ) , which is exactly what we want to avoid . Thus , it is a must to normalize the features before using them for computing the distances . As you point out , equal weighting may be not perfectly correct , however , this is what we can do without any domain knowledge .

5 ) >>
- It is a very good point too . Interestingly , the Los Angeles area contains many microclimates , which means that the daytime temperatures can vary as much as 36 ° F ( 19°C ) between inland areas such as the San Fernando Valley and the coastal Los Angeles Basin . The temperature differences between different areas are more obvious during the summer season ( than other seasons ) . For example , the average high temperature ( °F ) can vary as much as 26 ° F for July and August on 9 different regions ( Downtown LA , LAX , Santa Monica , Culver City , Long Beach , San Fernando Valley , Burbank , Santa Ana , and Anaheim ) . In contrast , it varies only 6° F for December and January in the same regions . Thus , it is more challenging to predict temperatures in the summer season , which is why we choose two months ( 7 and 8 ) that often shows extreme fluctuations of temperatures .
- The bigger picture of our work is investigating urban heat island effect in Los Angeles areas . Temperature is the most important factor in urban heat island and exhibits more variations ( and more difficult to predict ) . In contrast , other observations , such as Relative Humidity or Precipitation , are very stable in Los Angeles areas . They are very easy to predict and therefore cannot justify for a complex model .



6 ) >>
We use the lowercase w for data quality levels for each station and the upper case W for edge weights . The bold w is the vector having N elements and w_i is the data quality level of an i-th weather station . We have fixed the confusions in the updated draft .

7 ) >>
- In our paper , we assume that if two connected nodes ( weather stations ) are located in similar spatial features , these nodes highly likely observe similar meteorological measurements . Thus , if two connected nodes provide very different observations , we think that these observations are not reliable and could be noisy . Furthermore , if there is a group of connected nodes and some nodes observe significantly different measurements with other nodes in the group , we can say that the measurements are too heterogeneous and not reliable . In other words , the data quality level is associated with noisy as well as heterogeneous observations .
- For the anomaly detection , what we would like to propose is that the embeddings from our model can be used to visualize nodes based on their connectivity and observations . For example , in Figure 3 , the red dot ( v_22 ) is connected with the green dots ( v_19 , v_20 , v_21 , v_23 , v_25 , v_29 ) . Since these nodes have similar spatial features and are connected , it is expected to have similar observations . At t=0 , the distribution of the nodes seems like a cluster . However , v_25 is far away from other green nodes and the red node at t=4 . There are two possible reasons . First , observations of v_25 at t=4 may be too different with those of other green nodes and the red node . Second , observations of a node ( v_4 ) that is only connected to v_25 ( not other green nodes and the red node ) might be too noisy . In the first case , since it violates our assumption ( v_25 's observations should be similar with those of another green node . ) , the observations of v_25 at t=4 might be noisy or not reliable . In the second case , the observations of v_4 at t=4 might be noisy . Thus , we can detect potentially anomalous nodes by looking at the distribution of nodes . ( A bridge node is not necessary to be an anomaly . )
- The anomaly comes from temporal observations by comparing to neighbor observations . Thus , two aspects ( spatial and temporal ) are jointly considered .

8 ) >>
Thanks for your pointing out . We agree that the sentence you quote can cause some confusion . A bridge node in our paper is considered as a node connecting two ( or more ) clusters that consist of nodes having similar features . As a result , a bridge node is affected by two ( or more ) different groups of nodes simultaneously , and thus , the quality level at the bridge node is more susceptible than those of other nodes . However , it does n’t directly mean that it is necessary for a bridge node to have a lower data quality level .

9 ) >>
Thanks for your suggestion and we update the conference information in the revised version .

Quality , Clarity , Originality , Significance >>
We do not agree with the comment on significance . Our paper proposes a novel solution to automatically infer data quality levels based on local variations of graph signals and demonstrate that the quality levels can be used to reduce forecasting error and detect potentially anomalous observations . It provides a new idea on inferring interpretable quantity without explicit labels . We agree that there is limited work in ICLR on data quality , but it is definitely one essential hurdle for any representation learning model to work in practice .

Summary of the reviews :
Pros :
• A novel way to evaluate the quality of heterogeneous data sources .
• An interesting way to put the data quality measurement as a regularization term in the objective function .
Cons :
• Since the data quality is a function of local variation , it is unclear about the advantage of the proposed data quality regularization versus using a simple moving average regularization or local smoothness regularization .
• It is unclear about the advantage of using the Multi-layer graph convolutional networks versus two naïve settings for graph construction + simple LSTM , see detailed comments D1

Detailed comments :
D1 : Compared to the proposed approaches , there are two alternative naïve ways : 1 ) Instead of construct the similarity graph with Gaussian kernel and associated each vertex with different types of time - series , we can also construct one unified similarity graph that is a weighted combination of different types of data sources and then apply traditional LSTM ; 2 ) During the GCN phases , one can apply type -aware random walk as an extension to the deep walk that can only handle a single source of data . It is unclear about the advantage of using the Multi-layer graph convolutional networks versus these two naïve settings . Either some discussions or empirical comparisons would be a plus .


Thank you for your comments and suggestions to improve the paper . Below are our responses to the main points of your comments :

Cons 1.
>> It is an interesting point . A simple moving average or local smoothness regularization may improve the forecasting performance . However , these regularizations cannot infer the data quality level which is useful to understand time - varying graph signals .

Cons 2 and Detailed comments
>> Thanks for the suggestions of more baselines . For the former one , we do not have a specific way to get the unified similarity graph based on the different types of time - series . That is why we only consider the spatial ( static ) features to construct the graph structure . But it could be an interesting direction to explore .
It is a great idea to think different ways to cover K - hop neighbor nodes . Random deep walk is one of the ways . Although we have n’t compared our method to these random - walk - based methods , the multi-layer GCNs with the data quality networks are more flexible to learn latent interactions between temporal observations due to their nonlinearity and compositional property . It may be possible to improve forecasting quality by using the random walk method , however , we focus on the automatically inferring method of the data quality level , which is not easily achievable by other ways .

The paper takes a closer look at the analysis of SGD as variational inference , first proposed by Duvenaud et al . 2016
and Mandt et al. 2016 . In particular , the authors point out that in general , SGD behaves quite differently from Langevin diffusion due to the multivariate nature of the Gaussian noise . As the authors show based on the Fokker - Planck equation of the underlying stochastic process , there exists a conservative current ( a gradient of an underlying potential ) and a non-conservative current ( which might induce stationary persistent currents at long times ) . The non-conservative part leads to the fact that the dynamics of SGD may show oscillations , and these oscillations may even prevent the algorithm from converging to the ' right ' local optima . The theoretical analysis is carried - out very nicely , and the theory is supported by experiments on two-dimensional toy examples , and Fourier-spectra of the iterates of SGD .

This is a nice paper which I would like to see accepted . In particular I appreciate that the authors stress the importance
of ' non -equilibrium physics ' for understanding the SGD process . Also , the presentation is quite clear and the paper well written .

There are a few minor points which I would like to ask the authors to address :

1 . Why cite Kingma and Welling as a source for variational inference in section 3.1 ? VI is a much older field , and Kingma and Welling proposed a very special form of VI , namely amortized VI with inference networks . A better citation would be Jordan et al 1999 .

2 . I 'm not sure how much to trust the Fourier-spectra . In particular , perhaps the deviations from Brownian motion could also be due to the discrete nature of SGD ( i.e. that the continuous - time formalism is only an approximation of a discrete process ) . Could you elaborate on this ?

3 . Could you give the reader more details on how the uncertainty estimates on the Fourier transformations were obtained ?

Thanks .

Thank you for your comments . Please read below for our clarifications .

>> Why cite Kingma and Welling for variational inference , e.g. , cite Jordan ‘ 99 , VI is a much older field

Good point : we will also include older citations .

>> Not sure how much to trust Fourier spectra , deviations from Brownian motion can also be due to discretization

Note that the plot in Fig. 3a is the discrete Fourier transform of ( x_{k + 1 } - x_k ) _k . The trajectory is of length 10^5 epochs and sampled at each epoch ; we are thus sampling at a very high frequency , well above the Nyquist rate . Low frequency modes in the continuous - time dynamics will not be affected by such a discretization , high frequency modes might , see the right part of Fig. 3a .

The FFT , which is expected to be flat for Brownian motion , is distinctly non - flat in our experiments . This result is also predicted by other experiments in Sec. 4.1 and Fig. 3 b , and our theoretical results . So the Fourier spectra are just one more confirmation of the claim .

>> Give more details on how the uncertainty estimates on the Fourier transformations were obtained

This is described in the caption of Fig . 3 . The FFT is computed , independently , for the one-dimensional trajectory of each weight . The standard deviation across all the weights is depicted as the “ error band ” . The eigenmodes of the weight vector are also the eigenmodes of the trajectory of each weight ; it is indeed surprising that different weights have very similar amplitude .

The authors discuss the regularized objective function minimized by standard SGD in the context of neural nets , and provide a variational inference perspective using the Fokker - Planck equation . They note that the objective can be very different from the desired loss function if the SGD noise matrix is low rank , as evidenced in their experiments .

Overall the paper is written quite well , and the authors do a good job of explaining their thesis . However I was unable to identify any real novelty in the theory : the Fokker - Planck equation has been widely used in analysis of stochastic noise in MCMC samplers in recent years , and this paper mostly rephrases those results . Also the fact that SGD theory only works for isotropic noise is well known , and that there is divergence from the true loss function in case of low rank noise is obvious . Thus I found most of section 3 to be a reformulation of known results , including Theorem 5 and its proof .

Same goes for section 5 ; the symmetric - anti symmetric split is a common technique used in the stochastic MCMC literature over the last few years , and I did not find any new insight into those manipulations of the Fokker - Planck equation from this paper .

Thus I think that although this paper is written well , the theory is mostly recycled and the empirical results in Section 4 are known ; thus it is below acceptance threshold due to lack of novelty .

Thank you for your comments . Please see our clarifications below .

>> Unable to identify any novelty in the theory , reformulation of known results , empirical results are known

We are glad to help :
1 . While it is widely * believed * that SGD acts as an “ implicit regularizer ” , to the best of our knowledge we are first to * prove * that it performs variational inference : SGD minimizes an average potential along with an entropic regularization term .
2 . While someone may have noticed that mini-batch noise in deep networks is highly non-isotropic , nobody had connected this to convergence properties of SGD for deep nets .
3 . The fact that anisotropy in deep networks causes the potential Phi to be different than the function upon which SGD evaluates its gradients was * not known * , nor proven , before .
4 . The fact that the most likely trajectories of SGD for deep nets are limit cycles was * not known * , nor proven .
We have scouted the literature diligently , but of course it is possible that we may have missed work where any of the above empirical and theoretical results may have been described . We will gladly examine specific references if provided .

>> Fokker - Planck equation has been widely used before

We surely do not claim to be the first to use the Fokker - Planck equation ; it is a standard tool in the analysis of stochastic processes .

>> Fact that SGD theory only works for isotropic noise is well - known , that there is divergence from the true loss is obvious

The issue is not that there is “ divergence from the true loss ” , but precisely of what * nature * it is . To the best of our knowledge , we are the first to point out -- and prove -- that SGD for deep nets has limit cycles as its most likely trajectories . This is surely not obvious : in fact , most of the literature focuses on which * critical points * SGD converges to . We show that , with anisotropic noise , it converges to none . Quite non-obvious , frankly .

>> Common technique in stochastic MCMC , did not find any new insight into manipulations

MCMC theory constructs grad f and D given a log-likelihood Phi that one would like to draw samples from . This paper is about the reverse direction : given a grad f and a D , what is the Phi ? This is a novel question and pertinent to understanding the efficacy of SGD for deep networks ; it is not under the purview of the MCMC literature . We * decompose * grad f into symmetric and anti-symmetric terms and develop assumptions and theory that enables us to do so .

To emphasize , MCMC methods start with a given Phi , whereas we find the Phi . The two are completely opposite directions , even if some formulae might look familiar from the MCMC literature .

This paper develop theory to study the impact of stochastic gradient noise for SGD , especially for deep neural network models . It is shown that when the gradient noise is isotropic normal , SGD converges to a distribution tilted by the original objective function . However , when the gradient noise is non isotropic normal , which is shown common in many models especially in deep neural network models , the behavior of SGD is intriguing , which will not converge to the tilted distribution by the original objective function , sometimes more interestingly , will converge to limit cycles around some critical points of the original objective function . The paper also provides some hints on why using SGD can get good generalization ability than gradient descend .

I think the finding of this paper is interesting , and the technical details are correct . I still have the following comments .

First , Assumption 4 seems a bit too abstract . It is not easy to see what the assumption means . It would be better if an example is given , which is verified to satisfy the assumption .

Another comment is related to the overall content of this paper . Thought the paper point out that SGD will have the out -of-equilibrium behavior when the gradient noise is non isotropic normal , it remains to show how far away this stationary distribution is from the original distribution defined by the objective function .

Thank you for your comments . Please see our clarifications below .

>> Assumption 4 seems a bit too abstract , can you give an example

Example 13 illustrates the effects of the assumption ; we will point the readers to it in Sec . 3 . Another example is in three dimensions , where the assumption is akin to Helmholtz decomposition of a vector field into divergence - free and curl - free components . We allow the force j( x ) to be non-trivial , j( x ) neq 0 corresponds to broken detailed balance while j( x ) = 0 corresponds to detailed balance . This assumption is motivated by the second - law of thermodynamics as discussed in Appendix B.

>> How far away is the stationary distribution from the original one

The relation between the two is the offset described in Thm. 17 . This difference scales linearly with learning rate / batch-size ; which can be large in practice because deep networks are trained with small batch -sizes and / or large learning rates . The divergence of the matrix Q is also explicitly computable , see ( A13 ) and Remarks 19 - 20 . Doing so is however computationally challenging for large networks , and a subject of our future investigation .

This paper aims to learn hierarchical policies by using a recursive policy structure regulated by a stochastic temporal grammar . The experiments show that the method is better than a flat policy for learning a simple set of block - related skills in minecraft ( find , get , put , stack ) and generalizes better to a modification of the environment ( size of room ) . The sequence of subtasks generated by the policy are interpretable .

Strengths :
- The grammar and policies are trained using a sparse reward upon task completion .
- The method is well ablated ; Figures 4 and 5 answered most questions I had while reading .
- Theoretically , the method makes few assumptions about the environment and the relationships between tasks .
- The interpretability of the final behaviors is a good result .

Weaknesses :
- The implementation gives the agent a - 0.5 reward if it generates a currently unexecutable goal g’ . Providing this reward requires knowing the full state of the world . If this hack is required , then this method would not be useful in a real world setting , defeating the purpose of the sparse reward mentioned above . I would really like to see how the method performs without this hack .
- There are no comparisons to other multitask or hierarchical methods . Progressive Networks or Zero - Shot Task Generalization with Multi-Task Deep Reinforcement Learning seem like natural comparisons .
- A video to show what the environments and tasks look like during execution would be helpful .
- The performances of the different ablations are rather close . Please a standard deviation over multiple training runs . Also , why does figure 4.b not include a flat policy ?
- The stages are ordered in a semantically meaningful order ( find is the first stage ) , but the authors claim that the order is arbitrary . If this claim is going to be included in the paper , it needs to be proven ( results shown for random orderings ) because right now I do not believe it .

Quality :
The method does provide hierarchical and interpretable policies for executing instructions , this is a meaningful direction to work on .

Clarity :
Although the method is complicated , the paper was understandable .

Originality and significance :
Although the method is interesting , I am worried that the environment has been too tailored for the method , and that it would fail in realistic scenarios . The results would be more significant if the tasks had an additional degree of complexity , e.g . “ put blue block next to the green block ” “ get the blue block in room 2 ” . Then the sequences of subtasks would be a bit less linear ( e.g. , first need to find blue , then get , then find green , then put ) . At the moment the tasks are barely more than the actions provided in the environment .

Another impedance to the paper ’s significance is the number of hacks to make the method work ( ordering of stages , alternating policy optimization , first training each stage on only tasks of previous stage ) . Because the method is only evaluated on one simple environment , it unclear which hacks are for the method generally , and which hacks are for the method to work on the environment .

I am happy with the updates the authors made to the paper . The video and additional experiments are valuable , and I will increase my score accordingly .

However , the paper would strongly benefit from either a second test environment or a more complex grammar to showcase generality .

( nitpick ) : “ We do not include the standard deviations since there are no noticeable difference among them . ” including standard deviations is not just to compare the magnitude of the standard deviations , but to see whether the differences in means of the methods are within the standard deviations .

Thank you for your reviews and insights .

1 . “ Providing this reward requires knowing the full state of the world ”

Learning curves of training without the penalty have been added to Figure 5 . We find that the penalty does not have a significantly effect on the learning efficiency as shown in Figure 5 .

Reviewer may be confused by our phrasing . Sorry about that . Actually we want to stress that we do not provide the full state when using this penalty . Instead , a penalty only includes information about the agent ’s physical capacity and the nature of the target task , since it is given i ) when the agent attempts to execute tasks that exceeds its physical capacity , such as trying to put down a block when it is not carrying one or trying to pick - up another block where there is already one in its hands , ii ) or when it attempts to execute tasks that are irrelevant to that tasks .

Also , we only use this during training under the assumption that a given task in the training is always executable ( the necessary blocks are present in the environment ) . So when a penalty was given to a task of finding an object that does not exist in the environment , it was meant to save time in game playing and also gives training signical of what old tasks are relevant for the agent .

Finally , in testing , we do not use this penalty , and it does not affect the performance .

Therefore , we do not think that this will prevent us to apply the approach to the real world for the aforementioned reasons .

2 . “ There are no comparisons to other multitask or hierarchical methods ”
We have also evaluated H-DRLN ( Tessler et al. , 2017 ) .

3 . “ A video … would be helpful ”
Please refer to this link for the video ( with audio ) : https://www.dropbox.com/s/j5nw2cljpoofo9j/hrl_demo.mov?dl=0

4 . “ The performances of the different ablations are rather close . Please a standard deviation over multiple training runs . ”
We do not include the standard deviations since there are no noticeable difference among them . The alternating and 2 value functions mainly help accelerating the learning phase 1 . As shown in Figure 4a , the acceleration is significant ( full model is the first one that switches to phase 2 ) . In phase 2 ( see Figure 4 b ) , the advantage of using the STG is very clear ( the blue curve reaches a plateau around an average reward of 0.8 ) and the others do not show a large improvement over the full model . So none of them reduces the training variance , but they all help increase the training efficiency .

5 . “ why does figure 4.b not include a flat policy ” ”
We have added the flat policy .

6 . “ the authors claim that the order is arbitrary ”
Sorry for the confusion . We will clarify this . What we meant was for arbitrary order , we can still train the hierarchical policy , but a semantically meaningful order is indeed very important for the training efficiency . And we do provide this order as weak supervision .

7 . “ unclear which hacks are for the method generally , and which hacks are for the method to work on the environment . ”
They are not environment dependent .
i ) The order of the stages comes from semantic meanings of the tasks , so it depends on the tasks but does not depend on the environment . E.g. , you may train a real robot on the same tasks in the same order in a real environment . In fact , the generated interpretable hierarchical plans can be directly used for the same tasks in different environments without additional training as long as there are equivalent primitive actions .
ii ) Alternating is purely for the optimization of the neural nets . As the experimental results show , we can also train the hierarchical policies without it .
iii ) The first phase in the 2 - phase curriculum is for the global policy to learn what the goals of the previous tasks are , so that in the second phase , it knows when to repeat the same task and when to stop . This is also not tailored to any specific environment .

Summary :
This paper proposes an approach to learning hierarchical policies in a lifelong learning context . This is achieved by stacking policies - an explicit " switch " policy is then used to decide whether to execute a primitive action or call the policy of the layer below it . Additionally , each task is encoded in a human-readable template , which provides interpretability .
Review :
Overall , I found the paper to be generally well - written and the core idea to be interesting . My main concern is about the performance against existing methods ( no empirical results are provided ) , and while it does provide interpretability , I am not sure that other approaches ( e.g. Tessler et al. 2017 ) could not be slightly modified to do the same . I think the paper could also benefit from at least one more experiment in a different , harder domain .

I have a few questions and comments about the paper :

The first paragraph claims " This precludes transfer of previously learned simple skills to a new policy defined over a space with differing states or actions " . I do not see how this approach avoids suffering from the same problem ? Additionally , approaches such as agent-space options [ Konidaris and Barto . Building Portable Options : Skill Transfer in Reinforcement Learning , IJCAI 2007 ] get around at least the state part .

I do not quite follow what is meant by " a global policy is assumed to be executable by only using local policies over specific options " . It sounds like this is saying that the inter-option policy can pick only options , and not primitive actions , which is obviously untrue . Can you clarify this sentence ?

In section 3.1 , it may be best to mention that the policy accepts both a state and task and outputs an action . This is stated shortly afterwards , but it was confusing because section 3.1 says that there is a single policy for a set of tasks , and so obviously a normal state- action policy would not work here .

At the bottom of page 6 , are there any drawbacks to the instruction policy being defined as two independent distributions ? What if not all skills are applicable to all items ?

In section 5 , what does the " without grammar " agent entail ? How is the sampling from the switch and instruction policies done in this case ?

While the results in Figures 4 and 5 show improvement over a flat policy , as well as the value of using the grammar , I am *very * surprised there is no comparison to existing methods . For example , Tessler 's H-DRLN seems like one obvious comparison here , since it learns when to execute a primitive action and when to reuse a skill .

There were also some typos / small issues ( I may have missed some ) :

pg 3 : " In addition , previous work usually useS ... "
pg 3 . " we encode a human instruction to LEARN A ... " ( ?)
pg 4 . " ... with A stochastic temporal grammar ... "
pg 4 . " ... described above through A / THE modified ... "
pg 6 . " ... TOTALLING six colors ... "
There are some issues with the references ( capital letters missing e.g. Minecraft )

It also would be preferable if the figures could appear after they are referenced in the text , since it is quite confusing otherwise . For example , Figure 2 contains V ( s , g ) , but that is only defined much later on . Also , I struggled to make out the yellow box in Figure 2 , and the positioning of Figure 3 on the side is not ideal either .

Thank you for your comments and suggestions .

1 . “ performance against existing methods ”
We have added the comparison with Tessler et al. ( 2017 ) , i.e. , H-DRLN , as suggested ( see Table 1 and Figure 4 ) . H-DRLN indeed also has the concept of reusing previously learned skills but the advantages of our approach are very obvious :
1 ) Each H-DRLN in Tessler et al. ( 2017 ) can only learn one task like “ Stack blue , ” whereas ours can learn a set of tasks , like { “ Stack x ”} , where x can be different colors . In fact , each curve of H-DRLN we show in Figure 4 is only for training one particular task ( i.e. , “ Get white ” and “ Stack white ” respectively ) , whereas all the other curves are for training the whole set of tasks .
2 ) H-DRLN treats all the old tasks { DSN_i} and the actions on the same level and learns Q ( s , a ) and Q(s , DSN_i ) . This setting is clearly not scalable . In the original paper , the learning was done where there were only 4 old tasks . However , in our settings , we have as many as 18 old tasks . And clearly from the results , H-DRLN can not learn good policies for new tasks as the input space of the Q(s , DSN_i ) function becomes unbearably large ( it is similar to learning a policy based on a very large action space ) .
3 ) Our policy is also learning the semantics of the tasks from the instructions , thus it has better generalization . As we show in the experiments on the { “ Put x on top of y”} tasks , ours generalizes well in noval ( x , y ) combinations unseen in training whereas H-DRLN does not have this capability .

2 . “ one more experiment in a different , harder domain ”
We have tested more difficult tasks { “ Put x on top of y”} , and have also evaluated our hierarchical policy in a zero - shot setting for unseen ( x , y ) combinations . See Table 1 , Figure 4 and also Section 5.3 and Section 5.4 . As shown in the results , flat policy and H-DRLN ( Tessler et al. , 2017 ) fail to learn good policies for { ” Stack x ”} tasks and the new { “ Put x on top of y”} tasks . This manifests that the tasks are actually quite challenging for current RL methods .

3 . Questions about the paper :

1 ) “ I do not see how this approach avoids suffering from the same problem ”
Policies on different levels are learned on different corpus ( e.g. , pi_0 does not know the word “ get ” ) and on different action spaces ( e.g. , pi_0 can be trained on actions excluding “ pick up ” and “ put down ” ) . They also do not need to share a same state encoding module , so the forms of states can be different ( e.g. , we may use symbolic states for a global policy while its local policy uses raw pixels as states ) .

2 ) “ It sounds like this is saying that the inter-option policy can pick only options , and not primitive actions ”
We are referring to some of the existing methods like Kulkarni et al . ( 2016 ) , Andreas et al. ( 2017 ) where the set of necessary options for a global policy is predefined and a task is executed only based on these given options . Other methods like Tessler et al. ( 2017 ) do not have this limitation . We will clarify this .

3 ) “ are there any drawbacks to the instruction policy being defined as two independent distributions ”
It is for simplicity . For more complex instructions , we may use GRUs or LSTMs to train the instruction generator , but the training will be slower .

4 ) “ How is the sampling from the switch and instruction policies done in this case ? ”
As we stated in the text , the sampling was done w.r.t. equation ( 1 ) and ( 2 ) when not using the STG .

We agree with the other editing comments and have fixed them in the updated version .

This paper introduces an iterative method to build hierarchical policies . At every iteration , a new meta policy feeds in task id to the previous policy and mixes the results with an ' augmented ' policy . The resulting policy is somewhat interpretable as the task id being sampled by the meta policy corresponds to one of the subgoals that are manually designed .

One of the limitation of the method is that appropriate subgoals and curriculum must be hand designed . Another one is that the model complexity grows linearly with the number of meta iterations .

The comparison to non-hierarchical models is not totally fair in my opinion . According to the experiment , the flat policy performs much worse than the hierarchical , but it is unclear how much of this is due to the extra capacity of the model of the unfolded hierarchical policy and how much of that is due to the hierarchy . In other words , it is unclear if hierarchy is actually useful , or just the task curriculum and model capacity staging .

The paper does not appear to be fully self contained in term of notations , in particular regarding the importance sampling I could not find the definitions of mu , and regarding the STG I could not find the definition of q and rho .

The experimental results are a bit confusing . In the learning curves that are shown , it is not clear exactly when the set of task is expanded , nor when the hierarchical policy iteration occurs . Also , some curves are lacking the flat baseline .

Thank you for your reviews .

1 . “ a new meta policy feeds in task id to the previous policy ”
Actually , the global policy feeds an instruction in human language rather than a task ID to the previous policy . Compared to using task IDs , this i ) improves the interpretability of the policy and ii ) facilitates the generalization of the policy in noval scenarios / tasks thanks to the semantics of the instructions .

2 . “ appropriate subgoals and curriculum must be hand designed ”
Actually we let the global policy at each level to explore the appropriate subgoals for the new tasks among all previously learned tasks . The learning efficiency of our approach does depend on the given curriculum as existing curriculum - based RL training approaches do . We regard this as a weak supervision from human knowledge . In fact , compared to some recent work ( Kulkarni et al. , 2016 ; Andreas et al. , 2017 ) which specifically provide the necessary sub-goals and / or the order of the subgoals for a task , we do not think our setting is any less general .

3 . “ The comparison to non-hierarchical models is not totally fair ”
First , the training of flat policy is also strictly following the curriculum we use for our model . It is always finetuned based on the policy trained for the previous task set . Second , the biggest benefit of our hierarchical policy comes from the more efficient exploration thanks to reusing old skills and learning the STG . As the updated Figure 4 b shows , when training for more complex tasks , the flat policy can not yield any positive reward as achieving the goals requires a fairly long sequence of primitive actions and precise operations ( e.g. , picking up and putting down the correct blocks at the correct locations ) .

4 . “ I could not find the definitions of mu , and regarding the STG I could not find the definition of q and rho ”
Mu was introduced in the second paragraph of Section 4.1 . q and rho were defined in the second paragraph of Section 3.3.

5 . “ The experimental results are a bit confusing ”
Sorry for the confusion . As we explained in Section 4 , we adopt a 2 - phase curriculum learning where the task set was expanded in the second phase . For curriculum - based training , the dip of a curve comes from the switch from phase 1 to phase 2 , thus also indicates when the task set was expanded . For non - curriculum based training where there is no dip in the reward , the expansion starts from the first episode . Note that in Figure 4 b , we only show the phase 2 of our curriculum , so the expansion starts from the first episode for all curves in this figure .

We have also added the flat baseline for the Figure 4b . The reason we did n’t include that in the previous version was that the flat policy failed to learn anything meaningful for the complex tasks .

The paper proposes to learn a custom translation or rotation invariant kernel in the Fourier representation to maximize the margin of SVM . Instead of using Monte Carlo approximation as in the traditional random features literature , the main point of the paper is to learn these Fourier features in a min-max sense . This perspective leads to some interesting theoretical results and some new interpretation . Synthetic and some simple real - world experiments demonstrate the effectiveness of the algorithm compared to random features given the fix number of bases .

I like the idea of trying to formulate the feature learning problem as a two - player min-max game and its connection to boosting . As for the related work , it seems the authors have missed some very relevant pieces of work in learning these Fourier features through gradient descent [ 1 , 2 ] . It would be interesting to compare these algorithms as well .

[ 1 ] Zichao Yang , Marcin Moczulski , Misha Denil , Nando de Freitas , Alex Smola , Le Song , Ziyu Wang . Deep Fried Convnets . ICCV 2015 .
[ 2 ] Zichao Yang , Alexander J. Smola , Le Song , Andrew Gordon Wilson . A la Carte — Learning Fast Kernels . AISTATS 2015 .

Similar to our work , paper [ 2 ] also considers learning the Fourier spectrum of a shift - invariant kernel , where the spectrum is parameterized as a mixture of ( a fixed number of ) Gaussians or a piecewise linear function , which can definitely fit in our min-max formulation . However , in comparison , our end - to - end method is more general since it does n’t rely on any specific parameterization . Paper [ 1 ] is also interesting and relevant since it draws connections between spectrally learned kernel machines and deep neural networks . As we mention in the conclusion , it is an exciting future direction to build our method into a deep neural network . We thank the reviewer for pointing out these references , and we ’ll add them to the related work section .



In this paper , the authors proposed an interesting algorithm for learning the l1 - SVM and the Fourier represented kernel together . The model extends kernel alignment with random feature dual representation and incorporates it into l1 - SVM optimization problem . They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity . Under some conditions about the quality of the solution to the nonconvex optimization , they provide the convergence and the sample complexity . Empirically , they show the performances are better than random feature and the LKRF .

I like the way they handle the nonconvexity component of the model . However , there are several issues need to be addressed .

1 , In Eq. ( 6 ) , although due to the convex - concave either min-max or max - min are equivalent , such claim should be explained explicitly .

2 , In the paper , there is an assumption about the peak of random feature " it is a natural assumption on realistic data that the largest peaks are close to the origin " . I was wondering where this assumption is used ? Could you please provide more justification for such assumption ?

3 , Although the proof of the algorithm relies on the online learning regret bound , the algorithm itself requires visit all the data in each update , and thus , it is not suitable for online learning . Please clarify this in the paper explicitly .

4 , The experiment is weak . The algorithm is closely related to boosting and MKL , while there is no such comparison . Meanwhile , Since the proposed algorithm requires extra optimization w.r.t. random feature , it is more convincing to include the empirical runtime comparison .

Suggestion : it will be better if the author discusses some other model besides l1 - SVM with such kernel learning .



@1 : We mention the minimax theorem in the proof , in the appendix . We can add a brief clarification in the main paper .

@2 : The only assumption ( for the theorems to hold ) is that Algorithm 1 finds an eps-approximate global maximum . Our discussion on band - limitedness of real - world data is simply to argue that this non- convex problem is plausibly easy on realistic optimization landscapes : low -frequency features ( on the same scale as the RBF bandwidth parameter ; see Appendix A.1 ) are informative .

@3 : That ’s correct , just as in boosting . We are happy to find a way to further emphasize the distinction , to reduce confusion .

@4 :
- As mentioned in the paper , MKL methods take > 100 times longer on datasets as large as CIFAR - 10 .
- Unlike the selling point of methods such as LKRF and Quasi-Monte Carlo , our method has much greater expressivity ( thus evidently saturates at a much higher accuracy ) . Hence , the value of a quantitative wall clock time comparison is unclear . We believe that the existing discussion ( primal like RF ; parallelizable ; reasonable wall clock time in practice ) suffices to address qualitative questions on efficiency as compared to other paradigms .
- Is there a specific boosting method the reviewer believes to be related enough , so as to require an end - to - end comparison ? We found that it ’s unclear how to choose an ensemble in boosting for fair comparison with learning an optimal translation - invariant kernel ( an infinite - dimensional continuous family ) . As far as we know , though our theoretical analysis bears a strong relationship to boosting , the end - to - end methodology is somewhat dissimilar .

@Suggestion : We agree that considering state - of - the- art settings and applications is an important and interesting direction ( as we note in the conclusion ) . As we mentioned in another review , any convex kernel machine admitting a dual could fit in our min-max formulation , while the min-max SVM objective captures the structure of learning a kernel for any such kernel machine .


In this paper the authors consider learning directly Fourier representations of shift / translation invariant kernels for machine learning applications . They choose the alignment of the kernel to data as the objective function to optimize . They empirically verify that the features they learned lead to good quality SVM classifiers . My problem with that paper is that even though at first glance learning adaptive feature maps seems to be an attractive approach , authors ' contribution is actually very little . Below I list some of the key problems . First of all the authors claim in the introduction that their algorithm is very fast and with provable theoretical guarantees . But in fact later they admit that the problem of optimizing the alignment is a non-convex problem and the authors end up with a couple of heuristics to deal with it . They do not really provide any substantial theoretical justification why these heuristics work in practice even though they observe it empirically . The assumptions that large Fourier peaks happen close to origin is probably well - justified from the empirical point of view , but it is a hack , not a well established well - grounded theoretical method ( the authors claim that in their experiments they found it easy to find informative peaks , even in hundreds of dimensions , but these experiments are limited to the SVM setting , I have no idea how these empirical findings would translate to other kernelized algorithms using these adaptive features ) . The Langevin dynamics algorithm used by the authors to find the peaks ( where the gradient is available ) gives only weak theoretical guarantees ( as the authors actually admit ) and this is a well known method , certainly not a novelty of that paper . Finally , the authors notice that " In the rotation - invariant case , where Ω is a discrete set , heuristics are available " . That is really not very informative ( the authors refer to the Appendix so I carefully read that part of the Appendix , but it is extremely vague , it is not clear at all how the Langevin dynamics can be " emulated " by a discrete Markov chain that mixes fast ; the authors do not provide any justification of that approach , what is the mixing time ?; how the " good emulation property " is exactly measured ? ) . In the conclusions the authors admit that : " Many theoretical questions remain , such as accelerating the search for Fourier peaks " . I think that the problem of accelerating this approach is a critical point that this publication is missing . Without this , it is actually really hard to talk about general mechanism of learning adaptive Fourier features for kernel algorithms ( which is how the authors present their contribution ) ; instead we have a method heavily customized and well - tailored to the ( not particularly exciting ) SVM scenario ( with optimization performed by the standard annealing method ; it is not clear at all whether for other downstream kernel applications this approach for optimizing the alignment would provide good quality models ) that uses lots of task specific hacks and heuristics to efficiently optimize the alignment . Another problem is that it is not clear at all to me how authors ' approach can be extended to non shift - invariant kernels that do not benefit from Bochner 's Theorem . Such kernels are very related to neural networks ( for instance PNG kernels with linear rectifier nonlinearities correspond to random layers in NNs with ReLU ) and in the NN context are much more interesting that radial basis function or in general shift- invariant kernels . A general kernel method should address this issue ( the authors just claim in the conclusions that it would be interesting to explore the NN context in more detail ) .

To sum it up , it is a solid submission , but in my opinion without a substantial contribution and working only in a very limited setting when it is heavily relying on many unproven hacks and heuristics .

The reviewer ’s summary ( “ they choose the alignment of the kernel to data as the objective function to optimize ” ) appears to have missed our main contribution , which is our formulation of the kernel learning problem as a min-max game whose Nash equilibrium gives the optimal kernel . This is harder than simply maximizing kernel alignment , which is the objective considered by most previous works ; it is also more useful and principled , as this optimizes the generalization bound directly . Our contribution lies in the provable black - box reduction from computing this Nash to solving * adversarially weighted * instances of kernel alignment . We are concerned that this confusion possibly underlies the reviewer ’s score and conclusion .


@ Langevin : We never claim that the use of Langevin gradient is the crux ( or novelty ) of the paper . Again , our contribution is giving a theoretically sound reason and strong empirical evidence to use multiple rounds of * adversarially weighted * kernel alignment rather than the uniformly weighted one ( which the reviewer possibly had in mind ) . The role of Langevin is to provide an end - to - end pipeline for this reduction which works well in practice ; we do n’t believe that this makes the entire methodology a “ hack ” .

@ Spherical harmonics : The mention of the Markov chain on spherical harmonic indices should be viewed as a side note for practical implementation . Finding a band -limited Fourier peak in discrete space is * easier * in practice , even though there is no gradient ; we first mention that enumeration of indices is possible , with no possible optimization error ( unlike the continuous case ) . By “ emulating ” Langevin dynamics , we refer to a random walk on the lattice of valid indices . Again , the role of all discussion here on Monte Carlo for optimization is to complete a practical end - to - end pipeline , in this case for the rotation - invariant version .

@Non-convexity : An end- to -end polynomial - time guarantee with no assumptions would be a * significant * breakthrough : it would entail an optimal way to train two - layer neural nets with cosine activations . Our reduction connects this daunting task to a classic and natural non-convex problem ( high -dimensional FFT ) , an active area of theoretical research as pointed out in the conclusion .

@ Fourier assumption : We do n’t believe that the hypothesis that large Fourier peaks are close to the origin is such a controversial one . This is essentially the same as why \ell_2 regularization is widely adopted and well - justified ( see paper [ 1 ] ) . Algorithmically , as we point out in a response to another review , the assumption does not even need to show up . However , we could allay this concern more rigorously via an explicitly - enforced \ell_2 regularizer or constraint . Since multiple reviewers have mentioned this , we ’ll revise the manuscript to clarify .

@Downstream kernel methods : Our method applies to downstream supervised kernel algorithms at least as well as kernel alignment , a widely considered objective in MKL . ( Note that kernel alignment is a special case of our method , with T=1 round of boosting . ) We found that a comprehensive presentation and evaluation of the plethora of kernel methods would distract from the main focus . We further note that the min-max formulation is possible for any convex kernel machine admitting a dual : e.g. support vector regression , kernel ridge regression , and statistical testing ( which we can add to the appendix ) . The min-max SVM objective captures the structure of learning a kernel for any such kernel machine .

@ More expressive kernels : Shift-invariant kernels are very expressive and general , compared to kernel families with any comparable theory ( existing work in random features ) . In ICML / NIPS / JMLR 2016 -2017 , there is a huge amount of research solely on efficiently approximating ( not learning ) a * single * RBF kernel ( see , e.g. papers [ 3 , 4 , 5 , 6 ] ) . We also agree that non-shift-invariant kernels are a very interesting direction to consider . However , much care must be taken , as some restriction on the kernel must be chosen ; otherwise , overfitting is inevitable ( due to the no-free-lunch theorem ) .

@ReLU -NN : To fully address the issue of learning a two - layer ReLU neural network is a well - known NP - hard problem ( paper [ 2 ] ) . We agree that hardness results should n’t prevent us from advancing . However , any non-trivial improvement on this problem should surely deserve a separate paper , which is why we mention it as an interesting future direction .


[ 1 ] Kakade et al . On the Complexity of Linear Prediction : Risk Bounds , Margin Bounds , and Regularization , NIPS 2008 .
[ 2 ] Klivans , Sherstov . Cryptographic Hardness for Learning Intersections of Halfspaces , FOCS 2006 .
[ 3 ] Yu et al . Orthogonal random features , NIPS 2016 .
[ 4 ] Lyu. Spherical Structured Feature Maps for Kernel Approximation , ICML 2017 .
[ 5 ] Avron et al . Quasi- Monte Carlo Feature Maps for Shift-Invariant Kernels , JMLR 2016 .
[ 6 ] Dao et al . Gaussian Quadrature for Kernel Features , NIPS 2017 .

SUMMARY :
This work is about learning the validity of a sequences in specific application domains like SMILES strings for chemical compounds . In particular , the main emphasis is on predicting if a prefix sequence could possibly be extended to a complete valid sequence . In other words , one tries to predict if there exists a valid suffix sequence , and based on these predictions , the goal is to train a generative model that always produces valid sequences . In the proposed reinforcement learning setting , a neural network models the probability that a certain action ( adding a symbol ) will result in a valid full sequence . For training the network , a large set of ( validity - ) labelled sequences would be needed . To overcome this problem , the authors introduce an active learning strategy , where the information gain is re-expressed as the conditional mutual information between the the label y and the network weights w , and this mutual information is maximized in a greedy sequential manner .
EVALUATION :
CLARITY & NOVELTY : In principle , the paper is easy to read . Unfortunately , however , for the reader is is not easy to find out what the authors consider their most relevant contribution . Every single part of the model seems to be quite standard ( basically a network that predicts the probability of a valid sequence and an information - gain based active learning strategy ) - so is the specific application to SMILES strings what makes the difference here ? Or is is the specific greedy approximation to the mutual information criterion in the active learning part ? Or is it the way how you augment the dataset ? All these aspects might be interesting , but somehow I am missing a coherent picture .
SIGNIFICANCE : it is not entirely clear to me if the proposed " pruning " strategy for the completion of prefix sequences can indeed be generally applied to sequence modelling problems , because in more general domains it might be very difficult to come up with reasonable validity estimates for prefixes that are significantly shorter than the whole sequence . I am not so familiar with SMILES strings -- but could it be that the experimental success reported here is mainly a result of the very specific structure of valid SMILES strings ? But then , what can be learned for general sequence validation problems ?

UPDATE : Honestly , outside the scope of SMILES strings , I still have some concerns regarding reasonable validity estimates for prefixes that are significantly shorter than the whole sequence ...



Our main contribution is the formulation of the problem as learning a Q function . To learn this function , however , we need informative data . For Python strings , where no positive data is available , we propose an active learning strategy to learn efficiently . For SMILES , where existing positive data is available , we propose a data augmentation strategy which allows us to obtain informative negative samples . We chose to describe our Q function with a recurrent neural network ( LSTM ) , but any other similar model ( GRU ) could have been used as well .

To further demonstrate the the importance of our contribution , we ’ve updated the SMILES experiments to include a comparison with previous work on validity of samples from VAE prior – a challenging domain where benchmarks exist . Here , our model sets the new state - of - the-art .


Solving the validity learning problem in arbitrary domains can at worst be highly intractable . We believe practical solutions are only available when the validity rules are simple enough to be learned from data . Our approach is able to learn validity models in two very different domains , Python expressions and SMILES strings , demonstrating its capacity for generalization .

The proposed active learning method is expected to be more beneficial in domains where shorter sequences demonstrate rules of validity that also apply to longer strings — that is , the nature of the governing validity rules does not change a great deal as sequences get longer .

The proposed approach is applicable to any sequence validity problem in which our Q function is learnable from data . We considered the problems of learning the validity of python expressions and SMILES sequences because these are relatively simple problems that are also useful in practice and challenging for existing methods .

The authors use a recurrent neural network to build generative models of sequences in domains where the vast majority of sequences is invalid . The basic idea , outlined in Eq. 2 , is moderately straightforward : at each step , use an approximation of the Q function for subsequences of the appropriate length to pick a valid extension . There are numerous details to get right . The writing is mostly clear , and the examples are moderately convincing . I wish the paper had more detailed arguments and discussions .

I question the appropriateness of Eq. 2 as a target . A correctly learned model will put positive weight on valid sequences , but it may be an arbitrarily slow way to generate diverse sequences , depending on the domain . For instance , imagine a domain of binary strings where the valid sequences are the all 1 sequence , or any sequence beginning with a 0 . Half the generated sequences would be all 1 's in this situation , right ? And it 's easy to construct further examples that are much worse than this ?

The use of Bayesian active learning to generate the training set feels like an elegant idea . However , I wish there were more clarity about what was ad hoc and what was n't . For instance , I think the use of dropout to get q is suspect ( see for instance https://arxiv.org/abs/1711.02989), and I 'd prefer a little more detail on statements like " The nonlinearity of g( · ) means that our Monte
Carlo approximation is biased , but still consistent . " Do we have any way of quantifying the bias ? Is the statement about K=16 being reasonable a statement about bias , variance , or both ?

For Python strings :
- Should we view the fact that high values of tau give a validity of 1.0 as indicative that the domain 's constraints are fairly easy to learn ?
- " The use of a Boltzmann policy allows us to tune the temperature parameter to identify policies
which hit high levels of accuracy for any learned Q-function approximation . " This is only true to the extent the domain is sufficiently " easy " right ? Is the argument that even in very hard domains , you might get this by just having an RNN which memorized a single valid sequence ( assuming at least one could be found ) ?
- What 's the best explanation for * why * the active model has much higher diversity ? I understand that the active model is picking examples that tell us more about the uncertainty in w , but it 's not obvious to me that means higher diversity . Do we think this is a universal property of domains ?
- The highest temperature active model is exploring about half of valid sequences ( modulo the non-tightness of the bound ) ? Have you tried gaining some insight by generating thousands of valid sequences manually and seeing which ones the model is rejecting ?
- The coverage bound is used only for for Python expressions , right ? Why not just randomly sample a few thousand positives and use that to get a better estimate of coverage ? Since you can sample from the true positive set , it seems that your argument from the appendix about the validation set being " too similar to the training set " does n't apply ?
- It would be better to see a comparison to a strong non - NN baseline . For instance , I could easily make a PCFG over Python math expressions , and use rejection sampling to get rid of those that are n't exactly length 25 , etc .?

I question how easy the Python strings example is . In particular , it might be that it 's quite an easy example ( compared to the SMILES ) example . For SMILES , it seems like the Bayesian active learning technique is not by itself sufficient to create a good model ? It is interesting that in the solubility domain the active model outperforms , but it would be nice to see more discussion / explanation .

Minor note : The incidence of valid strings in the Python expressions domain is ( I believe ) > 1/5000 , although I guess 1 in 10,000 is still the right order of magnitude .

If I could score between " marginal accept " and " accept " I would .

In our new SMILES experiments , instead of the active learning we propose a data augmentation strategy which generates informative negative samples . Table 4 shows that this strategy allows us to outperform previous state - of - the- art results [ Kusner et al . ( 2017 ) ] .

The reason for not using our active learning strategy with SMILES is because , while it does learn to discover strings that are technically valid , they are not chemically - realistic . This is why our initial active learning results in the SMILES domain were mixed . By instead augmenting an existing set of realistic molecules , we are able to more efficiently explore the space of realistic SMILES strings .


We have updated and extended our SMILES experiments . We now provide comparisons of our work with a state - of - art context - free grammar based approach [ Kusner et al . ( 2017 ) ] .

For python expressions , the active model sees a lot more valid sequences during training , and thus gets better at modelling a large range of those . The passive one does n’t see as many examples of valid sequences , and so does n’t learn their general properties as well . Looking at the generated data , both methods struggle with correlated changes like brackets . One possible fix is to use variable length sequences to learn the usage of brackets from shorter sequences , which can then be generalised to longer sequences .

About the claim regarding Boltzmann sampling being used to generate high validity samples at low enough temperatures , indeed , we mean that it can just generate the same one valid sequence with no sequence diversity . Tau at 1.0 validity could perhaps give an indication of the difficulty of the problem domain .


We have investigated the quality of the biased Monte Carlo information gain estimator . For active learning , the bias would only matter if it affects the relative ordering of different choices . The bias here preserves ordering . That is , if info_gain ( x _1 ) > info_gain ( x _2 ) then E [ info_gain_MC ( x_1 ) ] > E[ info_gain_MC ( x_2 ) ] . K=16 was a statement regarding variance – note that some variance is n’t much of an issue for us , after all we are intentionally ‘ injecting ’ noise at the Boltzmann sampling stage , in order to obtain diverse samples .

The paper https://arxiv.org/abs/1711.02989 refers to variational Gaussian dropout . We use Bernoulli dropout , which is a theoretically - grounded way of obtaining uncertainty estimates in neural networks . This method has already been used to obtain uncertainty estimates in Bayesian neural networks in several previous works :

https://arxiv.org/abs/1506.02142
https://arxiv.org/abs/1512.05287
https://arxiv.org/abs/1703.02910


While the proposed target distribution is not uniform over $ \mathcal { X}_ + $ , it has the following advantages :

( a ) It functions as an indicator of validity , giving zero probability mass to invalid sequences .
( b ) It can be combined with generative models trained on real - world data which do not generate uniform samples . The proposed method can then be used to eliminate , at each step during the sequence generation in such models , those next actions ( characters ) that will lead to invalid sequences , improving the validity of the sequences generated .
( c ) It is invariant to changes in the training data distribution ( active learning strategy ) .
( d ) It can handle padded sequences with no extra effort .
( d ) It is numerically stable -- with the output at each step being in the range [ 0 , 1 ] , rather than perhaps the ratio of sequences with that prefix that can lead to valid sequences , which tends to 0 with increasing sequence length and has typical scale that varies with step t.

We also considered and tested as target a distribution that would be uniform over all sequences if trained with data distributed uniformly from $ \mathcal { X}$ . We found however that : ( a ) this only held for fixed length sequences and was not appropriate for padded sequences ; ( b ) the requirement of uniform data from $ \mathcal { X}$ prevented us from using active learning or any already existing data and ; ( c ) the resulting method suffered from severe numerical / optimisation issues .


Overall : Authors casted discrete structure generation as a planning task and they used Q-learning + RNNs to solve for an optimal policy to generate valid sequences . They used RNN for sequential state representation and Q-learning for encoding expected value of sub-actions across trajectory - constraining each step 's action to valid subsequences that could reach a final sequence with positive reward ( valid whole sequences ) .

Evaluation : The approach centers around fitting a Q function with an oracle that validates sub-sequences . The Q function is supported by a sequence model for state representation . Though the approach seems novel and well crafted , the experiments and results ca n't inform me which part of the modeling was critical to the results , e.g. was it the ( 1 ) LSTM , ( 2 ) Q-function fitting ? Are there other simpler baseline approaches to compare against the proposed method ? Was RL really necessary for the planning task ? The lack of a baseline approach for comparison makes it hard to judge both results on Python Expressions and SMILES . The Python table gives me a sense that the active learning training data generation approach provides competitive validity scores with increased discrete space coverage . However the SMILES data set is a little mixed for active vs passive - authors should try to shed some light into that as well .

In conclusion , the approach seems novel and seem to fit well with the RL planning framework . But the lack of baseline results make it hard to judge significance of the work .

The reason for not using our active learning strategy with SMILES is because while it does learn to discover strings that are technically valid , they are not chemically - realistic . This is why our initial active learning results in the SMILES domain were mixed . By instead augmenting an existing set of realistic molecules , we are able to more efficiently explore the space of realistic SMILES strings .

In our new SMILES experiments , instead of the active learning we propose a data augmentation strategy which generates informative negative samples . Table 4 shows that this strategy allows us to outperform previous state - of - the- art results [ Kusner et al . ( 2017 ) ] .


We have updated and extended our SMILES experiments . We now provide comparisons of our work with a state - of - art context - free grammar based approach [ Kusner et al . ( 2017 ) ] . This more clearly demonstrates the significance of our contribution .


Our main contribution is the formulation of the problem as learning a Q function . To learn this function , however , we need informative data . For Python strings , where no positive data is available , we propose an active learning strategy to learn efficiently . For SMILES , where existing positive data is available , we propose a data augmentation strategy which allows us to obtain informative negative samples . We chose to describe our Q function with a recurrent neural network ( LSTM ) , but any other similar model ( GRU ) could have been used as well .


The paper present online algorithms for learning multiple sequential problems . The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm . Experimental results are given on different multi-task instances . The contributions are interesting and experimental results seem promising . But the paper is difficult to read due to many different ideas and because some algorithms and many important explanations must be found in the Appendix ( ten sections in the Appendix and 28 pages ) . Also , most of the paper is devoted to the study of algorithms for which the expected target scores are known . This is a very strong assumption . In my opinion , the authors should have put the focus on the DU4 AC algorithm which get rids of this assumption . Therefore , I am not convinced that the paper is ready for publication at ICLR '18 .
* Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks . The gap is so large that I am not convinced on the fairness of the comparison . For instance , BA3C ( Algorithm 2 in Appendix C ) does not have the knowledge of the target scores while others heavily rely on this knowledge .
* I do not see how the single output layer is defined .
* As said in the general comments , in my opinion Section 6 should be developped and more experiments should be done with the DUA4C algorithm .
* Section 7.1 . It is not clear why degradation does not happen . It seems to be only an experimental fact .

Thank you for the reviews . We address your comments below :

> In my opinion , the authors should have put the focus on the DU4 AC algorithm which get rids of this assumption .
We believe that the Doubling Paradigm is an important part of the paper and thus , as requested by the reviewer , we have added additional results for the DUA4C agent .

Apart from MT1 , we now show results on another 6 task instance ( MT2 ) , one 8 task instance ( MT4 ) and one 12 task instance ( MT5 ) .
In all the cases , the DUA4C agent outperforms the BA3C agent and is able to perform well on all the MTIs .
We are still running the DUA4C agent on the 21 task instance and will be able to add the results on the same in the camera-ready version of the paper . These results have increased the quality of our work and we hope the reviewer raises his score in the light of these new experiments .


> Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks . The gap is so large that I am not convinced on the fairness of the comparison . For instance , BA3C ( Algorithm 2 in Appendix C ) does not have the knowledge of the target scores while others heavily rely on this knowledge .

As stated in Section 4.1 , we do believe that the lackluster performance of BA3C agent is due to the uniform sampling of the tasks . The DUA4C agent is not provided with the baselines either and it is nevertheless able to beat the BA3C agent by a margin on all the MTIs . The experiments with DUA4C verify our claim that it is indeed the probability distribution over the tasks that causes the huge improvement in our agents .


> I do not see how the single output layer is defined .

As stated in Section 3 , the single output layer is a superset of all the actions in different tasks . Take an MTI with Pong and Breakout . Pong has valid actions as up , down , and no - op ( do nothing ) . Breakout has valid actions as left , right and no -op . The single output layer will have valid actions as up , down , left , right and no -op . While playing an episode of Pong , if the agent chooses left or right ( non- valid actions for Pong ) , it would be treated as a no - op action .
In all our experiments , since we deal with Atari Games , we set the output layer as all the possible 18 actions in ALE with non-valid actions as a no-op .
You can now see how not providing the identity of the task makes learning hard . The agent on seeing a frame is supposed to figure out what is the valid action subset first and thus , learning is harder .


> As said in the general comments , in my opinion Section 6 should be developed and more experiments should be done with the DUA4C algorithm .

We have hopefully addressed the issue of developing DUA4C further with the new experiments .


> Section 7.1 . It is not clear why degradation does not happen . It seems to be only an experimental fact .

While we do agree that we have n’t provided with a theoretical explanation of why degradation does n’t happen , Section 7.1 does provide with an intuition for why the algorithm is able to prevent catastrophic forgetting . We reiterate : Catastrophic Forgetting in our agents is avoided due to the way in which we sample the tasks . The probability of a task getting sampled in our agents is higher for those tasks on which the agent is currently bad at . Once the agent becomes good on a task , if degradation has happened on a task which was previously good , the agent will switch back to the other task and will thus ensure that it trains more on the degraded task .



The authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful . They design and explore several approaches to the active learning ( or active sampling ) problem , from a basic
change to the distribution to UCB to feature - based neural - network based RL . The domain is video games . All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks ( one multitask problem had 21 tasks ) .


Pros :

- very promising results with an interesting active learning approach to multitask RL

- a number of approaches developed for the basic idea

- a variety of experiments , on challenging multiple task problems ( up to 21 tasks / games )

- paper is overall well written / clear

Cons :

- Comparison only to a very basic baseline ( i.e. uniform sampling )
Could n't comparisons be made , in some way , to other multitask work ?



Additional comments :

- The assumption of the availability of a target score goes against
the motivation that one need not learn individual networks .. authors
say instead one can use ' published ' scores , but that only assumes
someone else has done the work ( and furthermore , published it ! ) .

The authors do have a section on eliminating the need by doubling an
estimate for each task ) which makes this work more acceptable ( shown
for 6 tasks or MT1 , compared to baseline uniform sampling ) .

Clearly there is more to be done here for a future direction ( could be
mentioned in future work section ) .

- The averaging metrics ( geometric , harmonic vs arithmetic , whether
or not to clip max score achieved ) are somewhat interesting , but in
the main paper , I think they are only used in section 6 ( seems like
a waste of space ) . Consider moving some of the results , on showing
drawbacks of arithmetic mean with no clipping ( table 5 in appendix E ) , from the appendix to
the main paper .


- The can be several benefits to multitask learning , in particular
time and / or space savings in learning new tasks via learning more
general features . Sections 7.2 and 7.3 on specificity / generality of
features were interesting .



--> Can the authors show that a trained network ( via their multitask
approached ) learns significantly faster on a brand new game
( that 's similar to games already trained on ) , compared to learning from
scratch ?

--> How does the performance improve / degrade ( or the variance ) , on the
same set of tasks , if the different multitask instances ( MT_i )
formed a supersets hierarchy , ie if MT_2 contained all the
tasks / games in MT_1 , could training on MT_2 help average
performance on the games in MT_1 ? Could go either way since the network
has to allocate resources to learn other games too . But is there a pattern ?



- ' Figure 7.2 ' in section 7.2 refers to Figure 5 .


- Can you motivate / discuss better why not providing the identity of a
game as an input is an advantage ? Why not explore both
possibilities ? what are the pros / cons ? ( section 3 )






Thanks for reviewing the paper , the comments and questions ! We believe addressing these questions will increase the quality of the work , and we will certainly do that .

> Comparison only to a very basic baseline ( i.e. uniform sampling ) . Could n't comparisons be made , in some way , to other multitask work ?

We do make a direct comparison to another multi-task work . As stated in Section 5 , the tasks in MT4 ( 8 task instance ) are exactly the same as those used in Actor Mimic Networks ( Parisotto et al. , 2015 ) . AMNs achieve a q_am of 0.79 while all of our agents achieve a q_am greater than 0.9 .


> The assumption … future work section ) .

Before we go ahead , we would like to reiterate that we see the baselines as target scores that we want to achieve on the tasks . As we have shown in Appendix G , it ’s not necessary to take them from published works , a human being could try solving a task and set his score as the target as well . Our algorithm is robust to target scores as well as seen in the same Appendix , i.e you could choose ( reasonably ) bigger targets as well .

We however also believe that the Doubling Paradigm is an important part of the paper and thus , as requested by the reviewer , we have added additional results for the DUA4C agent . Apart from MT1 , we now show results on another 6 task instance ( MT2 ) , one 8 task instance ( MT4 ) and one 12 task instance ( MT5 ) . We are still running the DUA4C agent on the 21 task instance and will be able to add the results on the same in the camera-ready version of the paper . In all the cases , the DUA4C agent outperforms the BA3C agent and is able to perform well on all the MTIs . These results have increased the quality of our work and we thank the reviewer again for raising these requests .


> Can the authors show that a trained network ( via their multitask approached ) learns significantly faster on a brand new game ( that 's similar to games already trained on ) , compared to learning from scratch ?

The work we have presented focuses specifically on Multi-task learning only and not transfer learning and thus , we did n’t show results on transfer learning . While we have n’t shown explicit results on transfer learning , we STRONGLY believe that it will indeed be the case that the MTAs will learn faster on a new similar game . This is attributed to the fact that all the agents in our work learn task agnostic features ( as shown in Section 7 ) and having learned these features beforehand will speed up training on a similar new task . All in all , we are currently designating transfer learning as future work .


> How does the performance improve / degrade ( or the variance ) , on the same set of tasks , if the different multitask instances ( MT_i ) formed a supersets hierarchy , ie if MT_2 contained all the tasks / games in MT_1 , could training on MT_2 help average performance on the games in MT_1 ? Could go either way since the network has to allocate resources to learn other games too . But is there a pattern ?

We do have a supersets hierarchy in the MTIs we ’ve chosen . Note that MT1 is a subset of MT5 . We see that it is indeed the case that the network has allocated resources to learn other games too . For an A5C agent trained on MT5 , the q_am for just the MT1 tasks is 0.697 . For the A5C agent trained on MT1 , the q_am is 0.799 . Please note that the size of the network is same in both the cases . Clearly , the network has allocated some of its representational power to learn the other games . We , however , do not claim this to be a pattern and this forms an interesting direction for further work . We thank the reviewer for this question . This provides further insight into how the network is allocating its resources for multi-tasking .


> ' Figure 7.2 ' in section 7.2 refers to Figure 5 .

We apologize for the typo . We ’ve fixed it in the revision .


> Can you motivate / discuss better why not providing the identity of a game as an input is an advantage ? Why not explore both possibilities ? what are the pros / cons ? ( section 3 )

Not providing the identity of the game is clearly not an advantage . This is because the agent now has to figure out the subset of actions which make sense for the task ( if the actions not valid for a task are chosen , it is treated as a no - op action ) . It makes the setup harder to solve . The motivation behind doing this is that in real - world problems , the identity of the tasks might not be provided . We point out that in spite of not providing the identity of the tasks , the agents perform quite well on the MTIs .

In this paper active learning meets a challenging multitask domain : reinforcement learning in diverse Atari 2600 games . A state of the art deep reinforcement learning algorithm ( A3C ) is used together with three active learning strategies to master multitask problem sets of increasing size , far beyond previously reported works .

Although the choice of problem domain is particular to Atari and reinforcement learning , the empirical observations , especially the difficulty of learning many different policies together , go far beyond the problem instantiations in this paper . Naive multitask learning with deep neural networks fails in many practical cases , as covered in the paper . The one concern I have is perhaps the choice of distinct of Atari games to multitask learn may be almost adversarial , since naive multitask learning struggles in this case ; but in practice , the observed interference can appear even with less visually diverse inputs .

Although performance is still reduced compared to single task learning in some cases , this paper delivers an important reference point for future work towards achieving generalist agents , which master diverse tasks and represent complementary behaviours compactly at scale .

I wonder how efficient the approach would be on DM lab tasks , which have much more similar visual inputs , but optimal behaviours are still distinct .


Thank you for the positive reviews . We address your comments below :

> The choice of distinct of Atari games to multitask learn may be almost adversarial .

We agree with the reviewer that the choice of tasks in our paper could be adversarial because the state spaces are very different visually . This was intentional ( we point it out in the caption of Fig 1 ) with the purpose of raising the standard of the results and strengthens the work presented because , in spite of the state spaces being so visually different , the agents are able to perform very well on all the tasks as the results show .


> How efficient would the algorithm be for visually similar tasks ?

As we claim in the introduction to Section 7 , an ideal MTA performs well due to learning task - agnostic abstract features which help it generalize across multiple tasks . In the case where tasks have visually similar state spaces , finding such features is clearly easier . We thus believe solving visually similar tasks are easier .
Applying the framework to environments apart from Atari has currently been left as future work because of time and computational constraints .

This paper proposes a novel adaptive learning mechanism to improve results in ergodic cooperation games . The algorithm , tagged ' Consequentialist Conditional Cooperation ' , uses outcome - based accumulative rewards of different strategies established during prior training . Its core benefit is its adaptiveness towards diverse opposing player strategies ( e.g. selfish , prosocial , CCC ) while maintaining maximum reward .

While the contribution is explored in all its technical complexity , fundamentally this algorithm exploits policies for selfish and prosocial strategies to determine expected rewards in a training phase . During operation it then switches its strategy depending on a dynamically - calculated threshold reward value ( considering variation in agent-specific policies , initial game states and stochasticity of rewards ) relative to the total reward of the played game instance . The work is contrasted to tit-for- tat approaches that require complete observability and operate based on expected future rewards . In addition to the observability , approximate Markov TFT ( amTFT ) methods are more processing - intense , since they fall back on a game 's Q-function , as opposed to learned policies , making CCC a lightweight alternative .

Comments :

The findings suggest the effectiveness of that approach . In all experiments CCC - based agents fare better than agents operating based on a specific strategy . While performing worse than the amTFT approach and only working well for larger number of iterations , the outcome - based evaluation shows benefits . Specifically in the PPD game , the use of CCC produces interesting results ; when paired with cooperate agents in the PPD game , CCC - based players produce higher overall reward than pairing cooperative players ( see Figure 2 , ( d ) & ( e ) ) . This should be explained . To improve the understanding of the CCC - based operation , it would further be worthwhile to provide an additional graph that shows the action choices of CCC agents over time to clarify behavioural characteristics and convergence performance .

However , when paired with non-cooperative players in the risky PPD game , CCC players lead to an improvement of pay-offs by around 50 percent ( see Figure 2 , ( e ) ) , compared to payoff received between non-cooperative players ( - 28.4 vs. - 18 , relative to - 5 for defection ) . This leads to the question : How much CCC perform compared to random policy selection ? Given its reduction of processing - intensive and need for larger number of iterations , how much worse is the random choice ( no processing , independent of iterations ) ? This is would be worthwhile to appreciate the benefit of the proposed approach .

Another point relates to the fishing game . The game is parameterized with the rewards of + 1 and + 3 . What is the bases for these parameter choices ? What would happen if the higher reward was + 2 , or more interestingly , if the game was extended to allow agents to fish medium - sized fish ( + 2 ) , in addition to small and large fish . Here it would be interesting to see how CCC fares ( in all combinations with cooperators and defectors ) .

Overall , the paper is well - written and explores the technical details of the presented approach . The authors position the approach well within contemporary literature , both conceptually and using experimental evaluation , and are explicit about its strengths and limitations .

Presentation aspects :
- Minor typo : Page 2 , last paragraph of Introduction : `... will act act identically . '
- Figure 2 should be shifted to the next page , since it is not self - explanatory and requires more context .


We have made several additions to the paper that were suggested by the reviewer ( the paper updated paper can be viewed via the PDF link above ) . We think these suggestions make the contribution more clear . We thank the reviewer for these comments .

>>> Specifically in the PPD game , the use of CCC produces interesting results ; when paired with cooperate agents in the PPD game , CCC - based players produce higher overall reward than pairing cooperative players ( see Figure 2 , ( d ) & ( e ) ) . This should be explained . **

This is a good catch ! Actually this is due to variance in the payoffs .

The 2 cooperator payoff is -.74 with a standard error ( calculated assuming that each matchup is independent so using the empirical standard deviation / sqrt ( n - 1 ) ) of +/- .56 while the 2 CCC payoff is -.22 with a CI of +/- .36 . Thus , these payoffs are statistically indistinguishable .

On the other hand 2 defectors get a payoff of - 5.8 with a standard error of 2.2 , so ( D ,D ) is quite statistically distinguishable from ( C , C ) or ( CCC , CCC ) .

This stochasticity only occurs in the rPPD because of the random nature of the defect-payoff . A defection in the standard PPD means the partner loses 2 points deterministically whereas in the rPPD the partner only realizes a large loss of - 20 with a relatively small probability of .1.

In other words , the standard errors of the mean payoffs in the other games ( eg. Fishery , standard PPD ) are tiny and can be ignored but do need to be acknowledged in the rPPD .

In the current version we have relegated the full tournament results to the appendix and edited what we show in the text to better reflect our problem definition .

**>>> worthwhile to provide an additional graph that shows the action choices of CCC agents over time to clarify behavioural characteristics and convergence performance .**

This is a good suggestion . We have added a figure that shows trajectories of behavior of a CCC agent with a C or D partner for the Fishery game .

** >>> How much CCC perform compared to random policy selection ? **

We note that while random policy selection will yield approximately a payoff of .5 * ( D, D ) + .5* ( C ,D ) when paired with a defect partner this random policy will no longer have the incentive properties of CCC are such that if our agent commits to CCC then their partner does better by cooperating than by defecting .

By comparison , if our agent commits to a random policy this incentive property no longer holds and so the best response for a partner is to always defect .

** The game is parameterized with the rewards of + 1 and + 3 . What is the bases for these parameter choices ? What would happen if the higher reward was + 2 , or more interestingly , if the game was extended to allow agents to fish medium - sized fish ( + 2 ) , in addition to small and large fish . Here it would be interesting to see how CCC fares ( in all combinations with cooperators and defectors ) . **
** **
The choice of + 1 / + 3 was made rather arbitrarily ( in most behavioral studies of cooperation the key question is one of the benefit / cost ratio , typically a ratio of 2:1 or 3:1 is used , we simply copied that here ) .

We agree that more games are important for testing the robustness of CCC and other strategies for solving social dilemmas but we leave this for future work .

**>> Typos / figure 2 clarity **
** **
We thank the reviewer for these catches , we have fixed both of them .


This paper studies learning to play two - player general - sum games with state ( Markov games ) with imperfect information . The idea is to learn to cooperate ( think prisoner 's dilemma ) but in more complex domains . Generally , in repeated prisoner 's dilemma , one can punish one 's opponent for noncooperation . In this paper , they design an apporach to learn to cooperate in a more complex game , like a hybrid pong meets prisoner 's dilemma game . This is fun but I did not find it particularly surprising from a game - theoretic or from a deep learning point of view .

From a game - theoretic point of view , the paper begins with a game - theoretic analysis of a cooperative strategy for these markov games with imperfect information . It is basically a straightforward generalization of the idea of punishing , which is common in " folk theorems " from game theory , to give a particular equilibrium for cooperating in Markov games . Many Markov games do not have a cooperative equilibrium , so this paper restricts attention to those that do . Even in games where there is a cooperative solution that maximizes the total welfare , it is not clear why players would choose to do so . When the game is symmetric , this might be " the natural " solution but in general it is far from clear why all players would want to maximize the total payoff .

The paper follows with some fun experiments implementing these new game theory notions . Unfortunately , since the game theory was not particularly well - motivated , I did not find the overall story compelling . It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information , but one could have illustrated the game theory equally well with other techniques .

In contrast , the paper " Coco -Q : Learning in Stochastic Games with Side Payments " by Sodomka et. al. is an example where they took a well - motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning . I would think that generalizing such solution concepts to stochastic games and / or deep learning might be more interesting .

It should also be noted that I was asked to review another ICLR submission entitled " MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING " which amazingly introduced the same " Pong Player ’s Dilemma " game as in this paper .

Notice the following suspiciously similar paragraphs from the two papers :

From " MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING " :
We also look at an environment where strategies must be learned from raw pixels . We use the method
of Tampuu et al. ( 2017 ) to alter the reward structure of Atari Pong so that whenever an agent scores a
point they receive a reward of 1 and the other player receives − 2 . We refer to this game as the Pong
Player ’s Dilemma ( PPD ) . In the PPD the only ( jointly ) winning move is not to play . However , a fully
cooperative agent can be exploited by a defector .

From " CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION " :
To demonstrate this we follow the method of Tampuu et al. ( 2017 ) to construct a version of Atari Pong
which makes the game into a social dilemma . In what we call the Pong Player ’s Dilemma ( PPD ) when an agent
scores they gain a reward of 1 but the partner receives a reward of − 2 . Thus , in the PPD the only ( jointly ) winning
move is not to play , but selfish agents are again tempted to defect and try to score points even though
this decreases total social reward . We see that CCC is a successful , robust , and simple strategy in this
game .

We thank the reviewer for their thorough review . We have made several changes in exposition and presentation . We hope these address the reviewer 's concerns .

>> The paper follows with some fun experiments implementing these new game theory notions . Unfortunately , since the game theory was not particularly well - motivated , I did not find the overall story compelling . It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information , but one could have illustrated the game theory equally well with other techniques .

From reading the reviewers ' comments , we realize that we should have been much clearer front with our problem definition . We have edited the text substantially to make this clearer .

Our goal is to consider a question of agent design : how should we build agents that can enter into social dilemmas and achieve high payoffs with partners that are themselves trying to achieve high payoffs ?

In particular , we seek to answer this question for social dilemmas where actions of a partner are not observed .

This question is quite different from just making agents that cooperate all the time ( since those agents are easily exploited by defectors ) . It is related to , but also different from , the question of computing cooperative equilibria .

See the reply to R3 above for a more in depth discussion about the desiderata from past literature that we seek to satisfy in order to construct a “ good ” strategy for imperfect information social dilemmas .

>> The reviewer asks whether maximizing the sum of the payoffs is the “ right ” solution
We agree with this criticism . While in symmetric games ( eg. bargaining ) behavioral experiments show that often view the symmetric sum of payoffs to be a natural focal point while in asymmetric games they do not ( see eg. the chapter on bargaining in Kagel & Roth Handbook of Experimental Economics or more recent work on inequality in social dilemmas eg . Hauser , Kraft -Todd , Rand , Nowak & Norton 2016 ) .

The question of how to choose the “ correct ” focal points for playing with humans comes down to asking what should the “ right ” utility function be for training the cooperative strategies ( see Charness & Rabin ( 2002 ) for a generic utility function that can express many social goals ) . Note that CCC can then be applied using these new C strategies just as in the current work .

However , figuring out the correct utility function to use here is far beyond the scope of this paper and is likely quite context dependent . This is an important direction for future research and we have made this point clear the paper .


>> Similar paragraphs
We are also the authors of the other paper , it is earlier / related work to this paper ( in the sense that it asks about designing agents that can solve social dilemmas ) , though it covers substantially different ground ( perfectly observed games ) .

We apologize if there is something unclear from the current text . We do not mean to imply that this paper ( CCC ) introduces the PPD . Rather , it is the earlier paper ( amTFT ) that introduces the PPD as an environment and the CCC paper uses it for robustness checks .

The point of the PPD in this paper is to ask whether the other work is superceded by the CCC . Indeed , the techniques proposed in the amTFT paper can ONLY work in perfectly observed games .

By contrast , CCC is a good strategy for imperfectly observed games . Since any perfectly observed game is trivially also an imperfectly observed one one may think that CCC is just a strictly better strategy than amTFT ( and thus the other paper is subsumed by this one ) .

The point of the PPD experiments in this paper is to show that there are classes of perfectly observed games where CCC performs similarly to amTFT ( normal PPD ) but there are also those where CCC fails but amTFT succeeds ( risky PPD ) .

We have changed the text to make these points clearer and to attribute credit transparently .


The main result specifies a ( trigger ) strategy ( CCC ) and corresponding algorithm that leads to an efficient outcome in social dilemmas , the theoretical basis of which is provided by theorem 1 . This underscores an algorithm that uses a prosocial adjustment of the agents rewards to encourage efficient behaviour . The paper makes a useful contribution in demonstrating that convergence to efficient outcomes in social dilemmas without the need for agents to observe each other 's actions . The paper is also clearly written and the theoretical result is accompanied by some supporting experiments . The numerical experiments show that using CCC strategy leads to an increase in the proportion of efficient equilibrium outcomes . However , in order to solidify the experimental validation , the authors could consider a broader range of experimental evaluations . There are also a number of items that could be added that I believe would strengthen the contribution and novelty , in particular :

Some highly relevant references on ( prosocial ) reward shaping in social dilemmas are missing , such as Babes , Munoz de cote and Littman , 2008 and for the ( iterated ) prisoner 's dilemma ; Vassiliades and Christodoulou , 2010 which all provide important background material on the subject . In addition , it would be useful to see how the method put forward in the paper compares with other ( reward - shaping ) techniques within MARL ( especially in the perfect information case in the pong players ' dilemma ( PPD ) experiment ) such as those already mentioned . The authors could , therefore , provide more detail in relating the contribution to these papers and other relevant past work and existing algorithms .

The paper also omits any formal discussion on the equilibrium concepts being used in the Markov game setting ( e.g. Markov Perfect Equilibrium or Markov - Nash equilibrium ) which leaves a notable gap in the theoretical analysis .

There are also some questions that to me , remain unaddressed namely :

i . the model of the experiments , particularly a description of the structure of the pong players ' dilemma in terms of the elements of the partially observed Markov game described in definition 1 . In particular , what are the state space and transitions ?

ii . the equilibrium concepts being considered i.e. does the paper consider Markov perfect equilibria . Some analysis on the conditions that under which the continuation equilibria e.g. cooperation in the social dilemma is expected to arise would also be beneficial .

iii . Although the formal discussion is concerned with Markov games ( i.e. repeated games with stochastic transitions with multiple states ) the experiments ( particularly the PPD ) appear to apply to repeated games ( this could very much be cleared up with a formal description of the games in the experimental sections and the equilibrium concept being used ) .

iv . In part 1 of the proof of the main theorem , it seems unclear why the sign of the main inequality has changed after application of Cauchy convergence in probability ( equation at the top of the page ) . As this is an important component of the proof of the main result , the paper would benefit from an explanation of this step ?


We thank the reviewer for their comments . They have pointed out weaknesses in our presentation of our results . We have edited the text substantially and hope that our contributions and claims are much clearer .

**>>> R3 asks about the relationship of our work to prior work on reward shaping in MARL . **

We are happy to add these references to the main text and discuss them . One important thing to note is that the prior work mentioned by the reviewer has dealt with perfectly observed games rather than partially observed ones .

We have added a longer discussion of how our work is related to existing work on MARL , equilibrium finding , and reward shaping .

We specifically discuss one of the examples the reviewer gives : Babes et al . 2008 use reward shaping in the repeated Prisoner 's Dilemma to construct an agent that does well against fixed opponents as well as can lead learner “ followers ” to cooperate with it . However , in order to do this they first need to compute a value function for a known “ good ” strategy ( they use Pavlov , a variant of tit-for - tat ) and use this for shaping . This is possible for the basic one ( or multi-memory ) PD but does n't scale well to general Markov games ( in particular partially observed ones ) .

By contrast , the CCC agent creates similar incentives by switching between two pre-computed strategies in a predictable way . The computation of these two strategies does not require anything other than standard self - play .

Combining these ideas is an interesting direction for future research but beyond the scope of our paper .

** **
**>> R3 asks for formalization for the state spaces / transition functions / etc … in our games .**

We are happy to add more details of the games to the paper ( as well as release the code upon publication ) . Our games do not permit a compact enumeration of the states and transitions ( which is precisely why we are interested in moving beyond tabular methods to eg. deep RL ) . For example , in the PPD the full set of states is the set of RAM states in Atari Pong .

**>> R3 asks about equilibrium concepts in our Markov game setting **

While much existing work is framed in terms of finding good equilibria , our work is more related to questions raised by Axelrod ( 1984 ) who asks : if one is to enter a social dilemma with an ( unknown ) partner , how should one behave ?

The work on tit-for-tat ( TFT , and related strategies such as Win-Stay-Lose - Shift / Pavlov ) comes up with the answer that one should play a strategy that is :

* simple
* nice ( begins by cooperating )
* not exploitable
* forgiving ( provides a way to return to cooperation after a defection )
* incentivizes cooperation from its partner ( that is , a partner who commits to cooperation will get a higher payoff than a partner who commits to defection )

Our main contribution is to find a way to construct a strategy which satisfies the Axelrod desiderata in * partially observed * Markov games which require deep RL for function approximation .

Note that these desiderata are different from equilibrium desiderata . For example , tit-for-tat , one of the most heavily studied strategies in the Prisoner 's Dilemma , is actually not a symmetric equilibrium strategy ( because the best response to TFT is always cooperate ) . Rather , these desiderata are about agent design or about good strategies to commit to .

We do not claim that CCC forms an equilibrium with itself as there may be local deviations to improve one 's payoff , however , our theorem shows that the partner of a CCC agent maximizes its asymptotic payoff by playing a policy that cooperates with the CCC agent ( thus we preserve TFT - like incentive properties ) .

We have edited the text to make our problem statement / results clearer .

We only focus on equilibrium for computational reasons in the case of the D policy for which we have made an assumption that ( D ,D ) forms an equilibrium in policies which only condition on agent 's observations ( this is related to the notion of a belief free equilibrium in repeated games Ely et al. 2005 ) .


**>> R3 asks “ it seems unclear why the sign of the main inequality has changed after application of Cauchy convergence in probability ( equation at the top of the page ) ” **
** **
We apologize for any confusion . The equation at the top of the page uses the convention

P( X ) < epsilon

while the next equations use the notation

P ( ~X ) > ( 1 - epsilon )

We have changed these to both use P ( ~X ) > ( 1 - epsilon ) so that it is more clear .


The present manuscript attempts to address the problem of mode collapse in GANs using a constrained mixture distribution for the generator , and an auxiliary classifier which predicts the source mixture component , plus a loss term which encourages diversity amongst components .

All told the proposed method is quite incremental , as mixture GANs / multi-generators have been done before . The Inception scores are good but it 's widely known now that Inception scores are a deeply flawed measure , and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice . If the generator were to generate one template per class for which the Inception network 's p( y|x ) had low entropy , the Inception score would be quite high even though the model had only memorized one image per class . For claims surrounding mode collapse in particular , evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al ( 2017 ) would be the gold standard . Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score .

The mixing proportions are fixed to the uniform distribution , and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity . This seems quite dubious .

Finally , their own qualitative results indicate that they 've simply moved the problem , with clear evidence of mode collapse in one of their mixture components in figure 5c , 4th row from the bottom . Indeed , this does nothing to address the problem of mode collapse in general , as there is nothing preventing individual mixture component GANs from collapsing .

Uncited prior work includes Generative Adversarial Parallelization of Im et al ( 2016 ) . Also , if I 'm not mistaken this is quite similar to an AC - GAN , where the classes are instead randomly assigned and the generator conditioning is done in a certain way ; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component . More discussion of this would be warranted .

Other notes :
- The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice .
- " As a result , the optimization order in 1 can be reversed " this does not accurately characterize the source of the issues , see , e.g. Goodfellow ( 2015 ) " On distinguishability criteria ... " .
- Section 3 : the second last sentence of the third paragraph is vague and does n't really say anything . Of course parameter sharing leverages common information . How does this help to train the model effectively ?
- Section 3 : Since JSD is defined between two distributions , it is not clear what JSD_pi( P_ G1 , P_ G2 , ... ) refers to . The last line of the proof of theorem 2 leaps to calling this term a Jensen - Shannon divergence but it 's not clear what the steps are ; it looks like a regular KL divergence to me .
- Section 3 : Also , is the classifier being trained to maximize this divergence or just the generator ? I assume the latter .
- The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions ( pi ) .
- " ... which further minimizes the objective value " -- it minimizes a term that you introduced which is constant with respect to your learnable parameters . This is not a selling point , and I 'm not sure why you bothered mentioning it .
- There 's no mention of the substitution of log ( 1 - D ( x ) ) for - log ( D ( x ) ) and its effect on the interpretation as a Jensen - Shannon divergence ( which I 'm not sure was quite right in the first place )
- Section 4 : does the DAE introduced in DFM really introduce that much of a computational burden ?
- " Symmetric Kullback Liebler divergence " is not a well - known measure . The standard KL is asymmetric . Please define it .
- Figure 2 is illegible in grayscale .
- Improved - GAN score in Table 1 is misleading , as this was their no - label baseline . It 's fine to include it but indicate it as such .

Update : many of my concerns were adequately addressed , however I still feel that calling this an avenue to " overcome mode collapse " is misleading . This seems aimed at improving coverage of the support of the data distribution ; test log likelihood bounds via AIS ( there are GAN baselines for MNIST in the Wu et al manuscript I mentioned ) would have been more compelling quantitative evidence . I 've raised my score to a 5 .

**** Note 1 : The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice .

==== Answer : We do not understand exactly what you meant by ill-posedness . Can please you further clarify this note ?

**** Note 2 : " As a result , the optimization order in 1 can be reversed " this does not accurately characterize the source of the issues , see , e.g. Goodfellow ( 2015 ) " On distinguishability criteria ... " .

==== Answer : Here , we simply mentioned the issue discussed in The GAN tutorial ( Goodfellow , 2016 ) : “ Simultaneous gradient descent does not clearly privilege min max over max min or vice versa . We use it in the hope that it will behave like min max but it often behaves like max min . ”

**** Note 3 : Section 3 : the second last sentence of the third paragraph is vague and does n't really say anything . Of course parameter sharing leverages common informaNtion . How does this help to train the model effectively ?

==== Answer : We discussed in Section 5.2 , Model Architectures that our experiment showed that when the parameters are not tied between the classifier and discriminator , the model learns slowly and eventually yields lower performance .

**** Note 4 : Section 3 : Since JSD is defined between two distributions , it is not clear what JSD_pi( P_ G1 , P_ G2 , ... ) refers to . The last line of the proof of theorem 2 leaps to calling this term a Jensen - Shannon divergence but it 's not clear what the steps are ; it looks like a regular KL divergence to me .

==== Answer : The general definition of JSD is :
JSD_pi( P_1 , P_2 , … P_n ) = H( sum_{i=1..n} ( pi_i * P_i ) ) - sum_{i=1..n} ( pi_i * H( P_i )
Where H( P ) is the Shannon entropy for distribution P. Due to limited space , we showed more details of the derivation of L( G_1 :K ) in Appendix B.

**** Note 5 : Section 3 : Also , is the classifier being trained to maximize this divergence or just the generator ? I assume the latter .

==== Answer : It is the latter . Based on Eq. 2 , the classifier is trained to minimize its softmax loss , and based on the optimal solution for the classifier , the generators , by minimizing their objective function , will maximize the JSD divergence .

**** Note 6 : The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions ( pi ) . - " ... which further minimizes the objective value " – it minimizes a term that you introduced which is constant with respect to your learnable parameters . This is not a selling point , and I 'm not sure why you bothered mentioning it .

==== Answer : Please refer to our answer to comment 3.

**** Note 7 : There 's no mention of the substitution of log ( 1 - D ( x ) ) for - log ( D ( x ) ) and its effect on the interpretation as a Jensen - Shannon divergence ( which I 'm not sure was quite right in the first place )

==== Answer : We said in the end of Section 3 : “ In addition , we adopt the non-saturating heuristic proposed in ( Goodfellow et al. , 2014 ) to train G_{1 : K} by maximizing log D( G_k ( z ) ) instead of minimizing log D( 1 - G_k ( z ) ) . ”

**** Note 8 : Section 4 : does the DAE introduced in DFM really introduce that much of a computational burden ?

==== Answer : It was stated in Section 5.3 , paragraph 2 in ( Warde - Farley & Bengio , 2017 ) that : “ we achieve a higher Inception score using denoising feature matching , using denoiser with 10 hidden layers of 2,048 rectified linear units each . ” That means the DAE adds more than 40 million parameters .

**** Note 9 : “ Symmetric Kullback Liebler divergence ” is not a well - known measure . The standard KL is asymmetric . Please define it . - Figure 2 is illegible in grayscale .

==== Answer : Symmetric Kullback Liebler is the average of the KL and reverse KL divergence . As per your suggestion , we will define it in the paper . Regarding Figure 2 , we tried different shapes for the real and generated data points , but due the small size if figure , they are just clusters of red and blue points . We will try different approaches to make the figure more legible .

**** Note 10 : Improved - GAN score in Table 1 is misleading , as this was their no - label baseline . It 's fine to include it but indicate it as such .

==== Answer : We will take your advice and make it clear that Improve - GAN score in Table 1 is for the unsupervised version .

**** Comment 4 : Finally , their own qualitative results indicate that they 've simply moved the problem , with clear evidence of mode collapse in one of their mixture components in figure 5c , 4th row from the bottom . Indeed , this does nothing to address the problem of mode collapse in general , as there is nothing preventing individual mixture component GANs from collapsing .

==== Answer : If we look carefully at samples shown in previously published papers ( such as Figure 4 of the Improved GAN paper that showed samples generated by semi-supervised GAN trained on CIFAR - 10 with feature matching ) , there are often broken samples that look similar .

Solving mode collapse for a single - generator GAN is out of scope of this paper . As discussed in Introduction , we acknowledged the challenges of training a single generator , and therefore we took the multi-generator approach . We did not seek to improve within-generator diversity but instead improve among - generator diversity . The intuition is that GAN can be pretty good for narrow - domain datasets , so if a group of generators learns to partition the data space , and each of them focuses on a region of the data space , then they together can do a good job too . Finally , the use of a classifier to enforce divergence among generators makes our method relatively easy to integrate with other single - generator models that achieved improvement regarding the mode collapsing problem .

**** Comment 5 : Uncited prior work includes Generative Adversarial Parallelization of Im et al ( 2016 ) . Also , if I 'm not mistaken this is quite similar to an AC - GAN , where the classes are instead randomly assigned and the generator conditioning is done in a certain way ; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component . More discussion of this would be warranted .

==== Answer : Generative Adversarial Parallelization ( GAP ) trains many pairs of GAN , periodically swap the discriminators ( generators ) randomly , and finally selects the best GAN based on GAM evaluation . When we discussed methods in the multi-generator approach , we focused on mixture GAN and as a result neglected GAP . It is fair to discuss GAP as an approach to reduce the mode collapsing problem .

In AC - GAN , the label information and the noise are concatenated and then fed into the generator network . In our model , generators have different weights in the first layer , so they are mapped to the first hidden layer differently . MGAN and AC - GAN both add the log-likelihood of the correct class to the objective function , but the motivation is very different . Our idea started by asking how to force generators to generate different data , while AC - GAN 's motivation is to leverage the label information from training data . So , the two works are totally independent and happens to share some similarities . Our paper focuses on unsupervised GAN , so we did not discuss semi-supervised methods .

**** Comment 3 : The mixing proportions are fixed to the uniform distribution , and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity . This seems quite dubious .

**** Note 6 : The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions ( pi ) . - " ... which further minimizes the objective value " – it minimizes a term that you introduced which is constant with respect to your learnable parameters . This is not a selling point , and I 'm not sure why you bothered mentioning it .

==== Answer : Our theorem 3 shows that by means of maximizing the divergence among the generated distributions p_G_k ( ⋅ ) , in an ideal case , our proposed MGAN can recover the true data distribution wherein each p_G_k describes a mixture component in this data distribution . Although this theorem gives more insightful understanding of our MGAN as well as its behavior , it requires a strict setting wherein we need to specify the number of mixtures and the mixing proportions a priori . Stating this theorem , we want to emphasize that maximizing the divergence among the generated distributions p_G_k is an efficient way to encourage the generators to produce diverse data that can occupy multiple modes in the real data . Moreover , since GAN requires training a single generator that can cover multiple data modes , it is much harder to train , and always ends up with missing of data modes . In contrast , our MGAN aims at training each generator to cover one or a few data modes , hence being easier to train , and reducing the missed data modes . In addition , due to the fact that each generator can cover some data modes , the number of generators K can be less than the number of data modes as shown in Figure 6 wherein samples generated from 3 or 4 generators can well cover a mixture of 8 Gaussians .

Given the fact that we are learning from the empirical data distribution , we develop a further theorem to clarify that if we wish to learn the mixing proportion π , the optimal solution is the uniform distribution . The idea is that the optimal generators will learn to partition the empirical data into K disjoint sets of roughly equal size , and each generator approximates a set . In addition , due to the fact that the discrete distribution p_A_k is well - approximated by a continuous generator G_k , the data points in each A_k occupies several groups or clusters . Again , Figure 6 illustrates this point . In Figure 6 b , each of the 2 generators ( yellow and blue ) covers 4 modes . In Figure 6 c , one generator ( dark green ) covers 2 modes and the other two generators ( yellow and blue ) covers 3 modes . In Figure 6d , each of the four generators ( yellow , blue , dark green and dodger blue ) cover 2 modes .

For details of our theorem , please refer to this link : https://app.box.com/s/jjr5kt69uxbr0aikrm0d9cdp2jj95wa0

**** Comment 2 : The Inception scores are good but it 's widely known now that Inception scores are a deeply flawed measure , and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice . If the generator were to generate one template per class for which the Inception network 's p( y|x ) had low entropy , the Inception score would be quite high even though the model had only memorized one image per class . For claims surrounding mode collapse in particular , evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al ( 2017 ) would be the gold standard . Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score .

==== Answer : We chose Inception Score because at the time we set up our experiment , it was the most widely accepted metrics , so it would be easier for us to compare with many baselines . We did acknowledge that any quantitative metric has its weakness and Inception Score is no exception . Therefore , we included a lot of samples in the paper and looked at them from different angle . It can be noticed that our samples , in terms of quality , are far better than those shown in previously published papers . In addition , we looked at samples generated by each of the generators to check whether they trap Inception Score by memorizing a few examples from each class . We saw no sign of trapping as samples generated by each generator were diverse , especially on diverse datasets such as STL - 10 or ImageNet . Therefore , we believe that our method achieved higher Inception Score than single - GAN methods not because it trapped the score , but because each of the generators learned to model a different subset of the training data . As a result , our generated samples are more diverse and at the same time more visually appealing . For the mentioned reasons , we strongly believe the use of Inception Score in our experiment to evaluate our proposed method is valid and plausible .

As per your suggestion , we looked for GAN baselines using the AIS loglikelihood , but we found no GAN baseline . Regarding Frechet Inception ( FID ) distance , our model got an FID of 26.7 for Cifar - 10 . Some baselines we collected from ( Heusel et al. , 2017 ) are 37.7 for the original DCGAN , 36.9 for DCGAN using Two Time-scale Update rule ( DCGAN + TTUR ) , 29.3 for WGAN - GP ( Gulrajani , 2017 ) FID of 29.3 , and 24.8 for WGAN - GP using TTUR . It is noteworthy that lower FID is better , and that the base model for MGAN is DCGAN . Therefore , in terms of FID , MGAN ( 26.7 ) is 28 % better than DCGAN ( 37.7 ) and DCGAN using TTUR ( 36.9 ) and is 9 % better than WGAN - GP ( 29.3 ) , which uses ResNet architecture . This example further shows evidence that our proposed method helps to address the mode collapsing problem .

We gratefully thank the reviewer for the detailed and valuable comments and notes . It took us a while to thoughtfully answer all the comments , and the following are our answers . Due to the limited number of characters per comment , we will answer in several posts :

**** Comment 1 : All told the proposed method is quite incremental , as mixture GANs / multi-generators have been done before .

==== Answer : As discussed in related work , there are previous attempts following the multi-generators approach , but they are different from our proposed method . Mix + GAN is totally different as it 's based on the min-max theorem and set up mixed strategies for both generators and discriminators . AdaGAN train generators sequentially in a manner similar to AdaBoost , thus having some disadvantages as we discussed . MAD - GAN , at a first glance , looks somewhat similar to our proposed method in terms of model design , but there are some key differences . First , it uses a multi-class discriminator , which outputs D_k ( x ) as the probability that x generated by G_k for k = 1 , 2 , … K , and D_{K + 1 } ( x ) as the probability that x came from the training data . The gradient signal for each generator k comes from the loss function E_{x~p_G_k} [ log ( 1 - D_{k + 1 } ( x ) ] , which is similar to that in a standard GAN . So , it might be vulnerable to the issue discussed in the Improved GAN paper : “ Because the discriminator processes each example independently , there is no coordination between its gradients , and thus no mechanism to tell the outputs of the generator to become more dissimilar to each other . ” Our proposed method is distinguished in the use of a classifier to enforce JSD divergence among generators . In addition , the use of a separate classifier makes our method easier to integrate with other single - generator GAN models . There is also extension to our method that do not apply to MAD - GAN . We can use the classifier to cluster the train data , and then further train each generator in a different cluster .

In terms of performance , our method is far superior than Mix + GAN both in terms of Inception Scores and sample quality . The AdaGAN only presents experiment on MNIST . MAD - GAN mostly performed experiment on narrow - domain datasets , and they did not report any quantitative data on diverse datasets and did not release code as well .

Summary :

The paper proposes a mixture of generators to train GANs . The generators used have tied weights except the first layer that maps the random codes is generator specific , hence no extra computational cost is added .


Quality / clarity :

The paper is well written and easy to follow .

clarity : The appendix states how the weight tying is done , not the main paper , which might confuse the reader , would be better to state this weight tying that keeps the first layer free in the main text .

Originality :

Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper , the difference in this paper is in weight tying between generators of the mixture , the first layer is kept free for each generator .

General review :

- when only the first layer is free between generators , I think it is not suitable to talk about multiple generators , but rather it is just a multimodal prior on the z , in this case z is a mixture of Gaussians with learned covariances ( the weights of the first layer ) . This angle should be stressed in the paper , it is in fine , * one generator * with a multimodal learned prior on z!

- Taking the multimodal z further , can you try adding a mean to be learned , together with the covariances also ? see if this also helps ?

- in the tied weight case , in the synthetic example , can you show what each " generator " of the mixture learn ? are they really learning modes of the data ?

- the theory is for general untied generators , can you comment on the tied case ? I do n't think the theory is any more valid , for this case , because again your implementation is one generator with a multimodal z prior . would be good to have some experiments and see how much we loose for example in term of inception scores , between tied and untied weights of generators .


We gratefully thank the reviewer for the thoughtful and insightful comments . It took us a while to answer all the reviews as well as to run additional experiments as suggested . Our answers are the following :

**** Comment 1 : when only the first layer is free between generators , I think it is not suitable to talk about multiple generators , but rather it is just a multimodal prior on the z , in this case z is a mixture of Gaussians with learned covariances ( the weights of the first layer ) . This angle should be stressed in the paper , it is in fine , * one generator * with a multimodal learned prior on z!

==== Answer : The first hidden layer actually has 4x4x512 = 8,192 dimensions ( for Cifar - 10 ) . So , untying weights in the first layer effectively maps the noise prior to a different distribution in R^8192 ( with a different mean and covariances ) for each generator . So , our proposed method is different from a GAN with a multimodal prior .

**** Comment 2 : taking the multimodal z further , can you try adding a mean to be learned , together with the covariances also ? see if this also helps ?

==== Answer : We tried to learn the mean and covariance of the prior for each generator , but the result was not much different from the standard GAN .

**** Comment 3 : in the tied weight case , in the synthetic example , can you show what each " generator " of the mixture learn ? are they really learning modes of the data ?

==== Answer : Following your suggestion , we revised figure 6 so that data points generated by different generators have different colors . As you can see , generators learned different modes of the data .

**** Comment 4 : the theory is for general untied generators , can you comment on the tied case ? I do n't think the theory is any more valid , for this case , because again your implementation is one generator with a multimodal z prior . would be good to have some experiments and see how much we loose for example in term of inception scores , between tied and untied weights of generators .

==== Answer : In theory , tying weights will add constraints to the optimization of the objective function for G_{1 : K} in Eq . 4 . For example , if we tie weights in all layers and generators differ only in the mean and variance of the noise prior , the result was similar to the standard GAN like we reported in comment 2 . Untying weights in the first layer , however , achieved good results like we discussed in the paper . Finally , as per your request , we conducted experiments without parameter sharing . Surprisingly , when we trained 4 generators without parameter sharing and each generator has 128 feature maps in the penultimate layer , the model failed to learn . The model even failed to learn when we set beta to 0 . When we reduced the number of feature maps in the penultimate layer for each generator to 32 , they managed to learn and achieved an Inception Score of 7.42 . So , we hypothesize that added benefit of our parameter sharing scheme is to balance the capacity of generators and that of the discriminator / classifier .

MGAN aims to overcome model collapsing problem by mixture generators . Compare to traditional GAN , there is a classifier added to minimax formulation . In training , MGAN is optimized towards minimizing the Jensen - Shannon Divergence between mixture distributions from generator and data distribution . The author also present that using MGAN to achive state - of - art results .

The paper is easy to follow .

Comment :

1 . Seems there still no principle to choose correct number of generators but try different setting . Although most parameters of generators are shared , the result various .
2 . Parameter sharing seems is a trick in MGAN model . Could you provide experiment results w/ o parameter sharing .



We gratefully thank reviewers for the insightful comments . We have endeavored to address as much as we can , including running additional experiments as suggested , thus it has taken us a while .

**** Comment 1 : Seems there still no principle to choose correct number of generators but try different setting . Although most parameters of generators are shared , the result various .

==== Answer : We agree that we do n’t have any principle to choose the correct number of generators for our proposed model , as choosing the correct number of clusters for Gaussian mixture model ( GMM ) and other clustering methods . If we wish to specify an appropriate number of generators automatically , we would need to go for a Bayesian nonparametric extension , similarly to going from GMM to Dirichlet Process Mixtures . Within the scope of this work , our motivation is that GAN works pretty well on narrow - domain dataset but poorly on diverse dataset ; So , if we can efficiently train many generators while enforcing divergence among them , they can work well too . In general , more generators tend to work better .

**** Comment 2 : Parameter sharing seems is a trick in MGAN model . Could you provide experiment results w/ o parameter sharing .

==== Answer : We did experiment without parameters sharing among generators and found an interesting behavior . When we trained 4 generators without parameter sharing and each generator has 128 feature maps in the penultimate layer , the model failed to learn . The model even failed to learn when we set beta to 0 . When we reduced the number of feature maps in the penultimate layer for each generator to 32 , they managed to learn and achieved an Inception Score of 7.42 . So , we hypothesize that added benefit of parameter sharing is to help balance the capacity of generators and that of the discriminator / classifier .


This paper investigates residual networks ( ResNets ) in an empirical way . The authors argue that shallow layers are responsible for learning important feature representations , while deeper layers focus on refining the features . They validate this point by performing a series of lesion study on ResNet .

Overall , the experiments and discussions in the first part of Section 4.2 and 4.3 appears to be interesting , while other observations are not quite surprising . I have two questions :
1 ) What is the different between the layer - dropping experiment in sec 4.2 and that in [ Veit , et al , Residual networks are exponential ensembles of relatively shallow networks ] ? What is the main point here ?
2 ) I do n't quite understand the first paragraph of sec 4.5 . Could you elaborate more on this ?


We thank reviewer for his remarks , and positive assessment .

In his first point , reviewer asks what is difference between our 4.2 and Veit et al . We cite Veit et al , and extend his observations . Our novel observation is that blocks in residual network have different function , and only subset of blocks focus on iterative inference . More specifically , some blocks have large l2 ratio ( ratio of output to input norms for given block ) , and cannot be dropped without drastic effect on performance . This allows us to specify concretely in what sense residual network performs iterative inference . We made edits in text to clarify this .

In his second point reviewer requests clarification on first paragraph of 4.5 . First paragraph of 4.5 reads : “ Given the iterative inference view , we now study the effects of sharing residual blocks . Contrary to ( Liao & Poggio , 2016 ) we observe that naively sharing the higher ( iterative refinement ) residual blocks of a Resnets in general leads to overfitting ( especially for deeper Resnets ) . ” . First , we say that our results suggest that residual network perform iterative inference , and that top blocks are performing similar function ( feature refinement ) , there it is plausible that top blocks in residual network should be shareable . However , during this investigation , we report a surprising observation that has not been made before ( Liao & Poggio tested relatively small ResNets ) that when we share layers of residual network , it leads to drastic overfitting . In Fig.8 we compare Resnet -110 - shared and Resnet - 32 , where Resnet -110 - shared has same number of parameters as Resnet - 32 . We observe strong overfitting ( train accuracy remains the same , while validation accuracy is much lower for Resnet110 ) . We made edits in text to clarify this first paragraph .


The author unveils some properties of the resnets , for example , the cosine loss and l2 ratio of the layers .
I think the author should place more focus to study " real " iterative inference with shared parameters rather than analyzing original resnets .

In resnet without sharing parameters , it is quite ambiguous to say whether it is doing representation learning or iterative refinement .

1 . The cosine loss is not meaningful in the sense that the classification layer is trained on the output of the last residual block and fixed . Moving the classification layer to early layers will definitely result in accuracy loss . Even in non-residual network , we can always say that the vector h_{i+ 1} - h_i is refining h_i towards the negative gradient direction . The motivation of iterative inference would be to generate a feature that is easier to classify rather than to match the current fixed classifier . Thus the final classification layer should be retrained for every addition or removal of residual blocks .

2 . The l2 ratio . The l2 ratio is small for higher residual layers , I 'm not sure how much this phenomenon can prove that resnet is actually doing iterative inference .

3 . In section 4.4 it is shown that unrolling the layers can improve the performance of the network . However , the same can be achieved by adding more unshared layers . I think the study should focus more on whether shared or unshared is better .

4 . Section 4.5 is a bit weak in experiments , my conclusion is that currently it is still limited by batch normalization and optimization , the evidence is still not strong enough to show that iterative inference is advantageous / disadvantageous .

The the above said , I think the more important thing is how we can benefit from iterative inference interpretation , which is relatively weak in this paper .

In his second point , reviewer asks why small l2 ratio in higher layers can be seen as a proof for iterative inference . Small l2 ratio suggests that 1st order taylor expansion is accurate . Decreasing l2 ratio from lower to higher layers further supports iterative inference , because model descending down the loss surface should converge towards the end . We made edits in revision to clarify these points .

In his third point , reviewer mentions that it is not clear from 4.4 if adding shared or unshared layers is better . Section 4.5 is devoted to this question and concludes that shared layers do not lead to same gains as unshared . In particular , we observe that naively sharing parameters of the top residual blocks leads both to underfitting ( worse training accuracy than its unshared counterpart ) and overfitting ( given similar training accuracy , the model with shared parameters model has significantly lower validation performances ) .
This issue appears to be related to the activation explosion we observe in the shared Resnet model . We believe that developing better strategy to control the activation norm increase through the layer could help addressing this issue .

In his fourth point , reviewer remarks results of 4.5 are limited by optimization and using batch normalization . We agree in this sense , that optimization of shared residual network seems intrinsically difficult . Given our results from previous sections , we believe that bridging these difficulties in training , will result in a strong shared residual network .

Overall , our novelty is in showing one specific , formal way in which residual networks perform iterative inference , and then exploring it in both unshared , and shared case ( both by training shared network , as well as unrolling unshared network to more steps than it was trained on ) .


We thank reviewer 2 for his thoughtful review . To address the comments we added new plots to paper ( 2 sections in Appendix with references in main text ) , and revised text to clarify mentioned points .

We would like to open by clarifying our definition of iterative inference . Iterative inference ( as specified in “ Formalizing iterative inference section ” ) is defined in our paper as descending the loss surface of the network that is specified by current set of parameters , rather than finding generally better features . Our contribution is showing that residual network maximizes the alignment of block output with steepest direction in the aforementioned loss surface , specified by the network current parameters . The usage of fixed classifier is therefore justifiedby our Taylor expansion . We made these points clearer in revised version , and also added more detailed math derivation .

The central objection of reviewer is our focus on iterative inference without shared weights . Our objective is to study a form of iterative inference ( as specified above ) implemented by regular residual networks . However , we devoted large section of paper to shared residual network . We fully agree there is room for doubt if some of the results transfer to true iterative ( shared ) residual network , and we thank reviewer for this remark . To address this , we plot cosine loss , l2 ratio , and intermediate accuracy on shared residual networks , on which we observe very similar trends to unshared residual network . We posted them anonymously here : https://ibb.co/hd1F9m ( red is close to output , dotted is on validation set ) , and also included them in the revision in appendix .

Now , we respond in detail to each point in turn .

In his first point , reviewer raises objection to using fixed classifier . We would like to stress that our definition of iterative inference is descending down the network loss surface here defined by fixed classifier . We made this point clearer in the revision . We also include plot of all the metrics for “ true ” iterative inference in shared residual network , https://ibb.co/hd1F9m ( red is close to output , dotted is on validation set ) , also added to Appendix .

Next objection is that trivially non-residual network could be seen as revising h_{t + 1 } towards good accuracy , and therefore cosine loss is not a meaningful metric . We are in agreement that in a non-residual network we do expect increase in accuracy . However , we do not expect from non-residual network :
Layer output to be aligned with gradient of loss with respect of hidden state , and perform small iterative refinement . Layer output always should increase accuracy as stated by reviewer , but this is very different from small iterative steps that are aligned with dL / dh .
Layer to generalize when applied to more steps than it was trained ( Sec 4.4 ) , both on train and test distribution
Final layers to focus only on borderline examples ( as specified in text , examples that are close to being either correctly classified , or misclassified )
, which are non-trivial things we report . To further support generalization to more steps we revised text to highlight maintained negative cos loss and reduction of loss on training set . We plot evolution of cosine loss for unrolled steps in appendix : https://ibb.co/f64YC6. To further support claim about iterative refinement , we plot l2 ratio for experiments in 4.3 , 4.4 ( also included in https://ibb.co/f64YC6).


This paper shows that residual networks can be viewed as doing a sort of iterative inference , where each layer is trained to use its “ nonlinear part ” to push its values in the negative direction of the loss gradient . The authors demonstrate this using a Taylor expansion of a standard residual block first , then follow up with several experiments that corroborate this interpretation of iterative inference . Overall the strength of this paper is that the main insight is quite interesting — though many people have informally thought of residual networks as having this interpretation — this paper is the first one to my knowledge to explain the intuition in a more precise way .

Some weaknesses of the paper on the other hand — some of the parts of the paper ( e.g. on weight sharing ) are only somewhat related to the main topic of the paper . In fact , the authors moved the connection to SGD to the appendix , which I thought would be * more * related . Additionally , parts of the paper are not as clearly written as they could be and lack rigor . This includes the mathematical derivation of the main insight — some of the steps should be spelled out more explicitly . The explanation following is also handwavey despite claims to being formal .

Some other lower level thoughts :
* Regarding weight sharing for residual layers , I do n’t understand why we can draw the conclusion that the initial gradient explosion is responsible for the lower generalization capability of the model with shared weights . Are there other papers in literature that have shown this connection ?
* The name “ cosine loss ” suggests that this function is actually being minimized by a training procedure , but it is just a value that is being plotted … perhaps just call it the cosine ?
* I recommend that the authors also check out Figurnov et al CVPR 2017 ( " Spatially Adaptive Computation Time for Residual Networks " ) which proposes an “ adaptive ” version of ResNet based on the intuition of adaptive inference .
* The plots in the later parts of the paper are quite small and hard to read . They are also spaced together too tightly ( horizontally ) , making it difficult to immediately see what each plot is supposed to represent via the y-axis label .
* Finally , the citations need to be fixed ( use \citep{} instead of \cite{} )



We thank reviewer for his positive assessment and useful feedbacks .

In his first point reviewer remarks that sharing residual network is less interesting than studying SGD connection . We agree with reviewer 1 and will move back the connection to SGD in the main text in camera ready version . We do believe that studying weight sharing in residual network is important as well , because it implements ‘ true ’ iterative inference ( i.e. where same function is applied ) .

Next , reviewer suggests we should improve writing of some parts of paper including mathematical derivation . We address this remark in revision of the paper , by making the derivation more explicit ( step from ( 3 ) to ( 4 ) ) .

In the next point reviewer asks if interpretation in 4.5 that gradient explosion leads to overfitting is justified . We would like to clarify we observe both underfitting ( worse training accuracy of the shared model compared to the unshared model ) and overfitting ( given similar training accuracy , the Resnet with shared parameter has significantly lower validation performances ) . We also observe that the activations explode during the forward propagation of the Resnet with shared parameters due to the the repeated application of the same layer . By better controlling the activation norm increase using unshared batch normalization , we show that we can reduce the performance gap between the Resnet with shared parameters and the Resnet with unshared parameters , for both training accuracy and validation accuracy . We have updated the text in section 4.5 to clarify this point .

We thank for reference . We introduced suggested edits in revision . We agree that “ cosine loss ” name can be misleading . For camera ready we will change it .


The authors consider the task of program synthesis in the Karel DSL . Their innovations are to use reinforcement learning to guide sequential generation of tokes towards a high reward output , incorporate syntax checking into the synthesis procedure to prune syntactically invalid programs . Finally they learn a model that predicts correctness of syntax in absence of a syntax checker .

While the results in this paper look good , I found many aspects of the exposition difficult to follow . In section 4 , the authors define objectives , but do not clearly describe how these objectives are optimized , instead relying on the read to infer from context how REINFORCE and beam search are applied . I was not able to understand whether syntactic corrected is enforce by way of the reward introduced in section 4 , or by way of the conditioning introduced in section 5.1 . Discussion of the experimental results coould similarly be clearer . The best method very clearly depends on the taks and the amount of available data , but I found it difficult to extract an intuition for which method works best in which setting and why .

On the whole this seems like a promising paper . That said , I think the authors would need to convincingly address issues of clarity in order for this to appear .

Specific comments

- Figure 2 is too small

- Equation 8 is confusing in that it defines a Monte Carlo estimate of the expected reward , rather than an estimator of the gradient of the expected reward ( which is what REINFORCE is ) .

- It is not clear the how beam search is carried out . In equation ( 10 ) there appear to be two problems . The first is that the index i appears twice ( once in i=1.. N and once in i \in 1..C ) , the second is that λ_r refers to an index that does not appear . More generally , beam search is normally an algorithm where at each search depth , the set of candidate paths is pruned according to some heuristic . What is the heuristic here ? Is syntax checking used at each step of token generation , or something along these lines ?

- What is the value of the learned syntax in section 5.2 ? Presumaly we need a large corpus of syntax - checked training examples to learn this model , which means that , in practice , we still need to have a syntax - checker available , do we not ?

We thank the reviewer for the detailed comments on how to improve the exposition of our paper , which we included in the revised version .

- Figure 2 ’s size was increased to make the model clearer .

- Reinforce vs . Monte Carlo estimate of the expected reward :
Equation 8 indeed describes how we estimate the expected reward , we also added the form of the estimator of the expected gradient for a sample i to make what me meant clearer

Beam search and Approximate probabilities
Equation 10 indeed had a typo . The i in “ i \in 1.. C ” should have been a “ r ” , making the product a product of the probability of the C programs sampled from the approximate probability distribution . At a search depth of d , the heuristic used to prune candidate paths is the probability of the prefix ( which you can think of as the product of equation ( 5 ) but limited to the first d terms ) .

We arrive at a search depth d with a set of S candidates . For each of these S candidates , we obtain the probability of the next token using the softmax . Combining the probability of this token with the product of the whole path that comes before it , we obtain the probability for S * ( nb_possible_token ) possible paths . We only keep the S best ones ( possibly removing the ones that have reached a termination symbol ) and repeat the step at the depth d+ 1 .
We end up at the end with a set of S samples which are going to be used as the basis for our approximate distribution . We have added more description of the process to make it clearer .

When syntax checking is available , whether in its learned form or not , it is implicitly included as its contribution is introduced just before the softmax ( see Figure 2 if you can zoom in ) . A token judged non-syntactically correct would have a probability of zero , so the probability of the path containing it would be zero and would therefore not be included into the promising paths going to the next stage .


- Is there value in learning syntax ?
It might be possible to have access to a large amount of programs in a language without having access to a syntax checker , such as for example if we have downloaded a large amount of programs from a code repository . Moreover , it might be useful even for common languages : Note that what we require is a bit different to a traditional syntax checker : answering the question “ is this program syntactically correct ” , which any compiler would give ; as opposed to what we have in equation 13 which corresponds to “ Do these first t tokens contain no syntax error and may therefore be a valid prefix to a program ” . The syntax checker we need has to return a decision even for non- complete program , therefore it would require some work to transform current compilers to return such answers .
Finally , as shown in our experiments , using a learned syntax checker might perform better than using a formal one , as it can capture what represents an “ idiomatic ” program vs . a technically correct one .





The paper presents a reinforcement learning - based approach for program synthesis . The proposed approach claims two advantages over a baseline maximum likelihood estimation - based approach . MLE - based methods penalize syntactically different but semantically equivalent programs . Further , typical program synthesis approaches do n't explicitly learn to produce correct syntax . The proposed approach uses a syntax - checker to limit the next-token distribution to syntactically - valid tokens .

The approach , and its constituent contributions , i.e. of using RL for program synthesis , and limiting to syntactically valid programs , are novel . Although both the contributions are fairly obvious , there is of course merit in empirically validating these ideas .

The paper presents comparisons with baseline methods . The improvements over the baseline methods is small but substantial , and enough experimental details are provided to reproduce the results . However , there is no comparison with other approaches in the literature . The authors claim to improve the state - of - the-art , but fail to mention and compare with the state - of - the - art , such as [ 1 ] . I do find it hard to trust papers which do not compare with results from other papers .

Pros :
1 . Well - written paper , with clear contributions .
2 . Good empirical evaluation with ablations .

Cons :
1 . No SOTA comparison .
2 . Only one task / No real - world task , such as Excel Flashfill .

[ 1 ] : " Neural Program Meta-Induction " , Jacob Devlin , Rudy Bunel , Rishabh Singh , Matthew Hausknecht , Pushmeet Kohli

We thank the reviewer for the comments on the paper , and for pointing out the missing related work .

It is difficult to perform an exact comparison between the two papers as they are solving two different problems . The model developed in [ 1 ] ( Devlin et al , 2017 ) performs program induction : i.e. it produces the output world on a new input world where the desired program semantics are encoded in the network itself . On the other hand , in our case , we perform program synthesis , i.e. generate a program in the Karel DSL that performs the desired transformation from input to output .

Using the terminology of Devlin et al. , what we describe is closest to the meta-induction approach : strong cross task knowledge sharing but no task specific learning . Overall , our MLE baseline architecture will correspond to the Devlin et al . meta-induction architecture if the decoder was trained to generate program tokens instead of output worlds . This precision was added to the paper

-> No real - world task such as FlashFill ?
The FlashFill DSL considered in previous neural program synthesis work such as Robust Fill is essentially a functional language comprising of compositions of a sequence of functions . In this work , we wanted to increase the complexity of the DSL one step further to better understand what neural architectures are more appropriate for learning programs with such complexity . Concretely , the Karel DSL consists of complex control - flow such as nested loops and conditionals , which are not present in the FlashFill DSL . The difference of performance of meta-induction on FlashFill ( ~ 70 % from Figure 7 of [ 2 ] ) vs. KarelDSL ( ~ 40 % from Figure 4 of [ 1 ] ) points towards Karel being a more complex dataset .

Learning Karel programs can also be considered close to a real - world task as this language is used to teach introductory programming to Stanford students , and the program synthesis models can be used to help students if they are having difficulty in writing correct programs .


[ 1 ] Jacob Devlin , Rudy Bunel , Rishabh Singh , Matthew Hausknecht , Pushmeet Kohli . Neural Program Meta-Induction . In NIPS , 2017
[ 2 ] Jacob Devlin , Jonathan Uesato , Surya Bhupatiraju , Rishabh Singh , Abdel -rahman Mohamed , and Pushmeet Kohli . Robustfill : Neural program learning under noisy I /O . In ICML , 2017


This is a nice paper . It makes novel contributions to neural program synthesis by ( a ) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and ( b ) using a syntax checker ( or a learned approximation thereof ) to prevent the synthesizer from outputting any syntactically - invalid programs , thus pruning the search space . In experiments , the proposed method synthesizes correct Karel programs ( non-trivial programs involving loops and conditionals ) more frequently than synthesizers trained using only maximum likelihood supervised training .

I have a few minor questions and requests for clarification , but overall the paper presents strong results and , I believe , should be accepted .


Specific comments / questions follow :


Figure 2 is too small . It would be much more helpful ( and easier to read ) if it were enlarged to take the full page width .

Page 7 : " In the supervised setting ... " This suggests that the syntax LSTM can be trained without supervision in the form of known valid programs , a possibility which might not have occurred to me without this little aside . If that is indeed the case , that 's a surprising and interesting result that deserves having more attention called to it ( I appreciated the analysis in the results section to this effect , but you could call attention to this sooner , here on page 7 ) .

Is the " Karel DSL " in your experiments the full Karel language , or a subset designed for the paper ?

For the versions of the model that use beam search , what beam width was used ? Do the results reported in e.g . Table 1 change as a function of beam width , and if so , how ?


We thank the reviewer for the comments on the part of the paper that need clarification . We incorporated his feedback in the new version . Here are answers to the raised questions :

-> Figure 2 is too small
The size of Figure 2 was increased to make it full page width .

-> “ The syntaxLSTM can be trained without supervision in the form of known valid programs ”
While the syntax LSTM can be trained without access to known valid programs when RL - based training is employed ( because gradient will flow to it through the softmax defining the probability of each token ) , we point out in the experiments section that we were n’t successful in training models using only RL training . As a result , it would not be accurate to claim that it can be trained without any valid programs as supervision .

-> Is the Karel DSL the full Karel language ?
The exact description of our DSL is in Appendix B . It does n’t exactly match the full Karel language , as most notably there is no possibility to define subroutines . We made this clearer in the paper .

-> What was the beam width used ?
All of our experiments used a beam width of 64 . We did n’t study the effect of this hyperparameter and chose it as the maximum width we could afford based on available GPU memory . In the limit , using an extremely large beam size would be equivalent to computing the complete sum for the expected reward but this is not feasible for any applications where program would be longer than a few tokens .


This paper investigates a new approach to prevent a given classifier from adversarial examples . The most important contribution is that the proposed algorithm can be applied post-hoc to already trained networks . Hence , the proposed algorithm ( Stochastic Activation Pruning ) can be combined with algorithms which prevent from adversarial examples during the training .

The proposed algorithm is clearly described . However there are issues in the presentation .

In section 2 - 3 , the problem setting is not suitably introduced .
In particular one sentence that can be misleading :
“ Given a classifier , one common way to generate an adversarial example is to perturb the input in direction of the gradient … ”
You should explain that given a classifier with stochastic output , the optimal way to generate an adversarial example is to perturb the input proportionally to the gradient . The practical way in which the adversarial examples are generated is not known to the player . An adversary could choose any policy . The only thing the player knows is the best adversarial policy .

In section 4 , I do not understand why the adversary uses only the sign and not also the value of the estimated gradient . Does it come from a high variance ? If it is the case , you should explain that the optimal policy of the adversary is approximated by “ fast gradient sign method ” .

In comparison to dropout algorithm , SAP shows improvements of accuracy against adversarial examples . SAP does not perform as well as adversarial training , but SAP could be used with a trained network .

Overall , this paper presents a practical method to prevent a classifier from adversarial examples , which can be applied in addition to adversarial training . The presentation could be improved .


We thank Reviewer2 for thorough comments . We are glad that the reviewer appreciated the value of an adversarial defense technique that can be applied post-hoc and our exposition . We reply to specific points below :

1 . You are correct , the defender does not know the what policy any actual adversary will use , only what the optimal adversary might do , thus the objective of minimizing the worst - case performance . We are improving the draft to be clearer in this regard .

2 . Regarding : “ In section 4 , I do not understand why the adversary uses only the sign and not also the value of the estimated gradient . ” : The reason why we are considering only the sign is because we cap the infinity norm of the adversarial perturbation . This leads to taking a step of equal size in each input dimension and thus the gradient magnitude does not come into play . This approach is standard in the recent academic study of adversarial examples and follows work by Goodfellow et al . ( 2014 ) , which showed that imperceptible adversarial examples could be produced efficiently in this manner ..

One motivation for considering the infinity norm ( vs L2 or L1 ) for constraining the size of an adversarial perturbation is that it accords more closely with perceptual similarity . For example , it ’s possible to devise a perturbation with small L2 norm that is perceptually obvious because it moves a small group of pixels a large amount .

Naturally , a stronger adversary might pursue an iterative approach rather than making one large perturbation . To this end , we are currently running experiments with iterative attacks and the initial results are promising - SAP continues to significantly outperform the dense model . We will add these results to the paper when they are ready .

3 . We are grateful for the reviewer ’s suggestions for improving the exposition and are currently working to revise the draft in accordance with these recommendations . To start , we have improved some of the ( previously ) confusing language that might have failed to distinguish between the optimal adversary and some arbitrary adversary which may not apply the optimal perturbation .


This paper propose a simple method for guarding trained models against adversarial attacks . The method is to prune the network ’s activations at each layer and renormalize the outputs . It ’s a simple method that can be applied post- training and seems to be effective .

The paper is well written and easily to follow . Method description is clear . The analyses are interesting and done well . I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods .

Could you elaborate more on the findings from Fig 1.c Seems that the DENSE model perform best against randomly perturbed images . Would be good to know if the authors have any intuition why is that the case .

There are some interesting analysis in the appendix against some other methods , it would be good to briefly refer to them in the main text .

I would be interested to know more about the intuition behind the proposed method . It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method .

Also would like to see some notes about computation complexity of sampling multiple times from a larger multinomial .

Again I am not familiar about different kind of existing adversarial attacks , the paper seem to be mainly focus on those from Goodfellow et al 2014 . Would be good to see the performance against other forms of adversarial attacks as well if they exist .

Thanks for your clear review of our paper . We are glad that you appreciated both the method and the clarity of exposition .

1 . Regarding Fig 1.c : While dense models are susceptible to adversarial attack , they are actually quite robust to random noise . The purpose of reporting the results of this experiment is to provide context for the other results . Because dense models are not especially vulnerable to random noise , we are not surprised that they perform well here .

2 . Thanks for the suggestion that the analysis in the appendix should be summarized within the body of the paper . Per your request , we have added an additional subsection ( 5.3 ) in the current draft that briefly describes the baselines and we have included a corresponding figure that shows the quantitative results for each .

3 . While we are reluctant to present an explanation for a phenomena that we do not fully understand , we are happy to share the intuitions that guided us in developing the algorithm :

Originally we were looking sparsifying the weights and / or activations of the network . We were encouraged by results , e.g. https://arxiv.org/abs/1510.00149, showing high accuracy with sparsified weights ( as by pruning ) . We thought that by sparsifying a network , we might maintain high accuracy while lowering the Lipschitz constant and thus conferring some robustness against small perturbations . We later drew some inspiration from randomized algorithms that sparsify matrices by randomly dropping entries according to their weights and scaling up the survivors to produce a sparse matrix with similar spectral properties to the original .

4 . Sampling from the multinomial is fast . Without getting into detail about how many random bits are needed , given uniform samples , we can convert to a sample from a multinomial by performing a binary search . So it ’s roughly k log ( n ) where k is the number of samples and n is the number of activations . As a practical concern , sampling from the multinomial in our algorithms does not comprise a significant computational obstacle .

5 . As you correctly point out , In our experiments , we adopt approach from Goodfellow et al. of evaluating with adversarial perturbations produced by taking a single step with capped infinity norm . However , we generate these attacks differently for each model . Against our stochastic models , the adversary produces the attack by estimating the gradient with MC samples .

6 . Per your suggestions we have compared against a stronger modes of attack , namely an iterative update where we take multiple small updates , each of capped infinity norm . In these experiments , SAP continues to outperform the dense model significantly . We are currently compiling these results and will add them to the draft when ready .


The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights / activations . Empirically the authors demonstrate , on two different task domains , that one can trade off some accuracy for a little robustness -- qualitatively speaking .

On one hand , the approach is simple to implement and has minimal impact computationally on pre-trained networks . On the other hand , I find it lacking in terms of theoretical support , other than the fact that the added stochasticity induces a certain amount of robustness . For example , how does this compare to random perturbation ( say , zero - mean ) of the weights ? This adds stochasticity as well so why and why not this work ? The authors do not give any insight in this regard .

Overall , I still recommend acceptance ( weakly ) since the empirical results may be valuable to a general practitioner . The paper could be strengthened by addressing the issues above as well as including more empirical results ( if nothing else ) .



Thanks for the thoughtful review of our paper . We are glad that you recognize the empirical strength of the result and the simplicity of the method . We are also share your desire for greater theoretical understanding .

Regarding : “ how does this compare to random perturbation ( say , zero - mean ) of the weights ? ” .
We ran this experiment , and found that it did not help . Additionally , for a more direct comparison , we compared against zero-mean Gaussian noise applied to the activations . We call this method Random Noisy Activations ( RNA ) . It was previously described only in Appendix B , but we have now added a brief description to section 5 and reported the quantitative results in Figure 5 .

Despite extensive empirical study , precisely why our method works but random noise on the activations does not remains unclear . While we can imagine some ways of spinning a theoretical story post-hoc , the honest answer is that we do not yet possess a solid theoretical explanation . We share your desire for a greater understanding and plan to investigate this direction further in future work .

*** TL ; DR : Per your suggestions , we have improved the draft by running additional experiments . Please find in Figure 5 results for 0 - mean gaussian noise applied to weights with sigma values {.01 , .02 , … , .05 } , as well as results for several other sensible baselines and greater detail in Appendix B .***


SUMMARY :

The authors reinvent a 20 years old technique for adapting a global or component - wise learning rate for gradient descent . The technique can be derived as a gradient step for the learning rate hyperparameter , or it can be understood as a simple and efficient adaptation technique .


GENERAL IMPRESSION :

One central problem of the paper is missing novelty . The authors are well aware of this . They still manage to provide added value .
Despite its limited novelty , this is a very interesting and potentially impactful paper . I like in particular the detailed discussion of related work , which includes some frequently overlooked precursors of modern methods .


CRITICISM :

The experimental evaluation is rather solid , but not perfect . It considers three different problems : logistic regression ( a convex problem ) , and dense as well as convolutional networks . That 's a solid spectrum . However , it is not clear why the method is tested only on a single data set : MNIST . Since it is entirely general , I would rather expect a test on a dozen different data sets . That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \alpha_0 and \beta .

The extensions in section 5 do n't seem to be very useful . In particular , I can not get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem . Analyzing the actual adaptive algorithm would be very interesting . In contrast , the present result is trivial and of no interest at all , since it requires knowing a good parameter setting , which defeats a large part of the value of the method .


MINOR POINTS :

page 4 , bottom : use \citep for Duchi et al . ( 2011 ) .

None of the figures is legible on a grayscale printout of the paper . Please do not use color as the only cue to identify a curve .

In figure 2 , top row , please display the learning rate on a log scale .

page 8 , line 7 in section 4.3 : " the the " ( unintended repetition )

End of section 4 : an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something ?


> One central problem of the paper is missing novelty . The authors are well aware of this . They still manage to provide added value . Despite its limited novelty , this is a very interesting and potentially impactful paper . I like in particular the detailed discussion of related work , which includes some frequently overlooked precursors of modern methods .

Thank you very much for your evaluation and encouraging words .

> The experimental evaluation is rather solid , but not perfect . It considers three different problems : logistic regression ( a convex problem ) , and dense as well as convolutional networks . That 's a solid spectrum . However , it is not clear why the method is tested only on a single data set : MNIST . Since it is entirely general , I would rather expect a test on a dozen different data sets . That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \alpha_0 and \beta .

Please note that we provide experimental evaluation on a non-MNIST data set , specifically CIFAR - 10 ( Section 4.3 on page 8 and Figure 2 on page 7 ) .

> The extensions in section 5 do n't seem to be very useful . In particular , I can not get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem . Analyzing the actual adaptive algorithm would be very interesting . In contrast , the present result is trivial and of no interest at all , since it requires knowing a good parameter setting , which defeats a large part of the value of the method .

We agree with your assessment that the analysis in Section 5.1 is significantly restricted and this is a limitation of the current paper . There remains much to be done in this respect , and a theoretical convergence analysis is a highly desired future work . Please note that a convergence analysis of the technique in the multidimensional quadratic case is available in a separate work , which we will highlight prominently in the de-anonymized final revision of the paper .

> MINOR POINTS

Thank you for pointing these out , we will fix them in the final revision .


The authors consider a method ( which they trace back to 1998 , but may have a longer history ) of learning the learning rate of a first - order algorithm at the same time as the underlying model is being optimized , using a stochastic multiplicative update . The basic observation ( for SGD ) is that if \ theta_{t + 1 } = \theta_t - \alpha \nabla f( \theta_t ) , then \partial / \partial\alpha f ( \theta_{t + 1 } ) = - < \nabla f ( \theta_t ) , \ nabla f ( \theta_{t + 1 } ) > , i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate \alpha .

I have seen this before for SGD ( the authors do not claim that the basic idea is novel ) , but I believe that the application to other algorithms ( the authors explicitly consider Nesterov momentum and ADAM ) are novel , as is the use of the multiplicative and normalized update of equation 8 ( particularly the normalization ) .

The experiments are well - presented , and appear to convincingly show a benefit . Figure 3 , which explores the robustness of the algorithms to the choice of \alpha_0 and \beta , is particularly nicely - done , and addresses the most natural criticism of this approach ( that it replaces one hyperparameter with two ) .

The authors highlight theoretical convergence guarantees as an important future work item , and the lack of them here ( aside from Theorem 5.1 , which just shows asymptotic convergence if the learning rates become sufficiently small ) is a weakness , but not , I think , a critical one . This appears to be a promising approach , and bringing it back to the attention of the machine learning community is valuable .

> I have seen this before for SGD ( the authors do not claim that the basic idea is novel ) , but I believe that the application to other algorithms ( the authors explicitly consider Nesterov momentum and ADAM ) are novel , as is the use of the multiplicative and normalized update of equation 8 ( particularly the normalization ) .

> The experiments are well - presented , and appear to convincingly show a benefit . Figure 3 , which explores the robustness of the algorithms to the choice of \alpha_0 and \beta , is particularly nicely - done , and addresses the most natural criticism of this approach ( that it replaces one hyperparameter with two ) .

Thank you very much for your evaluation and your encouraging feedback .

Figure 3 was produced with exactly the purpose that you described , and we are very glad that this was noticed and found useful .

> The authors highlight theoretical convergence guarantees as an important future work item , and the lack of them here ( aside from Theorem 5.1 , which just shows asymptotic convergence if the learning rates become sufficiently small ) is a weakness , but not , I think , a critical one . This appears to be a promising approach , and bringing it back to the attention of the machine learning community is valuable .

We agree that a theoretical convergence analysis is a highly desired future work and is a limitation of the current paper . We also agree with the assessment that the approach appears promising and therefore we would like to bring it to the attention of the larger community .



This paper revisits an interesting and important trick to automatically adapt the stepsize . They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize . Such simple trick alleviates the effort in tuning stepsize , and can be incorporated with popular stochastic first - order optimization algorithms , including SGD , SGD with Nestrov momentum , and Adam . Surprisingly , it works well in practice .

Although the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick , considering their performance , I vote for acceptance . But before that , there are several issues need to be addressed .

1 , the derivation of the update of \alpha relies on the expectation formulation . I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick .

2 , The derivation of the multiplicative rule of HD relies on a reference I can not find . Please include this part for self - containing .

3 , As the authors claimed , the Maclaurin et.al . 2015 is the most related work , however , they are not compared in the experiments . Moreover , the empirical comparisons are only conducted on MNIST . To be more convincing , it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet .

Minors :

In the experiments results figures , after adding the new trick , the SGD algorithms become more stable , i.e. , the variance diminishes . Could you please explain why such phenomenon happens ?

Thank you for your encouraging evaluation and for the improvements suggested .

> 1 , the derivation of the update of \alpha relies on the expectation formulation . I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick .

We do not have theoretical results about the effect of the minibatch size and gradient variance on the hypergradient descent ( HD ) algorithm . Considering that the reviewer was potentially referring to experimental evidence , we will make sure to include experimental results with varying minibatch sizes in an appendix in the final revision of this paper .

> 2 , The derivation of the multiplicative rule of HD relies on a reference I can not find . Please include this part for self - containing .

Thank you for pointing this out . The mentioned reference for the multiplicative HD rule is now made accessible online , and can be located with a Google search of the title .

> 3 , As the authors claimed , the Maclaurin et.al . 2015 is the most related work , however , they are not compared in the experiments . Moreover , the empirical comparisons are only conducted on MNIST . To be more convincing , it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet .

As you point out , Maclaurin et al. ( 2015 ) is a highly related work , which introduces the term “ hypergradient ” and similarly performs gradient - based updates of hyperparameters through a reversible higher - order automatic differentiation setup .

However , note that in the approach in Maclaurin et al. ( 2015 ) a regular optimization procedure is truncated to a fixed number N of “ elementary ” iterations ( such as N = 100 in the paper ) , at the end of which the derivative of an objective is propagated all the way through this N inner optimization iterations ( the “ reversibility ” trick introduced in the paper is for making this possible in practice ) , and the resulting hypergradient is used in an outer optimization of M “ meta ” iterations ( such as M=50 in the paper ) . Our technique , in contrast , is an online adaptation of a hyperparameter ( in particular , the learning rate ) at each iteration of optimization , and does not perform derivative propagation through an inner optimization that consists of many iterations . The techniques are thus not directly comparable as competing alternatives . For instance , it is not straightforward to replicate our learning rate trajectory through the VGGNet / CIFAR - 10 experiment of 78125 iterations ( Figure 2 on page 7 , rightmost column ) in the reversible learning algorithm due to ( 1 ) uninformative gradients beyond a few hundred iterations ( see Section 4 “ Limitations ” in Maclaurin et al. 2015 ) and ( 2 ) potentially prohibitive memory requirements . Having said this , we believe that it would be interesting to compare the behavior of our algorithm for the initial 100 iterations with the 100 - iteration learning - rate schedules reported in Maclaurin et al. ( 2015 ) and we intend to add such an experiment in the appendix in the final revision of the paper .

> Moreover , the empirical comparisons are only conducted on MNIST .

Please note that the paper does report non-MNIST empirical comparisons , specifically CIFAR - 10 ( Section 4.3 on page 8 and Figure 2 on page 7 ) .

> Minors : In the experiments results figures , after adding the new trick , the SGD algorithms become more stable , i.e. , the variance diminishes . Could you please explain why such phenomenon happens ?

As far as we can observe , the variance does not diminish , and the method behaves in a similar way to how regular SGD does with a good choice of the learning rate , as for example 10e - 2 in the case of logistic regression . We would be interested in looking into this more carefully if you could point us to an experiment / figure where this behavior with SGD happens .

Thank you once more for all these constructive comments and suggested additions that allow us to improve the paper .


The paper universal value function type ideas to learn models of how long the current policy will take to reach various states ( or state features ) , and then incorporates these into model - predictive control . This looks like a reasonable way to approach the problem of model - based RL in a way that avoids the covariate shift produced by rolling learned transition models forward in time . Empirical results show their method outperforming Hindsight Experience Replay ( which looks quite bad in their experiments ) , DDPG , and more traditional model - based learning . It also outperforms DDPG quite a bit in terms of sample efficiency on a real robotic arm . They also show the impact of planning horizon on performance , demonstrating a nice trade - off .

There are however a couple of relevant existing papers that the authors miss referencing / discussing :
- " Reinforcement Learning with Unsupervised Auxiliary Tasks " ( Jarderberg et al , ICLR 2017 ) - uses predictions about auxiliary tasks , such as effecting maximum pixel change , to obtain much better sample efficiency .
- " The Predictron : End- To- End Learning and Planning " ( Silver et al , ICML 2017 ) , which also provides a way of interpolating between model - based and model - free RL .

I do n't believe that these pieces of work subsume the current paper , however the authors do need to discuss the relationship their method has with them and what it brings .

** UPDATE Jan 9 : Updated my rating in light of authors ' response and updated version . I recommend that the authors find a way to keep the info in Section 4.3 ( Dynamic Goal and Horizon Resampling ) in the paper though , unless I missed where it was moved to . **


Thank you for your feedback ! We have edited the paper to address all of the issues that you ’ve raised ( see below ) . We would appreciate any further feedback that you might have to provide .

We have added a discussion of the two papers suggested ( Jaderberg et al ICML 2017 , Silver et al , ICML 2017 ) to the paper in the related work section ( Section 5 , paragraph 6 ) . Our method shares the same motivation as those papers : to increase the amount of supervision in model - free RL to achieve sample - efficient learning . We also include recent work on distributional RL ( Bellemare et. al. , 2017 ) as another example of this general idea .

We were able to obtain the code for hindsight experience replay ( HER ) from the original authors . Using this code as reference , we improved our implementation by incorporating their hyperparameter settings and implementation details , including ones that we had difficulty deducing from the original HER paper . The performance of HER on our tasks did improve , and we have updated Figure 2 with the new results . At this point , with the help of the original authors , we are confident that our implementation of HER is accurate . An observation we would like to make is that the purpose of HER is not necessarily to improve the sample efficiency of tasks where dense rewards are sufficient for DDPG to learn . Rather , a big selling point of HER is that it can improve the asymptotic performance of DDPG in tasks with sparse rewards . To test this hypothesis , we ran an additional baseline of DDPG with sparse rewards ( - 1 if the goal state is not reach , 0 if it is ) . HER definitively outperforms this baseline , so our results confirm that HER helps with sparse- reward tasks . We wanted to improve the sample efficiency of DDPG , and not necessarily DDPG ’s feasibility for sparse- reward tasks , and so we focused on tasks that DDPG could already solve , albeit after many samples . It may be that the benefits of HER shine through on tasks where dense rewards will not lead to good policies .


This paper proposes a " temporal difference model learning " , a method that aims to combine the benefits of model - based and model - free RL . The proposed method essentially learns a time - varying goal - conditional value function for a specific reward formulation , which acts as a surrogate for a model in an MPC - like setting . The authors show that the method outperforms some alternatives on three continuous control domains and real robot system .

I believe this paper to be borderline , but ultimately below the threshold for acceptance . On the positive side , there are certainly some interesting ideas here : the notion of goal - conditioned value functions as proxies for a model , and as a means of merging model - free and model - based approaches is very really interesting , and hints at a deeper structure to goal - conditioned value functions in general . Ultimately , though , I feel that there are two main issues that make this research feel as though it is still ultimately in the earlier stages : 1 ) the very large focus on the perspective that this approach is unifying model - based and model - free RL , when it fact this connection seems a bit tenuous ; and 2 ) the rather lackluster experimental results , which show only marginal improvement over purely model - based methods ( at the cost of much additional complexity ) , and which make me wonder if there 's an issue with their implementation of prior work ( namely the Highsight Experience Replay algorithm ) .

To address the first point , although the paper stresses it to a very high degree , I ca n't help but feel that the connection that the claimed advance of " unifying model - based and model - free RL " is overstated . As far as I can tell , the connection is as follows : the learned quantity here is a time - varying goal - conditioned value function , and under some specific definition of reward , we can interpret the constraint that this value function equal zero as a proxy for the dynamics constraint in MPC . But the exact correspondence between this and the MPC formulation only occurs for a horizon of size zero : longer horizons require a multi-step MPC for the definition of the model - free and model - based correspondence . The fact that the action selection of a model - based method and this approach have some function which looks similar ( but only under certain conditions ) , just seems like a fairly odd connection to highlight so heavily .

Rather , it seems to me that what 's happening here is really quite simple : the authors are extending goal - conditioned value functions to the case of non-stationary finite horizon value functions ( the claimed " key insight " in eq ( 5 ) is a completely standard finite- horizon MDP formulation ) . This seems to describe perfectly well what is happening here , and it does also seem intuitive that this provides an advantage over stationary goal - conditioned value functions : just as goal conditioned value functions offer the advantage of considering " every state as a goal " , this method can consider " every state as a goal for every time horizon " . This seems interesting enough on its own , and I admit I do n't see the need for the method to be yet another claimed unification of model - free and model - based RL.

I would also suggest that the authors look into the literature on how TD methods implicitly learn models ( see e.g. Boyan 1997 " Least-squares temporal difference learning " , and Parr et al. , 2007 " An analysis of linear models ... " ) . In these works it has been shown that least squares TD methods ( at least in the linear feature setting ) , implicitly learn a dynamics model in feature space , but only the " projection " of the reward function is actually needed to learn the TD weights . In building the proposed value functions , it seems like the authors are effectively solving for multiple rewards simultaneously , which would effectively preserve the learned dynamics model . I feel like this may be an interesting line of analysis for the paper if the authors _do_ want to stick with the notion of the method as unifying model - free and model - based RL.

All these points may ultimately just be a matter of interpretation , though , if not for the second issue with the paper , which is that the results seem quite lackluster , and the claimed performance of HER seems rather suspicious . But instead , the authors evaluate the algorithm on just three continuous control tasks ( and a real robot , which is more impressive , but the task here is still so extremely simple for a real robot system that it really just qualifies as a real - world demonstration rather than an actual application ) . And in these three settings , a model - based approach seems to work just as well on two of the tasks , and may soon perform just as well after a few more episodes on the last task ( it does n't appear to have converged yet ) . And despite the HER paper showing improvement over traditional policy approaches , in these experiments plain DDPG consistently performs as well or better than HER .

Thank you for your feedback !

To address the reviewer ’s concerns about the experiments , we ran our algorithm on more difficult tasks and have updated the experimental results . We would like to emphasize is that a primary goal of our method is to achieve both sample - efficiency and good final performance . While the asymptotic performance of TDMs may not always be far better than that the model - free methods , TDMs are substantially more sample - efficient , as shown in the updated Figure 2 . We consider this important , as sample efficiency is important in many real - world tasks , such as robotics , where collecting data is expensive . For the model - based baseline , a concern brought up was that , “ model - based approach … may soon perform just as well after a few more episodes on the last task ( it does n't appear to have converged yet ) . ” We ran the half - cheetah experiments for more iterations and see that the trend is the same : TDM converges to a better solution than the model - based baseline , by a significant margin . We also evaluated our method on two substantially more complex 3D locomotion tasks using the “ ant ” quadrupedal robot . We tested two tasks : asking the ant to run to a target location , and asking it to achieve a target location at the same time as a target velocity ( this is meant to be representative , e.g. , of the ant attempting to make a jump ) . The latter task is particularly interesting , since the ant cannot maintain both the position and velocity goal at the same time , and must therefore achieve it at a single time step . TDMs significantly outperform the model - based baseline on these tasks as well . This supports the central claim of the paper , in Section 1 , paragraph 5 , which is that TDMs achieve learning times comparable to model - based methods , but asymptotic performance that is comparable to model - free algorithms . We believe that this kind of improvement in learning performance is of substantial interest to the reinforcement learning community .

We were able to obtain the code for hindsight experience replay ( HER ) from the original authors . Using this code as reference , we improved our implementation by incorporating their hyperparameter settings and implementation details , including ones that we had difficulty deducing from the original HER paper . The performance of HER on our tasks did improve , and we have updated Figure 2 with the new results . At this point , with the help of the original authors , we are confident that our implementation of HER is accurate . An observation we would like to make is that the purpose of HER is not necessarily to improve the sample efficiency of tasks where dense rewards are sufficient for DDPG to learn . Rather , a big selling point of HER is that it can improve the asymptotic performance of DDPG in tasks with sparse rewards . To test this hypothesis , we ran an additional baseline of DDPG with sparse rewards ( - 1 if the goal state is not reach , 0 if it is ) . HER definitively outperforms this baseline , so our results confirm that HER helps with sparse- reward tasks . We wanted to improve the sample efficiency of DDPG , and not necessarily DDPG ’s feasibility for sparse- reward tasks , and so we focused on tasks that DDPG could already solve , albeit after many samples . It may be that the benefits of HER shine through on tasks where dense rewards will not lead to good policies .

We appreciate your comments regarding the connection between model - based and model - free RL . In this paper , we presented two main contributions : one is a connection between model - free and model - based reinforcement learning , and another is an algorithm derived from this connection . We have edited the paper ( throughout the paper , and notably in Section 3.2 , Paragraph 2 ) to balance the presentation better between these two components , and to avoid overstating the connection . We would be happy to incorporate any other concrete suggestions you might have .

Thank you for the references to the earlier work connecting TD - methods and model - based methods . We have added a discussion to this work in the related works ( Section 5 , paragraph 2 ) . While these papers also show a connection between TD - methods and model - based methods , their objective is rather different from ours . Boyan shows an exact equivalence between a learned model and learned value function , but this requires a tabular value function which effective keeps track of every state-action - next-state transition . Parr shows that for linear function approximators , a value function extracted using a learned model is the same as a value function learned with TD - learning . Rather than analyzing equivalence at convergence , our primary contribution is how we can achieve sample complexity comparable to model - based RL while retaining the favorable asymptotic performance of model - free RL in complex tasks with function approximation .

We believe that we have addresses all the issues raised by the reviewer . We would be happy to discuss and address any additional concerns .

This is an interesting direction . There is still much to understand about the relative strengths and limitations of model based and model free techniques , and how best to combine them , and this paper discusses a new way to address this problem . The empirical results are promising and the ablation studies are good , but it also makes me wonder a bit about where the benefit is coming from .

Could you please put a bit of discussion in about the computational and memory cost . TDM is now parameterized with ( state , action ( goal ) state , and the horizon tau ) . Essentially it is now computing the distance to each possible goal state after starting in state ( s , a ) and taking a fixed number of steps .
It seems like this is less compact than learning a 1 - step dynamics model directly .
The results are better than models in some places . It seems likely this is because the model - based approach referenced does n’t do multi-step model fitting , but essentially TDM is , by being asked to predict and optimize for C steps away . If models were trained similarly ( using multi-step loss ) would models do as well as TDM ?
How might this be extended to the stochastic setting ?


Thank you for your feedback !

One concern brought up is the computation and memory cost of using TDMs . To address this concern , we have added discussion of this point in Section 4.3 , paragraph 2 , as well as Figure 4 in the appendix . In short , the learning for TDM and DDPG both have the number of updates per environment step as a hyperparameter , which largely determines the computation cost . The empirical result we got is that as we increased the number of updates , the performance of TDM increased while the performance of DDPG stayed the same or degraded . TDMs can benefit from more computation ( number of updates per environment step ) than DDPG since they can learn a lot more by relabeling goal states and horizon tau ; we see this more as a benefit as it means TDMs can extract more information from the same amount of data . We also would like to point out that one advantage of doing more computation at training time is that test time is relatively fast : to do multi-step planning , we simply set tau=5 in our TDM , whereas a typical multi-step model - based planning approach would need to unroll a model over five time steps and optimize over all intermediate actions . Furthermore , we hope that this , in addition to the ablative studies in section 6.2 , addresses the concern that , “ … it also makes me wonder a bit about where the benefit is coming from . ”

For stochastic environments , the TDM would learn the expected distance to the goal , rather than the exact distance . We have added this discussion to the second paragraph of the conclusion .

We agree that a more in - depth discussion of the connection to multi-step models would be appropriate . We ’ve added discussion of two related works in Section 5 , paragraph 4 . One critical distinction between these methods and TDMs is that TDMs can be viewed as goal conditioned models : the prediction is made T steps into the future , conditioned on a policy that is trying to reach a particular state . Most model learning methods do not condition on a policy , requiring them to take in an entire sequence of future actions , which greatly increases the input space .

The authors propose a generative method that can produce images along a hierarchy of specificity , i.e. both when all relevant attributes are specified , and when some are left undefined , creating a more abstract generation task .

Pros :
+ The results demonstrating the method 's ability to generate results for ( 1 ) abstract and ( 2 ) novel / unseen attribute descriptions , are generally convincing . Both quantitative and qualitative results are provided .
+ The paper is fairly clear .

Cons :
- It is unclear how to judge diversity qualitatively , e.g. in Fig. 4 ( b ) .
- Fig. 5 could be more convincing ; " bushy eyebrows " is a difficult attribute to judge , and in the abstract generation when that is the only attribute specified , it is not clear how good the results are .


We are glad the reviewer thought that our paper was clear and has convincing results . Below we provide some clarifications to address the cons raised by the reviewer :

1 . How to judge diversity in Fig. 4 b ) :
The new version provides a clearer caption to Fig. 4 b ) explaining the qualitative results for MNIST -A . Hopefully this should make judging diversity easier .

2 . Explanation for Fig. 5 :
We have updated the text on Page. 9 , to better explain the qualitative results for CelebA , in addition to further examples in Appendix A.9.

Further , we would like to point out a subtle distinction between attributes which are ‘ absent ’ and attributes which are ‘ unspecified ’ . ‘ Absence ’ means that the value of an attribute is set to 0 , while unspecified means that we do n’t know whether the attribute should be set ( 1 ) or unset ( 0 ) . Thus , for Fig. 5 , there are a total of 16 attributes specified ( 15 unset and 1 set ) ( for the most abstract generation example , male=* , smiling= * ) , as opposed to just ‘ bushy eyebrows ’ being specified ( which is one of the premises of the reviewer ’s concern ) .


This paper presented a multi-modal extension of variational autoencoder ( VAE ) for the task " visually grounded imagination . " In this task , the model learns a joint embedding of the images and the attributes . The proposed model is novel but incremental comparing to existing frameworks . The author also introduced new evaluation metrics to evaluate the model performance concerning correctness , coverage , and compositionality .

Pros :
1 . The paper is well - written , and the contribution ( both the model and the evaluation metric ) potentially can to be very useful in the community .
2 . The discussion comparing the related work / baseline methods is insightful .
3 . The proposed model addresses many important problems , such as attribute learning , disentanged representation learning , learning with missing values , and proper evaluation methods .

Cons / questions :
1 . The motivation of the model choice of q is not clear .
Comparing to BiVCCA , apart from the differences that the author discussed , a big difference is the choice of q. BiVCCA uses two inference networks q( z|x ) and q( z|y ) , while the proposed method uses three . q( z|x ) , q ( z|y ) , and q( z|x , y ) . How does such model choice affect the final performance ?

2 . Baselines are not necessarily sufficient .
The paper compared the vanilla version of BiVCCA but not the one with factorized representation version . In the original VAECCA paper , the extension of using factorized representation ( private and shared ) improved the performance ] . The author should also compare this extension of VAECCA .

3 . Some details are not clear .
a ) How to set / learn the scaling parameter \lambda_y and \beta_y ? If it is set as hyper - parameter , how does the performance change concerning them ?
b ) Discussion of the experimental results is not sufficient . For example , why JMVAE performs much better than the proposed model when all attributes are given . What is the conclusion from Figure 4 ( b ) ? The JMVAE seems to generate more diverse ( better coverage ) results which are not consistent with the claims in the related work . The same applies to figure 5 .


We are glad the reviewer thought that our contribution would be useful to the community and that our work addresses important problems . Below we address specific cons / questions raised by the reviewer :

1 . The motivation for the model choice of q:
In general , when there is information asymmetry in the two modalities ( i.e. one instance in modality y corresponds to many instances in modality x ) , we believe it is beneficial to learn with a joint inference network and retrofit ( like TELBO does ) as opposed to trying to explain both modalities using the unimodal inference network ( say for q( z| y ) ) where I ( y ) < I ( x ) , since it is difficult for the latent variable in this case to explain ' all ' the instances of the second ( information rich ) modality .

2 . Comparison to private -VCCA :
A direct comparison to private-VCCA ( from Wang.et.al . ( VAECCA ) ) can not be made since the private-VCCA model does not have a mechanism to do ‘imagination ’ . Concretely , the private-VCCA model cannot be applied for multimodal translation , i.e. going from x to y or vice versa , since in this model , p( x | h_x , z ) needs access to h_x and z , while the inference networks that condition on 'y ' are just q( h_y | y ) and q( z| y ) , meaning there is no path to go from ‘y ’ to ‘x ’ , which is needed for conditional generation .
We implemented a modified version of private - VCCA and / extended / it to the BiVCCA setting ( i.e. learning two inference networks ) to obtain a version that / can / do conditional generation . We call this model imagination - private-BiVCCA . Concretely , we learn three inference networks per modality : q( h_x | y ) , q ( h_y|y ) and q( z| y ) . Now , we see that given ‘y ’ , we can sample h_x and z , and thus , feed it through p( x | h_x , z ) to generate images . Similar to Wang. et.al. , we regularize the latent space in this model using dropout , searching for values in the range {0 , 0.2 , 0.4 } . As with the BiVCCA model we present in this paper , we also search for \mu in range { 0.3 , 0.5 , 0.7} . We find that this approach does about as well as the best BiVCCA method we already report in the paper and is still substantially worse than JMVAE or TELBO .

3 . Details of the hyperparameters , \lambda_y and \beta_y :
Firstly , we clarified notation , replacing \beta_y with \gamma in the revised version . We have added details of how performance changes based on different hyperparameters for all the objective functions in appendix A.6 . Concretely , the important parameters to tweak for TELBO are \lambda_y ( with \ gamma ) being less important . For JMVAE , both \lambda_y and \alpha turn out to be fairly important , while for BiVCCA , \lambda_y is important while the choice of \mu is marginally important .

4 . Why is JMVAE better at correctness when all attributes are specified ?
Page . 6 explains the tradeoff between correctness and coverage implicit in choices of JMVAE v.s. TELBO objectives . Briefly , our initial submission showed in appendix A.1. that JMVAE fits an inference network optimizing the KL divergence between the aggregate posterior ( for the set of images given a label y_i ) and the unimodal inference network q( z| y_i ) , to ‘ cover ’ the aggregate posterior . This ensures that the samples from q( z| y_i ) are better tuned to work with p( x | z ) , since the elbo( x , y ) term has a likelihood function p_{\theta_x} ( x | z) which is fed samples from q( z| x , y ) , whose aggregate posterior q( z| y_i ) tries to match . In contrast TELBO regularizes q( z| y_i ) to be ‘ close ’ to p( z ) which leads to better coverage , but does not lead naturally to the expectation that we would achieve better correctness than a model like JMVAE , which in some sense has a tighter coupling between the likelihood and the q( z| y ) terms . If the aggregate posterior matches the prior better , the gap in correctness between JMVAE and TELBO would reduce . Making this happen is an active research area in variational inference ( see [ A ] , [ B ] ) .

5 . Clarifications about qualitative results :
The new version provides a clearer caption to Fig. 4 b ) explaining the qualitative results for MNIST - A and adds new text on Page . 9 explaining the qualitative results on CelebA . In addition , we show more qualitative results in Appendix A.9 ( Page. 18 , Figure. 13 ) discussing results comparing TELBO and JMVAE in terms of diversity .

References :
[ A ] : Tomczak , Jakub M. , and Max Welling . 2017 . “ VAE with a VampPrior . ” arXiv [ cs.LG ] . arXiv. http://arxiv.org/abs/1705.07120.
[ B ] : Hoffman , Matthew D. , and Matthew J. Johnson. 2016 . “ Elbo Surgery : Yet Another Way to Carve up the Variational Evidence Lower Bound . ” In Workshop in Advances in Approximate Bayesian Inference , NIPS . http://approximateinference.org/accepted/HoffmanJohnson2016.pdf.

The paper proposes a method for generating images from attributes . The core idea is to learn a shared latent space for images and attributes with variational auto-encoder using paired samples , and additionally learn individual inference networks from images or attributes to the latent space using unpaired samples . During training the auto-encoder is trained on paired data ( image , attribute ) whereas during testing one uses the unpaired data to generate an image corresponding to an attribute or vice versa . The authors propose handling missing data using a product of experts where the product is taken over available attributes , and it sharpens the prior distribution . The authors evaluate their method using correctness i.e. if the generated images have the desired attributes , coverage i.e. if the generated images sample unspecified attributes well , and compositionality i.e. if images can be generated from unseen attributes . Although the proposed method performs slightly poor compared to JMVAE in terms of concreteness when all attributes are provided , it outperforms when some of the attributes are missing ( Figure 4a ) . It also outperforms existing methods in terms of coverage and compositionality .

Major comments :

The paper is well written , and summarizes its contribution succinctly .

I did not fully understand the ' retrofitting ' idea . If I understood correctly , the authors first train \theta and \phi and then fix \theta to train \phi_x and \phi_y . If that is true , then is \calL ( \theta , \phi , \phi_x , \ phi_y ) are right cost function since one does not maximize all three ELBO terms when optimizing \theta ? Please clarify ?

Minor comments :

- ' in order of increasing abstraction ' , does the order of gender -> smiling or not -> hair color matter ? Or , is male , * , blackhair a valid option ?

- what are the image sizes for the CelebA dataset

- page 5 : double the

- Which multi-label classifier is used to classify images in attributes ?

We thank the reviewer for the encouraging feedback on the paper . Below we provide clarifications to specific concerns .

1 . Clarifications about “ retrofitting ” :
As explained on Page . 3 , last paragraph , and by the reviewer , the retrofitting idea is that we first train \theta and \phi and then fix \theta to train \phi_x and \phi_y . As explained in the paper , we just optimize the first term in the cost function with respect to \theta and fix \theta_x and \theta_y when training the last two ELBO terms respectively . This is what we refer to as ‘ retrofitting ’ . To clarify things further , we write the cost in terms of L ( \theta_x , \theta_y , \phi_x , \phi_y , \ phi ) to clarify which parameters get used in each of the ELBO terms . However , conceptually , we still retain the notation specifying the overall cost using \theta for succinctness , and for the possibility of performing semi-supervised learning with the objective ( footnote , Page. 4 ) , which is beyond the scope of the current paper .

2 . Order of attributes :
Thanks a lot for raising this point ! We have updated the paper with clarifications ( Page . 1 ) explaining that the order of attributes does not matter .

3 . Image sizes for CelebA :
The updated version of the paper contains the image sizes used for both MNIST - A and CelebA . The sizes used are the same across both the datasets , namely 64x64 .

4 . Details of the multi-label classifier :
Page. 15 provides more details of the multi-label classifier used for evaluating imagination on MNIST - A dataset . The multi-label classifier is a 2 layer convolutional neural network with four heads , where each head is a 1 hidden layer MLP trained end - to - end to optimize for the log-likelihood of the correct attribute label for an image .

Summary :

The paper proposes a framework for constructing spherical convolutional networks ( ConvNets ) based on a novel synthesis of several existing concepts . The goal is to detect patterns in spherical signals irrespective of how they are rotated on the sphere . The key is to make the convolutional architecture rotation equivariant .

Pros :

+ novel / original proposal justified both theoretically and empirically
+ well written , easy to follow
+ limited evaluation on a classification and regression task is suggestive of the proposed approach 's potential
+ efficient implementation

Cons :

- related work , in particular the first paragraph , should compare and contrast with the closest extant work rather than merely list them
- evaluation is limited ; granted this is the nature of the target domain

Presentation :

While the paper is generally written well , the paper appears to conflate the definition of the convolutional and correlation operators ? This point should be clarified in a revised manuscript .

In Section 5 ( Experiments ) , there are several references to S^2CNN . This naming of the proposed approach should be made clear earlier in the manuscript . As an aside , this appears a little confusing since convolution is performed first on S^2 and then SO ( 3 ) .

Evaluation :

What are the timings of the forward / backward pass and space considerations for the Spherical ConvNets presented in the evaluation section ? Please provide specific numbers for the various tasks presented .

How many layers ( parameters ) are used in the baselines in Table 2 ? If indeed there are much less parameters used in the proposed approach , this would strengthen the argument for the approach . On the other hand , was there an attempt to add additional layers to the proposed approach for the shape recognition experiment in Sec. 5.3 to improve performance ?

Minor Points :

- some references are missing their source , e.g. , Maslen 1998 and Kostolec , Rockmore , 2007 , and Ravanbakhsh , et al. 2016 .

- some sources for the references are presented inconsistency , e.g. , Cohen and Welling , 2017 and Dieleman , et al . 2017

- some references include the first name of the authors , others use the initial

- in references to et al . or not , appears inconsistent

- Eqns 4 , 5 , 6 , and 8 require punctuation

- Section 4 line 2 , period missing before " Since the FFT "

- " coulomb matrix " --> " Coulomb matrix "

- Figure 5 , caption : " The red dot correcpond to " --> " The red dot corresponds to "

Final remarks :

Based on the novelty of the approach , and the sufficient evaluation , I recommend the paper be accepted .



Thank you for the detailed and balanced review .

RE Related work : we have expanded the related work section a little bit in order to contrast with previous work . ( Unfortunately there is no space for a very long discussion )

RE Convolution vs correlation : thank you for pointing this out . Our reasoning had been that :
1 ) Everybody in deep learning uses the word " convolution " to mean " cross -correlation " .
2 ) In the non-commutative case , there are several different but essentially equivalent convolution - like integrals that one can define , with no really good reason to prefer one over the other .

But we did not explain this properly . We think a reasonable approach is to call something group convolution if , for the translation group it specializes to the standard convolution , and similarly for group correlations . This seems to be what several others before us have done as well , so we will follow this convention . Specifically , we will define the ( group ) cross - correlation as :
psi \star f( g ) = int psi( g^{ - 1 } h ) f( h ) dh .

RE The S^2CNN name : we have now defined this term in the introduction , but not changed it , because the paper is called " Spherical CNN " and S^2-CNN is just a shorthand for that name .

RE Timings : we have added timings , memory usage numbers , and number of parameters to the paper . It is not always possible to compare the number of parameters to related work because those numbers are not always available . However , we can reasonably assume that the competing methods did their own cross-validation to arrive at an optimal model complexity for their architecture . ( Also , in deep networks , the absolute number of parameters can often vary widely between architectures that have a similar generalization performance , making this a rather poor measure of model complexity . )

RE References and other minor points : we have fixed all of these issues . Thanks for pointing them out .

The focus of the paper is how to extend convolutional neural networks to have built - in spherical invariance . Such a requirement naturally emerges when working with omnidirectional vision ( autonomous cars , drones , ... ) .

To get invariance on the sphere ( S^2 ) , the idea is to consider the group of rotations on S^2 [ SO ( 3 ) ] and spherical convolution [ Eq . ( 4 ) ] . To be able to compute this convolution efficiently , a generalized Fourier theorem is useful . In order to achieve this goal , the authors adapt tools from non - Abelian [ SO ( 3 ) ] harmonic analysis . The validity of the idea is illustrated on 3D shape recognition and atomization energy prediction .

The paper is nicely organized and clearly written ; it fits to the focus of ICLR and can be applicable on many other domains as well .


Thank you very much for taking the time to review our work .

First off , this paper was a delight to read . The authors develop an ( actually ) novel scheme for representing spherical data from the ground up , and test it on three wildly different empirical tasks : Spherical MNIST , 3D - object recognition , and atomization energies from molecular geometries . They achieve near state - of - the- art performance against other special - purpose networks that are n't nearly as general as their new framework . The paper was also exceptionally clear and well written .

The only con ( which is more a suggestion than anything ) -- it would be nice if the authors compared the training time / # of parameters of their model versus the closest competitors for the latter two empirical examples . This can sometimes be an apples- to- oranges comparison , but it 's nice to fully contextualize the comparative advantage of this new scheme over others . That is , does it perform as well and train just as fast ? Does it need fewer parameters ? etc.

I strongly endorse acceptance .

Thank you for the kind words , we 're glad you like our work !

Our models for SHREC17 and QM7 both use only about 1.4M parameters . On a machine with 1 Titan X GPU , training the SHREC17 model takes about 50 hours , while the QM7 model takes only about 3 hours . Memory usage is 8 GB for SHREC ( batchsize 16 ) and 7GB for QM7 ( batchsize 20 ) .

We have studied the SHREC17 paper [ 1 ] , but unfortunately it does not state the number of parameters or training time for the various methods . It does seem likely that each of the competition participants did their own cross validation , and arrived at an appropriate model complexity for their method . It is thus unlikely that the strong performance of our model relative to others can be explained by its size ( especially since 1.4M parameters is not considered very large anymore ) .

For QM7 , it looks like Montavon et al. used about 760 k parameters ( we have deduced this from the description of their network architecture ) . Since the model is a simple multi-layer perceptron applied to a hand - designed feature representation , we expect that it is substantially faster to train than our model ( though indeed comparing a spherical CNN to an engineered features + MLP approach is a bit of an apples - to-oranges comparison ) . Raj et al . use a non-parametric method , so there is no parameter count or training time to compare to .

[ 1 ] M. Savva et al . SHREC ’17 Track Large - Scale 3D Shape Retrieval from ShapeNet Core55 , Eurographics Workshop on 3D Object Retreival ( 2017 ) .

The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task . While the proposed idea might be too simple , the authors show the importance of it via thorough experiments . It also seems to be easily integrated into existing RNN systems without heavy tuning as shown in the experiments .

* One advantage of proposed idea claimed against the skip-RNN is that the Skim-RNN can generate the same length of output sequence given input sequence . It is not clear to me whether the output prediction on those skimmed tokens is made of the full hidden state ( updated + copied ) or a first few dimensions of the hidden state . I assume that the full hidden states are used for prediction . It is somehow interesting because it may mean the prediction heavily depends on small ( d ' ) part of the hidden state . In the second and third figures of Figure 10 , the model made wrong decisions when the adjacent tokens were both skimmed although the target token was not skimmed , and it might be related to the above assumption . In this sense , it would be more beneficial if the skimming happens over consecutive tokens ( focus on a region , not on an individual token ) .

* This paper would gain more attention from practitioners because of its practical purpose . In a similar vein , it would be also good to have some comments on training time as well . In a general situation where there is no need of re-training , training time would be meaningless , however , if one requires updating the model on the fly , it would be also meaningful to have some intuition on training time .

* One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state . In this sense , it makes this manuscript more comprehensive if there are some comparisons with RNNs with limited - sized hidden dimensions ( say 10 or 20 ) . So that readers can check benefits of the skim RNN against skip-RNN and small -sized RNN .


Thank you for your insightful and supportive comments ; we make a few clarifications and discuss additional experiments inspired by your suggestions .

Clarification :
- Output of skimmed tokens : The output of skimmed tokens is the full hidden state ( concatenating updated and copied parts ) .

Suggestions :
- Focusing on region : Thank you for your suggestion , and we will consider this approach in future work .

- Training time : Since Skim-RNN needs to compute outputs for both RNNs ( big and small ) during training , it requires more time for the same number of training steps . For instance , in SQuAD , Skim -LSTM takes 8 hours of training whereas LSTM takes 5 hours until convergence . However , in terms of number of training steps , they both require approximately 18 k steps . We will include this in any final version of the paper .

- Comparison with small hidden size : When the hidden size becomes 10 ( from 100 ) for vanilla RNN , there is 3.4 % accuracy drop in SST ( 7.1x less FLOP ) and 6.1 % accuracy drop in Rotten Tomatoes ( 7.1x less FLOP ) . There is a clear trade - off between accuracy and FLOP when smaller hidden size is used . We are currently experimenting with other datasets and will include them in the next revision .


Summary : The paper proposes a learnable skimming mechanism for RNN . The model decides whether to send the word to a larger heavy - weight RNN or a light - weight RNN . The heavy - weight and the light - weight RNN each controls a portion of the hidden state . The paper finds that with the proposed skimming method , they achieve a significant reduction in terms of FLOPS . Although it does n’t contribute to much speedup on modern GPU hardware , there is a good speedup on CPU , and it is more power efficient .

Contribution :
- The paper proposes to use a small RNN to read unimportant text . Unlike ( Yu et al. , 2017 ) , which skips the text , here the model decides between small and large RNN .

Pros :
- Models that dynamically decide the amount of computation make intuitive sense and are of general interests .
- The paper presents solid experimentation on various text classification and question answering datasets .
- The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation ( increase in accuracy in some tasks ) .
- The paper is well written , and the presentation is good .

Cons :
- Each model component is not novel . The authors propose to use Gumbel softmax , but does compare other gradient estimators . It would be good to use REINFORCE to do a fair comparison with ( Yu et al. , 2017 ) to see the benefit of using small RNN .
- The authors report that training from scratch results in unstable skim rate , while Half pretrain seems to always work better than fully pretrained ones . This makes the success of training a bit adhoc , as one need to actively tune the number of pretraining steps .
- Although there is difference from ( Yu et al. , 2017 ) , the contribution of this paper is still incremental .

Questions :
- Although it is out of the scope for this paper to achieve GPU level speedup , I am curious to know some numbers on GPU speedup .
- One recommended task would probably be text summarization , in which the attended text can contribute to the output of the summary .

Conclusion :
- Based on the comments above , I recommend Accept

Thank you for your insightful and supportive comments ; we discuss additional experiments following your suggestions and make a few clarifications .

Suggestions :
- Other gradient methods : Thank you for your suggestion , and we found that REINFORCE substantially underperforms ( less than 20 % accuracy on SST ) Gumbel - Softmax within 50 k steps of training . We suspect that this is due to the high variance of REINFORCE , which becomes even worse in our case where the sample space exponentially increases with the sequence length . We found that temperature annealing is not as bad as REINFORCE , but the accuracy is still ~ 0.5 % lower than Gumbel - Softmax and the convergence is slower . We will include this in any final version of the paper .


- Text summarization : We agree that it is an appropriate application for Skim-RNN . We will consider it for potential future work .


Clarifications :
- Adhoc training due to required pretraining : while pretraining definitely helps in QA , we would like to emphasize that no - pretraining still performs well ( classification results are without pretraining ) , and there is no added cost of pretraining ; that is , pretraining + finetuning has a similar training time to training from scratch .

- GPU speed up : Theoretically Skim-RNN could have speed up on GPU . However , because parallelization has log-time cost , this would be negligible compared to other costs .


This paper proposes a skim-RNN , which skims unimportant inputs with a small RNN while normally processes important inputs with a standard RNN for fast inference .

Pros .
- The idea of switching small and standard RNNs for skimming and full reading respectively is quite simple and intuitive .
- The paper is clearly written with enough explanations about the proposal method and the novelty .
- One of the most difficult problems of this approach ( non-differentiable ) is elegantly solved by employing gumbel - softmax
- The effectiveness ( mainly inference speed improvement with CPU ) is validated by various experiments . The examples ( Table 3 and Figure 6 ) show that the skimming process is appropriately performed ( skimmed unimportant words while fully read relevant words etc . )
Cons .
- The idea is quite simple and the novelty is incremental by considering the difference from skip-RNN .
- No comments about computational costs during training with GPU ( it would not increase the computational cost so much , but gumbel - softmax may require more iterations ) .

Comments :
- Section 1 , Introduction , 2nd paragraph : ‘ peed ’ -> ‘ speed ’ ( ? )
- Equation ( 5 ) : It would be better to explain why it uses the Gumbel distribution . To make ( 5 ) behave like argmax , only temperature parameter seems to be enough .
- Section 4.1 : What is “ global training step ” ?
- Section 4.2 , “ We also observe that the F1 score of Skim -LSTM is more stable across different configurations and computational cost . ” : This seems to be very interesting phenomena . Is there some discussion of why skim -LSTM is more stable ?
- Section 4.2 , the last paragraph : “ Table 6 shows ” -> “ Figure 6 shows ”


Thank you for your insightful and supportive comments ; we discuss additional experiments inspired your suggestions and make a few clarifications .


Suggestions :
- Training cost with GPU : Thank you for the suggestion , and we report training cost in two dimensions : memory and time . Assuming d/ d’=100 / 20 on SQuAD , memory consumption is only ~ 5 % more than vanilla RNN . Since Skim-RNN needs to compute outputs for both RNNs ( big and small ) during training , it requires more time for the same number of training steps . For instance , on SQuAD , Skim -LSTM takes 8 hours of training whereas LSTM takes 5 hours until convergence . However , in terms of number of training steps , they both require approximately 18 k steps .

- Why Gumbel - softmax and not just temperature : we used Gumbel - Softmax mainly due to its theoretical guarantee shown in Jang et al ( 2017 ) . We experimented with temperature annealing only , and found that the accuracy is ~ 0.5 % lower on SQuAD and convergence is a little slower . While there is some advantage of Gumbel - softmax , It seems temperature annealing is also an effective technique .

- Typos : thank you for correcting them and we have fixed them in the current revision .


Clarifications :
- Sec 4.1 global training step : We meant just “ training step ” , and we fixed it in the most recent revision .

- Sec 4.2 stableness : We actually meant that LSTM accuracy dips ( is unstable ) when we use smaller hidden state size to reduce FLOP , while Skim-LSTM accuracy does not dip even with high reduction in FLOP .


Update 1/11/18:

I 'm happy with the comments from the authors . I think the explanation of non-saturating vs saturating objective is nice , and I 've increased the score .

Note though : I absolutely expect a revision at camera-ready if the paper gets accepted ( we did not get one ) .

Original review :
The paper is overall a good contribution . The motivation / insights are interesting , the theory is correct , and the experiments support their claims .

I ’m not sure I agree that this is “ unifying ” GANs and VAEs , rather it places them within the same graphical model perspective . This is very interesting and a valuable way of looking at things , but I do n’t see this as reshaping how we think of or use GANs . Maybe a little less hype , a little more connection to other perspectives would be best . In particular , I ’d hope the authors would talk a little more about f-GAN , as the variational lower - bound shown in this work is definitely related , though this work uniquely connects the GAN lower bound with VAE by introducing the intractable “ posterior ” , q( x | y ) .

Detailed comments :
P1 : I see f-GAN as helping link adversarial learning with traditional likelihood - based methods , notably as a dual - formulation of the same problem . It seems like there should be some mention of this .

P2 :
what does this mean : “ generated samples from the generative model are not leveraged for model learning ” . The wording is maybe a little confusing .

P5 :
So here I think the connection to f-GAN is even clearer , but it is n’t stated explicitly in the paper : the discriminator defines a lower - bound for a divergence ( in this case , the JSD ) , so it ’s natural that there is an alternate formulation in terms of the posterior ( as it is called in this work ) . As f-GAN is fairly well - known , not making this connection here I think isolates this work in a critical way that makes it seem that similar observations have n’t been made .

P6 :
" which blocks out fake samples from contributing to learning ” : this is an interesting way of thinking about this . One potential issue with VAEs / other MLE - based methods ( such as teacher - forcing ) is that it requires the model to stay “ close ” to the real data , while GANs do not have such a restriction . Would you care to comment on this ?

P8 :
I think both the Hjelm ( BGAN ) and Che ( MaliGAN ) are using these weights to address credit assignment with discrete data , but BGAN does n’t use a MLE generator , as is claimed in this work .

General experimental comments :
Generally it looks like IWGAN and AA - VAE do as is claimed : IWGANs have better mode coverage ( higher inception scores ) , while AA - VAEs have better likelihoods given that we ’re using the generated samples as well as real data . This last one is a nice result , as it ’s a general issue with RNNs ( teacher forcing ) and is why we need things like scheduled sampling to train on the free-running phase . Do you have any comments on this ?

It would have been nice to show that this works on harder datasets ( CelebA , LSUN , ImageNet ) .

By “ unifying ” we meant this work proposes a unified statistical view of VAEs and GANs . We will revise the title to avoid confusion .

P1 & P5 :
We have discussed f-GAN ( Nowozin et al. , 2016 ) in the related work section . f- GAN and most previous works that analyze GANs are based on the `saturated` objective of GANs , i.e. , min_G log ( 1 - D ( G ( z ) ) ) . In particular , f-GAN and a few other works showed that with this objective , GANs involve * minimizing a variational lower bound * of some f-divergence ( Nowozin et al. , 2016 ) or mutual information between x and y ( the real / fake indicator ) ( Huszar et al. , 2016 ; Li et al. , 2016 ) .

In contrast , our work is based on the `non-saturated` objective of the original GAN , i.e. , max_G log D ( G ( z ) ) . The two objectives have the same fixed point solution , but the `non-saturated` one avoids the vanishing gradient issue of the `saturated` one , and is more widely - used in practice . However , very few formal analysis ( e.g. , Arjovsky & Bottou , 2017 ) has been done on the `non-saturated` objective . Our results in Lemma .1 is a generalization of the previous theorem in ( Arjovsky & Bottou , 2017 ) by allowing non-optimal discriminators in the analysis ( please see the last paragraph of P5 for more details ) .

We will make these clearer in the revised version .

P2 & P6 :
The sentence “ generated samples from … learning ” in P2 meant the same as “ blocks out fake samples from contributing to learning ” in P6 as the reviewer noted . We will polish the statement . Thanks for pointing it out .

By the analysis in the paper and common empirical observations , GANs ( which involve min KL ( Q||P ) ) suffer from mode missing issue . That is , the learned generation distribution tends to concentrate to few large modes of the real data distribution . In contrast , VAEs and other MLE -based methods ( which involve min KL ( P||Q ) ) suffer from the issue of covering all data modes as well as small - density regions in - between . In this sense , GANs in practice are more “ restricted ” to stay close to data modes , and generate samples that are generally less diverse and more plausible .

P8 :
BGAN for discrete data rephrases generator G as conditional distribution g( x|z ) , and evaluates the explicit conditional likelihood g( x|z ) for training . BGAN for continuous data does not have such parameterization . We will update the statements to fix the issue . Thanks for pointing this out .

Experiments :
It can be very interesting to apply the techniques in AA - VAE to augment the MLE training of RNNs , which has not been explored in this paper . A related line of research is adversarial training of RNNs which applies a discriminator on the RNN samples . To our knowledge , such approaches suffer from optimization difficulty due to , e.g. , the discrete nature of samples ( e.g. , text samples ) . In contrast , AA - VAE avoids the issue as generated samples are used in the same way as real data examples by maximizing the “ likelihood ” of good samples selected by the discriminator . We are happy to explore more in this direction in the future .

This paper focuses mainly on establishing formal connections between GANs , VAEs , and other deep generative models through new formulations of them . Technique transfer between research lines , e.g. , IWGAN and AA - VAE , serves to showcase the benefit of the unified statistical view . We will validate the new techniques on harder datasets as suggested , and show the results soon .

The authors develops a framework interpreting GAN algorithms as performing a form of variational inference on a generative model reconstructing an indicator variable of whether a sample is from the true of generative data distributions . Starting from the ‘ non-saturated ’ GAN loss the key result ( lemma 1 ) shows that GANs minimizes the KL divergence between the generator ( inference ) distribution and a posterior distribution implicitly defined by the discriminator . I found the paper IWGAN and especially the AAVAE experiments quite interesting . However the paper is also very dense and quite hard to follow at times - In general I think the paper would benefit from moving some content ( like the wake - sleep part of the paper ) to the appendix and concentrating more on the key results and a few more experiments as detailed in the comments / questions below .

Q1 ) What would happen if the KL- divergence minimizing loss proposed by Huszar ( see e.g http://www.inference.vc/an-alternative-update-rule-for-generative-adversarial-networks/) was used instead of the “ non -saturated ” GAN loss - would the residial JSD terms in Lemma 1 cancel out then ?

Q2 ) In Lemma 1 the negative JSD term looks a bit nasty to me e.g. in addition to KL divergence the GAN loss also maximises the JSD between the data and generative distributions . This JSD term acts in a somewhat opposite direction of the KL - divergence that we are interested in minimizing . Can the authors provide some more detailed comments / analysis on these two somewhat opposed terms - I find this quite important to include given the opposed direction of the JSD versus the KL term and that the JSD is ignored in e.g. section 4.1 ? secondly did the authors do any experiments on the the relative sizes of these two terms ? I imagine it would be possible to perform some low- dimensional toy experiments where both terms were tractable to compute numerically ?

Q3 ) I think the paper could benefit from some intuition / discussion of the posterior term q^r ( x|y ) in lemma 1 composed on the prior p_theta0 ( x ) and discriminator q^r ( y|x ) . The terms drops out nicely in math however i had a bit of a hard time wrapping my head around what minimizing the KL - divergence between this term and the inference distribution p( x Iy ) . I know this is a kind of open ended question but i think it would greatly aid the reader in understanding the paper if more ‘ guidance ’ is provided instead of just writing “ .. by definition this is the posterior . ’

Q4 ) In a similar vein to the above . It would be nice with some more discussion / definitions of the terms in Lemma 2 . e.g what does “ Here most of the components have exact correspondences ( and the same definitions ) in GANs and InfoGAN ( see Table 1 ) ” mean ?

Q5 ) The authors state that there is ‘ strong connections ’ between VAEs and GANs . I agree that both ( after some assumptions ) both minimize a KL - divergence ( table 1 ) however to me it is not obvious how strong this relation is . Could the authors provide some discussion / thoughts on this topic ?

Overall i like this work but also feel that some aspects could be improved : My main concern is that a lot of the analysis hinges on the JSD term being insignificant , but the authors to my knowledge does but provide any prof / indications that this is actually true . Secondly I think the paper would greatly benefit from concentration on fewer topics ( e.g. maybe drop the RW topic as it feels a bit like an appendix ) and instead provide a more throughout discussion of the theory ( lemma 1 + lemma 2 ) as well as some more experiments wrt JSD term .


Q1 ) Huszar proposed to optimize a loss that combines the `saturated` and `non-saturated` losses . We have cited and briefly discussed this work ( Sønderby et al. , 2017 ) in the related work section . As with most previous studies , the analysis of the combined loss in the blog and ( Sønderby et al. , 2017 ) is based on the assumption that the discriminator is near optimal . With this assumption , Lemma .1 is simplified to Eq. ( 8 ) , and the residual JSD term cancels out with the combined loss . However , as discussed in the paper , Lemma .1 in general case ( Eq.6 ) does not rely on the optimality assumptions of the discriminator which are usually unwarranted in practice . Thus , Lemma . 1 can be seen as a generalization of previous results ( Sønderby et al. , 2017 ; Arjovsky and Bottou , 2017 ) to account for broader situations . Also , the JSD term does not cancel out even with the combined loss .

Q2 ) We will update the paper to add more analysis of the JSD term . In particular , from the derivation of Lemma .1 in section C of the supplements , we can show the relative sizes of the KL and JSD term follow : JSD <= KL. Specifically , if we denote the RHS of Eq. ( 20 ) as - E_p ( y ) [ KL - KL_1 ] , then from Eq. ( 20 ) we have KL_1 <= KL. From Eqs . ( 22 ) and ( 23 ) , we further have JSD <= KL_1 . We therefore have JSD <= KL_1 <= KL . That is , the JSD is upper-bounded by the KL , and intuitively , if the KL is sufficiently minimized , the magnitude of JSD will also decrease .

Note that we did not mean that the JSD term is negligible . Indeed , most conclusions in the paper have taken into account the JSD . For example , JSD is * symmetric* ( rather than insignificant ) and will not affect the mode missing behavior of GANs endowed by the asymmetry of the KL . We have also noticed in the paper that the gradients of the JSD and the KLD cancel out when discriminator gives random guesses ( e.g. , when p_g=p_data ) . In the derivations of IWGAN in sections 4.1 and G , inspired from the JSD term in Lemma.1 , we also subtracted away the 2nd term of RHS of Eq. ( 38 ) which equals the JSD when k=1 . The approximation is necessary for computational tractability .

Q3 ) Figure .2 and the second point ( “ Training dynamics ” ) under Lemma .1 give an intuitive illustration of the posterior distribution q^r ( x|y ) . Intuitively , the posterior distribution is a mixture of p_data ( x ) and p_{g_\theta0 } ( x ) with the mixing weights induced from the discriminator distribution q^r ( y|x ) . Figure . 2 illustrates how minimizing the KL divergence between the inference distribution and the posterior can push p_g towards p_data , and how mode missing can happen .

Q4 ) We will add more definitions and explanations of the terms in Lemma . 2 . The key terms are listed in Table .1 to allow side - by-side comparison with the corresponding terms in GANs and InfoGANs . For example , the distribution q_\eta ( z|x , y ) in Lemma . 2 precisely corresponds to the distribution q_\eta ( z|x , y ) in InfoGAN ( defined in the text of Eq. ( 9 ) ) . We will make these clearer in the revised version . Thanks for the suggestion .

Q5 ) By “ strong ” we meant the connections reveal multiple new perspectives of GANs and VAEs as well as a broad class of their variants . Most of the discussions are presented in section 3.4 . For example , the reformulation of GANs links the adversarial approach to the classic Bayesian variational inference ( VI ) algorithm , which further opens up the opportunities of transferring the large volume of extensions of VI to the adversarial approach for improvement ( e.g. , the proposed IWGAN in the paper ) . Section 3.4 provides four examples of such new perspectives inspired by the new formulations and connections , each of which in turn leads to either an existing research direction or new broad discussions on deep generative modeling ( e.g. , section A ) . We hope this work can inspire even more insights and discussions on , e.g. , formal relations of adversarial approaches and Bayesian methods , etc .

The paper provides a symmetric modeling perspective ( " generation " and " inference " are just different naming , the underlying techniques can be exchanged ) to unify existing deep generative models , particularly VAEs and GANs . Someone had to formally do this , and the paper did a good job in describing the new view ( by borrowing the notations from adversarial domain adaptation ) , and demonstrating its benefits ( by exchanging the techniques in different research lines ) . The connection to weak - sleep algorithm is also interesting . Overall this is a good paper and I have little to add to it .

One of the major conclusions is GANs and VAEs minimize the KL Divergence in opposite directions , thus are exposed to different issues , overspreading or missing modes . This has been noted and alleviated in [ 1 ] .

Is it possible to revise the title of the paper to specifically reflect the proposed idea ? Other papers have attempted to unify GAN and VAE from different perspectives [ 1 , 2 ] .

[ 1 ] Symmetric variational autoencoder and connections to adversarial learning . arXiv:1709.01846
[ 2 ] Adversarial variational Bayes : Unifying variational autoencoders and generative adversarial networks . arXiv:1701.04722 , 2017 .


Minor : In Fig. 1 , consider to make “ ( d ) ” bold to be consistent with other terms .

Thanks for the valuable and encouraging comments .

- Our work is indeed a couple of months earlier than [ 1 ] , and is discussed in [ 1 ] . The work of [ 1 ] focuses on alleviating the asymmetry of the KL Divergence minimized by VAEs . It discusses the connection of the new symmetric VAE variant and GANs , but does not reveal that GANs involve minimizing a KL Divergence in an opposite direction , nor focus on the underlying connections between original VAEs and GANs as we do .

In section 3.4 point 2 ) and section F of the supplements , we discussed some existing work on alleviating the mode overspreading issue of VAEs by augmenting original VAE objective with GANs related objectives . The work of [ 1 ] falls into this category ( though in [ 1 ] the symmetric VAE is motivated purely from VAEs perspective ) . We will include the discussion of [ 1 ] in the revised version .

- Our work aims at developing a unified statistical view of VAEs and GANs through new formulations of them . The unified view provides a tool to analyze existing deep generative model research , and naturally enables technique transfer between research lines . This is different from other work [ 1 , 2 ] which combines VAE and GAN objectives to form a new model / algorithm instance . We acknowledge that a clearer , specific title can alleviate confusions . Thanks for the suggestion .

The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar . The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks . The authors further propose to enhance the network with cascaded adversarial training , that is , learning against iteratively generated adversarial inputs , and showed improved performance against harder attacks .

The idea proposed is fairly straight - forward . Despite being a simple approach , the experimental results are quite promising . The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights .

As pointed out in section 4.2 , increasing the regularization coefficient leads to degenerated embeddings . Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings , for example , normalizing the inputs before sending it to the bidirectional or pivot loss , or use cosine distance etc .?

Table 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one - step attacks than clean test set , which is a bit counter- intuitive . Do the authors have any insight on this ?

Comments :
1 . The writing of the paper could be improved . For example , " Transferability analysis " in section 1 is barely understandable ;
2 . Arrow in Figure 3 are not quite readable ;
3 . The paper is over 11 pages . The authors might want to consider shrink it down the recommended length .

Thank you for the valuable reviews .

Q1 - As pointed out in section 4.2 , increasing the regularization coefficient leads to degenerated embeddings . Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings , for example , normalizing the inputs before sending it to the bidirectional or pivot loss , or use cosine distance etc .?
( Ans ) We would like to mention that every regularization technique including weight decay ( L1 / L2 regularization ) has the problem of degenerated embeddings if we weigh more on the regularized term . We have applied regularization after normalizing embeddings ( divided by the standard deviation of the embeddings ) . As we increase lambda_2 , the mean of the embeddings remains the same , but , the standard deviation of the embeddings becomes large . That means intra class variation becomes large . We eventually observed degenerated embeddings ( lower accuracy on the clean example ) for large lambda_2 which is the same phenomenon with our original version of implementation .

Q2 - Table 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one - step attacks than clean test set , which is a bit counter- intuitive . Do the authors have any insight on this ?
( Ans ) The purpose of cascade adversarial training is to improve the robustness against “ iterative ” attack . And we showed the effectiveness of the method showing increased accuracy against iterative attack , but , at the expense of decreased accuracy against one - step attack .
We have observed this phenomenon ( the networks shown to be robust against iterative attacks tends to less robust against one - step attacks ) in various conditions including ensemble adversarial training . We feel that in somehow , there is trade - off between them . Based on our extensive empirical experiments , it was very hard to increase robustness against both one - step attack and iterative attack . It is a still open question why this is so difficult , but , we assume that high dimensionality of the input is one reason for this . That means once we find good defense for some adversarial direction , there exists another adversarial direction which breaks the defense .
Finally , we would like to mention that even though we have observed decreased accuracy for the one-step attack from cascade adversarial training , accuracy gain on the iterative attacks in white box setting helps increase in robustness against black box attack ( for both one - step and iterative attacks . )

Q3 - The writing of the paper could be improved . For example , " Transferability analysis " in section 1 is barely understandable ;
( Ans ) We ’ve updated the manuscript . Essentially , the detailed analysis can be found in new section 3.1.

Q4 - Arrow in Figure 3 are not quite readable ;
( Ans ) We ’ve re-drawn the arrow from e to e+8 instead of e+4 to increase readability in revised version .

Q5 - The paper is over 11 pages . The authors might want to consider shrink it down the recommended length .
( Ans ) We ’ve updated the manuscript . Thanks for your recommendation .


This paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding space . This is an interesting idea which , from a robustness point of view ( Xu et al , 2013 ) makes sense . Note that a similar strategy has been used in the recent past under the name of stability training . The proposed method works well on CIFAR and MNIST datasets . My main concerns are :

- The adversarial objective and the stability objective are potentially conflicting . Indeed when the network misclassifies an example , its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version ( that of the ground truth label ) . Have the authors considered this issue ? Can they elaborate more on how they with this ?

- It may be significantly more difficult to make this work in such setting due to the dimensionality of the data . Did the authors try such experiment ? It would be interesting to see these results .

Lastly , The insights regarding label leaking are not compelling . Label leaking is not a mysterious phenomenon . An adversarially trained model learns on two different distributions . Given the fixed size of the hypothesis space explored ( i.e. , same architecture used for vanilla and adversarial training ) , It is natural that the statistics of the simpler distribution are captured better by the model . Overall , the paper contains valuable information and a method that can contribute to the quest of more robust models . I lean on accept side .




Thank you for the valuable reviews .

Q1 .- The adversarial objective and the stability objective are potentially conflicting . Indeed when the network misclassifies an example , its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version ( that of the ground truth label ) . Have the authors considered this issue ? Can they elaborate more on how they with this ?
( Ans ) Yes , we totally agree with that the objective of the image classification and similarity objective can be potentially conflicting each other . We would like to address that our distance based loss can be considered as a way of regularization . Like every regularization , for example , weight L1 / L2 regularization which penalizes the large value of the weights , too much regularization can always damage the training process .
If the classifier misclassifies the clean version , that means the example is hard example . Adversarial version of the hard example will also be misclassified . Instead of trying to always encourage to produce ground truth , the similarity loss will encourage adversarial version to mimic the clean version . We can think this is somewhat analogous to student - teacher learning where student network is trained with the soft target from the teacher network instead of conventional hard target .

Q2 - It may be significantly more difficult to make this work in such setting due to the dimensionality of the data . Did the authors try such experiment ? It would be interesting to see these results .
( Ans ) We have applied low level similarity learning for ImageNet dataset . We observed similar results ( A network trained with pivot loss showed improved accuracy for white box iterative attacks compared to the network trained with adversarial training only . ) We will augment those results in the final version . Due to the lack of computing resources , however , we have n’t tried training several similar networks with different initialization and testing those networks under white box and black box scenario .
If you are talking about the dimensionality of the embedding , we actually have applied similarity loss on un - normalized logits ( the layer right before the softmax layer ) where the dimension of the embedding is exact the same with the number of labels which we do n’t think a problem . We have tried applying similarity loss on intermediate neurons ( that means different size of the embeddings ) , and found that applying similarity loss is efficient when we apply this at the very end of the network .

Q3 - Lastly , The insights regarding label leaking are not compelling
( Ans ) There is no clear evidence that the distribution of the one-step adversarial images is simpler than the distribution of the clean images . Adversarial images have meant to be created to fool the network . If we think the images with random noise , and train a network with clean and noisy images , we observe the decreased accuracy for noisy images since we lost the information due to the noise . Considering the fact that the adversarial images can be viewed as images with additive noise , label leaking phenomenon is not well understood since we actually added some noise which is intentional . Correlation analysis reveals the reason behind this effect .


The paper presents a novel adversarial training setup , based on distance based loss of the feature embedding .

+ novel loss
+ good experimental evaluation
+ better performance
- way too long
- structure could be improved
- pivot loss seems hacky

The distance based loss is novel , and significantly different from prior work . It seems to perform well in practice as shown in the experimental section .
The experimental section is extensive , and offers new insights into both the presented algorithm and baselines . Judging the content of the paper alone , it should be accepted .

However , the exposition needs significant improvements to warrant acceptance . First , the paper is way too long and unfocused . The recommended length is 8 pages + 1 page for citations . This paper is 12 + 1 pages long , plus a 5 page supplement . I 'd highly recommend the authors to cut a third of their text , it would help focus the paper on the actual message : pushing their new algorithm . Try to remove any sentence or word that does n't serve a purpose ( help sell the algorithm ) .
The structure of the paper could also be improved . For example the cascade adversarial training is buried deep inside the experimental section . Considering that it is part of the title , I would have expected a proper exposition of the idea in the technical section ( before any results are presented ) . While condensing the paper , consider presenting all technical material before evaluation .
Finally , the pivot " loss " seems a bit hacky . First , the pivot objective and bidirectional loss are exactly the same thing . While the bidirectional loss is a proper loss and optimized as such ( by optimizing both E^adv and E ) , the pivot objective is no loss function , as it does not correspond to any function any optimization algorithm could minimize . I 'd recommend the just remove the pivot objective , or at least not call it a loss .

In summary , the results and presented method are good , and eventually deserve publication . However the exposition needs to significantly improve for the paper to be ready for ICLR .

Thank you for the valuable reviews .

Q1 – Way too long , structure could be improved
( Ans ) Thanks for your feedback . We have changed the structure / length of the paper in revised version . In revised version , the contents are essentially the same with those in the previous version . We only changed structure / length of the paper . As the reviewer suggested , we have included exposition of the idea in the technical section before the experimental results .
Only the transferability analysis has been included before the experimental results since we found it is necessary to show ‘ higher transferability of the iterative adversarial examples between defended networks ’ to introduce our proposed cascade adversarial training .
Thank you very much for your valuable feedback which greatly improved the quality of the revised manuscript .

Q2 - pivot loss seems hacky
( Ans ) Initially , we did n’t want to hurt the accuracy for the clean data , and that motivated us to invent pivot loss . In pivot loss , we assume clean embeddings as ground truth embeddings that adversarial embeddings have to mimic .
Bidirectional loss and pivot loss have the same mathematical form , however , in pivot loss , embeddings from the clean examples are treated as constants ( non-trainable ) . For pivot loss , it actually computes gradients with back - propagation , but only through embeddings computed from adversarial images .
We can also think this is somewhat analogous to student - teacher learning where student network ( adversarial embedding ) is trained with the soft target from the teacher network ( clean embedding ) instead of conventional hard target .


This paper presents methods to reduce the variance of policy gradient using an action dependent baseline . Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state . The paper :
( 1 ) shows that using separate baselines for actions , each of which can depend on the state and other actions is bias - free
( 2 ) derive the optimal action - dependent baseline , showing that it does not degenerate into state - only dependent baseline , i.e. there is potentially room for improvement over state - only baselines .
( 3 ) suggests using marginalized action - value ( Q ) function as a practical baseline , generalizing the use of value function in state - only baseline case .
( 4 ) suggests using MC marginalization and also using the " average " action to improve computational feasibility
( 5 ) combines the method with GAE techniques to further improve convergence by trading off bias and variance

The suggested methods are empirically evaluated on a number of settings . Overall action - dependent baseline outperform state- only versions . Using a single average action marginalization is on par with MC sampling , which the authors attribute to the low quality of the Q estimate . Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance .

I find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space . In light of such results , one might change the policy space to enforce such structure .

Notes :
- Elaborate further on the assumption made in Eqn 9 . Does it mean that the actions factors can not share ( too many ) parameters in the policy construction , or that shared parameters can only be applied to the state ?
- Eqn 11 should use \simeq
- How can the notion of average be extended to handle multi-modal distributions , or categorical or structural actions ? Consider expanding on that in section 4.5.
- The discussion on the DAG graphical model is lacking experimental analysis ( where separate baselines models are needed ) . How would you train such baselines ?
- Figure 4 is impossible to read in print . The fonts are too small for the numbers and the legends .


Thank you for the thorough review ! We have updated the paper based on your suggestions . We have added discussions on categorical distributions to Section 4.4 , sub-section 2 and discussions on the generic DAG graphical model to Appendix E , last 3 paragraphs . We have addressed your key point below and incorporated the discussion into the revised article .

> Elaborate further on the assumption made in Eqn 9 . Does it mean that the actions
> factors can not share ( too many ) parameters in the policy construction , or that shared
> parameters can only be applied to the state ?

The assumption made in Eqn 9 is primarily for the theoretical analysis to be clean , and is not required to run the algorithm in practice . In particular , even without this assumption , the proposed baseline is bias -free . When the assumption holds , the optimal action - dependent baseline has a clean form which we can analyze thoroughly . As noted by the reviewer , the assumption is not very unrealistic . Some examples where these assumptions hold include multi-agent settings where the policies are conditionally independent by construction , cases where the policy acts based on independent components [ 1 ] of the observation space , and cases where different function approximators are used to control different actions or synergies [ 2 , 3 ] without weight sharing .

[ 1 ] Y. Cao et al . Motion Editing With Independent Component Analysis , 2007 .
[ 2 ] E. Todorov , Z. Ghahramani , Analysis of the synergies underlying complex hand manipulation , 2004 .
[ 3 ] E. Todorov , W. Li , X. Pan , From task parameters to motor synergies : A hierarchical framework for approximately optimal control of redundant manipulators , 2005 .

In this paper , the authors investigate variance reduction techniques for agents with multi-dimensional policy outputs , in particular when they are conditionally independent ( ' factored ' ) . With the increasing focus on applying RL methods to continuous control problems and RTS type games , this is an important problem and this technique seems like an important addition to the RL toolbox . The paper is well written , the method is easy to implement , and the algorithm seems to have clear positive impact on the presented experiments .

- The derivations in pages 4 - 6 are somewhat disconnected from the rest of the paper : the optimal baseline derivation is very standard ( even if adapted to the slightly different situation situated here ) , and for reasons highlighted by the authors in this paper , they are not often used ; the ' marginalized ' baseline is more common , and indeed , the authors adopt this one as well . In light of this ( and of the paper being quite a bit over the page limit ) - is this material ( 4.2->4.4 ) mostly not better suited for the appendix ? Same for section 4.6 ( which I believe is not used in the experiments ) .

- The experimental section is very strong ; regarding the partial observability experiments , assuming actions are here factored as well , I could see four baselines
( two choices for whether the baseline has access to the goal location or not , and two choices for whether the baseline has access to the vector $ a_{ - i} $ ) . It 's not clear which two baselines are depicted in 5 b - is it possible to disentangle the effect of providing $ a_{ - i} $ and the location of the hole to the baseline ?

( side note : it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit ' iffy ' ; the agent requires information to train , but is not provided the information to act . Out of curiosity , is it intended as an experiment to verify the need for better baselines ? Or as a ' fair ' training procedure ? )

- Minor : in equation 2 - is the correct exponent not t' ? Also since $ \rho_\pi$ is define with a scaling $ ( 1 - \gamma ) $ ( to make it an actual distribution ) , I believe the definition of $ \eta$ should also be multiplied by $ ( 1 - \gamma ) $ ( as well as equation 2 ) .

Thank you for the clear and encouraging review ! We have addressed your key points below and incorporated the discussion into the revised article .

> The derivations in pages 4 - 6 are somewhat disconnected from the rest of the paper : the
> optimal baseline derivation is very standard ( even if adapted to the slightly different
> situation situated here ) , and for reasons highlighted by the authors in this paper , they
> are not often used ; the ' marginalized ' baseline is more common , and indeed , the authors
> adopt this one as well . In light of this ( and of the paper being quite a bit over the page
> limit ) - is this material ( 4.2->4.4 ) mostly not better suited for the appendix ? Same for
> section 4.6 ( which I believe is not used in the experiments ) .

Thank you for your suggestion . We have moved the derivation and the general actions exposition to Appendices B- D and E , respectively , and have referenced only the important conclusions in the main text .

> The experimental section is very strong ; regarding the partial observability experiments ,
> assuming actions are here factored as well , I could see four baselines ( two choices for
> whether the baseline has access to the goal location or not , and two choices for whether
> the baseline has access to the vector $ a_{ - i} $ ) . It 's not clear which two baselines are
> depicted in 5 b - is it possible to disentangle the effect of providing $ a_{ - i} $ and the
> location of the hole to the baseline ?
>
> ( side note : it is an interesting idea to include information not available to the agent as
> input to the baseline though it does feel a bit ' iffy ' ; the agent requires information to
> train , but is not provided the information to act . Out of curiosity , is it intended as an
> experiment to verify the need for better baselines ? Or as a ' fair ' training procedure ? )

Thank you for this observation . We have updated the experiments to compare baseline1 =state + action + goal vs baseline2 =state +action , and have generated results for more random seeds ( 5 ) . Similarly , the multi-agent experiment is comparing whether the baseline has access to the state of other agents or not , in addition to a single agent ’s state + action . Our primary goal in these experiments was to see if providing additional information can reduce variance and help train faster . At test time , both policies are required to act based on the same information , and hence this is a ‘ fair ’ procedure . Similar approaches of using additional information during training time have been employed in recent related works [ 1 , 2 ] , which we have referenced in the paper .

[ 1 ] Lowe et al . Multi- Agent Actor -Critic for Mixed Cooperative -Competitive Environments , 2017 .
[ 2 ] Levine , et al . End-to -end training of deep visuomotor policies , 2016 .

> Minor : in equation 2 - is the correct exponent not t' ? Also since $ \rho_\pi$ is define with
> a scaling $ ( 1 - \gamma ) $ ( to make it an actual distribution ) , I believe the definition of
> $ \eta$ should also be multiplied by $ ( 1 - \gamma ) $ ( as well as equation 2 ) .

Thank you for the detailed questions and comments ! The correct exponent is $ t ’- t$ because what is being computed is the cumulative discounted return starting from time t . Thank you also for catching our error with the $ ( 1 - \gamma ) $ . We have corrected this in the manuscript in Section 3.3.

The paper proposes a variance reduction technique for policy gradient methods . The proposed approach justifies the utilization of action - dependent baselines , and quantifies the gains achieved by it over more general state - dependent or static baselines .


The writing and organization of the paper is very well done . It is easy to follow , and succinct while being comprehensive . The baseline definition is well - motivated , and the benefits offered by it are quantified intuitively . There is only one mostly minor issues with the algorithm development and the experiments need to be more polished .

For the algorithm development , there is an relatively strong assumption that z_i^ T z_j = 0 . This assumption is not completely unrealistic ( for example , it is satisfied if completely separate parts of a feature vector are used for actions ) . However , it should be highlighted as an assumption , and it should be explicitly stated as z_i^ T z_j = 0 rather than z_i^ T z_j approx 0 . Further , because it is relatively strong of an assumption , it should be discussed more thoroughly , with some explicit examples of when it is satisfied .

Otherwise , the idea is simple and yet effective , which is exactly what we would like for our algorithms . The paper would be a much stronger contribution , if the experiments could be improved .
- More details regarding the experiments are desirable - how many runs were done , the initialization of the policy network and action - value function , the deep architecture used etc .
- The experiment in Figure 3 seems to reinforce the influence of \lambda as concluded by the Schulman et. al. paper . While that is interesting , it seems unnecessary / non-relevant here , unless performance with action - dependent baselines with each value of \lambda is contrasted to the state- dependent baseline . What was the goal here ?
- In general , the graphs are difficult to read ; fonts should be improved and the graphs polished .
- The multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent 's baseline ?
- It 'd be great if Plot ( a ) and ( b ) in Figure 5 are swapped .

Overall I think the idea proposed in the paper is beneficial . Better discussing the strong theoretical assumption should be incorporated . Adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear manner . Particularly with some clarity on the experiments , I would be willing to increase the score .

Minor comments :
1 . In Equation ( 28 ) how is the optimal - state dependent baseline obtained ? This should be explicitly shown , at least in the appendix .
2 . The listed site for videos and additional results is not active .
3 . Some typos
- Section 2 - 1st para - last line : " These methods are therefore usually more sample efficient , but can be less stable than critic-based methods . " .
- Section 4.1 - Equation ( 7 ) - missing subscript i for b( s_t , a_t^{ - i} )
- Section 4.2 - \hat { Q} is just Q in many places

Thank you for the thoughtful review ! These suggestions and questions are reflected in the updated article . We have added the derivation of the optimal - state dependent baseline ( based on Greensmith , et al. , 2004 ) in Appendix A , and a video has since been uploaded to the site . We have added experiment details to Appendix G , and have clarified baselines for multi-agent settings in Section 5 ( Figure 4 b ) . Thank you also for noting the typos ! We have updated the manuscript to reflect these changes and included a clarification of our notation in Section 3.1 , paragraph 1 . We have addressed your key points below and incorporated the discussion into the new revision of the article .

> For the algorithm development , there is an relatively strong assumption that z_i^ T z_j = 0 . This
> assumption is not completely unrealistic ( for example , it is satisfied if completely separate parts
> of a feature vector are used for actions ) . However , it should be highlighted as an assumption ,
> and it should be explicitly stated as z_i^ T z_j = 0 rather than z_i^ T z_j approx 0 . Further , because
> it is relatively strong of an assumption , it should be discussed more thoroughly , with some
> explicit examples of when it is satisfied .

Thank you for this very important observation . We have revised the manuscript to state this assumption explicitly and have also provided examples where it is satisfied ( in Section 4.2 , paragraph 1 ) . We note however that this assumption is primarily for the theoretical analysis to be clean , and is not required to run the algorithm in practice . In particular , even without this assumption , the proposed baseline is bias -free . When the assumption holds , the optimal action - dependent baseline has a clean form which we can analyze thoroughly . As noted by the reviewer , the assumption is not very realistic . Some examples where these assumptions hold include multi-agent settings where the policies are conditionally independent by construction , cases where the policy acts based on independent components [ 1 ] of the observation space , and cases where different function approximators are used to control different actions or synergies [ 2 , 3 ] without weight sharing .

[ 1 ] Y. Cao et al . Motion Editing With Independent Component Analysis , 2007 .
[ 2 ] E. Todorov , Z. Ghahramani , Analysis of the synergies underlying complex hand manipulation , 2004 .
[ 3 ] E. Todorov , W. Li , X. Pan , From task parameters to motor synergies : A hierarchical framework for approximately optimal control of redundant manipulators , 2005 .

> The experiment in Figure 3 seems to reinforce the influence of \lambda as concluded by the
> Schulman et. al. paper . While that is interesting , it seems unnecessary / non-relevant here ,
> unless performance with action - dependent baselines with each value of \lambda is contrasted
> to the state- dependent baseline . What was the goal here ?

Thank you for the great question . Our goal was to emphasize that one does not lose out on temporal difference based variance reduction approaches like GAE , which are complimentary to reducing variance caused by high dimensionality of action space considered in this work . Considering the page limit and your suggestion , we have moved this discussion to Appendix F.


The paper was clearly written and pleasant to read . I liked the use of sparsity - and group-sparsity - promoting regularizers to select connections and decide how to expand the network .

A strength of the paper is that the proposed algorithm is interesting and intuitive , even if relatively complex , as it requires chaining a sequence of sub-algorithms . It was good to see the impact of each sub-algorithm studied separately ( to some degree ) in the experimental section . The results are overall strong .

It ’s hard for me to judge the novelty of the approach though , as I ’m not an expert on this topic .

Just a few points below :
- The experiments focus on a relevant continual learning problem , where each new task corresponds to learning a new class . In this setup , the method consistently outperforms EWC ( e.g. , Fig. 3 ) , as well as the progressive network baseline .
Did the authors also check the performance on the permuted MNIST benchmark , as studied by Kirkpatrick et al. and Zenke et al .? It would be important to see how the method fares in this setting , where the tasks are the same , but the inputs have to be remapped , and network expansion is less of an issue .

- Fig. 4 would be clearer if the authors showed also the performance and how much the selected connection subsets would change if instead of using the last layer lasso + BFS , the full L1 - penalized problem was solved , while keeping the rest of the pipeline intact .

- Still regarding the proposed selective retraining , the special role played by the last hidden layer seems slightly arbitrary . It may well be that it has the highest task - specificity , though this is not trivial to me . This special role might become problematic when dealing with deeper networks .

Q1 . Did the authors also check the performance on the permuted MNIST benchmark , as studied by Kirkpatrick et al. and Zenke et al .? It would be important to see how the method fares in this setting , where the tasks are the same , but the inputs have to be remapped , and network expansion is less of an issue .

A . Following your suggestion , we have also experimented on the permuted MNIST dataset using feedforward network with 2 hidden layers and included the results in the Appendix section ( See Figure 6 ) . As expected , DEN achieves performance comparable to batch models such as STL or MTL , significantly outperforming both DNN - EWC and DNN - Progressive while obtaining a network that has significantly less number of parameters .

Q2 . Fig. 4 would be clearer if the authors showed also the performance and how much the selected connection subsets would change if instead of using the last layer lasso + BFS , the full L1 - penalized problem was solved , while keeping the rest of the pipeline intact .

A . The suggested comparative study is already done in Fig . 4 . DNN - L1 shows the results using the full L1 - penalized regularizer instead of the last layer lasso + BFS . This result shows that selective retraining is indeed useful in reducing time complexity of training and perform selective knowledge transfer to obtain better accuracy .

For better understanding of the BFS process , we updated the figure that illustrates the selective retraining process to include arrows ( Leftmost figure of Figure 2 ) .

Q3 . Still regarding the proposed selective retraining , the special role played by the last hidden layer seems slightly arbitrary . It may well be that it has the highest task - specificity , though this is not trivial to me . This special role might become problematic when dealing with deeper networks .

A . The last hidden layer is not the only layer that is learned to be task -specific , as the BFS process selects units that are useful for the given task at all layers of the network and retrains them .

To show that selective retraining does not become problematic with deeper networks , we performed additional experiments on the CIFAR - 100 dataset with a 8 - layer network which is a slight modification of AlexNet . The results show that DEN obtains similar performance gain over baselines even with this deeper network .


The topic is of great interest to the community , and the ideas explored by the authors are reasonable , but I found the conclusion less - than-clear . Mainly , I was not sure how to interpret the experimental findings , and did not have a clear picture of the various models being investigated ( e.g. " base DNN regularized with l2 " ) , or even of the criteria being examined . What is " learning capacity " ? ( If it 's number of model parameters , the authors should just say , " number of parameters " ) . The relative performance of the different models examined , plotted in the top row of Figure 3 , is quite different , and though the authors do devote a paragraph to interpreting the results , I found it slightly hard to follow , and was not sure what the bottom line was .

What does the " batch model " refer to ?

re . " 11.9 % p − 51.8 % p " ; remove " p " ?

Reference for CIFAR - 100 ? Explain abbreviation for both CIFAR - 100 and AWA - Class ?

re . " ... but when the number of tasks is large , STL works better since it has larger learning capacity than MTL " : is n't the number of parameters matched ? If so , why is the " learning capacity " different ? What do the authors mean exactly by " learning capacity " ?

re . Figure 3 , e.g. " Average per-task performance of the models over number of task t " : this is a general point , but usually the expression " < f ( x ) > vs. < x > " is used rather than " < f ( x ) > over < x > " when describing a plot .

" DNN : dase ( sic ) DNN " : how is this trained ?




Q1 . What does the " batch model " refer to ?
A . “ Batch model ” refers to models that are not trained in an incremental manner ; in other words , a batch model is trained with all tasks at hand , such as DNN-MTL or DNN-STL .

Q2. re. " 11.9 % p − 51.8 % p " ; remove " p " ?
A. %p stands for percent point and is a more accurate way of denoting absolute performance improvements compared to %.

Q3 . Reference for CIFAR - 100 ? Explain abbreviation for both CIFAR - 100 and AWA - Class ?
A . Thank you for the suggestion . We updated the reference for the CIFAR - 100 dataset and included the full dataset name for AWA in the revision . CIFAR is simply a dataset


Q4. re. " ... but when the number of tasks is large , STL works better since it has larger learning capacity than MTL " : is n't the number of parameters matched ? If so , why is the " learning capacity " different ? What do the authors mean exactly by " learning capacity " ?
A . By “ learning capacity ” , we are referring to the number of parameters in a network . DNN -MTL learns only a single network for all T tasks whereas DNN - STL learns T networks for T tasks . For the experiments that generated the plots in the top row of Figure 3 , we used the same network size for both DNN - STL and DNN-MTL , and therefore , DNN - STL used T times more parameters than DNN-MTL . For accuracy / network capacity experiments in the bottom row , we diversified the base network capacity for both baselines .

Q5 . " DNN : dase ( sic ) DNN " : how is this trained ?
A . Thank you for pointing out the typo . We have corrected it in the revision .


In this paper , the authors propose a method ( Dynamically Expandable Network ) that addresses issues of training efficiency , how to dynamically grow the network , and how to prevent catastrophic forgetting .

The paper is well written with a clear problem statement and description of the method for preventing each of the described issues . Interesting points include the use of an L1 regularization term to enforce sparsity in the weights , as well as the method for identifying which neurons have “ drifted ” too far and should be split . The use of timestamps is a clever addition as well .

One question would be how sparse training is done , and how this saves computation , especially with the breadth - first search described on page 5 . A critique would be that the base networks ( a two layer FF net and LeNet ) are not very compelling .

Experiments indicate that the method works well , with a clear improvement over progressive networks . Thus , though there is n’t one particular facet of the paper that leaps out , overall the method and results seem solid and worthy of publication .

Q1 . One question would be how sparse training is done , and how this saves computation , especially with the breadth - first search described on page 5 .

A . Sparse training is done at both initial network training ( Eq . ( 2 ) ) and selective retraining step ( Eq. ( 3 ) ) , using L1 regularizer . First , the initial network training obtains a network with sparse connectivity between neurons at consecutive layers .

Then , the selective retraining selects the neurons at the layer just before the output neurons of this sparse network , and then using the topmost layer neurons as starting vertices , it selects neurons at each layer that have connections to the selected upper-layer neurons ( See the leftmost model illustration of Figure 2 ) .

This results in obtaining a subnetwork of the original network that has much less number of parameters ( Figure 4 . ( b ) ) that can be trained with significantly less training time ( Figure 4 . ( a ) ) . The selected subnetwork also obtains substantially higher accuracy since it leverages only the relevant parts of the network for the given task ( Figure 4 . ( a ) ) .

Q2 . A critique would be that the base networks ( a two layer FF net and LeNet ) are not very compelling .

A . To show that our algorithm obtains performance improvements on any generic networks , we experimented with a larger network that consists of 8 layers , which is a slight modification of AlexNet . With this larger network , our algorithm achieved similar performance gain over the baseline models as in the original experiments . We included the new results in the revision .


This work proposes an approach to meta-learning in which temporal convolutions and attention are used to synthesize labeled examples ( for few - shot classification ) or action - reward pairs ( for reinforcement learning ) in order to take the appropriate action . The resulting model is general - purpose and experiments demonstrate efficacy on few - shot image classification and a range of reinforcement learning tasks .

Strengths

- The proposed model is a generic meta-learning useful for both classification and reinforcement learning .
- A wide range of experiments are conducted to demonstrate performance of the proposed method .

Weaknesses

- Design choices made for the reinforcement learning setup ( e.g. temporal convolutions ) are not necessarily applicable to few - shot classification .
- Discussion of results relative to baselines is somewhat lacking .

The proposed approach is novel to my knowledge and overcomes specificity of previous approaches while remaining efficient .

The depth of the TC block is determined by the sequence length . In few - shot classification , the sequence length can be known a prior . How is the sequence length determined for reinforcement learning tasks ? In addition , what is done at test - time if the sequence length differs from the sequence length at training time ?

The causality assumption does not seem to apply to the few - shot classification case . Have the authors considered lifting this restriction for classification and if so does performance improve ?

The Prototypical Networks results in Tables 1 and 2 do not appear to match the performance reported in Snell et al . ( 2017 ) .

The paper is well - written overall . Some additional discussion of the results would be appreciated ( for example , explaining why the proposed method achieves similar performance to the LSTM / OPSRL baselines ) .

I am not following the assertion in 5.2.3 that MAML adaption curves can be seen as an upper bound on the performance of gradient - based methods . I am wondering if the authors can clarify this point .

Overall , the proposed approach is novel and achieves good results on a range of tasks .

EDIT : I have read the author 's comments and am satisfied with their response . I believe the paper is suitable for publication in ICLR .

Please refer to our main response in an above comment that addresses the primary and shared questions amongst all reviewers . Here we respond to your specific comments .

“ The causality assumption does not seem to apply to the few - shot classification case . Have the authors considered lifting this restriction for classification and if so does performance improve ? ”

>>> This was done in line with past work on meta-learning ( such as Santoro et al . [ 5 ] ) for a maximally direct comparison . In general , past work ( and this work ) consider such processing because it ’s compatible with streaming over incoming data ( relevant for future large scale applications ) and it aligns well with future extensions on few - shot active learning ( where the model sequentially creates its own support set by querying an oracle to label a chosen image from the dataset ) .

“ The depth of the TC block is determined by the sequence length . In few - shot classification , the sequence length can be known a priori . How is the sequence length determined for reinforcement learning tasks ? In addition , what is done at test - time if the sequence length differs from the sequence length at training time ? ”

>>> In some RL tasks , there is a maximum episode length , and we can choose the depth of the TC block accordingly ( this true for all of the tasks considered in our paper ) . If the episode length is unbounded , or differs between training and test , we can simply choose a reasonable value ( depending on the problem ) and rely on the fact that the attention operation has an infinite receptive field . We can think of the TC block as producing “ local ” features within a long sequence , from which the attentive lookups can select pertinent information .

“ The Prototypical Networks results in Tables 1 and 2 do not appear to match the performance reported in Snell et al . ( 2017 ) . ”

>>> Snell et al . [ 4 ] found that using more classes at training time than at test time improved their model ’s performance . Their best results used 20 classes at training time and 5 at test time . To make their results comparable to prior work , we reported the performance of Prototypical Networks when the same number of classes was used at training and test time ( Appendix Tables 5 and 6 in their paper ) , as all of the methods we listed in Tables 1 & 2 might also benefit from this modification .

“ I am not following the assertion in 5.2.3 that MAML adaption curves can be seen as an upper bound on the performance of gradient - based methods . I am wondering if the authors can clarify this point . ”

>>> These set of experiments were conducted to demonstrate the advantage of having highly general meta-learners where the meta-algorithm is fully - learned , especially when it comes to task distributions with a lot of exploitable structure . These continuous control problems originally introduced by the MAML paper can be easily solved by the agent identifying which task it is in over the first couple timesteps and then proceeding to execute the optimal policy . In the above comment , which we should reword , we meant to say that MAML ’s performance demonstrates that gradient - based methods which take their update steps after several test rollouts are fundamentally disadvantaged compared to RNN - like methods for this problem .


The authors propose a model for sequence classification and sequential decision making . The model interweaves attention layers , akin to those used by Vaswani et al , with temporal convolution . The authors demonstrate superior performance on a variety of benchmark problems , including those for supervised classification and for sequential decision making .

Unfortunately , I am not an expert in meta-learning , so I can not comment on the difficulty of the tasks ( e.g. Omniglot ) used to evaluate the model or the appropriateness of the baselines the authors compare against ( e.g. continuous control ) .

The experiment section definitely demonstrate the effort put into this work . However , my primary concern is that the model seems somewhat lacking in novelty . Namely , it interweaves the Vaswani style attention with with temporal convolutions ( along with TRPO . The authors claim that Vaswani model does not incoporate positional information , but from my understanding , it actually does so using positional encoding . I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making . I think comparison to such a similar model would strengthen the novelty of this paper ( e.g. convolution is a superior method of incorporating positional information ) .

My second concern is that the authors do not provide analysis and / or intuitions on why the proposed models outperform prior art in few - shot learning . I think this information would be very useful to the community in terms of what to take away from this paper . In retrospect , I wish the authors would have spent more time doing ablation studies than tackling more task domains .

Overall , I am inclined to accept this paper on the basis of its experimental results . However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers ( who are hopefully more experienced in this domain ) .

Some minor feedback / questions for the authors :
- I would prefer mathematical equations as opposed to pseudocode formulation
- In the experiment section for Omniglot , when the authors say " 1200 classes for training and 432 for testing " , it sounds like the authors are performing zero - shot learning . How does this particular model generalize to classes not seen during training ?

Please refer to our main response in an above comment that addresses the primary and shared questions amongst all reviewers . Here we respond to your specific comments .

“ The authors claim that Vaswani model does not incorporate positional information , but from my understanding , it actually does so using positional encoding . I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making . I think comparison to such a similar model would strengthen the novelty of this paper ( e.g. convolution is a superior method of incorporating positional information . ”

>>> Vaswani et. al . [ 3 ] add to their feature vector a representation of where the example is in the sequence ( described in section 3.5 of [ 3 ] ) . This method crucially does not perform any local comparisons where the embedding of a particular image is modified depending on the others is being compared against ( which Matching Networks [ 2 ] found to be essential ) . For the final paper , we will conduct an ablation to show how much SNAILs performance degrades when TCs are replaced with this method . Preliminary experiments on the MDP problem using attention like in [ 3 ] ( with the positional encoding mentioned above ) performed marginally better than a random policy . We will include this ablation ( the [ 3 ] - style model on RL tasks ) in the final version of the paper .

“ In the experiment section for Omniglot , when the authors say " 1200 classes for training and 432 for testing " , it sounds like the authors are performing zero - shot learning . How does this particular model generalize to classes not seen during training ? ”

>>> During test - time , for the 1 - shot 5 - way and 5 - shot 5 - way problems , the model is given 1 labeled example of each of the 5 selected test classes and 5 labeled examples of each of the 5 selected test classes respectively . Therefore it is not zero-shot . The set of 432 test classes are not seen during training .


The paper proposes a general neural network structure that includes TC ( temporal convolution ) blocks and Attention blocks for meta-learning , specifically , for episodic task learning . Through intensive experiments on various settings including few - shot image classification on Omniglot and Mini- ImageNet , and four reinforcement learning applications , the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state - of - the- art methods . The experiment results seem solid and the proposed structure is with simple design and highly generalizable . The concern is that the contribution is quite incremental from the theoretical side though it involves large amount of experimental efforts , which could be impactful . Please see the major comment below .

One major comment :
- Despite that the work is more application oriented , the paper would have been stronger and more impactful if it includes more work on the theoretical side .
Specifically , for two folds :
( 1 ) in general , some more work in investigating the task space would be nice . The paper assumes the tasks are “ related ” or “ similar ” and thus transferrable ; also particularly in Section 2 , the authors define that the tasks follow the same distribution . But what exactly should the distribution be like to be learnable and how to quantify such “ related ” or “ similar ” relationship across tasks ?
( 2 ) in particular , for each of the experiments that the authors conduct , it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community ; some questions to answer include : when should we prefer the proposed combination of TC + attention blocks over the other methods ? The result from the paper seems to answer with “ in all cases ” but then that always brings the issue of “ overfitting ” or parameter tuning issue . I believe the paper would have been much stronger if either of the two above are further investigated .

More detailed comments :
- On Page 1 , “ the optimal strategy for an arbitrary range of tasks ” lacks definition of “ range ” ; also , in the setting in this paper , these tasks should share “ similarity ” or follow the same “ distribution ” and thus such “ arbitrariness ” is actually constrained .

- On Page 2 , the notation and formulation for the meta-learning could be more mathematically rigid ; the distribution over tasks is not defined . It is understandable that the authors try to make the paradigm very generalizable ; but the ambiguity or the abstraction over the “ task distribution ” is too large to be meaningful . One suggestion would be to split into two sections , one for supervised learning and one for reinforcement learning ; but both share the same design paradigm , which is generalizable .

- For results in Table 1 and Table 2 , how are the confidence intervals computed ? Is it over multiple runs or within the same run ? It would be nice to make clear ; in addition , I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests . The confidence intervals may not be clear without elaboration ; such is also concerning in the caption for Table 3 about claiming “ not statistically - significantly different ” because no significance test is reported .

- At last , some more details in implementation would be nice ( package availability , run time analysis ) ; I suppose the package or the source code would be publicly available afterwards ?

Please refer to our main response in an above comment that addresses the primary and shared questions amongst all reviewers . Here we respond to your specific comments .

“ in general , some more work in investigating the task space would be nice . The paper assumes the tasks are “ related ” or “ similar ” and thus transferrable ; also particularly in Section 2 , the authors define that the tasks follow the same distribution . But what exactly should the distribution be like to be learnable and how to quantify such “ related ” or “ similar ” relationship across tasks ? ”

>>> Measures of task similarity would certainly be useful in understanding how well we can expect a meta-learner to generalize . However , it remains an open problem and beyond the scope of our work -- our contribution is the proposed class of model architectures , which we experimentally validate on a number of benchmarks ( where there is a high degree of task similarity , and thus potential for meta-learning to succeed ) from the meta-learning literature .

“ On Page 2 , the notation and formulation for the meta-learning could be more mathematically rigid ; the distribution over tasks is not defined . It is understandable that the authors try to make the paradigm very generalizable ; but the ambiguity or the abstraction over the “ task distribution ” is too large to be meaningful . One suggestion would be to split into two sections , one for supervised learning and one for reinforcement learning ; but both share the same design paradigm , which is generalizable . ”

>>> Our formulation of the meta-learning problem is consistent with prior work , as one can see in MAML [ 1 ] and Matching Networks [ 2 ] .

“ For results in Table 1 and Table 2 , how are the confidence intervals computed ? Is it over multiple runs or within the same run ? It would be nice to make clear ; in addition , I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests . The confidence intervals may not be clear without elaboration ; such is also concerning in the caption for Table 3 about claiming “ not statistically - significantly different ” because no significance test is reported . ”

>>> The confidence intervals in Tables 1 & 2 are calculated over 10000 episodes of the evaluation procedure described in Section 5.1 ( 95 % confidence ) . The statistical significance in Table 3 is determined by a one-sided t-test with p=0.05 . We will make these clarifications in the final version of the paper .


The authors train an RNN to perform deduced reckoning ( ded reckoning ) for spatial navigation , and then study the responses of the model neurons in the RNN . They find many properties reminiscent of neurons in the mammalian entorhinal cortex ( EC ) : grid cells , border cells , etc . When regularization of the network is not used during training , the trained RNNs no longer resemble the EC . This suggests that those constraints ( lower overall connectivity strengths , and lower metabolic costs ) might play a role in the EC 's navigation function .

The paper is overall quite interesting and the study is pretty thorough : no major cons come to mind . Some suggestions / criticisms are given below .

1 ) The findings seem conceptually similar to the older sparse coding ideas from the visual cortex . That connection might be worth discussing because removing the regularizing ( i.e. , metabolic cost ) constraint from your RNNS makes them learn representations that differ from the ones seen in EC . The sparse coding models see something similar : without sparsity constraints , the image representations do not resemble those seen in V1 , but with sparsity , the learned representations match V1 quite well . That the same observation is made in such disparate brain areas ( V1 , EC ) suggests that sparsity / efficiency might be quite universal constraints on the neural code .

2 ) The finding that regularizing the RNN makes it more closely match the neural code is also foreshadowed somewhat by the 2015 Nature Neuro paper by Susillo et al . That could be worthy of some ( brief ) discussion .

Sussillo , D. , Churchland , M. M. , Kaufman , M. T. , & Shenoy , K. V. ( 2015 ) . A neural network that finds a naturalistic solution for the production of muscle activity . Nature neuroscience , 18( 7 ) , 1025-1033 .

3 ) Why the different initializations for the recurrent weights for the hexagonal vs other environments ? I 'm guessing it 's because the RNNs do n't " work " in all environments with the same initialization ( i.e. , they either do n't look like EC , or they do n't obtain small errors in the navigation task ) . That seems important to explain more thoroughly than is done in the current text .

4 ) What happens with ongoing training ? Animals presumably continue to learn throughout their lives . With on -going ( continous ) training , do the RNN neurons ' spatial tuning remain stable , or do they continue to " drift " ( so that border cells turn into grid cells turn into irregular cells , or some such ) ? That result could make some predictions for experiment , that would be testable with chronic methods ( like Ca2 + imaging ) that can record from the same neurons over multiple experimental sessions .

5 ) It would be nice to more quantitatively map out the relation between speed tuning , direction tuning , and spatial tuning ( illustrated in Fig. 3 ) . Specifically , I would quantify the cells ' direction tuning using the circular variance methods that people use for studying retinal direction selective neurons . And I would quantify speed tuning via something like the slope of the firing rate vs speed curves . And quantify spatial tuning somehow ( a natural method would be to use the sparsity measures sometimes applied to neural data to quantify how selective the spatial profile is to one or a few specific locations ) . Then make scatter plots of these quantities against each other . Basically , I 'd love to see the trends for how these types of tuning relate to each other over the whole populations : those trends could then be tested against experimental data ( possibly in a future study ) .

Thank you for your positive assessment and feedback .

The reviewer raised the interesting point on the potential connection between the sparse coding work by Olshausen and Field ( 1996 ) and our work ( note that Reviewer 3 also raised the same point ) . Indeed , the initial idea of this work is partly inspired by sparse coding . In the Discussion section , we have now included a detailed discussion of the relation of our work to the sparse coding work . While there are important conceptual similarities , there are also some important differences that need to be mentioned . For example , the grid-like response patterns are averaged responses , and are not linear filters based on the input to the network as the Gabor filters . In our network , the velocity response needs to be integrated and maintained in order for the neurons to be spatially tuned . Also , the sparsity constraint in Olshausen and Field could be interpreted as imposing a particular form of Bayesian prior , while in our work , we find it difficult to interpret that way . We also include a discussion on Sussillo et al. ( 2015 ) , which we agree is very relevant to our work . Thank you for both suggestions .

Concerning the different initializations , we want to clarify that we tried a few different initializations and found the results to be robust . As a way to show the robustness , we have presented the results with different initializations . We have slightly changed the text to make this point more explicit . We apologize for the confusion .

On the question of ongoing training - we think this is a great idea . We have thought about these issues , including training the same network to perform tasks in various environments , but on the other hand , we also think a systematic treatment of these issues goes beyond the scope of the current paper , which is just a proof - of - principle of the approach .

The reviewer also suggests a more quantitative characterization of the relation between different kinds of tuning properties . We have computed the selectivity indexes for different tuning properties along these lines . It appears that at the population level , the relation between these different indexes is complex . We now incorporate a figure in the Appendix . We also examined the experimental literature on the relation between different types of tuning in EC , and there is not enough evidence to draw any conclusions . Thus , we think this is an interesting and important question that should be informative for future experiments . But one might need some more sophisticated ways to perform this analysis in order to better reveal the dependence between different types of tuning . For example , maybe one could first do a systematic clustering based on the neurons ’ response , and examine the dependence of the tuning based on the clustering . We would like to look deeper into these issues in the near future . Thank you for this suggestion .


Congratulations on a very interesting and clear paper . While ICLR is not focused on neuroscientific studies , this paper clearly belongs here as it shows what representations develop in recurrent networks that are trained on spatial navigation . Interestingly , these include representations that have been observed in mammals and that have attracted considerable attention , even honored with a Nobel prize .

I found it is very interesting that the emergence of these representations was contingent on some regularization constraint . This seems similar to the visual domain where edge detectors emerge easily when trained on natural images with sparseness constraints as in Olshausen& Field and later reproduced with many other models that incorporate sparseness constraints .

I do have some questions about the training itself . The paper mentions a metabolic cost that is not specified in the paper . This should be added .

My biggest concern is about Figure 6a . I am puzzled why is the error is coming down before the boundary interaction ? Even more puzzling , why does this error go up again for the blue curve ( no interaction ) ? Should n’t at least this curve be smooth ?


Thank you for your positive assessment and feedback .

The reviewer raised the interesting point on the potential connection between the sparse coding work by Olshausen and Field ( 1996 ) and our work ( note that Reviewer 1 raised a similar point ) . Indeed , the initial idea behind this work is partly inspired by sparse coding . In the Discussion section , we have now included a detailed discussion of the relation of our work to the sparse coding work . We also think while there are important conceptual similarities , there are also some important differences that need to be mentioned . For example , the grid-like response patterns are averaged responses , they are not linear filters based on the input to the network as the Gabor filters . In our network , the velocity response needs to be integrated and maintained in order for the neurons to be spatially tuned . Also , the sparsity constraint in Olshausen and Field could be interpreted as imposing a particular form of Bayesian prior , while in our work , we find it difficult to interpret that way . We also add discussions on a few other related studies . We thank the reviewer for this suggestion .

In terms of Figure 6a , the increase of the blue curve after the boundary interaction is due to the accumulation of noise in the RNN , which would gradually increase the error ( roughly linearly , thinking about Brownian motion may help here ) without further boundary interactions . We now explain this in the caption of Figure 6 . The reason the blue curve is not entirely smooth is that it is the averaged error of a set of sample paths , and the number of paths is not very large . However , we think the general pattern is clear . In terms of why the error is coming down before the boundary interaction , one possible yet speculative reason is that the boundary - related firing has a certain width , thus the error-correction might already start before the boundary interaction . In fact , this would be a natural interpretation based on a previous model which used boundary responses to correct the grid-drift ( Hardcastle et al. , Neuron , 2015 ) . We note that one caveat to that interpretation is that , in our simulations , sometimes we do n’t observe such a drop in error before the boundary interactions . Also , it remains unclear if the error-correction mechanisms in our model are the same as those proposed previously . However , the consistent pattern we observe empirically is that error robustly reduced after the boundary interaction . We thus have focused on this robust empirical observation , without speculating on an exact mechanism , which remains to be elucidated in the future .


This paper aims at better understanding the functional role of grid cells found in the entorhinal cortex by training an RNN to perform a navigation task .

On the positive side :

This is the first paper to my knowledge that has shown that grid cells arise as a product of a navigation task demand . I enjoyed reading the paper which is in general clearly written . I have a few , mostly cosmetic , complaints but this can easily be addressed in a revision .

On the negative side :

The manuscript is not written in a way that is suitable for the target ICLR audience which will include , for the most part , readers that are not expert on the entorhinal cortex and / or spatial navigation .

First , the contributions need to be more clearly spelled out . In particular , the authors tend to take shortcuts for some of their statements . For instance , in the introduction , it is stated that previous attractor network type of models ( which are also recurrent networks ) “ [ ...] require hand - crafted and fined tuned connectivity patterns , and the evidence of such specific 2D connectivity patterns has been largely absent . ” This statement is problematic for two reasons :

( i ) It is rather standard in the field of computational neuroscience to start from reasonable assumptions regarding patterns of neural connectivity then proceed to show that the resulting network behaves in a sensible way and reproduces neuroscience data . This is not to say that demonstrating that these patterns can arise as a byproduct is not important , on the contrary . These are just two complementary lines of work . In the same vein , it would be silly to dismiss the present work simply because it lacks spikes .

( ii ) the authors do not seem to address one of the main criticisms they make about previous work and in particular " [ a lack of evidence ] of such specific 2D connectivity patterns " . My understanding is that one of the main assumptions made in previous work is that of a center - surround pattern of lateral connectivity . I would argue that there is a lot of evidence for local inhibitory connection in the cortex . Somewhat related to this point , it would be insightful to show the pattern of local connections learned in the RNN to see how it differs from the aforementioned pattern of connectivity .

Second , the navigation task used needs to be better justified . Why training a network to predict 2D spatial location from velocity inputs ? Why is this a reasonable starting point to study the emergence of grid cells ? It might be obvious to the authors but it will not be to the ICLR audience . Dead-reckoning ( i.e. , spatial localization from velocity inputs ) is of critical ecological relevance for many animals . This needs to be spelled out and a reference needs to be added . As a side note , I would have expected the authors to use actual behavioral data but instead , the network is trained using artificial trajectories based on " modified Brownian motion ” . This seems like an important assumption of the manuscript but the issue is brushed off and not discussed . Why is this a reasonable assumption to make ? Is there any reference demonstrating that rodent locomotory behavior in a 2D arena is random ?

Figure 4 seems kind of strange . I do not understand how the “ representative units ” are selected and where the “ late ” selectivity on the far right side in panel a arises if not from “ early ” units that would have to travel “ far ” from the left side … Apologies if I am missing something obvious .

I found the study of the effect of regularization to be potentially the most informative for neuroscience but it is only superficially treated . It would have been nice to see a more systematic treatment of the specifics of the regularization needed to get grid cells .

Thank you for your positive assessment and feedback . Below we address the concerns raised in the review . We have taken the suggestions in the review to make this manuscript as accessible to the general ICLR audience as possible .

1 . Concerning the attractor model to motivate our model , we agree with the reviewer that our initially statement is unclear and potentially misleading . We have thus revised the statements in the Introduction to better motivate our work . Specifically , we now present a more balance view of the two types of models , and explicitly present our modeling as a complementary / alternative approach . Also we are now more specific about the assumptions made in the attractor network model . In particular we point out that it is the subtle asymmetry in the weights we are mostly concerned about , not the local inhibition . We agree with the reviewer that local inhibition is not uncommon in cortex . However , these models also assume systematic weight asymmetry in the connectivity matrix , which is responsible for the movement of the bump attractor for tracking the animal ’s location . Without such asymmetry , grids would not emerge in these models . For example , please see Eq. ( 2 ) in Couey et al . " Recurrent inhibitory circuitry as a mechanism for grid formation . " Just to be clear , we are not claiming that such connectivity pattern does not exist in the brain . We are just making the points that 1 ) this assumed pattern seems too restrictive , 2 ) so far there is no evidence for it . Furthermore , now we also motivate our work by the need for having a model that could account for other types of response patterns in Entorhinal Cortex . Note that such issues have been largely ignored previously .
As for the connections in our trained RNN , we did perform some preliminary analysis . We now include a discussion on this point in the final paragraph of the Discussion .

2 . Concerning the justification of the navigation task -
We completely agree that it would be useful to make the ecological relevance of dead-reckoning task explicit , in particular for the ICLR audience . We have discussed this point and add relevant references there . We thank the reviewer for this helpful suggestion .

On the issues of using artificial v.s. animal ’s real trajectory to train the model - Historically there is a long debate in neuroscience on the use of artificial or naturalistic stimuli , in particular for vision . For example , see Rust and Movshon " In praise of artifice . " In the present paper , we did n’t use the animal ’s real running trajectories for multiple reasons . First , we did n’t have access to enough data of animal ’s running trajectories . Second , from a theoretical point of view , it seems advantageous to start with simple trajectories that we have complete control over . Third , grid-like responses have been reported in various animal species , including rat , mouse , bat and humans . The locomotory behavior of these different animal species can be quite different . For example , there are qualitative differences between rats and mice . We thus started from simple movement statistics to see what we can get . Having said that , we do agree with the reviewer that it would be very interesting to test the model using real trajectories . We briefly discussed these points in section 2.2 . Also , as a side note , we have been talking to experimental colleagues to get data of the mice ’s running behavior .

3 . The representative units are picked manually based on their spatial firing pattern . We simply identified a few units that clearly show grid-like or boundary - related responses , and marked them after the dimensionality reduction using t-SNE . We want to emphasize that the main point of Figure 4 is to show that early during training all responses are similar to the boundary related responses , and only as training continues do the grid-like units emerge . This did not have to be true . Early in training some units could have boundary responses and others could have grid-like responses . If these spatial response patterns remained largely unchanged during training then all units would travel a short distance in the t-SNE figure . This is in fact the case for the boundary responses : the final spatial responses are very close to the responses that developed early in training and so these units do not travel much in this space . We have modified the text to make it more clear . Thank you for raising this point .

4 . In terms of the role of regularization , we agree that it is potentially interesting from the neuroscience point of view . While we think the most informative effect remains the one we show in the main text , we now also include a new figure in the Appendix for the triangular environment to better illustrate the effect of the metabolic cost and adding noise to the RNN units ( without being too redundant to the figure in the main text ) . We would be happy to include more simulations on this if the reviewer thinks that ’s necessary .


This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied .
While the attack model is quite general , the current bound is only valid for linear and NN with one hidden layer model , so the result is quite restrictive .

However the new bound is an " upper " bound of the worst - case performance which is very different from the conventional sampling based " lower " bounds . Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier .
This paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization ( loss + upper bound ) .
In conclusion , I recommend this paper to be accepted , since it presents a new and feasible direction of a principled approach to train a robust classifier , and the paper is clearly written and easy to follow .

There are possible future directions to be developed .

1 . Apply the sum-of-squares ( SOS ) method .
The paper 's SDP relaxation is the straightforward relaxation of Quadratic Program ( QP ) , and in terms of SOS relaxation hierarchy , it is the first hierarchy . One can increase the complexity going beyond the first hierarchy , and this should provides a computationally more challenging but tighter upper bound .
The paper already mentions about this direction and it would be interesting to see the experimental results .

2 . Develop a similar relaxation for deep neural networks .
The author already mentioned that they are pursuing this direction . While developing the result to the general deep neural networks might be hard , residual networks maybe fine thanks to its structure .

Thanks for the comments and thoughtful suggestions . Adding to the recommendations about the future work :

1 . Sum-of-squares ( SOS ) method -- It is indeed interesting to check whether the higher degree of SOS gives us sufficient tightness to significantly improve the results . An obvious bottleneck in trying this out is the expensive computation . Given that our objective is similar to MAXCUT , for which it is currently unknown whether higher degree SOS relaxations give better approximation ratios , it is apriori unclear how much we could gain .

2 . Develop a similar relaxation for deeper networks -- We agree with the reviewer that this is an interesting direction to pursue . In fact , we have already begun implementing an algorithm that works for arbitrary depth networks . As described in the response to reviewer 1 , the basic idea is that the adversarial loss for arbitrary ReLU networks can be written as a non-convex quadratic program , which can then be relaxed to an SDP and trained with similar ideas to the present paper . As the reviewer mentions , it ’s possible that resnets have additional structure that can be exploited efficiently , but our current proposal handles resnets as well .


The authors propose a new defense against security attacks on neural networks . The attack model involves a standard l_inf norm constraint . Remarkably , the approach outputs a security certificate ( security guarantee ) on the algorithm , which makes it appealing for security use in practice . Furthermore , the authors include an approximation of the certificate into their objective function , thus training networks that are more robust against attacks . The approach is evaluated for several attacks on MNIST data .

First of all , the paper is very well written and structured . As standard in the security community , the attack model is precisely formalized ( I find this missing in several other ML papers on the topic ) . The certificate is derived with rigorous and sound math . An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown . An innovative training criterion based on that certificate is proposed . Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks .

In summary , this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation . For me , it is a clear accept . The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al .


Thanks for your interest in our work and the pointer to the relevant recent work by Hein and Andriushschenko . We have fixed this omission and include a discussion of the paper in the newest uploaded version of our work . To summarize the comparison : Firstly , their work focuses on perturbations in l- 2 norm while ours considers the l-infty norm . Hence , there is no direct way to compare the experimental results . Theoretically , the general bound proposed for any l-p norm perturbation is similar to what we have in our work . However , the main challenge is to efficiently evaluate this bound . Hein and Andriushschenko show how to do this for p=2 . In our work , we consider the attack model where p = \infty . This makes a significant difference in the computations involved .


This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer . The upper bound is derived via ( 1 ) theorem of middle value ; ( 2 ) replace the middle value by the maximum ( eq 4 ) ; ( 3 ) replace the maximum of the gradient value ( locally ) by the global maximal value ( eq 5 ) ; ( 4 ) this leads to a non-convex quadratic program , and then the authors did a convex relaxation similar to maxcut to upper bound the function by a SDP , which then can be solved in polynomial time .

The main idea of using upper bound ( as opposed to lower bound ) is reasonable . However , I find there are some limitations / weakness of the proposed method :
1 . The method is likely not extendable to more complicated and more practical networks , beyond the ones discussed in the paper ( ie with one hidden layer )
2 . SDP while tractable , would still require very expensive computation to solve exactly .
3 . The relaxation seems a bit loose - in particular , in above step 2 and 3 , the authors replace the gradient value by a global upper bound on that , which to me seems can be pretty loose .

Thank you for the comments ! From our understanding of your review , it seems that there are three concerns , which we highlight and address below .

1 . “ The method is likely not extendable to more complicated and more practical networks , beyond the ones discussed in the paper ( ie with one hidden layer ) ”

Our general approach for obtaining networks with certified robustness can in fact extend to deeper networks . We have already begun implementing an algorithm that works for arbitrary depth networks . The basic idea is that the adversarial loss for arbitrary ReLU networks can be written as a non-convex quadratic program , which can then be relaxed to an SDP and trained with similar ideas to the present paper . We would be happy to give details if it would be helpful .


2 . “ SDP while tractable , would still require very expensive computation to solve exactly . ”

We would like to stress that we do not need to solve the SDP exactly . As discussed in Section 4 of our paper , our network can be trained via gradient descent on the dual . Even with inexact minimization of these dual variables , we get valid certificates . We acknowledge that training our model is slower than training regular networks , but it is not nearly as bad as if one had to exactly solve the SDP .

We also note that at test time , the trained dual variables directly provide a certificate ( with no extra computation ) and hence checking robustness at test time is no slower than generating predictions for a regular network .


3 . " The relaxation seems a bit loose - in particular , in above step 2 and 3 , the authors replace the gradient value by a global upper bound on that , which to me seems can be pretty loose . "

An important insight from our experiments is the following : while our SDP bound can be quite loose on arbitrary networks , optimizing against this SDP certificate leads to networks where this certificate is substantially tighter ( as seen in Figure 3 ) . Minimizing the SDP upper bound forces the optimizer to avoid where the bound is loose , as such points have higher objective values . Hence , the general looseness of the relaxation does not impede the utility of the relaxation as a way of obtaining provably robust networks .


--------------
Summary :
--------------
This paper presents a series of experiments on language emergence through referential games between two agents . They ground these experiments in both fully - specified symbolic worlds and through raw , entangled , visual observations of simple synthetic scenes . They provide rich analysis of the emergent languages the agents produce under different experimental conditions . This analysis ( especially on raw pixel images ) make up the primary contribution of this work .


--------------
Evaluation :
--------------
Overall I think the paper makes some interesting contributions with respect to the line of recent ' language emergence ' papers . The authors provide novel analysis of the learned languages and perceptual system across a number of environmental settings , coming to the ( perhaps uncontroversial ) finding that varying the environment and restrictions on language result in variations in the learned communication protocols .

In the context of existing literature , the novelty of this work is somewhat limited -- consisting primarily of the extension of multi-agent reference games to raw- pixel inputs . While this is a non-trivial extension , other works have demonstrated language learning in similar referring - expression contexts ( essentially modeling only the listener model [ Hermann et.al 2017 ] ) .

I have a number of requests for clarification in the weaknesses section which I think would improve my understanding of this work and result in a stronger submission if included by the authors .

--------------
Strengths :
--------------
- Clear writing and document structure .


- Extensive experimental setting tweaks which ablate the information and regularity available to the agents . The discussion of the resulting languages is appropriate and provides some interesting insights .


- A number of novel analyses are presented to evaluate the learned languages and perceptual systems .


--------------
Weaknesses :
--------------
- How stable are the reported trends / languages across multiple runs within the same experimental setting ? The variance of REINFORCE policy gradients ( especially without a baseline ) plus the general stochasticity of SGD on randomly initialized networks leads me to believe that multiple training runs of these agents might result is significantly different codes / performance . I am interested in hearing the author 's experiences in this regard and if multiple runs present similar quantitative and qualitative results . I admit that expecting identical codes is unrealistic , but the form of the codes ( i.e. primarily encoding position ) might be consistent even if the individual mappings are not ) .


- I do n't recall seeing descriptions of the inference - time procedure used to evaluate training / test accuracy . I will assume argmax decoding for both speaker and listener . Please clarify or let me know if I missed something .


- There is ambiguity in how the " protocol size " metric is computed . In Table 1 , it is defined as ' the effective number of unique message used ' . This comes back to my question about decoding I suppose , but does this count the ' inference - time ' messages or those produced during training ?
Furthermore , Table 2 redefines " protocol size " as the percentage of novel message . I assume this is an editing error given the values presented and take these columns as counts . It also seems " protocol size " is replaced with the term " lexicon " from 4.1 onward .

- I 'm surprised by how well the agents generalize in the raw pixel data experiments . In fact , it seems that across all games the test accuracy remains very close to the train accuracy .

Given the dataset is created by taking all combinations of color / shape and then sampling 100 location / floor color variations , it is unlikely that a shape / color combo has not been seen in training . Such that the only novel variations are likely location and floor color . However , taking Game A as an example , the probe classifiers are relatively poor at these attributes -- indicating the speaker 's representation is not capturing these attributes well . Then how do the agents effectively differentiate so well between 20 images leveraging primarily color and shape ?

I think some additional analysis of this setting might shed some light on this issue . One thought is to compute upper-bounds based on ground truth attributes . Consider a model which knows shape perfectly , but cannot predict other attributes beyond chance . To compute the performance of such a model , you could take the candidate set , remove any instances not matching the ground truth shape , and then pick randomly from the remaining instances . Something similar could be repeated for all attributes independently as well as their combinations -- obviously culminating in 100 % accuracy given all 4 . It could be that by dataset construction , object location and shape are sufficient to achieve high accuracy because the odds of seeing the same shape at the same location ( but different color ) is very low .

Given these are operations on annotations and do n't require time - consuming model training , I hope to see this analysis in the rebuttal to put the results into appropriate context .


- What is random chance for the position and floor color probe classifiers ? I do n't think it is mentioned how many locations / floor colors are used in generation .


- Relatively minor complaint : Both agents are trained via the REINFORCE policy gradient update rule ; however , the listener agent makes a fairly standard classification decision and could be trained with a standard cross-entropy loss . That is to say , the listener policy need not make intermediate discrete policy decisions . This decision to withhold available supervision is not discussed in the paper ( as far as I noticed ) , could the authors speak to this point ?



--------------
Curiosities :
--------------
- I got the impression from the results ( specifically the lack of discussion about message length ) that in these experiments agents always issued full length messages even though they did not need to do so . If true , could the authors give some intuition as to why ? If untrue , what sort of distribution of lengths do you observe ?

- There is no long term planning involved in this problem , so why use reinforcement learning over some sort of differentiable sampler ? With some re-parameterization ( i.e. Gumbel - Softmax ) , this model could be end - to - end differentiable .


--------------
Minor errors :
--------------
[ 2.2 paragraph 1 ] LSTM citation should not be in inline form .
[ 3 paragraph 1 ] ' Note that these representations do care some ' -> carry
[ 3.3.1 last paragraph ] ' still able comprehend ' --> to


-------
Edit
-------
Updating rating from 6 to 7 .

We thank the reviewer for their thorough review . We respond to the comments raised while we are in the process of making the necessary changes in the manuscript .

< review >
How stable are results ?
< / review >
Overall , results with REINFORCE in these non-stationary multi-agent environments ( where speakers and listeners are learning at the same time ) show instability , and -- as expected -- some of the experimental runs did not converge . However , we believe that the stability of the nature of the protocol ( rather than its existence ) is mostly influenced by the configuration of the game itself , i.e. , how constrained the message space is . As an example , games C & D impose constraints on the nature of the protocol since location encoding location on the messages is not an acceptable solution -- on runs that we had convergence , the protocols would always communicate about color . The same holds for game A ( position is a very good strategy since it uniquely identifies objects combined with the environmental pressure of many distractors ) . However , game B is more unconstrained in nature and the converged protocols were more varied . We will include a discussion of these observations in the updated manuscript .

< review >
Inference time procedure
< / review >
The reviewer is correct . At training time we sample , at test time we argmax . We will clarify this .

< review >
Protocol size vs lexicon
< / review >
Thank you for pointing this out . We will clarify the terminology .
Protocol size ( or lexicon -- we will remove this term and use protocol size only ) is the number of invented messages ( sequences of symbols ) .
In Table 1 , we report the protocol size obtained with argmax on training data .
In Table 2 , we report the number of novel messages , i.e. , messages that were not generated for the training data , on 100 novel objects .

< review >
Generalization on raw pixel data -- training and test accuracy are close
< / review >
This observation is correct . By randomly creating train and test splits , chances are that the test data contain objects of seen color and shape combination but unseen location . Neural networks ( and any other parametric model ) do better in these type of “ smooth ” generalizations caused by a continuous property like location .

< review >
However , taking Game A as an example , the probe classifiers are relatively poor at these attributes -- indicating the speaker 's representation is not capturing these attributes well .
Then how do the agents effectively differentiate so well between 20 images leveraging primarily color and shape ?
< / review >
In Game A , agents differentiate 20 objects leveraging primarily object position rather than color and shape .
In Game A , the listener needs to differentiate between 20 objects , and so , communicating about color and shape is not a good strategy as there are chances that there will be some other red cube , for example , on the distractor list . The probe classifiers are performing relatively poorly on these attributes ( especially on the object color ) whereas they perform very well on position ( which is in fact a good strategy ) , which as we find by our analysis is what the protocol describes . We note that location is a continuous variable ( which we discretize only for performing the probe analysis in Section 4.2 ) and so it is very unlikely that two objects have the same location , thus uniquely identifying objects among distractors . This is not the case for games C & D since the listener sees a variation of the speaker ’s target .
Moreover , we note , that object location is encoded across all games .

< review >
Upper-bound analysis based on ground truth attributes .
< / review >
We agree with the reviewer that an upper-bound analysis relying on gold information of objects will facilitate the exposition of results . Note that since location is a continuous variable , ground truth of location is not relevant .
color shape color & shape
A 0.37 0.24 0.80
B & C 0.93 0.90 0.98
D 0.89 0.89 0.98

We could perform the same analysis by discretizing the location in the same way we performed the probe analysis in Section 4.2 , however , the upper-bound results depend on the number of discrete locations we derive .
location color & location shape & location
A 0.69 0.95 0.92
B 0.97 0.99 0.99
( for C and D results for location are not applicable )






< review >
random chance of probe classifiers .
< / review >
When generating the dataset , we sample locations and floor colors from a continuous scale . For the probe classifiers , we quantize location by clustering each coordinate in 5 clusters ( and thus accuracy is reported by averaging the performance of the x and y probe classifiers with chance being at 20 % for each co-ordinate ) and floor colors in 3 clusters ( with chance being at 33 % ) . We will include the chance levels in Table 4 .

< review >
Why not use cross-entropy loss for listener ?
< / review >
We decided to train both agents via REINFORCE for symmetry . Given the nature of the listener ’s choice , we do n’t anticipate full supervision to have an effect other than speeding up learning .


< review >
What about message length ?
< / review >
Without any explicit penalty on the length of the messages ( Section 2 ) , agents are not motivated to produce shorter messages ( despite the fact that as the reviewer points , agents can decide to do so ) since this constrains the space of messages ( and thus the possibility of the speaker and listener agreeing on a successful naming convention ) . When we introduced a penalty on the length of the message ( Section 3 ) , agents produced shorter messages for the ambiguous messages ( since this strategy maximizes the total expected reward ) .

< review >
Why use reinforcement learning over some sort of differentiable sampler ?
< / review >
While a differentiable communication channel would make learning faster , it goes against the basic and fundamental principles of human communication ( and also against how this phenomenon is studied in language evolution ) . Simply put , having a differentiable channel would mean in practice that speakers can back - propagate through listeners ’ brains ( which unfortunately is not the case in real life :) ) We wanted to stay as close as possible to this communication paradigm , thus using a discrete communication channel .

This paper presents a set of studies on emergent communication protocols in referential games that use either symbolic object representations or pixel - level representations of generated images as input . The work is extremely creative and packed with interesting experiments .

I have three main comments .

* CLARITY OF EXPOSITION

The paper was rather hard to read . I 'll provide some suggestions for improvement in the minor- comments section below , but one thing that could help a lot is to establish terminology at the beginning , and be consistent with it throughout the paper : what is a word , a message , a protocol , a vocabulary , a lexicon ? etc .

* RELATION BETWEEN VOCABULARY SIZE AND PROTOCOL SIZE

In the compositional setup considered by the authors , agents can choose how many basic symbols to use and the length of the " words " they will form with these symbols . There is virtually no discussion of this interesting interplay in the paper . Also , there is no information about the length distribution of words ( in basic symbols ) , and no discussion of whether the latter was meaningful in any way .

* RELATION BETWEEN CONCEPT -PROPERTY AND RAW - PIXEL STUDIES

The two studies rely on different analyses , and it is difficult to compare them . I realize that it would be impossible to report perfectly comparable analyses , but the authors could at least apply the " topographic " analysis of compositionality in the raw- pixel study as well , either by correlating the CNN - based representational similarities of the Speaker with its message similarities , or computing similarity of the inputs in discretized , symbolic terms ( or both ? ) .

* MINOR / DETAILED COMMENTS

Section 1

How do you think emergent communication experiments can shed light on language acquisition ?

Section 2

In figure 1 , the two agents point at nothing .

\mathbf{v} is a set , but it 's denoted as a vector . Right below that , h^S is probably h^L ?

all candidates c \in C : or rather their representations \mathbf{v} ?

Give intuition for the reward function .

Section 3

We use the dataset of Visual Attributes ... : drop " dataset "

I think the pre-linguistic objects are not represented by 1 - hot , but binary vectors .

do care some inherent structure : carry

Note that symbols in V have no pre-defined semantics ... : This is repeated multiple times .

Section 3

I could n't find simulation details : how many training elements , and how is training accuracy computed ? Also , " training data " , " training accuracy " are probably misleading terms , as I suppose you measured performance on new combinations of objects .

I find " Protocol Size " to be a rather counterintuitive term : maybe call Vocabulary Size " Alphabet Size " , and Protocol Size " Lexicon Size " ?

State in Table 1 caption that the topographic measure will be explained in a later section . Also , the - 1 is confusing : you can briefly mention when you introduce the measure that since you correlate a distance with a similarity you expect an inverse relation ? Also , you mention in the caption that all Spearman rhos are significant , but where are they presented again ?

Section 3.2

Does the paragraph starting with " Note that the distractor " refer to a figure or table that is not there ? If not , it should be there , since it 's not clear what are the data that support your claims there . Also , you should explain what the degenerate strategy the agents find is .

Next paragraph :

- I find the usage of " obtaining " to refer to the relation between messages and objects strange .

- in which space are the reported pairwise similarities computed ?

- make clear that in the non-uniform case confusability is less influenced by similarity since the agents must learn to distinguish between similar objects that naturally co-occur ( sheep and goats )

- what is the expected effect on the naturalness of the emerged language ?

Section 3.3

adhere to , the ability to : " such as " missing ?

Is the unigram chimera distribution inferred from the statistics over the distribution of properties across all concepts or what ? ( please clarify . )

In Tables 2 and 3 , why is vocabulary size missing ?

In Table 2 , say that the protocol size columns report novel message percentage ** for the " test " conditions ***

Figure 2 : spelling of Levensthein

Section 3.3.2

while for languages ( c , d ) ... something missing .

with a randomly initialized ... : no a

More importantly , I do n't understand this " random " setup : if architecture was fixed and randomly initialized , how could something be learned about the structure of the data ?

Section 4

Refer to the images the agents must communicate about as " scenes " , since objects are just a component of them .

What are the absolute sizes of train and test splits ?

Section 4.1

we do not address this issue : the issue

Section 4.2

at least in the game C&D : games

Why is Appendix A containing information that logically follows that in Appendix B?


We would like to thank the reviewer for their review . We found their comments extremely helpful and we are in the process of updating the manuscript accordingly . We will upload the revised paper tomorrow . In the meantime , we respond here to the major comments .


< review >
* CLARITY OF EXPOSITION
< / review >
We will introduce the terminology together with the description of the game .

< review >
* RELATION BETWEEN VOCABULARY SIZE AND PROTOCOL SIZE
< / review >
Without any explicit penalty on the length of the messages ( Section 2 ) , agents are not motivated to produce shorter messages ( despite the fact that as the reviewer points , agents can decide to do so ) since this constrains the space of messages ( and thus the possibility of the speaker and listener agreeing on a successful naming convention ) , opting thus to always make use of the maximum possible length . When we introduced a penalty on the length of the message ( Section 3 ) , agents produced shorter messages for the ambiguous messages since this strategy maximizes the total expected reward .


< review >
* RELATION BETWEEN CONCEPT -PROPERTY AND RAW - PIXEL STUDIES
< / review >
Thanks for the suggestion . Correlating CNN - based representations with message similarities would not yield any new insight since these representations are the input to the message generation process . However , we ran the analysis on the symbolic representations of the images ( location cluster , color , shape , floor color cluster ) and the messages and found that the topographic similarities of the games are ordered as follows ( in parentheses we report the topographic $ \rho$ ) : game A ( 0.13 ) > game C ( 0.07 ) > game D ( 0.06 ) > game B ( 0.006 ) .
This ordering is in line with our qualitative analysis of the protocols presented in Section 4.1.

< review >
Figures / Tables for " Note that the distractor " paragraph and degenerate strategy .
< / review >
We will include in the manuscript the training curves that this paragraph refers to .
The degenerate strategy is that of picking a target at random from the topically relevant set of distractors , thus reducing the effective size of distractors .

< review >
" random " setup ...
< / review >
Despite the fact that the weights of the networks are random , since the message generation is a parametric process , similar inputs will tend to generate similar outputs , thus producing messages that retain ( at least to some small degree ) the structure of the input data , despite the fact that there is no learning at all .


This paper presents an analysis of the communication systems that arose when neural network based agents played simple referential games . The set up is that a speaker and a listener engage in a game where both can see a set of possible referents ( either represented symbolically in terms of features , or represented as simple images ) and the speaker produces a message consisting of a sequence of numbers while the listener has to make the choice of which referent the speaker intends . This is a set up that has been used in a large amount of previous work , and the authors summarize some of this work . The main novelty in this paper is the choice of models to be used by speaker and listener , which are based on LSTMs and convolutional neural networks . The results show that the agents generate effective communication systems , and some analysis is given of the extent to which these communications systems develop compositional properties – a question that is currently being explored in the literature on language creation .

This is an interesting question , and it is nice to see worker playing modern neural network models to his question and exploring the properties of the solutions of the phone . However , there are also a number of issues with the work .

1 . One of the key question is the extent to which the constructed communication systems demonstrate compositionality . The authors note that there is not a good quantitative measure of this . However , this is been the topic of much research of the literature and language evolution . This work has resulted in some measures that could be applied here , see for example Carr et al . ( 2016 ) : http://www.research.ed.ac.uk/portal/files/25091325/Carr_et_al_2016_Cognitive_Science.pdf

2 . In general the results occurred be more quantitative . In section 3.3.2 it would be nice to see statistical tests used to evaluate the claims . Minimally I think it is necessary to calculate a null distribution for the statistics that are reported .

3 . As noted above the main novelty of this work is the use of contemporary network models . One of the advantages of this is that it makes it possible to work with more complex data stimuli , such as images . However , unfortunately the image example that is used is still very artificial being based on a small set of synthetically generated images .

Overall , I see this as an interesting piece of work that may be of interest to researchers exploring questions around language creation and language evolution , but I think the results require more careful analysis and the novelty is relatively limited , at least in the way that the results are presented here .

We thank the reviewer for their comments .
For replying , we copy - paste the relevant part and comment on it .

< review > 1 . One of the key question ... Carr et al. ( 2016 ) : http://www.research.ed.ac.uk/portal/files/25091325/Carr_et_al_2016_Cognitive_Science.pdf "
< / review >

We agree with the reviewer that there are good existing measures . Our point was only that there is no mathematical definition and hence no definitive measure . In fact , we do include such a measure found in the literature on language evolution . Our topographic similarity measure ( which is introduced by Brighton & Kirby ( 2006 ) ) is in line with the measure introduced in 2.2.3 in Carr et al .. In Carr et al , the authors correlate Levenshtein message distances and triangle dissimilarities ( as obtained from humans ) . In our study , we correlate Levenshtein message distances and object dissimilarities as obtained by measuring cosine distance of the object feature norms ( which are produced by humans ) . We will make sure to make this connection to previous literature explicit in our description of the measure .

< review >
2 . In general the results occurred be more quantitative .... statistics that are reported .
< / review >

We agree with the reviewer that statistical tests are important , and we politely point out that our claims on 3.3.2 are in fact based on the reported numbers in Table 1 “ topographic ρ ” column . However , we will evaluate the statistical significance of the “ topographic ρ ” measure by calculating the null distribution via a repeated shuffling of the Levenshtein distances ( or an additional test if the reviewer has an alternative suggestion ) .

< review >
3 . As noted above the main novelty of this work is the use of contemporary network models
< / review >

We believe the novelty of this work is to take the well - defined and interesting questions that the language evolution literature has posed and try to scale them up to contemporary deep learning models and materials , i.e. , realistic stimuli in terms of objects and their properties ( see Section 3 ) , raw pixel stimuli ( see Section 4 ) and neural network architectures ( see Section 2 ) . This kind of interdisciplinary work can not only inform current models on their strengths and weaknesses ( as we note in Section 4 we find that neural networks starting from raw pixels cannot out - of - the - box process easily stimuli in a compositional way ) , but also open up new possibilities for language evolution research in terms of more realistic model simulations . We believe that this might not have been clear from the manuscript and will update the abstract and conclusion to reflect the premises of the work .

< review >
One of the advantages of this is that it makes it possible to work with more complex data stimuli , such as images . However , unfortunately the image example that is used is still very artificial being based on a small set of synthetically generated images .
< / review >

More complex image stimuli and realistic simulations is where we are heading . However , we ( as a community ) first need to understand how these models behave with raw pixels before scaling them up to complex stimuli . The nature of this work was to lay the groundwork on this question and investigate the properties of protocols in controlled ( yet realistic in terms of nature ) environments where we can tease apart clearly the behaviour of the model given the small number of variations of the pixel stimuli ( object color / shape / position and floor color ) . Performing the type of careful analysis we did for complex scenes is substantially harder due to the very large number of factors we would have to control ( diverse objects of multiple colors , shapes , sizes , diverse backgrounds etc ) so it puts into question to what degree we could have achieved a similar degree of introspection by immediately using more complex datasets in the current study .

< review >
Overall , I see this as an interesting piece of work that may be of interest to researchers exploring questions around language creation and language evolution , but I think the results require more careful analysis and the novelty is relatively limited , at least in the way that the results are presented here .
< / review >

We will upload an updated version of our paper by the end of this week containing
1 ) the statistical test of the null distribution
2 ) clarifications regarding the topographic measure and
3 ) we will clarify the main contributions of this work and better relate it to the existing literature in language evolution

Moreover , we would be really happy to conduct further analyses and clarify the exposition of results . If the reviewer has specific suggestions on this , we would like to hear them in order to improve the quality of the manuscript and strengthen our submission .


The paper proposes a simple classification task for learning feature extractors without requiring manual annotations : predicting one of four rotations that the image has been subjected to : by 0 , 90 , 180 or 270 º . Then the paper shows that pre-training on this task leads to state - of - the- art results on a number of popular benchmarks for object recognition , when training classifiers on top of the resulting representation .

This is a useful discovery , because generating the rotated images is trivial to implement by anyone . It is a special case of the approach by Agrawal et al 2015 , with more efficiency .

On the negative side , this line of work would benefit from demonstrating concrete benefits . The performance obtained by pre-training with rotations is still inferior to performance obtained by pre-training with ImageNet , and we do have Image Net so there is no reason not to use it . It would be important to come up with tasks for which there is not one ImageNet , then techniques such as that proposed in the paper would be necessary . However rotations are somewhat specific to images . There may be opportunities with some type of medical data .

Additionally , the scope of the paper is a little bit restricted , there is not that much to take home besides the the following information : " predicting rotations seems to require a lot of object category recognition " .





We thank the reviewer for the valuable feedback . Here we will answer to his / her comments .

Comment :
" This is a useful discovery , because generating the rotated images is trivial to implement by anyone . It is a special case of the approach by Agrawal et al 2015 , with more efficiency . "

-----

Answer :
We would like to mention that our method is NOT a special case of the Egomotion method of Agrawal et al 2015 . More specifically , the Egomotion method employs a ConvNet model with siamese like architecture that takes as input TWO CONSECUTIVE VIDEO FRAMES and is trained to predict ( through regression ) their camera transformation . Instead , in our approach , the ConvNet takes as input A SINGLE IMAGE to which we have applied a random geometric transformation ( rotation ) and is trained to recognize ( through classification ) this geometric transformation WITHOUT HAVING ACCESS TO THE INITIAL IMAGE . These are two fundamentally different approaches .

To make this difference more clear , consider , for instance , the case where we feed the siamese ConvNet model of the Egomotion method with a pair of images that include an image and a rotated copy of it . In this case , training the ConvNet to predict the difference in rotation would lead to learning some very trivial features ( e.g. , it would only need to compare the four corners of the image in order to solve the task ) .

On the contrary , since the ConvNet used in our approach takes as input a single image , it cannot recognize the type of geometric transformation applied between consecutive frames and used in the Egomotion method . Instead , our approach requires utilizing geometric transformations ( e.g. , the image rotations of 0 , 90 , 180 , and 270 degrees ) that transform the image in such a manner that is unambiguous to infer the applied transformation without having access to the initial image .

We believe that the above differences force our ConvNet model to learn different and , in our opinion , more high level features from the Egomotion method .

Comments :
" On the negative side , this line of work would benefit from demonstrating concrete benefits . The performance obtained by pre-training with rotations is still inferior to performance obtained by pre-training with ImageNet , and we do have Image Net so there is no reason not to use it . It would be important to come up with tasks for which there is not one ImageNet , then techniques such as that proposed in the paper would be necessary . However rotations are somewhat specific to images . There may be opportunities with some type of medical data . "

" Additionally , the scope of the paper is a little bit restricted , there is not that much to take home besides the the following information : predicting rotations seems to require a lot of object category recognition . "

-----

Answer :
To be honest , we found the above criticism of our method unfair . It is true that one would ultimately like the performance of an unsupervised learning approach to surpass the performance of a fully supervised one , but , to the best of our knowledge , no published unsupervised learning method manages to achieve this goal so far .

Furthermore , the type of data our work focuses on ( i.e. , visual data ) have attracted a tremendous amount of research interest as far as unsupervised learning is concernced . In fact , just over the last few years , one can cite numerous other unsupervised learning works focusing on exactly the same type of data ( e.g. , [ 1 , 2, 3 , 4 , 5 ] ) , all of which have been very well received by the community and which have also appeared on top-rank computer vision or machine learning conferences .

[ 1 ] Learning to See by Moving
[ 2 ] Unsupervised Visual Representation Learning by Context Prediction
[ 3 ] Colorful Image Colorization .
[ 4 ] Unsupervised learning of visual representations by solving jigsaw puzzles .
[ 5 ] Representation Learning by Learning to Count

Strengths :
* Very simple strategy for unsupervised learning of deep image features . Simplicity of approach is a good quality in my view .
* The rationale for the effectiveness of the approach is explained well .
* The representation learned from unlabeled data is shown to yield strong results on image categorization ( albeit mostly in scenarios where the unsupervised features have been learned from the * same * dataset where classification is performed -- more on this below ) .
* The image rotations are implemented in terms of flipping and transposition , which do not create visual artifacts easily recognizable by deep models .

Weaknesses :
* There are several obvious additional experiments that , in my view , would greatly strengthen this work :
1 . Nearly all of the image categorization results ( with the exception of those in Table 4 ) are presented for the contrived scenario where the unsupervised representation is learned from the same training set as the one used for the final supervised training of the categorization model . This is a useless application scenario . If labels for the training examples are available , why not using them for feature learning given that this leads to improved performance ( see results in Tables ) ? More importantly , this setup does not allow us to understand how general the unsupervised features are . Maybe they are effective precisely because they have been learned from images of the 10 classes that the final classifier needs to distinguish ... I would have liked to see some results involving unsupervised learning from a dataset that may contain classes different from those of the final test classification or , even better , from a dataset of randomly selected images that lack categorical coherence ( e.g. , photos randomly picked from the Web , such as Flickr pics ) .
2 . In nearly all the experiments , the classifier is built on top of the frozen unsupervised features . This is in contrast with the common practice of finetuning the entire pretrained unsupervised net on the supervised task . It 'd be good to know why the authors opted for the different setup and to see in any case some supervised finetuning results .
3 . It would be useful to see the accuracy per class both when using unsupervised features as well as fully - supervised features . There are many objects that have a canonical pose / rotation in the world . Forcing the unsupervised features to distinguish rotations of such objects may affect the recognition accuracy for these classes . Thus , my request for seeing how the unsupervised learning affects class -specific accuracy .
4 . While the results in Table 2 are impressive , it appears that the different unsupervised learning methods reported in this table are based on different architectures . This raises the question of whether performance gains are due to the better mechanism for unsupervised learning or rather the better network architecture .
5 . I do understand that using only 0 , 90 , 180 and 270 degree rotations eliminates the issue of potentially recognizable artifacts . Nevertheless , it 'd be interesting to see what happens empirically when the number of discrete rotations is increased , e.g. , by including 45 , 135 , 225 and 315 degree rotations . And what happens if you use only 0 and 180 ? Or only 90 and 270 ?
* While the paper is easy to understand , at times the writing is poor and awkward ( e.g. , opening sentence of intro , first sentence in section 2.2 ) .

We thank the reviewer for the valuable feedback . This is the 1st part of the answer to his / her comments .

Comment :
" Nearly all of the image categorization results ( with the exception of those in Table 4 ) are presented for the contrived scenario where the unsupervised representation is learned from the same training set as the one used for the final supervised training of the categorization model . This is a useless application scenario . If labels for the training examples are available , why not using them for feature learning given that this leads to improved performance ( see results in Tables ) ? "

Answer :
We disagree with the above statement that we do not present enough evaluation results for cases where a different training set is used between the unsupervised and supervised tasks .

In Table 4 ( which is Table 7 in the revised version of the paper ) we evaluate the unsupervised learned features on THREE DIFFERENT PASCAL tasks : image classification , object detection , and object segmentation ( the segmentation results were added in the revised version of the paper ) . These correspond to core problems in computer vision and evaluating the transferability of learned ConvNet features on those tasks is one of the most widely used and well - established benchmark for unsupervised representation learning methods [ 1 , 2,3,4 ] .

Moreover , in Figure 5.b we evaluate our unsupervised representation learning method on a semi-supervised setting when only a small part of the available training data are labelled and we demonstrate that our method can leverage the unlabelled training data to improve its accuracy on the test set .

Furthermore , regarding the evaluation experiments that utilize the same training set for both the unsupervised and supervised learning ( e.g. CIFAR - 10 and ImageNet classification tasks ) , we note that this type of experiments have been proposed and are extensively used in all prior feature learning methods [ 1 , 2, 3,4 ] . Therefore , they provide a well - established benchmark based on which we can compare to prior approaches . The reason why this is considered to be a useful benchmark is because it allows one to evaluate the quality of the unsupervised learned features by directly comparing them with the features learned in supervised way on the same training set ( which provides an upper bound on the performance of the unsupervided features ) .

------

Comment :
" More importantly , this setup does not allow us to understand how general the unsupervised features are . Maybe they are effective precisely because they have been learned from images of the 10 classes that the final classifier needs to distinguish . I would have liked to see some results involving unsupervised learning from a dataset that may contain classes different from those of the final test classification or , even better , from a dataset of randomly selected images that lack categorical coherence ( e.g. , photos randomly picked from the Web , such as Flickr pics ) . "

Answer :
In general we believe that the primary goal of unsupervised representation learning is to learn image representations appropriate for understanding ( e.g. , recognizing or detecting ) the visual concepts that were " seen " during training . Learning features that generalize on " unseen " visual concepts is indeed a desirable property but it is something that even supervised representation learning methods might struggle with it and is not the ( main ) scope of our paper . Nevertheless , as requested by the reviewer , we added in the revised version of our paper an evaluation of our unsupervised learned features on the scene classification task of Places205 benchmark ( see Table 6 ) . Note that for the scene classification results , the unsupervised features were learned on ImageNet that contains classes different from those of the scene classification task of Places205 .

------

[ 1 ] Richard Zhang et al , Colorful Image Colorization .
[ 2 ] Jeff Donahue et al , Adversarial Feature Learning .
[ 3 ] Noroozi and Favaro , Unsupervised learning of visual representations by solving jigsaw puzzles .
[ 4 ] Piotr Bojanowski and Armand Joulin , Unsupervised Learning by Predicting Noise .

This is the 2nd part of the answer to the reviewer 's comments .

Comment :
" In nearly all the experiments , the classifier is built on top of the frozen unsupervised features . This is in contrast with the common practice of finetuning the entire pretrained unsupervised net on the supervised task . It 'd be good to know why the authors opted for the different setup and to see in any case some supervised finetuning results . "

Answer :
We believe that this comment is inaccurate . First of all , for the PASCAL results in Table 4 ( i.e. , PASCAL classification , PASCAL detection , and the newly added PASCAL segmentation results ) , we FINETUNE THE ENTIRE NETWORK ( see the last 3 columns of this table ) . Furthermore , in general for the experimental evaluation of our method on natural images ( section 3.2 ) , we want to emphasize that we follow THE SAME EVALUATION SETUP that prior unsupervised feature learning methods have used [ 1 , 2, 3 , 4 ] and we did not propose a new one ( this also allows us to compare with these approaches ) .

Regarding the experiments presented in section 3.1 ( that includes results in CIFAR - 10 ) , those were meant as a proof of concept of our work and to help better analyze various aspects of our approach before we move onto the more challenging , but also much more time consuming , experiments on ImageNet ( see section 3.2 ) . Therefore , for the experimental evaluation in CIFAR - 10 we mimicked the evaluation setup that is employed by prior approaches on ImageNet ( i.e. , unsupervised feature learning on ImageNet and then training non-linear classifiers on top of them for the ImageNet classification task ) . Other than that we did not have any particular reason for not fine - tuning the learned features . However , as requested by the reviewer , we added experiments with fine-tuning in the revised version of the paper ( see Table 3 ) . We observe that by fine-tuning the unsupervised learned features this further improves the classification performance , thus reducing even more the gap with the supervised case .

------

Comment :
" It would be useful to see the accuracy per class both when using unsupervised features as well as fully - supervised features . There are many objects that have a canonical pose / rotation in the world . Forcing the unsupervised features to distinguish rotations of such objects may affect the recognition accuracy for these classes . Thus , my request for seeing how the unsupervised learning affects class -specific accuracy . "

Answer :
We added such results in Tables 8 and 9 ( in appendix B ) of the revised version of the paper .

------

Comment :
" While the results in Table 2 are impressive , it appears that the different unsupervised learning methods reported in this table are based on different architectures . This raises the question of whether performance gains are due to the better mechanism for unsupervised learning or rather the better network architecture . "

Answer :
Indeed , each entry in Table 2 ( Table 3 in the revised manuscript ) has a different network architecture . It was not really possible for us to implement our method with each of those architectures and so those results are just indicative and not meant for direct comparison ( we added this clarification in the revised manuscript - see caption of Table 3 ) . The main bulk of experiments that directly compares our approach against other ( more recent and more relevant ) approaches is presented in section 3.2 of our paper . Regarding Table 2 , we believe the most interesting and remarkable finding is the very small performance gap between our unsupervised feature learning method and the supervised case ( that both use exactly the same network architecture ) .

------

Comment :
" I do understand that using only 0 , 90 , 180 and 270 degree rotations eliminates the issue of potentially recognizable artifacts . Nevertheless , it 'd be interesting to see what happens empirically when the number of discrete rotations is increased , e.g. , by including 45 , 135 , 225 and 315 degree rotations . And what happens if you use only 0 and 180 ? Or only 90 and 270 ? "

Answer :
We thank the reviewer for this suggestion . Please see Table 2 and relevant discussion in paragraph `` " Exploring the quality of the learned features w.r.t. the number of recognized rotations " ( section 3.1 ) in the revised version of the paper .

------

[ 1 ] Richard Zhang et al , Colorful Image Colorization .
[ 2 ] Jeff Donahue et al , Adversarial Feature Learning .
[ 3 ] Noroozi and Favaro , Unsupervised learning of visual representations by solving jigsaw puzzles .
[ 4 ] Piotr Bojanowski and Armand Joulin , Unsupervised Learning by Predicting Noise .

** Paper Summary **
This paper proposes a self - supervised method , RotNet , to learn effective image feature from images by predicting the rotation , discretized into 4 rotations of 0 , 90 , 180 , and 270 degrees . The authors claim that this task is intuitive because a model must learn to recognize and detect relevant parts of an image ( object orientation , object class ) in order to determine how much an image has been rotated .
They visualize attention maps from the first few conv layers and claim that the attend to parts of the image like faces or eyes or mouths . They also visualize filters from the first convolutional layer and show that these learned filters are more diverse than those from training the same model in a supervised manner .
They train RotNet to learn features of CIFAR - 10 and then train , in a supervised manner , additional layers that use RotNet feature maps to perform object classification . They achieve 91.16 % accuracy , outperforming other unsupervised feature learning methods . They also show that in a semi-supervised setting where only a small number of images of each category is available at training time , their method outperforms a supervised method .
They next train RotNet on ImageNet and use the learned features for image classification on ImageNet and PASCAL VOC 2007 as well as object detection on PASCAL VOC 2007 . They achieve an ImageNet and PASCAL classification score as well as an object detection score higher than other baseline methods .
This task requires the ability to understand the types , the locations , and the poses of the objects presented in images and therefore provides a powerful surrogate supervision signal for representation learning . To demonstrate the effectiveness of the proposed method , the authors evaluate it under a variety of tasks with different settings .



** Paper Strengths **
- The motivation of this work is well - written .
- The proposed self - supervised task is simple and intuitive . This simple idea of using image rotation to learn features , easy to implement image rotations without any artifacts
- Requiring no scale and aspect ratio image transformations , the proposed self - supervised task does not introduce any low - level visual artifacts that will lead the CNN to learn trivial features with no practical value for the visual perception tasks .
- Training the proposed model requires the same computational cost as supervised learning which is much faster than training image reconstruction based representation learning frameworks .
- The experiments show that this representation learning task can improve the performance when only a small amount of annotated examples is available ( the semi-supervised settings ) .
- The implementation details are included , including the way of implementing image rotations , different network architectures evaluated on different datasets , optimizers , learning rates with weight decayed , batch sizes , numbers of training epochs , etc .
- Outperforms all baselines and achieves performance close to , but still below , fully supervised methods
- Plots rotation prediction accuracy and object recognition accuracy over time and shows that they are correlated



** Paper Weaknesses **
- The proposed method considers a set of different geometric transformations as discrete and independent classes and formulates the task as a classification task . However , the inherent relationships among geometric transformations are ignored . For example , rotating an image 90 degrees and rotating an image 180 degrees should be closer compared to rotating an image 90 degrees and rotating an image 270 degrees .
- The evaluation of low - level perception vision task is missing . In particular , evaluating the learned representations on the task of image semantic segmentation is essential in my opinion . Since we are interested in assigning the label of an object class to each pixel in the image for the task , the ability to encode semantic image feature by learning from performing the self - supervised task can be demonstrated .
- The figure presenting the visualization of the first layer filters is not clear to understand nor representative of any finding .
- ImageNet Top -1 classification results produced by Split -Brain ( Zhang et al. , 2016 b ) and Counting ( Noroozi et al. , 2017 ) are missing which are shown to be effective in the paper [ Representation Learning by Learning to Count ] ( https://arxiv.org/abs/1708.06734).
- An in - depth analysis of the correlation between the rotation prediction accuracy and the object recognition accuracy is missing . Showing both the accuracies are improved over time is not informative .
- Not fully convinced on the intuition , some objects may not have a clear direction of what should be “ up ” or “ down ” ( symmetric objects like balls ) , in Figure 2 , rotated image X^3 could plausibly be believed as 0 rotation as well , do the failure cases of rotation relate to misclassified images ?
- “ remarkably good performance ” , “ extremely good performance ” – vague language choices ( abstract , conclusion )
- Per class breakdown on CIFAR 10 and / or PASCAL would help understand what exactly is being learned
- In Figure 3 , it would be better to show attention maps on rotated images as well as activations from other unsupervised learning methods . With this figure , it is hard to tell whether the proposed model effectively focuses on high level objects .
- In Figure 4 , patterns of the convolutional filters are not clear . It would be better to make the figures clear by using grayscale images and adjusting contrast .
- In Equation 2 , the objective should be maximizing the sum of losses or minimizing the negative . Also , in Equation 3 , the summation should be computed over y = 1 ~ K , not i = 1 ~ N.



** Preliminary Evaluation **
This paper proposes a self - supervised task which allows a CNN to learn meaningful visual representations without requiring supervision signal . In particular , it proposes to train a CNN to recognize the rotation applied to an image , which requires the understanding the types , the locations , and the poses of the objects presented in images . The experiments demonstrate that the learned representations are meaningful and transferable to other vision tasks including object recognition and object detection . Strong quantitative results outperforming unsupervised representation learning methods , but lacking qualitative results to confirm / interpret the effectiveness of the proposed method .

We thank the reviewer for the valuable feedback . This is the 1st part of the answer to his / her comments .

Comment :
" The proposed method considers a set of different geometric transformations as discrete and independent classes and formulates the task as a classification task . However , the inherent relationships among geometric transformations are ignored . For example , rotating an image 90 degrees and rotating an image 180 degrees should be closer compared to rotating an image 90 degrees and rotating an image 270 degrees . "

Answer :
We thank the reviewer for this suggestion on how to further improve the performance of our method ( we will certainly try to explore if a modification of this type can be of any help ) . However , based on our intuition , we feel that the above modification to the rotation prediction task will most probably not have any positive effect with respect to the paper 's goal of unsupervised representation learning simply because the rotation prediction task that we propose is used here just as a proxy for learning semantic representations . Moreover , it is debatable if the representations of two images that differ , e.g. , by 90 degrees should always be closer than the representations of two images that differ by 180 degrees ( if this is what the reviewer means ) . In any case , we would be glad to explore the above enhancement to our method proposed by the reviewer and report any positive findings in the final version of the paper .

A first experiment that we did towards that direction is to modify the target distributions used in the cross entropy loss during the training of the rotation prediction task : more specifically , instead of using target distributions that place the entire probability mass on the ground truth rotation ( as before ) , we used distributions that also allow some small probability mass to be placed on the rotations that differ only by 90 degrees from the ground truth rotation .
However , this modification did not offer any performance improvement when the learned features were tested on the CIFAR - 10 classification task . On the contrary , the classification accuracy was slightly reduced from 89.06 to 88.91 .

------

Comment :
" The evaluation of low - level perception vision task is missing . In particular , evaluating the learned representations on the task of image semantic segmentation is essential in my opinion . "

Answer :
We agree with the reviewer . Unfortunately we did not have this experiment ready before the submission deadline . However , we added now the segmentation results in the revised version of the paper ( see Table 7 ) ; we observe that again our method demonstrates state - of - the- art performance among the unsupervised approaches .

------

Comment :
" In Figure 4 , patterns of the convolutional filters are not clear . It would be better to make the figures clear by using grayscale images and adjusting contrast . "

Answer :
In the revised version of the paper , we tried to improve the clarity of Figure 4 by further increasing the contrast .

------

Comment :
" The figure ( Figure 4 ) presenting the visualization of the first layer filters is not clear to understand nor representative of any finding . "

Answer :
However , we disagree with the statement that the visualizations of the 1st layer filters is not representative of any finding . It is true that the visualization of the 1st layer filters does not ( directly ) reveal the nature of the higher level features that a network learns , which is also what we are interested to understand . However , it very clearly demonstrates the nature of the low - level features that a network learns , which is also of interest , and , in our case , it shows that these features are very similar to those that a supervised object classification network learns . Due to the above reason , this type of visualization has been extensively used both for supervised methods [ 1 ] and for unsupervised methods [ 2 , 3 ,4 ] .

Furthermore , concerning the interpretation of the higher level features learned by our method , since it is difficult to provide a similar visualization as the one for the 1st layer filters , we choose to visualize instead the attention maps that those layers generate .

------

[ 1 ] Alex Krizhevsky et al , ImageNet Classification with Deep Convolutional Neural Networks .
[ 2 ] Jeff Donahue et al , Adversarial Feature Learning .
[ 3 ] Noroozi and Favaro , Unsupervised learning of visual representations by solving jigsaw puzzles .
[ 4 ] Piotr Bojanowski and Armand Joulin , Unsupervised Learning by Predicting Noise .

This is the 2nd part of the answer to the reviewer 's comments .

Comment :
" ImageNet Top -1 classification results produced by Split -Brain ( Zhang et al. , 2016 b ) and Counting ( Noroozi et al. , 2017 ) are missing which are shown to be effective in the paper . "

Answer :
The ImageNet Top -1 classification results for the NON - LINEAR classifiers of the Split -Brain and the Counting methods are missing because those methods do not report those results in their papers . However , in Table 4 ( it is Table 7 in the revised version of the paper ) we compare against the Split -Brain and Counting methods on the PASCAL tasks ( i.e. , classification , detection , and segmentation ) and our method demonstrates state - of - the- art results . Furthermore , in the revised version of our paper we added the ImageNet Top - 1 classification results for LINEAR classifiers of our method as well as prior methods that have reported such results ( including Split - Brain and Counting methods ) and again our approach achieves state - of - the - art results ( see Table 5 ) .

-------

Comment :
" An in - depth analysis of the correlation between the rotation prediction accuracy and the object recognition accuracy is missing . Showing both the accuracies are improved over time is not informative . "

Answer :
First we would like to clarify how we created the object recognition accuracy curve in Figure 5 a and in general what Figure 5b demonstrates . In order to create the object recognition accuracy curve , in each training snapshot of RotNet ( i.e. , every 20 epochs ) , we pause its training procedure and we train from scratch ( until convergence ) a non-linear object classifier on top of the so far learned RotNet features ( specifically the 2nd conv. block features ) . The object recognition accuracy curve depicts the accuracy of those non-linear object classifiers after the end of their training while the rotation prediction accuracy curve depicts the accuracy of the RotNet at those snapshots . Therefore , Figure 5a demonstrates the following fact : as the ability of the RotNet features for solving the rotation prediction task improves ( i.e. , as the rotation prediction accuracy increases ) , their ability to solve the object recognition task improves as well ( i.e. , the object recognition accuracy also increases ) .

We also did another experiment towards clarifying the possible existence of a relation between the two tasks but this time we explored their relation in the opposite direction , i.e. , we used input features learnt on the object recognition task in order to see their effectiveness on training a small network for the rotation prediction task . Specifically , the rotation prediction network that we train on the CIFAR10 dataset has the same architecture as the 3rd ( and last ) conv . block of a NIN based ConvNet and this network is applied on top of the feature maps generated by the 2nd conv . block of a NIN based ConvNet trained on the object prediction task of CIFAR10 . The rotation classification accuracy that this hybrid model achieves is 88.05 , which is relatively close to the 93.0 classification accuracy achieved by a NIN based ConvNet trained solely on the rotation prediction task ( and despite the fact that the first 2 conv . blocks of the hybrid model have been trained only with images of 0 degrees orientation ) .

If the reviewer would like to specify any additional concrete experiment ( reasonably easy to implement ) that could be used to further clarify the existence of such a relation between the two tasks , we would be happy to implement and test it .

------




This is the 3rd part of the answer to the reviewer 's comments .

Comment :
" Not fully convinced on the intuition , some objects may not have a clear direction of what should be “ up ” or “ down ” ( symmetric objects like balls ) , in Figure 2 , rotated image X^3 could plausibly be believed as 0 rotation as well , do the failure cases of rotation relate to misclassified images ? "

Answer :
Regarding the fact that some images might have ambiguous orientation , we note that this type of training examples comprise only a small part of the dataset and can essentially be seen as a small amount of label noise , which thus poses no problem for learning . On the contrary , the great majority of the used images have an unambiguous orientation . Therefore , the ConvNet , by trying to solve the rotation prediction task , will eventually be forced to learn object -specific features . This is also evidenced by the very strong experimental performance of these features when applied on a variety of different tasks including those of object recognition , object detection , object segmentation , and scene classification tasks ( section 3 of the paper ) .

Concerning the question posed by the reviewer if there is any connection between failure cases for rotation prediction and misclassifications w.r.t. object recognition , we did the following test in order to explore if there is any such correlation : first , we define as y0 a binary variable that indicates if an image is misclassified in the object recognition task by a fully supervised model , as y1 a binary variable that indicates if an image is misclassified in the object recognition task by our unsupervised learned RotNet model ( by training a non-linear classifier on top of the RotNet features ) , and as x a continuous variable that indicates the fraction of rotations ( out of the 4 possible ones per image ) that are misclassified by RotNet . The point biserial correlation coefficient ( https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pointbiserialr.html) between the y 1 and x variables on CIFAR - 10 is 0.1473 with p-value 1.286e - 49 while between the y0 and x variables is 0.1799 with p-value=1.5404e-73 . Therefore , it seems that there is little correlation between failing to classify the rotations of an image and failing to classify the object that it depicts . Moreover , this holds regardless if we use a fully supervised object classifier ( 0.1799 correlation ) or if we use an object classifier based on features learnt on the rotation prediction task .

------

Comment :
" “ remarkably good performance ” , “ extremely good performance ” – vague language choices ( abstract , conclusion ) "

Answer :
We rephrased the corresponding text to make it even more clear that the above statements relate to the state - of - the- art experimental results achieved by our method , which surpass prior approaches by a significant margin .

------

Comment :
" Per class breakdown on CIFAR 10 and / or PASCAL would help understand what exactly is being learned "

Answer :
We added such results in Tables 8 and 9 ( in appendix B ) of the revised version of the paper .

------

Comment :
" In Figure 3 , it would be better to show attention maps on rotated images as well as activations from other unsupervised learning methods . With this figure , it is hard to tell whether the proposed model effectively focuses on high level objects . "

Answer :
In the revised version of the paper in Figure 3 we added attention maps generated by a supervised model . By comparing them with those of our unsupervised model we observe that both of them focus on similar areas of the image in order to accomplish their task . Also , in Figure 6 ( in appendix A ) , we added the attention maps of the rotated versions of the images . We observe that the attention maps of all the rotated images is roughly the same which means the attention maps are equivariant w.r.t. image rotations . This practically means that in order to accomplish the rotation prediction task the network focuses on the same object parts regardless of the image rotation .

------

Comment :
" In Equation 2 , the objective should be maximizing the sum of losses or minimizing the negative . Also , in Equation 3 , the summation should be computed over y = 1 ~ K , not i = 1 ~ N. "

Answer :
We thank the reviewer for identifying the above typos . We fixed them in the revised version of the paper ( see equation 3 ) .

------

This paper proposes a network with the standard soft-attention mechanism for classification tasks , where the global feature is used to attend on multiple feature maps of local features at different intermediate layers of CNN . The attended features at different feature maps are then used to predict the final classes by either concatenating features or ensembling results from individual attended features . The paper shows that the proposed model outperforms the baseline models in classification and weakly supervised segmentation .

Strength :
- It is interesting idea to use the global feature as a query in the attention mechanism while classification tasks do not naturally involve a query unlike other tasks such as visual question answering and image captioning .

- The proposed model shows superior performances over GAP in multiple tasks .

Weakness :
- There are a lot of missing references . There have been a bunch of works using the soft-attention mechanism in many different applications including visual question answering [ A - C ] , attribute prediction [ D ] , image captioning [ E , F ] and image segmentation [ G ] . Only two previous works using the soft-attention ( Bahdanau et al. , 2014 ; Xu et al. , 2015 ) are mentioned in Introduction but they are not discussed while other types of attention models ( Mnih et al. , 2014 ; Jaderberg et al. , 2015 ) are discussed more .

- Section 2 lacks discussions about related work but is more dedicated to emphasizing the contribution of the paper .

- The global feature is used as the query vector for the attention calculation . Thus , if the global feature contains information for a wrong class , the attention quality should be poor too . Justification on this issue can improve the paper .

- [ H ] reports the performance on the fine - grained bird classification using different type of attention mechanism . Comparison and justification with this method can improve the paper . The performance in [ H ] is almost 10 % point higher accuracy than the proposed model .

- In the segmentation experiments , the models are trained on extremely small images , which is unnatural in segmentation scenarios . Experiments on realistic settings should be included . Moreover , [ G ] introduces a method of using an attention model for segmentation , while the paper does not contain any discussion about it .


Overall , I am concerned that the proposed model is not well discussed with important previous works . I believe that the comparisons and discussions with these works can greatly improve the paper .

I also have some questions about the experiments :
- Is there any reasoning why we have to simplify the concatenation into an addition in Section 3.2 ? They are not equivalent .

- When generating the fooling images of VGG - att , is the attention module involved , or do you use the same fooling images for both VGG and VGG - att ?

Minor comments :
- Fig. 1 -> Fig. 2 in Section 3.1 . If not , Fig. 2 is never referred .

References
[ A ] Huijuan Xu and Kate Saenko . Ask , attend and answer : Exploring question - guided spatial attention for visual question answering . In ECCV , 2016 .
[ B ] Zichao Yang , Xiaodong He , Jianfeng Gao , Li Deng , and Alex Smola . Stacked attention networks for image question answering . In CVPR , 2016 .
[ C ] Jacob Andreas , Marcus Rohrbach , Trevor Darrell , and Dan Klein . Deep compositional question answering with neural module networks . In CVPR , 2016 .
[ D ] Paul Hongsuck Seo , Zhe Lin , Scott Cohen , Xiaohui Shen , and Bohyung Han . Hierarchical attention networks . arXiv preprint arXiv:1606.02393 , 2016 .
[ E ] Quanzeng You , Hailin Jin , Zhaowen Wang , Chen Fang , and Jiebo Luo . Image captioning with semantic attention . In CVPR , 2016 .
[ F ] Jonghwan Mun , Minsu Cho , and Bohyung Han . Text - Guided Attention Model for Image Captioning . AAAI , 2017 .
[ G ] Seunghoon Hong , Junhyuk Oh , Honglak Lee and Bohyung Han , Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network , In CVPR , 2016 .
[ H ] Max Jaderberg , Karen Simonyan , Andrew Zisserman , Koray Kavukcuoglu , Spatial Transformer Networks , NIPS , 2015




We thank the reviewer for the comments .

Please find as follows , our point- by-point response to the concerns raised in the weakness section .

1 and 2 - missing references and discussion about related work : We thank the reviewer for pointing us to the most recent relevant literature regarding the proposed attention scheme . We have provided a brief discussion of the suggested works in the third paragraph of Sec. 1 . A more thorough treatment is taken up in Sec. 2 ( Related Work ) , which has now been reorganised to more exhaustively capture the variety of existing approaches in the area of attention in deep neural networks .
We have also produced an experimental comparison against the progressive attention mechanism of Seo et al. , incorporated into the VGG model and trained using the global feature as the query , for the task of classification of CIFAR datasets . The details of the implementation are provided in appendix Sec. A.2 . The results are compiled in the updated Table 1 . A quantitative evaluation of the above mechanism for the task of fine - grained recognition on the CUB and SVHN datasets is forthcoming and will be made available in the next revision .

3 - the global feature as a driver of attention : The global feature is indeed used as the query vector for our attention calculations . The global and local features vector do n't always need to be obtained from the same input image . In fact , in our framework , we can extract the local features from a given image A : call that the target image . The global feature vector can be obtained from another image B , which we call the query image . Under the proposed attention scheme , it is expected that the attention maps will highlight objects in the target image that are ' similar ' to the query image object . The precise notion of ' similarity ' may be different for the two compatibility functions , where parameterised compatibility is likely to capture the concept of objectness while the dot-product compatibility is likely to learn a high - order appearance - based match between the query and target image objects . We are investigating the two different compatibility functions w.r.t. the above hypothesis . The experimental results in the form of visualisations will be made available in the next update .

4 - performance comparison with [ H ] : We are unable to compare with [ H ] for the task of fine - grained recognition due to a difference in dataset preprocessing . The CUB dataset in [ H ] has not been tighly cropped to the birds in the images . Thus , the network has access to the background information which in case of birds can offer useful information about their habitat and climate , something that is key to their classification .
However , note that our experimental comparison with the progressive attention mechanism of Seo et al. is forthcoming . The progressive attention scheme has been shown to outperform [ H ] for the task of attribute prediction , see Table 1 in [ D ] . Hence , by presenting a comparison with the former , we would be able to indirectly compare the proposed approach against the spatial transformer network architecture of [ H ].

5 - segmentation experiments and comparison : Our weakly supervised segmentation experiments make use of the Object Discovery dataset , a known benchmark in the community widely used for evaluating approaches developed for the said task [ I , J ] . We note that [ G ] presents an attention - based model for segmentation . Our work uses category labels for weak segmentation and is related to the soft-attention approach of [ G ] . However , unlike the aformentioned , we do not explicitly train our model for the task of segmentation using any kind of pixel - level annotations . We evaluate the binarised spatial attention maps , learned as a by-product of training for image classification , for their ability to segment objects . We have added this discussion to the paragraph on weakly supervised segmentation in Sec. 2 ( Related Work ) .


Our responses to the questions asked are below :

1 . concatenation vs addition : Given the existing free parameters between the local and the global image descriptors in a CNN pipeline , we can simplify the concatenation of the two descriptors to an addition operation , without loss of generality . This allows us to limit the parameters of the attention unit .

2 . generation of fooling images : When generating the fooling images of VGG - att , we do use the attention module . Thus , the fooling images for both VGG and VGG - att are conditioned on their respective architectures , and hence different .


We have incorporated the minor comment in the updated version .


References
[ I ] Dutt Jain , S. , & Grauman , K. ( 2016 ) . Active image segmentation propagation . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( pp. 2864-2873 ) .
[ J ] Rubinstein , M. , Liu , C. , & Freeman , W. T. ( 2016 ) . Joint Inference in Weakly - Annotated Image Datasets via Dense Correspondence . International Journal of Computer Vision , 119 ( 1 ) , 23-45 .


This paper proposed an end - to - end trainable hierarchical attention mechanism for CNN . The proposed method computes 2d spatial attention map at multiple layers in CNN , where each attention map is obtained by computing compatibility scores between the intermediate features and the global feature . The proposed method demonstrated noticeable performance improvement on various discriminative tasks over existing approaches .

Overall , the idea presented in the paper is simple yet solid , and showed good empirical performance . The followings are several concerns and suggestions .

1 . The authors claimed that this is the first end - to - end trainable hierarchical attention model , but there is a previous work that also addressed the similar task :
Seo et al , Progressive Attention Networks for Visual Attribute Prediction , in Arxiv preprint :1606.02393 , 2016

2 . The proposed attention mechanism seems to be fairly domain ( or task ) specific , and may not be beneficial for strong generalization ( generalization over unseen category ) . Since this could be a potential disadvantage , some discussions or empirical study on cross - category generalization seems to be interesting .

3 . The proposed attention mechanism is mainly demonstrated for single - class classification task , but it would be interesting to see if it can also help the multi-class classification ( e.g. image classification on MS-COCO or PASCAL VOC datasets )

4 . The localization performance of the proposed attention mechanism is evaluated by weakly - supervised semantic segmentation tasks . In that perspective , it would be interesting to see the comparisons against other attention mechanisms ( e.g. Zhou et al 2016 ) in terms of localization performance .


We thank the reviewer for the comments .
Our responses are provided below , numbered correspondingly :

1 . The missing comparison with the progressive attention mechanism of Seo et al. was unintentional , and the authors agree that their work is indeed closely related to the proposed work . We have now included a discussion on it in Sec. 2 ( Related Work ) . We have also produced an experimental comparison against the progressive attention mechanism , incorporated into the VGG architecture and trained using the global feature as the query , for the task of classification of CIFAR datasets . The details of the implementation are provided in appendix Sec. A.2 . The results are compiled in the updated Table 1 . A quantitative evaluation of the above mechanism for the task of fine - grained recognition on the CUB and SVHN datasets is forthcoming and will be made available in the next revision .

2 . For our detailed investigation of cross - category generalisation of the image features learned using the proposed attention scheme , we would like to point the reviewer to Sec. 5.3 . Here , we use the baseline and attention - enhanced models as off - the-shelf feature extractors . The models are trained for the tasks of classification on CIFAR datasets and are queried to obtain high - order representations of images from unseen datasets , such as the Action - 40 and Scene - 67 datasets . At train time , the training - set features are used to optimise a linear SVM as a classifier . At test time , we evaluate the quality of generalisation via the quality of classification of the test set based on the extracted features .

3 . The multi-class classification task can be posed as a set of single - class or one- hot -encoded classification tasks . In this regard , as confirmed experimentally , our proposed attention scheme would be able to offer performance benefits . However , a more direct multi-class classification , on datasets such as MS- COCO or PASCAL , can be covered in future versions given that the standard protocol to train and test such networks is typically lengthy . The models are usually pre-trained on ImageNet and fine - tuned on the above datasets , putting this a bit outside our scope for revision / addition at present .

4 . The comparison with the attention mechanism proposed by Zhou et al. was actually included in Fig. 9 ( " VGG - GAP " ) : the reference was missing , which we have now added . The model has been trained for the CIFAR - 10 dataset , as the classes of this dataset overlap with the Object Discovery dataset that includes car , horse and airplane as categories .


This paper proposes an end - to - end trainable attention module , which takes as input the 2D feature vector map and outputs a 2D matrix of scores for each map . The goal is to make the learned attention maps highlight the regions of interest while suppressing background clutter . Experiments conducted on image classification and weakly supervised segmentation show the effectiveness of the proposed method .

Strength of this paper :
1 ) Most previous work are all implemented as post-hoc additions to fully trained networks while this work is end - to - end trainable . Not only the newly added weights for attention will be learned , so are the original weights in the network .
2 ) The generalization ability shown in Table 3 is very good , outperforming other existing network by a large margin .
3 ) Visualizations shown in the paper are convincing .

Some weakness :
1 ) Some of the notations are unclear in this paper , vector should be bold , hard to differentiate vector and scalar .
2 ) In equation ( 2 ) , l_i and g should have different dimensionality , how does addition work ? Same as equation ( 3 )
3 ) The choice of layers to add attention modules is unclear to me . The authors just pick three layers from VGG to add attention , why picking those 3 layers ? Is it better to add attention to lower layers or higher layers ? Why is it the case that having more layers with attention achieves worse performance ?


We thank the reviewer for the comments .

1 ) notation : We have updated the paper , in particular Section 3 , to represent the vectors in bold to differentiate them from scalars .

2 ) potential for differing dimensionalities of l_i and g : There is some discussion of this in the second paragraph of Sec. 3.1 . We propose the use of one fully connected layer for each CNN layer s , which projects the local feature vectors of s to the dimensionality of g . These linear parameters are learned along with all other network parameters during end - to - end training . There is an implementation detail , though , which we had neglected to mention : in order to limit the network parameters at the classification stage , we actually project g to the lower- dimensional space of the local features l_i . A note of clarification on this has been added to the first paragraph of Sec. 4.

3 ) selection of layers for attention : A brief discussion on the choice of adding attention to higher layers as opposed to the lower ones was included in Sec. 3.3 . We have now augmented this discussion , in place , with further clarification on the specific layers that we choose for estimating the attention .
For l_i and g to be comparable using the proposed compatibility functions , they should be mapped to a common high - dimensional space . In other words , the effective filters operating over image patches in the layers s must represent relatively ‘ mature ’ features that are captured in g for the classification goal . We thus expect to see the greatest benefit in deploying attention relatively late in the pipeline to provide for the learning of these features in l_i . In fact , att2 architectures often outperform their att3 counterparts , as can be seen in Tables 1 and 2 .
Further , different kinds of class details are more easily accessible at different scales . Thus , in order to facilitate the learning of diverse and complementary attention - weighted features , we propose the use of attention over different spatial resolutions . The combination of the two factors stated above results in our deploying the attention units after the convolutional blocks that are late in the pipeline , but before their corresponding max- pooling operations , i.e. before a reduction in the spatial resolution .

Update :
On further consideration ( and reading the other reviews ) , I 'm bumping my rating up to a 7 . I think there are still some issues , but this work is both valuable and interesting , and it deserves to be published ( alongside the Naesseth et al. and Maddison et al. work ) .

-----------

This paper proposes a version of IWAE - style training that uses SMC instead of classical importance sampling . Going beyond the several papers that proposed this simultaneously , the authors observe a key issue : the variance of the gradient of these IWAE - style bounds ( w.r.t. the inference parameters ) grows with their accuracy . They therefore propose using a more- biased but lower - variance bound to train the inference parameters , and the more - accurate bound to train the generative model .

Overall , I found this paper quite interesting . There are a few things I think could be cleared up , but this seems like good work ( although I 'm not totally up to date on the very recent literature in this area ) .

Some comments :

* Section 4 : I found this argument extremely interesting . However , it ’s worth noting that your argument implies that you could get an O( 1 ) SNR by averaging K noisy estimates of I_K . Rainforth et al. suggest this approach , as well as the approach of averaging K^2 noisy estimates , which the theory suggests may be more appropriate if the functions involved are sufficiently smooth , which even for ReLU networks that are non-differentiable at a finite number of points I think they should be .

This paper would be stronger if it compared with Rainforth et al . ’s proposed approaches . This would demonstrate the real tradeoffs between bias , variance , and computation . Of course , that involves O ( K^2 ) or O ( K^3 ) computation , which is a weakness . But one could use a small value of K ( say , K=5 ) .

That said , I could also imagine a scenario where there is no benefit to generating multiple noisy samples for a single example versus a single noisy sample for multiple examples . Basically , these all seem like interesting and important empirical questions that would be nice to explore in a bit more detail .

* Section 3.3 : Claim 1 is an interesting observation . But Propositions 1 and 2 seem to just say that the only way to get a perfectly tight SMC ELBO is to perfectly sample from the joint posterior . I think there ’s an easier way to make this argument :

Given an unbiased estimator \hat { Z} of Z , by Jensen ’s inequality E[ log \hat { Z} ] ≤ log Z , with equality iff the variance of \hat { Z} = 0 . The only way to get an SMC estimator ’s variance to 0 is to drive the variance of the weights to 0 . That only happens if you perfectly sample each particle from the true posterior , conditioned on all future information .

All of which is true as far as it goes , but I think it ’s a bit of a distraction . The question is not “ what ’s it take to get to 0 variance ” but “ how quickly can we approach 0 variance ” . In principle IS and SMC can achieve arbitrarily high accuracy by making K astronomically large . ( Although [ particle ] MCMC is probably a better choice if one wants extremely low bias . )

* Section 3.2 : The choice of how to get low - variance gradients through the ancestor- sampling choice seems seems like an important technical challenge in getting this approach to work , but there ’s only a very cursory discussion in the main text . I would recommend at least summarizing the main findings of Appendix A in the main text .

* A relevant missing citation : Turner and Sahani ’s “ Two problems with variational expectation maximisation for time - series models ” ( http://www.gatsby.ucl.ac.uk/~maneesh/papers/turner-sahani-2010-ildn.pdf). They discuss in detail some examples where tighter variational bounds in state- space models lead to worse parameter estimates ( though in a quite different context and with a quite different analysis ) .

* Figure 1 : What is the x-axis here ? Presumably phi is not actually 1 - dimensional ?

Typos etc .:

* “ learn a particular series intermediate ” missing “ of ” .

* “ To do so , we generate on sequence y 1 : T ” s/ on / a / , I think ?

* Equation 3 : Should there be a ( 1 / K ) in Z ?

We thank the reviewer for taking the time to review our paper and for their helpful feedback .

% % % Relationship with Rainforth et al % % %

> We would like to clarify that Rainforth et al do not propose any algorithmic approach , they only express the IWAE and VAE objectives in a general form and then carry out a theoretical analysis on which we build . One can of course always use more Monte Carlo samples ( i.e. increase M in their notation ) to increase the fidelity of estimates . The interesting approach which you are suggesting of using K^2 estimates here could be achieved by running K SMC sweeps of K samples . We agree that this could be an interesting further extension on top of our suggested approach , but we did not have the time to actively investigate it for this revision .


% % % Propositions 1 and 2 seem to just say that the only way to get a perfectly tight SMC ELBO is to perfectly sample from the joint posterior ... % % %

> Your intuition here is correct - our proof relates to demonstrating this more formally and highlighting the fact that this requires a particular factorization of the model . Proving the “ if ” part of propositions 1 , 2 is trivial . However , proving the “ only if ” part of proposition 2 requires somewhat more care to show that the variance of the estimator can indeed only be zero when the variance of the weights is zero and that the variance of the weights can indeed only be zero if the intermediate targets incorporate all future information , implying a particular factorization of the generative model .


% % % .... The question is not “ what ’s it take to get to 0 variance ” but “ how quickly can we approach 0 variance ” ... % % %

> Though we agree with your sentiment that the speed of with 0 variance is approached is of critical importance and note that we provide empirical investigation of this through the experiments , we would like to reiterate that the key point of the result is to show that for 1 < K < inf one will never learn a perfect estimator unless a particular factorization can be achieved . This is at odds to cases where K=1 , for which infinite training iterations should always lead to q becoming the posterior if q is expressive enough to encode it and the problem is convex . In other words , in the IS case we can achieve exact posterior samples at test time with a finite K and a perfectly trained q for any model that can be represented by the inference network , but it the SMC case this is only possible if we also learn the optimal factorization of the model ( as per proposition 2 ) .


% % % Section 3.2 % % %

We believe that in practice , the bias introduced by ignoring this term is only very small . We have added a short summary of the results from Appendix A as suggested .


% % % Relevant missing citation : Turner and Sahani % % %

> Thank you , we have duly updated the paper to include this reference .


% % % Figure 1 : What is the x-axis here ? Presumably phi is not actually 1 - dimensional ?

> In general , we are considering each dimension of the gradient separately in our assessment and so this should be read as \nabla_{\phi_1} . Note that each dimension of the gradient being equally likely to be positive or negative corresponds to the overall gradient taking a completely random direction .


% % % Typos etc . % % %

* “ learn a particular series intermediate ” missing “ of ” .

> Thank you , now fixed .

* “ To do so , we generate on sequence y 1 : T ” s/ on / a / , I think ?

> Thank you , now fixed .

* Equation 3 : Should there be a ( 1 / K ) in Z ?

> Thank you , now fixed .


Thank you for your further consideration and bumping up your score .

Overall :
I had a really hard time reading this paper because I found the writing to be quite confusing . For this reason I can not recommend publication as I am not sure how to evaluate the paper ’s contribution .

Summary
The authors study state space models in the unsupervised learning case . We have a set of observed variables Y , we posit a latent set of variables X , the mapping from the latent to the observed variables has a parametric form and we have a prior over the parameters . We want to infer a posterior density given some data .

The authors propose an algorithm which uses sequential Monte Carlo + autoencoders . They use a REINFORCE - like algorithm to differentiate through the Monte Carlo . The contribution of this paper is to add to this a method which uses 2 different ELBOs for updating different sets of parameters .

The authors show the AESMC works better than importance weighted autoencoders and the double ELBO method works even better in some experiments .

The proposed algorithm seems novel , but I do not understand a few points which make it hard to judge the contribution . Note that here I am assuming full technical correctness of the paper ( and still can not recommend acceptance ) .

Is the proposed contribution of this paper just to add the double ELBO or does it also include the AESMC ( that is , should this paper subsume the anonymized pre-print mentioned in the intro ) ? This was very unclear to me .

The introduction / experiments section of the paper is not well motivated . What is the problem the authors are trying to solve with AESMC ( over existing methods ) ? Is it scalability ? Is it purely to improve likelihood of the fitted model ( see my questions on the experiments in the next section ) ?

The experiments feel lacking . There is only one experiment comparing the gains from AESMC , ALT to a simpler ( ? ) method of IWAE . We see that they do better but the magnitude of the improvement is not obvious ( should I be looking at the ELBO scores as the sole judge ? Does AESMC give a better generative model ? ) . The authors discuss the advantages of SMC and say that is scales better than other methods , it would be good to show this as an experimental result if indeed the quality of the learned representations is comparable .

We thank the reviewer for taking the time to read through our paper and for their helpful feedback .

We would like to reiterate the main contributions of the paper :
- Re-introduction of the AESMC algorithm which was first introduced by Anon ( 2017 ) alongside the similar approaches of Maddison et al. ( 2017 ) , and Naesseth et al . ( 2017 ) . We reiterate that our previous work Anon ( 2017 ) is only a preprint and so this work still constitutes the first introduction of AESMC to the literature .

- Additional theoretical insights about the ELBOs used for AESMC and the IWAE , in particular demonstrating that increasing the number of particles K can be detrimental to proposal learning .

- Introducing the alternating EBLO algorithm to ameliorate the problems about proposal learning that our theoretical insights highlight can occur for the original AESMC and IWAE algorithms .

Regarding the comments about the experiments , we ran three experiments to illustrate our points :
- The experiment described in section 5.1 provides evidence that the AESMC algorithm works on a time - series model where we know how to evaluate and maximize the log marginal likelihood exactly . Figure 2 demonstrates that AESMC works better than IWAE .

- In section 5.2 we empirically investigate our claims about the adverse effect of increasing number of particles K on learning q( x|y ) ( Figure 3 , left ) . We then run the ALT algorithm to ameliorate this effect on a time series data for which the experiments are in Figure 3 ( middle , right ) .

- Finally , we run both IWAE , AESMC and ALT on a neural network model where it is impossible to evaluate the log marginal likelihood exactly and we must resort to max ( ELBO_IS , ELBO_SMC ) as a proxy . This is a common practice in evaluating deep generative models .


[ After author feedback ]
I think the approach is interesting and warrants publication . However , I think some of the counter- intuitive claims on the proposal learning are overly strong , and not supported well enough by the experiments . In the paper the authors also need to describe the differences between their work and the concurrent work of Maddison et al. and Naesseth et al .

[ Original review ]
The authors propose auto-encoding sequential Monte Carlo ( SMC ) , extending the VAE framework to a new Monte Carlo objective based on SMC . The authors show that this can be interpreted as standard variational inference on an extended space , and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure . The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial .

The approach is interesting and the paper is well - written , however , I have some comments and questions :

- It seems clear that the AESMC bound does not in general optimize for q( x|y ) to be close to p( x|y ) , except in the IWAE special case . This seems to mean that we should not expect for q -> p when K increases ?
- Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made . If I 'm not mistaken K=1 is regular ELBO and not IWAE / AESMC ? Have you estimated the probability for positive vs . negative gradient values for K=10 ? To me it looks like the probability of it being larger than zero is something like 2 / 3 . K>10 is difficult to see from this plot alone .
- Is there a typo in the bound given by eq . ( 17 ) ? Seems like there are two identical terms . Also I 'm not sure about the first equality in this equatiion , is I^2 = 0 or is there a typo ?
- The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter - intuitive , especially learning the proposals for SMC using IS . Have you tried this for high - dimensional models as well ? Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE - type objective will collapse to something close to the the standard ELBO . For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1 ? Might this be a typo in 4.1 ? You still propose to learn the proposal parameters using SMC but with lower number of particles ? I suspect this lower number of particles might be model - dependent .

Minor comments :
- Section 1 , first paragraph , last sentence , " that " -> " than " ?
- Section 3.2 , " ... using which ... " formulation in two places in the firsth and second paragraph was a bit confusing
- Page 7 , second line , just " IS " ?
- Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates ?
- Section 5.2 , stochastic variational inference Hoffman et al. ( 2013 ) uses natural gradients and exact variational solution for local latents so I do n't think K=1 reduces to this ?

We thank the reviewer for taking the time to read through our paper and their insightful questions .

Minor comments have been incorporated in the revision of our submission .

We address the specific questions in turn :

% % % It seems clear that the AESMC bound does not , in general , optimize for q( x|y ) to be close to p( x|y ) ... % % %

> This is true , but is effectively equivalent to the fact that the perfect importance sampler is better than the perfect SMC sampler , even though the latter is generally much more powerful when not perfectly optimized , as is almost always the case and hence why SMC is typically a superior inference algorithm . As we show in the set of equations ( 12 - 13 ) for IWAE and ( 14 - 15 ) for AESMC , these objectives can be decomposed to a log marginal likelihood term and a KL term on an extended space . In propositions 1 and 2 , we prove that while we should expect q( x|y ) = p ( x|y ) when we optimize the IWAE objective perfectly , this is not the case for AESMC .


% % % Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made ... % % %

> The second paragraph of section 4 ( and the associated Figure 1 ) is supposed to give an intuition for why using more K might result in a worse q( x|y ) . The case K=1 is regular ELBO ( which is a special case of the IWAE ) and the other cases are IWAE with the corresponding number of particles . We formalize this intuition by introducing the notion of a signal - to- noise ratio .

The probabilities of the \grad_\phi ELBO estimator are estimate using 10000 Monte Carlo samples to be :
K=1 : 0.8704
K=10 : 0.6072
K=100 : 0.5226
K=1000 : 0.5036
We believe that this is quite conclusive for this simple example , but the aim here is not the assumption that this simple example will generalize , just to show that increase K can be harmful . However , our theoretical result on the signal - to- noise ratio does provide a generalization to general problems and shows that this effect must always manifest for sufficiently large K for any problem .


% % % Is there a typo in the bound given by eq . ( 17 ) ... % % %

> Neither are typos . The repeated term is because one gets an identical term in the bias and variance components of the bound , one of which goes to the numerator and one the denominator when we calculate the SNR . It is indeed the case that I^2=0 .


% % % The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive ... % % %

> We agree that discussion and results with regards to the ALT algorithm are at first counter- intuitive , but we believe this is because of the counter- intuitive nature of our observation that increasing K actually harms the training of the proposal q( x|y ) . Given this novel realization , using fewer particles to train the proposals becomes a natural thing to do , while our empirical results verify that it can lead to improvements in performance .

We have indeed tried running ALT where we update \theta ( generative parameters ) using SMC with 1000 particles and \phi ( proposal parameters ) using IS with 10 particles . The results of this are described in the third paragraph of section 5.2. and the results are in Figure 3 ( middle , right ) . We have also run the experiment on a neural network based model described in section 5.3 and Figure 4 .

This paper presents a reinforcement learning method for learning complex tasks by dividing the state space into slices , learning local policies within each slice , while ensuring that they do n't deviate too far from each other , while simultaneously learning a central policy that works across the entire state space in the process . The most closely related works to this one are Guided Policy Search ( GPS ) and " Distral " , and the authors compare and contrast their work with the prior work suitably .

The paper is written well , has good insights , is technically sound , and has all the relevant references . The authors show through several experiments that the divide and conquer ( DnC ) technique can solve more complex tasks than can be solved with conventional policy gradient methods ( TRPO is used as the baseline ) . The paper and included experiments are a valuable contribution to the community interested in solving harder and harder tasks using reinforcement learning .

For completeness , it would be great to include one more algorithm in the evaluation : an ablation of DnC which does not involve a central policy at all . If the local policies are trained to convergence , ( and the context omega is provided by an oracle ) , how well does this mixture of local policies perform ? This result would be instructive to see for each of the tasks .

The partitioning of each task must currently be designed by hand . It would be interesting ( in future work ) to explore how the partitioning could perhaps be discovered automatically .

Thank you for your very valuable feedback !

We have modified the paper to include comparisons between DnC and two different oracle - based ensembles of local policies in Appendix C . The first ablation of DnC never distills the policies together , training the local policies to convergence . This ablation performs poorly compared to DnC in most tasks : we hypothesize that the distillation step allows the local policies to escape the local minima that policy gradient methods generally suffer from . Similar observations have been noted in Mordatch et al . [ 1 ] , where trajectory optimization without distillation to a central neural network underperforms . The other ablation runs DnC , but returns the final local ensemble instead of the final global policy . We observe that this final local ensemble with oracle context performs only marginally better than the final global policy in most tasks , indicating that there is little loss in performance during the distillation process . For both of these variants , the central policy , which must operate successfully for a wide range of contexts , generalizes better to contexts that are slightly different than the training distribution . Considering that training and testing conditions will almost always differ slightly in practice , even if one has oracle access to the context , it might be beneficial to use the central policy due to its better generalization capability .

Automatic ways to perform the partitioning is indeed an interesting future direction ! As a step in this direction , we have updated the paper with a simple automated partitioning scheme in Appendix D. Partitions are automatically generated via a K-means clustering procedure on the initial state distribution to generate contexts , and find that DnC performs well in this case as well . We hope to pursue more elaborate partitioning schemes in future work .

[ 1 ] Mordatch et al , Interactive Control of Diverse Complex Characters with Neural Networks , NIPS 2015


The submission tackles an important problem of learning highly varied skills . The approach relies on dividing the task space into subareas ( defined by task context vectors ) over which individual policies are trained , but are still required to operate well on tasks outside their context .

The exposition is clear and the method is well - motivated . I see no issues with the mathematical correctness of the claims made in the paper . The experimental results show a convincing benefit over TRPO and Distral on a number of manipulation and locomotion tasks . I would like to have seen more discussion of the computational costs and scaling of the method over TRPO or Distral , as the pairwise KL divergence terms grow quadratically in the number of contexts .

While the method is well - motivated , the division of tasks into subareas seems arbitrarily chosen . It would be very useful for readers to see performance of the algorithm under other task decompositions to alleviate the worries that the algorithm is not sensitive to the decomposition choice .

I would also like to see more discussion of curriculum learning , which also aims at tackling a similar problem of reducing complexity in early stages of training by choosing on simper tasks and progressing to more complex . Would such progressive tasks decompositions work better in your framework ? Does your framework remove the need for curriculum learning ?

Overall , I believe this is in interesting piece of work and I believe would be of interest to ICLR community .

Thank you for your very valuable comments . We address your questions below .

In regard to the choice of partitions : to address any potential concern regarding the partitions , we added additional experiments in Appendix D where the partitions are determined automatically , rather than being hand -specified . It is true that some care must be taken to get reasonable partitions , although our experiments suggest that even a simple K-means method can produce good results automatically . In Appendix D , we evaluate DnC on contexts generated by a K-means clustering procedure on the initial state distribution for the Picking task , which performs comparably to our manually designed contexts , indicating that performance of DnC is not particular to our choice of decomposition . We intend to extend this procedure to all the tasks for the final version . We further believe that it ’s possible to find more sophisticated automatic methods to generate the decompositions , which would make for interesting future work .


Regarding the complexity of the pairwise KL divergence , we have updated the paper to include a discussion of the computational cost in the fourth paragraph of Section 4.2 . Empirically we find that the quadratic penalty is not a bottleneck for the problems we hope to address with DnC , since sampling the environment is by far the most computationally demanding operation .

In regard to the relationship with curriculum learning , we have now added some remarks at the end of the first paragraph of Section 2 . Investigating the use of progressive decompositions with our method is an interesting direction for future work !


This paper presents a method for learning a global policy over multiple different MDPs ( referred to as different " contexts " , each MDP having the same dynamics and reward , but different initial state ) . The basic idea is to learn a separate policy for each context , but regularized in a manner that keeps all of them relatively close to each other , and then learn a single centralized policy that merges the multiple policies via supervised learning . The method is evaluated on several continuous state and action control tasks , and shows improvement over existing and similar approaches , notably the Distral algorithm .

I believe there are some interesting ideas presented in this paper , but in its current form I think that the delta over past work ( particularly Distral ) is ultimately too small to warrant publication at ICLR . The authors should correct me if I 'm wrong , but it seems as though the algorithm presented here is virtually identical to Distral except that :
1 ) The KL divergence term regularizes all policies together in a pairwise manner .
2 ) The distillation step happens episodically every R steps rather than in a pure SGD manner .
3 ) The authors possibly use a TRPO type objective for the standard policy gradient term , rather than REINFORCE - like approach as in Distral ( this one point was n't completely clear , as the authors mention that a " centralized DnC " is equivalent to Distral , so they may already be adapting it to the TRPO objective ? some clarity on this point would be helpful ) .
Thus , despite better performance of the method over Distral , this does n't necessarily seem like a substantially new algorithmic development . And given how sensitive RL tasks are to hyperparameter selection , there needs to be some very substantial treatment of how the regularization parameters are chosen here ( both for DnC and for the Distral and centralized DnC variants ) . Otherwise , it honestly seems that the differences between the competing methods could be artifacts of the choice of regularization ( the alpha parameter will affect just how tightly coupled the control policies actually are ) .

In addition to this point , the formulation of the problem setting in many cases was also somewhat unclear . In particular , the notion of the contextual MDP is not very clear from the presentation . The authors define a contextual MDP setting where in addition to the initial state there is an observed context to the MDP that can affect the initial state distribution ( but not the transitions or reward ) . It 's entirely unclear to me why this additional formulation is needed , and ultimately just seems to confuse the nature of the tasks here which is much more clearly presented just as transfer learning between identical MDPs with different state distributions ; and the terminology also conflicts with the ( much more complex ) setting of contextual decision processes ( see : https://arxiv.org/abs/1610.09512). It does n't seem , for instance , that the final policy is context dependent ( rather , it has to " infer " the context from whatever the initial state is , so effectively does n't take the context into account at all ) . Part of the reasoning seems to be to make the work seem more distinct from Distral than it really is , but I do n't see why " transfer learning " and the presented contextual MDP are really all that different .

Finally , the experimental results need to be described in substantially more detail . The choice of regularization parameters , the precise nature of the context in each setting , and the precise design of the experiments is all extremely opaque in the current presentation . Since the methodology here is so similar to previous approaches , much more emphasis is required to better understand the ( improved ) empirical results in this eating .

In summary , while I do think the core ideas of this paper are interesting : whether it 's better to regularize policies to a single central policy as in Distral or whether it 's better to use joint regularization , whether we need two different timescales for distillation versus policy training , and what policy optimization method works best , as it is right now the algorithmic choices in the paper seem rather ad-hoc compared to Distral , and need substantially more empirical evidence .

Minor comments :
• There are several missing words / grammatical errors throughout the manuscript , e.g. on page 2 " gradient information can better estimated " .

Thank you for your valuable suggestions !

We have included specific experiment details in Appendix A . In particular , we ran an extensive penalty hyperparameter sweep for DnC , centralized DnC , and Distral on each task to select the appropriate parameter for each method . Since the initial version , we have also updated the experiments by conducting a finer hyperparameter sweep and by running experiments with 5 random seeds instead of 3 . We have updated the paper with the results obtained from these searches ( Figure 1 , Table 1 ) . We thus contend that the difference between the performance of the various methods is not contingent on the exact choice of hyperparameters , and is indeed a result of the algorithmic differences . If the reviewer has any other suggestions for how to address this concern , we would be happy to incorporate them . We have also included more comprehensive task information , which detail precisely what the contexts are in each task , in Appendix B . We have updated the paper to distinguish our use of the word “ context ” from contextual MDPs in Section 3 . We also clarify in Section 5 that our analysis ports Distral to the TRPO objective . While the original Distral paper uses soft Q-learning , we adapt the algorithm to TRPO , since empirically TRPO exhibits better performance on high -dimensional continuous control tasks . If the reviewer has further recommendations , we would be happy to address these as well .



We now address concerns regarding the differences between our method and Distral [ 1 ] . DnC and Distral not only have completely different motivations , but the technical differences between the two algorithms are substantial as well . It is worth noting that our experiments ( with hyperparameter searches and multiple random seeds ) over five varied tasks in the locomotion and manipulation settings clearly illustrate that the Distral method as described in prior work does not solve the challenging tasks in our evaluation , while our approach does . This extensive comparative evaluation already establishes a clear contribution over the prior work , as noted by the other two reviewers .

There are also significant conceptual differences . Distral considers a transfer learning setting , while the goal in our work is to obtain a single policy that succeeds on a single challenging task with stochastic structure . While both algorithms could be applied to both settings , we feel this conceptual difference is very important . Whereas our method is concerned with the performance of the central policy on the full state space , the Distral paper evaluates performance of the local policies on their respective domains .

Furthermore , Distral does not propose nor analyze the potential to solve challenging continuous control tasks with stochastic initial state distributions . The observation that decomposing the initial state distribution in this way leads to drastically improved performance is not at all obvious , and is a key insight of our work . We believe that this contribution will be highly relevant to researchers interested in solving complex continuous control tasks , and this contribution is not present in the Distral paper . In the updated paper , we also describe how to automate the process of generating these decompositions , and present results in Appendix D . We find that DnC with this automated partitioning performs comparably to the manual partitions outlined in the paper , without the need for any manual specification of partitions .

Both DnC and Distral maintain the core idea that optimizing local or instance - specific policies can simplify many tasks . This idea is not new , and is popular in the RL community after works related to guided policy search [ 2 ] . In fact , ideas of the same flavor are present even in older works like target propagation [ 3 ] where an optimization method generates targets for a supervised learning network . From a bird ’s - eye perspective , all these methods exploit the same principle , but a closer look at the technical details unveil significant differences .

For example , GPS observes that adding a regularization term to stay close to the central network helps with distillation and overall convergence . Distral rediscovers the exact KL regularization and supervised distillation procedure as GPS , albeit with neural networks as local policies . However , Distral ’s key innovation comes from carefully choosing the algorithms for local training and applying the method to challenging visual transfer learning scenarios , something that the basic guided policy search algorithm does not do . In the same way , we propose a modified method with pairwise KL regularization terms and a varied distillation schedule , and apply it to challenging stochastic initial state continuous control tasks , as compared to the discrete control setup in Distral . Furthermore , while Distral uses soft Q-learning for their discrete action tasks , we use TRPO due to its stable performance in continuous control tasks . From this perspective , we believe that the difference between our method and Distral is comparable , if not greater than , the difference between Distral and GPS .

In motivation , technical detail , and empirical performance , DnC varies significantly from Distral . Thus , we believe that the proposed method , DnC , is quite different from previous methods , a sentiment that is shared by the other two reviews as well .


[ 1 ] Teh. et al , Distral , NIPS 2017
[ 2 ] Levine , et al , Guided Policy Search , ICML 2013
[ 3 ] see references of Lee et al , Difference Target Propagation , ECML PKDD 2015


Summary of paper - This paper presents SMASH ( or the one - Shot Model Architecture Search through Hypernetworks ) which has two training phases ( one to quickly train a random sample of network architectures and one to train the best architecture from the first stage ) . The paper presents a number of interesting experiments and discussions about those experiments , but offers more exciting ideas about training neural nets than experimental successes .

Review - The paper is very well written with clear examples and an excellent contextualization of the work among current work in the field . The introduction and related work are excellently written providing both context for the paper and a preview of the rest of the paper . The clear writing make the paper easy to read , which also makes clear the various weaknesses and pitfalls of SMASH .

The SMASH framework appears to provide more interesting contributions to the theory of training Neural Nets than the application of said training . While in some experiments SMASH offers excellent results , in others the results are lackluster ( which the authors admit , offering possible explanations ) .

It is a shame that the authors chose to push their section on future work to the appendices . The glimmers of future research directions ( such as the end of the last paragraph in section 4.2 ) were some of the most intellectually exciting parts of the paper . This choice may be a reflection of preferring to highlight the experimental results over possible contributions to theory of neural nets .


Pros -
* Strong related work section that contextualizes this paper among current work
* Very interesting idea to more efficiently find and train best architectures
* Excellent and thought provoking discussions of middle steps and mediocre results on some experiments ( i.e. last paragraph of section 4.1 , and last paragraph of section 4.2 )
* Publicly available code

Cons -
* Some very strong experimental results contrasted with some mediocre results
* The balance of the paper seems off , using more text on experiments than the contributions to theory .
* ( Minor ) - The citation style is inconsistent in places .

=-=-=-= Response to the authors

I thank the authors for their thoughtful responses and for the new draft of their paper . The new draft laid plain the contribution of the memory bank which I had missed in the first version . As expected , the addition of the future work section added further intellectual excitement to the paper .

The expansion of section 4.1 addressed and resolved my concerns about the balance of the paper by effortless intertwining theory and application . I do have one question from this section - In table 1 , the authors report p-values but fail to include them in their interpretation ; what is purpose of including these p-values , especially noting that only one falls under the typical threshold for significance ?


Hi Reviewer2 ,

Thank you for your detailed and constructive feedback . We are preparing a revision and a complete response but would like a quick bit of clarification as to specifically which experimental results fall under the lackluster banner .

Thanks ,

Paper1 Authors

Summary
The paper presents an interesting view on the recently proposed MAML formulation of meta-learning ( Finn et al ) . The main contribution is a ) insight into the connection between the MAML procedure and MAP estimation in an equivalent linear hierarchical Bayes model with explicit priors , b ) insight into the connection between MAML and MAP estimation in non- linear HB models with implicit priors , c ) based on these insights , the paper proposes a variant of MALM using a Laplace approximation ( with additional approximations for the covariance matrix . The paper finally provides an evaluation on the mini Image Net problem without significantly improving on the MAML results on the same task .

Pro :
- The topic is timely and of relevance to the ICLR community continuing a current trend in building meta-learning system for few - shot learning .
- Provides valuable insight into the MAML objective and its relation to probabilistic models

Con :
- The paper is generally well - written but I find ( as a non-meta- learner expert ) that certain fundamental aspects could have been explained better or in more detail ( see below for details ) .
- The toy example is quite difficult to interpret the first time around and does not provide any empirical insight into the converge of the proposed method ( compared to e.g. MAML )
- I do not think the empirical results provide enough evidence that it is a useful / robust method . Especially it does not provide insight into which types of problems ( small / large , linear / non-linear ) the method is applicable to .


Detailed comments / questions :
- The use of Laplace approximation is ( in the paper ) motivated from a probabilistic / Bayes and uncertainty point-of-view . It would , however , seem that the truncated iterations do not result in the approximation being very accurate during optimization as the truncation does not result in the approximation being created at a mode . Could the authors perhaps comment on :
a ) whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization ( given the psd approximation to the Hessian ) , or does it only make sense after convergence ?
b ) the consequence of the approximation errors on the general convergence of the proposed method ( consistency and rate )

- Sec 4.1 , p5 : Last equation : Perhaps useful to explain the term $ log ( \phi_j^ * | \ theta ) $ and why it is not in subroutine 4 . Should $ \phi^ * $ be $ \hat \phi$ ?
- Sec 4.2 : “ A straightforward … ” : I think it would improve readability to refer back to the to the previous equation ( i.e. H ) such that it is clear what is meant by “ straightforward ” .
- Sec 4.2 : Several ideas are being discussed in Sec 4.2 and it is not entirely clear to me what has actually been adopted here ; perhaps consider formalizing the actual computations in Subroutine 4 – and provide a clearer argument ( preferably proof ) that this leads to consistent and robust estimator of \theta .
- It is not clear from the text or experiment how the learning parameters are set .
- Sec 5.1 : It took some effort to understand exactly what was going on in the example and particular figure 5.1 ; e.g. , in the model definition in the body text there is no mention of the NN mentioned / used in figure 5 , the blue points are not defined in the caption , the terminology e.g. “ pre-update density ” is new at this point . I think it would benefit the readability to provide the reader with a bit more guidance .
- Sec 5.1 : While the qualitative example is useful ( with a bit more text ) , I believe it would have been more convincing with a quantitative example to demonstrate e.g. the convergence of the proposal compared to std MAML and possibly compare to a std Bayesian inference method from the HB formulation of the problem ( in the linear case )
- Sec 5.2 : The abstract clams increased performance over MAML but the empirical results do not seem to be significantly better than MAML ? I find it quite difficult to support the specific claim in the abstract from the results without adding a comment about the significance .
- Sec 5.2 : The authors have left out “ Mishral et al ” from the comparison due to the model being significantly larger than others . Could the authors provide insight into why they did not use the ResNet structure from the tcml paper in their L-MLMA scheme ?
- Sec 6+7 : The paper clearly states that it is not the aim to ( generally ) formulate the MAML as a HB . Given the advancement in gradient based inference for HB the last couple of years ( e.g. variational , nested laplace , expectation propagation etc ) for explicit models , could the authors perhaps indicate why they believe their approach of looking directly to the MAML objective is more scalable / useful than trying to formulate the same or similar objective in an explicit HB model and using established inference methods from that area ?

Minor :
- Sec 4.1 “ … each integral in the sum in ( 2 ) … ” eq 2 is a product


We thank R1 for thorough and constructive comments ! We have attempted to address all concerns to the best of our ability .

> “ The toy example is quite difficult to interpret the first time around and does not provide any empirical insight into the converge of the proposed method ( compared to e.g. MAML ) … I do not think the empirical results provide enough evidence that it is a useful / robust method . Especially it does not provide insight into which types of problems ( small / large , linear / non-linear ) the method is applicable to . ”

We have substantially revised the toy example in Figure 5 and its explanation in the text in Section 5.1 to better demonstrate the proposed novel algorithm . In Figure 5 , we show various samples from the posterior of a model that is meta-trained on different sinusoids , when presented with a few datapoints ( in red ) from a new , previously unseen sinusoid . This sampling procedure is motivated by the connection we have made between MAML and HB inference . We emphasize that the quantified uncertainty evident in Figure 5 is indeed a desirable quality in a model that learns from a small amount of data .

> “ The use of Laplace approximation is ( in the paper ) motivated from a probabilistic / Bayes and uncertainty point-of-view . It would , however , seem that the truncated iterations do not result in the approximation being very accurate during optimization as the truncation does not result in the approximation being created at a mode .
> Could the authors perhaps comment on :
> a ) whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization ( given the psd approximation to the Hessian ) , or does it only make sense after convergence ?
> b) the consequence of the approximation errors on the general convergence of the proposed method ( consistency and rate ) ”

We have revised the paper in Section 3.1 to better convey the following : The exact equivalence between early stopping and a Gaussian prior on the weights in the linear case , as well as the implicit regularization to the parameter initialization in the nonlinear case , tells us that *every iterate of truncated gradient descent is a mode of an implicit posterior . * Therefore , in making this approximation , we are not required to take the gradient descent procedure of fast adaptation to convergence . We thus emphasize that the PSD approximation to the curvature provided by KFAC is indeed justifiable even before convergence .

> Sec 4.2 : Several ideas are being discussed in Sec 4.2 and it is not entirely clear to me what has actually been adopted here ; perhaps consider formalizing the actual computations in Subroutine 4 – and provide a clearer argument ( preferably proof ) that this leads to consistent and robust estimator of \theta .

Regarding the justification of using KFAC and the Laplace approximation to estimate \theta : We employ the insight from Martens ( 2014 ) . In summary , for the Laplace approximation , we require a curvature matrix that would ideally be the Hessian . However , it is infeasible to compute the Hessian for all but the simplest models . In its place , we use the KFAC approximation to the Fisher ; the Fisher itself can be seen as an approximation to the Hessian as : 1 ) It corresponds to the expected Hessian under the model 's own predictive distribution ; and 2 ) It is equivalent under common loss functions ( such as cross-entropy and squared error , which we employ ) to the Generalized Gauss Newton ( GGN ) Matrix ( Pascanu & Bengio 2014 , Martens 2014 ) . We would note that we are not the first to use the GGN as an approximation to the Hessian ( see , for example , Martens 2010 , Vinyals & Popev , 2012 ) .

Regarding the computation of the Laplacian loss in practice : In subroutine 4 , we replace H- hat with the approximation to the Fisher found in Eq. ( 2 ) of Ba et al . ( 2017 ) .

> It is not clear from the text or experiment how the learning parameters are set .

We have clarified this in Section 5.2 : We chose the regularization weight of 10^ - 6 via cross-validation ; all other parameters are set to the values reported in Finn et al . ( 2017 ) .

> Comments re : Section 5.1

We apologize for the unclear diagram in the previous version of the paper . The toy example has been substantially revised in Figure 5 and elaborated in Section 5.1 , as per our comment above .

> Sec 5.2 : The abstract clams increased performance over MAML but the empirical results do not seem to be significantly better than MAML ?

We note that Triantafillou et al. ( 2017 ) in NIPS 2017 reported a similar improvement after MAML was published in ICML 2017 , and so the standard seems to be that an improvement of about 1 % is publishable .

> “ Sec 5.2 : The authors have left out “ Mishral et al ” from the comparison due to the model being significantly larger than others . Could the authors provide insight into why they did not use the ResNet structure from the tcml paper in their L-MLMA scheme ? ”

Please see our detailed discussion with an author of TCML in the OpenReview comment thread ( https://openreview.net/forum?id=BJ_UL-k0b&noteId=r1aR9l5lG). In summary , our contribution is to reinterpret MAML as approximate inference in a hierarchical Bayesian model , rather than to provide an exhaustive empirical comparison over neural network architectures ( as the choice of architecture is largely orthogonal to the training loss or algorithm ) . Furthermore , the majority of other prior few - shot learning methods used the smaller architecture , so we felt that standardizing the architecture would provide a more informative comparison . Since we were able to obtain a number for SNAIL / TCML using the same architecture , we believe that this adequately rounds out the comparisons .

> “ Sec 6 +7 : The paper clearly states that it is not the aim to ( generally ) formulate the MAML as a HB . Given the advancement in gradient based inference for HB the last couple of years ( e.g. variational , nested laplace , expectation propagation etc ) for explicit models , could the authors perhaps indicate why they believe their approach of looking directly to the MAML objective is more scalable / useful than trying to formulate the same or similar objective in an explicit HB model and using established inference methods from that area ? ”

We intend the connection between MAML and HB to provide an avenue to incorporate insights from gradient - based inference , not as an explicit alternative to established inference procedures . To clarify , with the Laplace approximation , we are making the assumption that the posterior over \phi is a unimodal Gaussian with mean centered at the point estimate computed by a few steps of gradient descent during fast adaptation , and with covariance equal to the inverse Hessian evaluated at that point . However , we are not restricted to this assumption — we could potentially use another inference method ( such as the nested Laplace approximation , variational Bayes , expectation propagation , or Hamiltonian Monte Carlo ) to compute a more complex posterior distribution over \phi and to potentially improve performance . We may also incorporate insights from the recent literature on interpreting gradient methods as forms of probabilistic inference ( e.g. , Zhang & Sun et al. 2017 ) due the gradient - based nature of our method . This is interesting future work !

If the reviewer has specific suggestions for related works that present generic inference methods ( gradient - based or otherwise ) that can reliably deal with high - dimensional stimuli and high-dimensional models ( especially those that deal with raw images ) , we would be grateful to hear of them .

> Sec 4.1 “ … each integral in the sum in ( 2 ) … ” eq 2 is a product

Fixed — thank you !


We encourage R2 to let us know of any additional questions or concerns about the clarity of the paper ( especially things that could make the work clearer to a non-meta- learning audience ) .


=========================================
References

Ba ( 2017 ) . “ Distributed Second - Order Optimization using Kronecker -Factored Approximations . ” In ICLR 2017 .
Martens ( 2010 ) . " Deep learning via Hessian - free optimization . " In ICML 2010 http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf
Martens ( 2014 ) . " New insights and perspectives on the natural gradient method . " arXiv preprint arXiv:1412.1193 . https://arxiv.org/abs/1412.1193
Pascanu & Bengio ( 2013 ) . " Revisiting natural gradient for deep networks . " arXiv preprint arXiv:1301.3584 . https://arxiv.org/abs/1301.3584
Triantafillou et al. ( 2017 ) . “ Few - Shot Learning Through an Information Retrieval Lens . ” In NIPS 2017 . https://arxiv.org/abs/1707.02610
Vinyals & Povey ( 2012 ) . “ Krylov Subspace Descent for Deep Learning . ” In AISTATS 2012 . https://arxiv.org/abs/1111.4259
Zhang & Sun et al. ( 2017 ) . “ Noisy Natural Gradient as Variational Inference . ” https://arxiv.org/abs/1712.02390

The paper reformulates the model - agnostic meta-learning algorithm ( MAML ) in terms of inference for parameters of a prior distribution in a hierarchical Bayesian model . This provides an interesting and , as far as I can tell , novel view on MAML . The paper uses this view to improve the MAML algorithm . The writing of the paper is excellent . Experimental evalution is well done against a number of recently developed alternative methods in favor of the presented method , except for TCML which has been exluded using a not so convincing argument . The overview of the literature is also very well done .

We thank R2 for feedback . Regarding R2 ’s comment on the exclusion of TCML from the miniImage Net results table : Our detailed discussion with an author of TCML is in the OpenReview comment thread ( https://openreview.net/forum?id=BJ_UL-k0b&noteId=r1aR9l5lG). In summary , our contribution is to reinterpret MAML as approximate inference in a hierarchical Bayesian model , rather than to provide an exhaustive empirical comparison over neural network architectures ( as the choice of architecture is largely orthogonal to the training loss or algorithm ) . Furthermore , the majority of other prior few - shot learning methods used the smaller architecture , so we felt that standardizing the architecture would provide a more informative comparison . Since we were able to obtain a number for SNAIL / TCML using the same architecture , we believe that this adequately rounds out the comparisons .

MAML ( Finn + 2017 ) is recast as a hierarchical Bayesian learning procedure . In particular the inner ( task ) training is initially cast as point - wise max likelihood estimation , and then ( sec4 ) improved upon by making use of the Laplace approximation . Experimental evidence of the relevance of the method is provided on a toy task involving a NIW prior of Gaussians , and the ( benchmark ) MiniImage Net task .

Casting MAML as HB seems a good idea . The paper does a good job of explaining the connection , but I think the presentation could be clarified . The role of the task prior and how it emerges from early stopping ( ie a finite number of gradient descent steps ) ( sec 3.2 ) is original and technically non-trivial , and is a contribution of this paper .
The synthetic data experiment sec5 . 1 and fig5 is clearly explained and serves to additionally clarify the proposed method .
Regarding the MiniImage Net experiments , I read the exchange on TCML and agree with the authors of the paper under review . However , I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically , and show that not just TCML but other non-shallow architectures are not considered for comparison here . In addition , the point made by the TCML authors is fair ( " nothing prevented you from ... " ) and I would also recommend mentioning the reviewed paper 's authors ' decision ( not to test deeper architectures ) in the footnote . This decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her disposal .
The experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods .

I am assessing this paper as " 7 " because despite the merit of the paper , the relevance of the reformulation of MAML , and the technical steps involved in the reformulation , the paper does not eg address other forms ( than L-MAML ) of the task -specific subroutine ML -... , and the benchmark improvements are quite small . I think the approach is good and fruitful .


# Suggestions on readability

* I have the feeling the paper inverts $ \alpha , \beta$ from their use in Finn 2017 ( step size for meta - vs task - training ) . This is unfortunate and will certainly confuse readers ; I advise carefully changing this throughout the entire paper ( eg Algo 2,3 , 4 , eq 1 , last eq in sec3. 1 , eq in text below eq3 , etc )

* I advise avoiding the use of the symbol f , which appears in only two places in Algo 2 and the end of sec 3.1 . This is in part because f is given another meaning in Finn 2017 , but also out of general parsimony in symbol use . ( could leave the output of ML -... implicit by writing ML -... ( \ theta , T ) _j in the $ sum_j$ ; if absolutely needed , use another symbol than f)

* Maybe sec3 can be clarified in its structure by re-ordering points on the quadratic error function and early stopping ( eg avoiding to split them between end of 3.1 and 3.2 ) .

* sec6 " Machine learning and deep learning " : I would definitely avoid this formulation , seems to tail in with all the media nonsense on " what 's the difference between ML and DL ? " . In addition the formulation seems to contrast ML with hierarchical Bayesian modeling , which does not make sense / is wrong and confusing .

# Typos

* sec1 second parag : did you really mean " in the architecture or loss function " ? unclear .
* sec2 : over a family
* " common structure , so that " ( not such that )
* orthgonal
* sec2.1 suggestion : clarify that \theta and \phi are in the same space
* sec2. 2 suggestion : task -specific parameter $ \phi_j $ is distinct from ... parameters $ \phi_{j '} , j' \neq j}
* " unless an approximate ... is provided " ( the use of the subjunctive here is definitely dated :-) )
* sec3.1 task -specific parameters $ \phi_j $ ( I would avoid writing just \ phi altogether to distinguish in usage from \ theta )
* Gaussian-noised
* approximation of the it objective
* before eq9 : " that solves " : well , it does n't really " solve " the minimisation , in that it is not a minimum ; reformulate this ?
* sec4.1 innaccurate
* well approximated
* sec4 .2 an curvature
* ( Amari 1989 )
* For the the Laplace
* O ( n^3 ) : what is n ?
* sec5.2 ( Ravi and L 2017 )
* for the the


We thank R3 for thorough and constructive comments ! We have attempted to address them to the best of our ability .

We agree with R3 ’s characterization of the paper , but would like to clarify a small point for completeness :

> “ In particular the inner ( task ) training is initially cast as point - wise max likelihood estimation … ”

We cast the task -specific training in the inner loop as maximum a posteriori estimation ( instead of max likelihood ) , in which the induced prior is a result of gradient descent with early stopping ( termed “ fast adaptation ” ) . In particular , the induced prior serves to regularize the task -specific parameters to initial conditions ( the parameter initialization ) .

> “ Regarding the MiniImage Net experiments … ”

Our detailed discussion with an author of TCML is in the OpenReview comment thread ( https://openreview.net/forum?id=BJ_UL-k0b&noteId=r1aR9l5lG). In summary , our contribution is to reinterpret MAML as approximate inference in a hierarchical Bayesian model , rather than to provide an exhaustive empirical comparison over neural network architectures ( as the choice of architecture is largely orthogonal to the training loss or algorithm ) . Furthermore , the majority of other prior few - shot learning methods used the smaller architecture , so we felt that standardizing the architecture would provide a more informative comparison . Since we were able to obtain a number for SNAIL / TCML using the same architecture , we believe that this adequately rounds out the comparisons .

> The experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods .

We note that Triantafillou et al. ( 2017 ) in NIPS 2017 reported a similar improvement after MAML was published in ICML 2017 , and so the standard seems to be that an improvement of about 1 % is publishable .

> before eq9 : " that solves " : well , it does n't really " solve " the minimisation , in that it is not a minimum ; reformulate this ?

In the linear regression case , the iterate indeed solves the * regularized * minimization problem ( in particular , it is a solution that obtains the best trade - off ( wrt the regularization parameter ) between minimal objective and regularization costs ) . However , the iterate indeed does not solve the * unregularized * problem .

> “ … the paper does not eg address other forms ( than L-MAML ) of the task -specific subroutine ML -... , ”

We could potentially use another inference method ( such as the nested Laplace approximation , variational Bayes , expectation propagation , or Hamiltonian Monte Carlo ) to compute a more complex posterior distribution over task - specific parameters \phi . This is an interesting extension that we leave to future work .

> “ # Suggestions on readability ” & “ # Typos ”

Many thanks for catching all of these corrigenda — we ’ve corrected them in the revised paper ( as follows for the more major points ) :

- \alpha , \beta → \beta , \alpha
- replaced “ f ” with “ E_{x from task} [ - \log p( x | \ theta ) ]
- We kept the split of early stopping & the quadratic function between 3.1 , 3.2 since 3.2 is “ additional material ” and 3.1 is already dense . But , thank you for the suggestion .
- reformulated related work
- clarified that \theta and \phi are in the same space
- O ( n^3 ) → O ( d^3 ) for d-dimensional Kronecker factor

The authors introduce an algorithm in the subfield of conditional program generation that is able to create programs in a rich java like programming language . In this setting , they propose an algorithm based on sketches - abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable names . Conditioned on information such as type specification or keywords of a method they generate the method 's body from the trained sketches .

Positives :

• Novel algorithm and addition of rich java like language in subfield of ' conditional program generation ' proposed
• Very good abstract : It explains high level overview of topic and sets it into context plus gives a sketch of the algorithm and presents the positive results .
• Excellently structured and presented paper

• Motivation given in form of relevant applications and mention that it is relatively unstudied
• The hypothesis / the papers goal is clearly stated . It is introduced with ' We ask ' followed by two well formulated lines that make up the hypothesis . It is repeated multiple times throughout the paper . Every mention introduces either a new argument on why this is necessary or sets it in contrast to other learners , clearly stating discrepancies .
• Explanations are exceptionally well done : terms that might not be familiar to the reader are explained . This is true for mathematical aspects as well as program generating specific terms . Examples are given where appropriate in a clear and coherent manner
• Problem statement well defined mathematically and understandable for a broad audience
• Mentioning of failures and limitations demonstrates a realistic view on the project
• Complexity and time analysis provided
• Paper written so that it 's easy for a reader to implement the methods
• Detailed descriptions of all instantiations even parameters and comparison methods
• System specified
• Validation method specified
• Data and repository , as well as cleaning process provided
• Every figure and plot is well explained and interpreted
• Large successful evaluation section provided
• Many different evaluation measures defined to measure different properties of the project
• Different observability modes
• Evaluation against most compatible methods from other sources
• Results are in line with hypothesis
• Thorough appendix clearing any open questions

It would have been good to have a summary / conclusion / future work section

SUMMARY : ACCEPT . The authors present a very intriguing novel approach that in a clear and coherent way . The approach is thoroughly explained for a large audience . The task itself is interesting and novel . The large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promising . Even though no conclusive section is provided , the paper is not missing any information .


Thank you for your feedback about the paper . We will add a conclusion section to the final version of the paper .

This paper aims to synthesize programs in a Java-like language from a task description ( X ) that includes some names and types of the components that should be used in the program . The paper argues that it is too difficult to map directly from the description to a full program , so it instead formulates the synthesis in two parts . First , the description is mapped to a " sketch " ( Y ) containing high level program structure but no concrete details about , e.g. , variable names . Afterwards , the sketch is converted into a full program ( Prog ) by stochastically filling in the abstract parts of the sketch with concrete instantiations .

The paper presents an abstraction method for converting a program into a sketch , a stochastic encoder - decoder model for converting descriptions to trees , and rejection sampling - like approach for converting sketches to programs . Experimentally , it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program AST . The data is derived from an online repository of ~ 1500 Android apps , and from that were extracted ~ 150 k methods , which makes the data very respectable in terms of realisticness and scale . This is one of the strongest points of the paper .

One point I found confusing is how exactly the Combinatorial Concretization step works . Am I correct in understanding that this step depends only on Y , and that given Y , Prog is conditionally independent of X ? If this is correct , how many Progs are consistent with a typical Y? Some additional discussion of why no learning is required for the P( Prog | Y ) step would be appreciated .

I 'm also curious whether using a stochastic latent variable ( Z ) is necessary . Would the approach work as well using a more standard encoder - decoder model with determinstic Z?

Some discussion of Grammar Variational Autoencoder ( Kusner et al ) would probably be appropriate .

Overall , I really like the fact that this paper is aiming to do program synthesis on programs that are more like those found " in the wild " . While the general pattern of mapping a specification to abstraction with a neural net and then mapping the abstraction to a full program with a combinatorial technique is not necessarily novel , I think this paper adds an interesting new take on the pattern ( it has a very different abstraction than say , DeepCoder ) , and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques , in my opinion .


Thank you for your feedback about the paper . We answer your specific questions below .

Question : Am I correct in understanding that [ Combinatorial Concretization ] step depends only on Y , and that given Y , Prog is conditionally independent of X ? If this is correct , how many Progs are consistent with a typical Y?

Answer : Yes , Prog is conditionally independent of X given a sketch Y . In theory , there may be an infinite number of Progs for every Y. A simple example is two Progs that differ only in variable names , thereby corresponding to the same Y ; for another example , there can be very many expressions that match the type of an API method argument . However , in practice , we use certain heuristics to limit the space of Progs from a given Y ( these heuristics are abstractly captured by the distribution P( Prog | Y ) . In particular , these heuristics prioritize smaller , simpler programs over complex ones , and name local variables in a canonical way .

While we did n't collect this data systematically , our experience with the system suggests that under the heuristics actually implemented in it , a typical Y leads to only ~ 5 - 10 distinct Progs in our experiments . We will collect this data more thoroughly and add it to the paper .

Question : Some additional discussion of why no learning is required for the P( Prog | Y ) step would be appreciated .

Answer : In principle , this step could be made data-driven ; however , the resulting learning problem would be very difficult . This is because a single sketch used for training can correspond to many training programs that only differ in superficial details ( for example local variable names ) . Learning to decide which differences between programs are superficial and which are not , solely by looking at the syntax of programs , is hard . In contrast , our approach of heuristically choosing P( Prog | Y ) utilizes our domain knowledge of language semantics ( for example , that local variable names do not matter , and that some algebraic expressions are semantically equivalent ) . This knowledge allows us to limit the set of programs that we end up generating . We will clarify this in more detail in the paper .


Question : I 'm also curious whether using a stochastic latent variable ( Z ) is necessary . Would the approach work as well using a more standard encoder - decoder model with deterministic Z?

Answer : The randomness associated with the latent variable Z serves as a way to regularize the learning process ( a similar argument is made in the context of VAEs for the stochastic latent variable used during VAE learning ) . We were concerned that without the stochasticity ( i.e. , with a deterministic Z ) , training the model would be more likely to be affected by overfitting . Practically speaking , the stochasticity also serves as a way to ensure that we can generate a wide variety of possible programs from a given X. If Z was not random , a particular set of labels X will always result in exactly the same value of Z.

Comment : Some discussion of Grammar Variational Autoencoder ( Kusner et al ) would probably be appropriate .

Answer : Kusner et al ’s work proposes a VAE for context - free grammars . Being an auto-encoder it is a generative model , but it is not a conditional model such as ours . In their application towards synthesizing molecular structures , given a particular molecular structure , their model can be used to search the latent space for similar valid structures . In our setting , however , we are not given a sketch but only labels about the sketch , and our task is learn a conditional model that can predict a whole sketch given labels .

We will add the discussion about this work in the final version of the paper .


This is a very well - written and nicely structured paper that tackles the problem of generating / inferring code given an incomplete description ( sketch ) of the task to be achieved . This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps . The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously .

This paper has many strengths :
1 ) The writing is clear , and the paper is well - motivated
2 ) The proposed algorithm is described in excellent detail , which is essential to reproducibility
3 ) As stated previously , the approach is validated with a large number of real Android projects
4 ) The fact that the language generated is non-trivial ( Java-like ) is a substantial plus
5 ) Good discussion of limitations

Overall , this paper is a valuable addition to the empirical software engineering community , and a nice break from more traditional approaches of learning abstract syntax trees .

Thank you for your feedback about the paper .

This paper investigates the impact of character - level noise on various flavours of neural machine translation . It tests 4 different NMT systems with varying degrees and types of character awareness , including a novel mean Char system that uses averaged unigram character embeddings as word representations on the source side . The authors test these systems under a variety of noise conditions , including synthetic scrambling and keyboard replacements , as well as natural ( human -made ) errors found in other corpora and transplanted to the training and / or testing bitext via replacement tables . They show that all NMT systems , whether BPE or character - based , degrade drastically in quality in the presence of both synthetic and natural noise , and that it is possible to train a system to be resistant to these types of noise by including them in the training data . Unfortunately , they are not able to show any types of synthetic noise helping address natural noise . However , they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise .

This is a thorough exploration of a mostly under-studied problem . The paper is well - written and easy to follow . The authors do a good job of positioning their study with respect to related work on black - box adversarial techniques , but overall , by working on the topic of noisy input data at all , they are guaranteed novelty . The inclusion of so many character - based systems is very nice , but it is the inclusion of natural sources of noise that really makes the paper work . Their transplanting of errors from other corpora is a good solution to the problem , and one likely to be built upon by others . In terms of negatives , it feels like this work is just starting to scratch the surface of noise in NMT . The proposed mean Char architecture does n’t look like a particularly good approach to producing noise - resistant translation systems , and the alternative solution of training on data where noise has been introduced through replacement tables is n’t extremely satisfying . Furthermore , the use of these replacement tables means that even when the noise is natural , it ’s still kind of artificial . Finally , this paper does n’t seem to be a perfect fit for ICLR , as it is mostly experimental with few technical contributions that are likely to be impactful ; it feels like it might be more at home and have greater impact in a * ACL conference .

Regarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data , but even granting that such a resource does not yet exist , what is described here feels unnaturally artificial . First of all , errors learned from the noisy data sources are constrained to exist within a word . This tilts the comparison in favour of architectures that retain word boundaries ( such as the charCNN system here ) , while those systems may struggle with other sources of errors such as missing spaces between words . Second , if I understand correctly , once an error is learned from the noisy data , it is applied uniformly and consistently throughout the training and / or test data . This seems worse than estimating the frequency of the error and applying them stochastically ( or trying to learn when an error is likely to occur ) . I feel like these issues should at least be mentioned in the paper , so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise .

Also , it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data ( Table 6 ) . I realize that this is likely due to computational or time constraints , but it is worth providing some explanation in the text for why the experiments were conducted in this manner . On a related note , the line in the abstract stating that “ ... a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise ” implies that the other ( non-charCNN ) architectures could not learn these representations , when in reality , they simply were n’t given the chance .

Section 7.2 on the richness of natural noise is extremely interesting , but maybe less so to an ICLR audience . From my perspective , it would be interesting to see that section expanded , or used as the basis for future work on improve architectures or training strategies .

I have only one small , specific suggestion : at the end of Section 3 , consider deleting the last paragraph break , so there is one paragraph for each system ( charCNN currently has two paragraphs ) .

[ edited for typos ]

Thank you for the useful feedback . We agree that noisy input in neural machine translation is an under-studied problem .

Responses to specific comments :
1 . We agree that our work only starts to scratch the surface of noise in NMT and believe there ’s much more to be done in this area . We do believe that it ’s important to initiate a discussion of this issue in the ICLR community , for several reasons : ( a ) we study word and character representations for NMT , which is in line with the ICLR representation learning theme ; ( b) ICLR audience is very interested in neural machine translation and seminal work on NMT has been published in ICLR ( e.g. , Bahdanau et al. ’s 2015 paper on attention in NMT ) ; ( c ) ICLR audience is very interested in noise and adversarial examples , as evidenced by the plethora of recent papers on the topic . As reviewer 1 says , even though there are no fancy new methods in the paper , we believe that this kind of research belongs in ICLR .

2 . We agree that mean Char may not be the ideal architecture for capturing noise , but it ’s a simple , structure - invariant representation that works reasonably well . We have tried several other architectures , including a self - attention mechanism , but have n’t been able to improve beyond it . We welcome more suggestions and can include those negative results in new drafts of the paper .

3 . Training with noise has its limitations , but it ’s an effective method that can be employed by NMT providers and researchers easily and impactfully , as pointed out by reviewer 1 .

4 . In this work , we focus on word - level noise . Certainly , sentence - level noise is also important to learn , and we ’d like to see more work on this . We ’ll add this as another direction for future work . Note that while charCNN may have some advantage in dealing with word - level noise , it too suffers from increasing amounts of noise , similar to the other models we studied .

5 . Applying noise stochastically based on frequency in available corpora is an interesting suggestion , that can be done for the natural noise , but not so clear how to apply for synthetic noise . We did experiment with increasing amounts of noise ( Figure 1 ) , but we agree there ’s more to be done . We ’ll add this as another future work .

6 . ( To both reviewer 2 and 3 ) Regarding training other seq2seq models with noise : Our original intent was to test the robustness of pre-trained state - of - the- art models , but we also considered retraining them in this noisy paradigm . There are a number of design decisions that are involved here ( e.g. should the BPE dictionary be built on the noisy texts and how should thresholds be varied ? ) . That being said , we can investigate training using published parameter values , but worry these may be wholly inappropriate settings for the new noisy data .

7 . We ’ll modify the abstract to not give the wrong impression regarding what other architectures can learn .

8 . We included section 7.2 to demonstrate why synthetic noise is not very helpful in dealing with natural noise , as well as to motivate the development of better architectures .

9 . We ’ll correct the other small issues pointed to .


This paper empirically investigates the performance of character - level NMT systems in the face of character - level noise , both synthesized and natural . The results are not surprising :

* NMT is terrible with noise .

* But it improves on each noise type when it is trained on that noise type .

What I like about this paper is that :

1 ) The experiments are very carefully designed and thorough .

2 ) This problem might actually matter . Out of curiosity , I ran the example ( Table 4 ) through Google Translate , and the result was gibberish . But as the paper shows , it ’s easy to make NMT robust to this kind of noise , and Google ( and other NMT providers ) could do this tomorrow . So this paper could have real - world impact .

3 ) Most importantly , it shows that NMT ’s handling of natural noise does * not * improve when trained with synthetic noise ; that is , the character of natural noise is very different . So solving the problem of natural noise is not so simple … it ’s a * real * problem . Speculating , again : commercial MT providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper , but at much larger scale . So these methods could be applied in the real world . ( It would be excellent if an outcome of this paper was that commercial MT providers answered it ’s call to provide more realistic noise by actually providing examples . )

There are no fancy new methods or state - of - the- art numbers in this paper . But it ’s careful , curiosity - driven empirical research of the type that matters , and it should be in ICLR .

Thank you for the useful feedback .

1 . We agree that the topic has real - world impact for MT providers and will emphasize this in the conclusions .

2 . We would love to see MT providers use noisy data and we agree that the community would benefit from access to more noisy examples .


This paper investigates the impact of noisy input on Machine Translation , and tests simple ways to make NMT models more robust .

Overall the paper is a clearly written , well described report of several experiments . It shows convincingly that standard NMT models completely break down on both natural " noise " and various types of input perturbations . It then tests how the addition of noise in the input helps robustify the charCNN model somewhat . The extent of the experiments is quite impressive : three different NMT models are tried , and one is used in extensive experiments with various noise combinations .

This study clearly addresses an important issue in NMT and will be of interest to many in the NLP community . The outcome is not entirely surprising ( noise hurts and training and the right kind of noise helps ) but the impact may be . I wonder if you could put this in the context of " training with input noise " , which has been studied in Neural Network for a while ( at least since the 1990s ) . I.e. it could be that each type of noise has a different regularizing effect , and clarifying what these regularizers are may help understand the impact of the various types of noise . Also , the bit of analysis in Sections 6.1 and 7.1 is promising , if maybe not so conclusive yet .

A few constructive criticisms :

The way noise is included in training ( sec. 6.2 ) could be clarified ( unless I missed it ) e.g. are you generating a fixed " noisy " training set and adding that to clean data ? Or introducing noise " on - line " as part of the training ? If fixed , what sizes were tried ? More information on the experimental design would help .

Table 6 is highly suspect : Some numbers seem to have been copy -pasted in the wrong cells , eg. the " Rand " line for German , or the Swap / Mid / Rand lines for Czech . It 's highly unlikely that training on noisy Swap data would yield a boost of + 18 BLEU points on Czech -- or you have clearly found a magical way to improve performance .

Although the amount of experiment is already important , it may be interesting to check whether all se2seq models react similarly to training with noise : it could be that some architecture are easier / harder to robustify in this basic way .

[ Response read -- thanks ]
I agree with authors that this paper is suitable for ICLR , although it will clearly be of interest to ACL / MT - minded folks .

Thank you for the constructive feedback .
1 . Noise setup : when training with noise , we replace the original training set with a new , noisy training set . The noisy training set has exactly the same number of sentences and words as the training set , but noise is introduced according to the description in Section 4 . Therefore , we have one fixed noisy training set per each noise type . We ’ll clarify the experimental design in the paper .

2 . We had not thought to explore the relationship between the noise we are introducing as a corruption of the input and the training under noise paradigm you referenced . We might be mistaken , but normally , the corruption ( e.g. Bishop 95 ) is in the form of small additive gaussian noise . It is n’t immediately clear to us whether discrete perturbation of the input like we have here is equivalent , but would love suggestions on analyses we might do to investigate this insight further .

3 . Some cells in the mentioned rows in Table 6 were indeed copied from the French rows by error . We corrected the numbers and they are in line with the overall trends . Thank you for pointing this out . The corrected Czech numbers are in the 20s and the best performing system is the Rand + Key + Real setting .

4 . ( To both reviewer 2 and 3 ) Regarding training other seq2seq models with noise : Our original intent was to test the robustness of pre-trained state - of - the- art models , but we also considered retraining them in this noisy paradigm . There are a number of design decisions that are involved here ( e.g. should the BPE dictionary be built on the noisy texts and how should thresholds be varied ? ) . That being said , we can investigate training using published parameter values , but worry these may be wholly inappropriate settings for the new noisy data .

This is a highly interesting paper that proposes a set of methods that combine ideas from imitation learning , evolutionary computation and reinforcement learning in a novel way . It combines the following ingredients :
a ) a population - based setup for RL
b ) a pair-selection and crossover operator
c ) a policy - gradient based “ mutation ” operator
d ) filtering data by high - reward trajectories
e ) two -stage policy distillation

In its current shape it has a couple of major flaws ( but those can be fixed during the revision / rebuttal period ) :

( 1 ) Related work . It is presented in a somewhat ahistoric fashion . In fact , ideas for evolutionary methods applied to RL tasks have been widely studied , and there is an entire research field called “ neuroevolution ” that specifically looks into which mutation and crossover operators work well for neural networks . I ’m listing a small selection of relevant papers below , but I ’d encourage the authors to read a bit more broadly , and relate their work to the myriad of related older methods . Ideally , a more reasonable form of parameter - crossover ( see references ) could be compared to -- the naive one is too much of a straw man in my opinion . To clarify : I think the proposed method is genuinely novel , but a bit of context would help the reader understand which aspects are and which aspects are n’t .

( 2 ) Ablations . The proposed method has multiple ingredients , and some of these could be beneficial in isolation : for example a population of size 1 with an interleaved distillation phase where only the high - reward trajectories are preserved could be a good algorithm on its own . Or conversely , GPO without high - reward filtering during crossover . Or a simpler genetic algorithm that just preserves the kills off the worst members of the population , and replaces them by ( mutated ) clones of better ones , etc .

( 3 ) Reproducibility . There are a lot of details missing ; the setup is quite complex , but only partially described . Examples of missing details are : how are the high - reward trajectories filtered ? What is the total computation time of the different variants and baselines ? The x-axis on plots , does it include the data required for crossover / Dagger ? What are do the shaded regions on plots indicate ? The loss on \pi_S should be made explicit . An open-source release would be ideal .

Minor points :
- naively , the selection algorithm might not scale well with the population size ( exhaustively comparing all pairs ) , maybe discuss that ?
- the filtering of high - reward trajectories is what estimation of distribution algorithms [ 2 ] do as well , and they have a known failure mode of premature convergence because diversity / variance shrinks too fast . Did you investigate this ?
- for Figure 2a it would be clearer to normalize such that 1 is the best and 0 is the random policy , instead of 0 being score 0 .
- the language at the end of section 3 is very vague and noncommittal -- maybe just state what you did , and separately give future work suggestions ?
- there are multiple distinct metrics that could be used on the x- axis of plots , namely : wallclock time , sample complexity , number of updates . I suspect that the results will look different when plotted in different ways , and would enjoy some extra plots in the appendix . For example the ordering in Figure 6 would be inverted if plotting as a function of sample complexity ?
- the A2C results are much worse , presumably because batchsizes are different ? So I ’m not sure how to interpret them : should they have been run for longer ? Maybe they could be relegated to the appendix ?

References :
[ 1 ] Gomez , F. J. , & Miikkulainen , R. ( 1999 ) . Solving non-Markovian control tasks with neuroevolution .
[ 2 ] Larranaga , P. ( 2002 ) . A review on estimation of distribution algorithms .
[ 3 ] Stanley , K. O. , & Miikkulainen , R. ( 2002 ) . Evolving neural networks through augmenting topologies .
[ 4 ] Igel , C. ( 2003 ) . Neuroevolution for reinforcement learning using evolution strategies .
[ 5 ] Hausknecht , M. , Lehman , J. , Miikkulainen , R. , & Stone , P. ( 2014 ) . A neuroevolution approach to general atari game playing .
[ 6 ] Gomez , F. , Schmidhuber , J. , & Miikkulainen , R. ( 2006 ) . Efficient nonlinear control through neuroevolution .


Pros :
- results
- novelty of idea
- crossover visualization , analysis
- scalability

Cons :
- missing background
- missing ablations
- missing details

[ after rebuttal : revised the score from 7 to 8 ]

1 - Concerning “ Missing background ” : Thank you for pointing us to relevant literature on neuroevolution algorithms for reinforcement learning . Previously , we had only covered NEAT and evolutionary strategies ( ES , CMA - ES ) , but we have expanded our background section to include HyperNEAT , CoSyNE , SANE and ESP , since all these neuroevolution algorithms have been successfully applied to RL problems . Please see section 2.3 . We have also include a recent work by Lehman et. al on “ safe mutation ” for genetic algorithms .

2 - Concerning “ Missing ablations ” : We have added a section ( 4.4 ) on ablation studies for GPO . We consider the following 3 crucial components of GPO - 1 ) State-space crossover 2 ) Selection of candidates with highest fitness 3 ) Data-sharing for policy gradient RL during the mutation phase . To understand the behavior of these components , we compare performance when each of them is used in isolation for policy optimization . We further experiment with the scenario when one component is removed from GPO and the other two are used . This gives us a total of 6 algorithms which we plot along with GPO and our Single baseline in Figure 5 in the revision . It helps to define what we mean by a component “ not being used ” . For crossover , it means that we create the offspring by using the network parameters of the stronger parent ; for selection , it means that we disregard the fitness of candidates and select the population for the next generation at random ; for data-sharing , it means that policies in the ensemble do n’t share samples from other similar policies for PPO ( or A2C ) during mutation .

3 - Concerning “ Reproducibility ” : We have added a section ( 6.3 ) on implementation details for the crossover step . To train the binary policy ( Equation 1 in the revision ) , we reuse the trajectories from the parents ’ previous mutation phase rather than generating new samples . We filter the trajectories based on trajectory reward ( sum of the individual reward at each transition in the trajectory ) . For our experiments , we simply prune the worst 40 % trajectories from the dataset . We did not find the final GPO performance to be very sensitive to the % threshold hyperparameter . We will release our code on Github very soon .

4 - Concerning other missing details : For computation time , we include a section ( 6.4.1 ) on wall - clock time . For all the environments , we compute the time for GPO and break it down into crossover , selection and mutation phases . We compare the time with our strongest baseline - “ Joint ” . The x-axis on all the plots of episode- reward vs . timesteps includes the data required for crossover / Dagger , and the shaded region indicates the standard - error of the performance with different random seeds . These details , along with the loss on \pi_S ( Equation 1 ) , are in the revision .

5 - Concerning “ Minor points ” :

[ Scalable selection ] Yes , for a population of n , comparing all nC2 pairs is prohibitively expensive when n is large . This was indeed the case when we ran with a population size of 32 for the scalability plot ( Figure 7 ) . Our solution was to prune the population to k by probabilistic sampling ( probability = fitness ) , and then run selection over kC2 . Looking for more sophisticated and scalable alternatives is interesting future work .

[ Lack of diversity ] Yes , we did observe that maintaining a diverse population was challenging after 3 - 4 rounds of GPO ( algorithm 1 ) . We did some preliminary investigation with the Hopper environment , where we believed that some policies in the GPO ensemble were getting stuck in local minima , making the overall learning slow . We increased the randomness in the selection phase and found learning to proceed at a much more rapid pace . We need to explore this further .

[ Language at end of section 3 ] We have modified the section to include details on the fitness used by our experiments . Rather than dynamically adapting the weight of performance vs . KL fitness over the rounds of GPO , our current implementation puts all the weight on performance for all rounds , and relies on the randomness in the starting seed for different policies in the ensemble for diversity in the initial rounds .

[ Figure 6 with timesteps on x-axis ] We have included this figure in Appendix 6.4.2 . For the Walker environment , we observe that the sample - complexity for a population of 32 is quite competitive with our default GPO value of 8 .

[ A2C results ] A2C runs use the same batchsize as the PPO . We believe that the KL penalty in PPO prevents ( possibly destructive ) large updates to the policy distribution , and also the 10x more gradient steps in PPO allow for faster learning compared to A2C . A2C performance seems to be still improving when we end training for our experiments , and running them longer could see them match the PPO numbers . A2C results are moved to the Appendix in the revision .

The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network , adding it to the ensemble and selecting the strongest networks to remain ( under certain definitions of a " strong " network ) . The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks , although I would appreciate a wall - time comparison as well , as training the " crossover " network is presumably time - consuming .

It seems that for much of the paper , the authors could dispense with the genetic terminology altogether - and I mean that as a compliment . There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient - based learning for GPO , even if it makes it depart from what many in the field would consider " evolutionary " computing . Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning . I would perhaps even call this a distillation network rather than a crossover network . In many robotics tasks behavioral cloning is known for overfitting to expert trajectories , but that may not be a problem in this setting as " expert " trajectories can be generated in unlimited quantities .

1 - Concerning wall - time comparison : We have added a section in the Appendix ( 6.4.1 ) comparing wall - clock time for GPO and Joint . Both the algorithms are designed to use the multi-core parallelism on offer . We observe that GPO can be 1.5 to 2 times slower than Joint depending on the environment . Note that the timing numbers also depend on the number of iterations we run mutation ( policy gradient ) for before crossing over the policies , and we show the numbers for the default setting of these hyperparameters for all our experiments . For GPO , Mutate takes a good portion of the overall time due to communication overheads caused by data-sharing between policies . The crossover step takes moderate amount of time . We believe this is due to the following reasons - 1 ) for learning the binary policy ( Equation 1 ) , we reuse the trajectories from the parents ’ previous mutation phase rather than generating new samples ; 2 ) the losses in Equation 1 . and 2. are not minimized to convergence since the optimization ( first-order , Adam ) is only run for certain number of epochs . We provide the exact details in a new section in the Appendix ( 6.3 ) ; ( 3 ) the crossover phase is parallelized , i.e. once the parents are decided by the selection step , each crossover is done in an independent parallel process .

2 - Concerning use of term behavioral cloning : We completely agree with the reviewer that it ’s imperative that we use crisp terminology . To that end , in section 3.2 , where we first mention using imitation learning for the crossover , we expand on the differences between flavors of imitation learning ( a.k.a behavioral cloning and inverse RL ) , and explicitly say that all our references to imitation learning signify behavioral cloning .

3 - Concerning using “ crossover by distillation ” : We agree with the reviewer in that the high - level objective of the crossover step is to “ distill ” the knowledge from the parent policy networks into the offspring network . However , we believe that there are two main differences between the distillation network proposed in [ 1 ] and our procedure for crossover . Firstly , in [ 1 ] the soft targets for training the offspring network are computed using the arithmetic ( or geometric ) mean of the temperature - controlled outputs from parent networks . The argument is that different parent networks trained on similar data for similar amount of time represent different local minima point on the loss surface , and averaging leads to better generalization . In contrast , the parent policies in GPO have ( possibly ) visited disparate regions of the state- space and have ( possibly ) been trained on dissimilar data . Therefore , rather than averaging the output of the parents , we train another policy \pi_S to output the weighting , and do a weighted average . Secondly , the distillation network in [ 1 ] was trained for speech and object recognition tasks which do not have a temporal nature . However , the supervised training of the offspring in GPO should account for the compounding errors in the performance of the trained policy in areas of state- space different from the training data . Therefore , we add DAgger training to our crossover step , making it further different from vanilla distillation .

[ 1 ] Hinton et al. , Distilling the Knowledge in a Neural Network

This paper proposes a genetic algorithm inspired policy optimization method , which mimics the mutation and the crossover operators over policy networks .

The title and the motivation about the genetic algorithm are missing leading and improper . The genetic algorithm is a black - box optimization method , however , the proposed method has nothing to do with black - box optimization .

The mutation is a method to sample individual independence of the objective function , which is very different with the gradient step . Mimicking the mutation by a gradient step is very unreasonable .

The crossover operator is the policy mixing method employed in game context ( e.g. , Deep Reinforcement Learning from Self - Play in Imperfect -Information Games , https://arxiv.org/abs/1603.01121 ) . It is straightforward if two policies are to be mixed . Although the mixing method is more reasonable than the genetic crossover operator , it is strange to compare with that operator in a method far away from the genetic algorithm .

It is highly suggested that the method is called as population - based method as a set of networks is maintained , instead of as " genetic " method .

Another drawback , perhaps resulted from the " genetic algorithm " motivation is that the proposed method has not been well explained . The only explanation is that this method mimics the genetic algorithm . However , this explanation reveals nothing about why the method could work well -- a random exploration could also waste a lot of samples with a very high probability .

The baseline methods result in rewards much lower than those in previous experimental papers . It is problemistic that if the baselines have bad parameters .
1 . Benchmarking Deep Reinforcement Learning for Continuous Control
2 . Deep Reinforcement Learning that Matters

1 - Concerning title and missing motivation : The reviewer is correct in pointing out that genetic algorithms ( GA ) fall into the category of black - box optimization techniques . Their lack of exploiting the structure in the underlying tasks , e.g. the temporal nature in RL , explains their limited success in deep learning . Black - box techniques have been able to solve some RL problems , for example in [ 1 ] and most recently in [ 2 ] , but with unsatisfactory sample - complexity . Our goal with GPO was to buy the philosophy of genetic operators - mutation , selection and crossover - from GA , and marry it with model - free policy - gradient RL algorithm to achieve good sample complexity . We believe that the connection to GA is helpful because it may be possible to apply the myriad of advanced enhancements for general GA ( Section 3.1 in [ 2 ] ) to our policy optimization algorithm as well . For example , techniques to obtain quality diversity in GA population could be helpful for efficient exploration in large state-spaces . At the same time , using policy gradients as a plug-and - play component in our genetically - inspired algorithm enables us to exploit advances in policy gradients ; see , for instance , the difference in GPO performance with PPO compared to A2C .

There is prior work on opening up the GA / ES “ black - box ” to obtain improved performance and stability for RL . For example , in [ 3 ] , the authors suggest replacing the random mutations with perturbations guided by the gradients of the neural network output . A related idea was presented in [ 6 ] . [ 7 ] modifies the fitness function used in selection to aid exploration . We have updated section 2.3 with more neuroevolution algorithms which have been adjusted to work in the RL setting .

2 - Concerning lack of explanation for why the method works : The fact that our algorithm is not “ black - box ” enables us to investigate the sources of improvement . Firstly , as we show in Section 4.2 through experimentation , the crossover operator is able to transfer positive behavior from the parent policies to the offspring policy . Secondly , we do mutation through tried - and -tested algorithms like PPO / A2C and take the empirical success that they have enjoyed . Thirdly , our selection operator maintains high performance policies in the population . We believe the overall GPO performance is a culmination of these components . We have added a section ( 4.4 ) on ablation studies for GPO .

3 - Concerning baseline performance not same as other papers : We use the MuJoCo environments included as part of rllab [ 4 ] . The environments provided in the open-source release vary in their parameters from what the authors used for the paper ( https://github.com/rll/rllab/issues/157), and therefore it ’s hard to replicate their exact numbers . Regarding the numbers in [ 5 ] , please note that their evaluation is done with the Gym MuJoCo environments , which again differ from rllab MuJoCo in terms of parameters like coefficients for rewards , aliveness bonus etc . For completeness , we ran GPO on Gym MuJoCo environments and compared to Joint . We have added Appendix 6.2 for this . We also had a discussion with the authors of [ 5 ] on the variance between baselines numbers for different codebases ( rllab , openAIbaselines etc . ) . See Figure 6. in [ 5 ] for reference where “ Duan 2016 ” is the rllab framework we use . We believe that factors such as value function approximation ( Adam vs. LBFGS ) , observation / reward normalization method etc. lead to appreciable variation in baseline performance across codebases . Importantly , all these factors remain constant between GPO and the baselines for our results . Our baselines are very close to the rllab baselines ( Figure 29 . ) in [ 5 ] .


[ 1 ] Evolution Strategies as a Scalable Alternative to Reinforcement Learning
[ 2 ] Deep Neuroevolution : Genetic Algorithms Are a Competitive Alternative For Training Deep Neural Networks for Reinforcement Learning
[ 3 ] Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients
[ 4 ] Benchmarking Deep Reinforcement Learning for Continuous Control
[ 5 ] Deep Reinforcement Learning that Matters
[ 6 ] Parameter Space Noise for Exploration
[ 7 ] Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty - Seeking Agents

The paper describes a neural network - based approach to active localization based upon RGB images . The framework employs Bayesian filtering to maintain an estimate of the agent 's pose using a convolutional network model for the measurement ( perception ) function . A convolutional network models the policy that governs the action of the agent . The architecture is trained in an end - to - end manner via reinforcement learning . The architecture is evaluated in 2D and 3D simulated environments of varying complexity and compared favorably to traditional ( structured ) approaches to passive and active localization .

As the paper correctly points out , there is large body of work on map-based localization , but relatively little attention has been paid to decision theoretic formulations to localization , whereby the agent 's actions are chosen in order to improve localization accuracy . More recent work instead focuses on the higher level objective of navigation , whereby any effort act in an effort to improve localization are secondary to the navigation objective . The idea of incorporating learned representations with a structured Bayesian filtering approach is interesting , but it 's utility could be better motivated . What are the practical benefits to learning the measurement and policy model beyond ( i ) the temptation to apply neural networks to this problem and ( ii ) the ability to learn these in an end - to - end fashion ? That 's not to say that there are n't benefits , but rather that they are n't clearly demonstrated here . Further , the paper seems to assume ( as noted below ) that there is no measurement uncertainty and , with the exception of the 3D evaluations , no process noise .

The evaluation demonstrates that the proposed method yields estimates that are more accurate according to the proposed metric than the baseline methods , with a significant reduction in computational cost . However , the environments considered are rather small by today 's standards and the baseline methods almost 20 years old . Further , the evaluation makes a number of simplifying assumptions , the largest being that the measurements are not subject to noise ( the only noise that is present is in the motion for the 3D experiments ) . This assumption is clearly not valid in practice . Further , it is not clear from the evaluation whether the resulting distribution that is maintained is consistent ( e.g. , are the estimates over - / under-confident ? ) . This has important implications if the system were to actually be used on a physical system . Further , while the computational requirements at test time are significantly lower than the baselines , the time required for training is likely very large . While this is less of an issue in simulation , it is important for physical deployments . Ideally , the paper would demonstrate performance when transferring a policy trained in simulation to a physical environment ( e.g. , using diversification , which has proven effective at simulation - to- real transfer ) .

Comments / Questions :

* The nature of the observation space is not clear .

* Recent related work has focused on learning neural policies for navigation , and any localization - specific actions are secondary to the objective of reaching the goal . It would be interesting to discuss how one would balance the advantages of choosing actions that improve localization with those in the context of a higher - level task ( or at least including a cost on actions as with the baseline method of Fox et al . ) .

* The evaluation that assigns different textures to each wall is unrealistic .

* It is not clear why the space over which the belief is maintained flips as the robot turns and shifts as it moves .

* The 3D evaluation states that a 360 deg view is available . What happens when the agent can only see in one ( forward ) direction ?

* AML includes a cost term in the objective . Did the author ( s ) experiment with setting this cost to zero ?

* The 3D environments rely upon a particular belief size ( 70 x 70 ) being suitable for all environments . What would happen if the test environment was larger than those encountered in training ?

* The comment that the PoseNet and VidLoc methods " lack a strainghtforward method to utilize past map data to do localization in a new environment " is unclear .

* The environments that are considered are quite small compared to the domains currently considered for

* Minor : It might be better to move Section 3 into Section 4 after introducing notation ( to avoid redundancy ) .
* The paper should be proofread for grammatical errors ( e.g. , " bayesian " --> " Bayesian " , " gaussian " --> " Gaussian " )


UPDATES FOLLOWING AUTHORS ' RESPONSE

( Apologies if this is a duplicate . I added a comment in light of the authors ' response , but do n't see it and so I am updating my review for completeness ) .

I appreciate the authors 's response to the initial reviews and thank them for addressing several of my comments .

RE : Consistency
My concerns regarding consistency remain . For principled ways of evaluating the consistency of an estimator , see Bar -Shalom " Estimation with Applications to Tracking and Navigation " .

RE : Measurement / Process Noise
The fact that the method assumes perfect measurements and , with the exception of the 3D experiments , no process noise is concerning as neither assumptions are valid for physical systems . Indeed , it is this noise in particular that makes localization ( and its variants ) challenging .

RE : Motivation
The response did n't address my comments about the lack motivation for the proposed method . Is it largely the temptation of applying an end - to - end neural method to a new problem ? The paper should be updated to make the advantages over traditional approaches to active localization .

We thank the reviewer for their valuable comments and feedback .

> What are the practical benefits to learning the measurement and policy model ?
The example at the end of the paper ( see Figure 4 ) highlights the importance of deciding actions for fast and accurate localization . We agree that the benefits can be better motivated in the introduction and we are looking into restructuring the paper to have a motivating example in the introduction .

> it is not clear from the evaluation whether the resulting distribution that is maintained is consistent :
Looking at the output of the model manually , it seems that the estimates are consistent . It is very difficult to quantify the consistency of the resulting distribution because there is no straightforward way to calculate the ground - truth distribution / posterior in 3D environments .

> while the computational requirements at test time are significantly lower than the baselines , the time required for training is likely very large :
We designed the Active Markov Localization ( Slow ) baseline keeping this in mind . The proposed model was trained for 24hrs for all experiments . AML ( Slow ) represents the Generalized AML algorithm using the values of hyperparameters which maximize the performance while keeping the runtime for 1000 episodes below 24hrs in each environment . This means the runtime of AML ( Slow ) is comparable to the training time of the proposed model . However , we agree that this point should be stated explicitly and we have made relevant changes in the paper .

> The nature of the observation space is not clear .
The observation space in 2D environments is just the depth of the one column in front of the agent and in 3D environments , it is the 108x60 RGB image showing the first - person view of the agent .

> It is not clear why the space over which the belief is maintained flips as the robot turns and shifts as it moves .
This happens due to the transition function as each channel represents a quantized orientation ( North / East / West / South ) . The details of the transition function are provided in the appendix . For example , if the agent turns left , the probability of it facing north at any x-y coordinate becomes the probability of it facing west at the same - coordinate . This is why the belief flips when turning left . Similarly , the belief flips in the opposite direction when turning right and shifts when moving forward .

> The 3D evaluation states that a 360 deg view is available . What happens when the agent can only see in one ( forward ) direction ?
This seems to be a misunderstanding . The agent only sees in one forward direction , it needs to take actions to turn around to get the view in other directions . This misunderstanding might be due to the likelihood and belief presented in 4 directions . Note that each of these 4 channels represents the likelihood / belief of the agent ’s orientation being that direction , not the likelihood / belief of the view in that direction .

> AML includes a cost term in the objective . Did the author ( s ) experiment with setting this cost to zero ?
In our environment , all actions have the same cost . This is equivalent to setting the cost to zero ( i.e. it does not affect the optimal policy in the environment ) , but we found it helps the optimization of our model .

> What would happen if the test environment was larger than those encountered in training ?
We will need to discretize the test environment such that its belief is at most the size of the training environment , i.e. 70x70 . The discretization of the training environments can be changed according to the desired level of accuracy in the test environment . For example , if we discretize 35 m x 35 m environment to a grid of 35x35 , each cell would be a length of 1m . Due to this discretization , the model can make errors up to 0.5 m even if it predicts the correct cell . The discretization can be increased to 70x70 to reduce errors to 0.25 m .

> The comment that the PoseNet and VidLoc methods " lack a straightforward method to utilize past map data to do localization in a new environment " is unclear .
The network weights in these models memorize the environment . The model has no way to ingest information about the map as input , thus the model trained in one map cannot be transferred to another map . These models need to be retrained on any new map .


I have evaluated this paper for NIPS 2017 and gave it an " accept " rating at the time , but the paper was ultimately not accepted . This resubmission has been massively improved and definitely deserves to be published at ICLR .

This paper formulates the problem localisation on a known map using a belief network as an RL problem . The goal of the agent is to minimise the number of steps to localise itself ( the agent needs to move around to accumulate evidence about its position ) , which corresponds to reducing the entropy of the joint distribution over a discretized grid over theta ( 4 orientations ) , x and y . The model is evaluated on a grid world , on textured 3D mazes with simplified motion ( Doom environment ) and on a photorealistic environment using the Unreal engine . Optimisation is done through A3C RL . Transfer from the crude simulated Doom environment to the photorealistic Unreal environment is achieved .

The belief network consists of an observation model , a motion prediction model that allows for translations along x or y and 90deg rotation , and an observation correction model that either perceives the depth in front of the agent ( a bold and ambiguous choice ) and matches it to the 2D map , or perceives the image in front of the agent . The map is part of the observation .

The algorithm outperforms Bayes filters for localisation in 2D and 3D and the idea of applying RL to minimise the entropy of position estimation is brilliant . Minor note : I am surprised that the cognitive map reference ( Gupta et al , 2017 ) was dropped , as it seemed relevant .

We thank the reviewer for their valuable comments and feedback .

> Minor note : I am surprised that the cognitive map reference ( Gupta et al , 2017 ) was dropped , as it seemed relevant .
We agree that this reference is relevant , we have added the reference to the revision .


This is an interesting paper that builds a parameterized network to select actions for a robot in a simulated environment , with the objective of quickly reaching an internal belief state that is predictive of the true state . This is an interesting idea and it works much better than I would have expected .

In more careful examination it is clear that the authors have done a good job of designing a network that is partly pre-specified and partly free , in a way that makes the learning effective . In particular
- the transition model is known and fixed ( in the way it is used in the belief update process )
- the belief state representation is known and fixed ( in the way it is used to decide whether the agent should be rewarded )
- the reward function is known and fixed ( as above )
- the mechanics of belief update
But we learn
- the observation model
- the control policy

I 'm not sure that global localization is still an open problem with known models . Or , at least , it 's not one of our worst .

Early work by Cassandra , Kurien , et al used POMDP models and solvers for active localization with known transition and observation models . It was computationally slow but effective .

Similarly , although the online speed of your learned method is much better than for active Markov localization , the offline training cost is dramatically higher ; it 's important to remember to be clear on this point .

It is not obvious to me that it is sensible to take the cosine similarity between the feature representation of the observation and the feature representation of the state to get the entry in the likelihood map . It would be good to make it clear this is the right measure .

How is exploration done during the RL phase ? These domains are still not huge .

Please explain in more detail what the memory images are doing .

In general , the experiments seem to be well designed and well carried out , with several interesting extensions .

I have one more major concern : it is not the job of a localizer to arrive at a belief state with high probability mass on the true state - -- it is the job of a localizer to have an accurate approximation of the true posterior under the prior and observations . There are situations ( in which , for example , the robot has gotten an unusual string of observations ) in which it is correct for the robot to have more probability mass on a " wrong " state . Or , it seems that this model may earn rewards for learning to make its beliefs overconfident . It would be very interesting to see if you could find an objective that would actually cause the model to learn to compute the appropriate posterior .

In the end , I have trouble making a recommendation :
Con : I 'm not convinced that an end - to - end approach to this problem is the best one
Pro : It 's actually a nice idea that seems to have worked out well
Con : I remain concerned that the objective is not the right one

My rating would really be something like 6.5 if that were possible .






We thank the reviewer for their valuable comments and feedback .

Concerns regarding the objective function :
This is a very interesting point and we thank the reviewer for this observation . We agree that one of the tasks of the localizer is to accurately approximate the true posterior under the prior and observations . But another task is to learn to take actions which lead it to arrive at a belief state with high probability mass on the true location . We provide rewards only for the correct prediction of the location and not for the correct prediction of the posterior because of two primary reasons :
- Defining an appropriate reward function for the true posterior would require some way of estimating the true posterior , which is very difficult especially in 3D environments .
- We want the model to be penalized if it fails to take actions in order to reach a state where it can predict its correct location , even if its estimation of the posterior under the prior and the observations is accurate .

The second point can potentially be mitigated by having an auxiliary loss on the belief , which back - propagates only through the perceptual model . This will only reward the perceptual model for predicting the true posterior , and the policy loss would still penalize the whole model for taking unfavorable actions . However , this will still require defining a reward or a loss function for the true posterior , which is difficult as there is generally no straightforward way of computing the ground - truth posterior in 3D environments with unknown models .

> although the online speed of your learned method is much better than for active Markov localization , the offline training cost is dramatically higher :
We designed the Active Markov Localization ( Slow ) baseline keeping this in mind . The proposed model was trained for 24hrs for all experiments . AML ( Slow ) represents the Generalized AML algorithm using the values of hyperparameters which maximize the performance while keeping the runtime for 1000 episodes below 24hrs in each environment . This means that the runtime of AML ( Slow ) is comparable to the training time of the proposed model . However , we agree that this point should be stated explicitly and we have made relevant changes in the paper .

> How is exploration done during the RL phase ?.
Exploration is done implicitly using the stochastic policy in Asynchronous Advantage Actor -Critic method . As in the original A3C paper , to encourage exploration , we used an entropy loss scale of 0.01 .

> Please explain in more detail what the memory images are doing .
Memory images are a part of the map information given to the agent in 3D Environments . They are used to calculate the likelihood given the current observation of the agent as follows : The perceptual model is used to get the feature representation of all the memory images and the current agent observation . The likelihood of each state in the set of memory images is calculated by taking the cosine similarity of the feature representation of the agent ’s observation with the feature representation of the memory image .

> It is not obvious to me that it is sensible to take the cosine similarity between the feature representation of the observation and the feature representation of the state to get the entry in the likelihood map :
The basic assumption here is that images containing the same “ landmark ” ( unique texture or object ) would have similar representations ( e.g. high inner product value ) . Taking cosine similarity is similar to the standard attention operation commonly used in Deep Learning , which is exponentiated inner product . The cosine similarity , in contrast , is scaled to remain within a range of values , which can help training stability and prevent the likelihood model from becoming too sharp .


Summary :

The authors propose a method to make exploration in really sparse reward tasks more efficient . They propose a method called Workflow Guided Exploration ( WGE ) which is learnt from demonstrations but is environment agnostic . Episodes are generated by first turning demonstrations to a workflow lattice . This lattice encodes actions which are in some sense similar to those in the demonstration . By rolling out episodes which are randomly sampled from this set of similar actions for each encountered state , it is claimed that other methods like Behavor Cloning + RL ( BC - then - RL ) can be outperformed in terms of number of sample complexity since high reward episodes can be sampled with much higher probability .

A novel NN architecture ( DOMNet ) is also presented which can embed structured documents like HTML webpages .

Comments :

- The paper is well - written and relevant literature is cited and discussed .
- My main concern is that while imitation learning and inverse reinforcement learning are mentioned and discussed in related work section as classes of algorithms for incorporating prior information there is no baseline experiment using either of these methods . Note that the work of Ross and Bagnell , 2010 , 2011 ( cited in the paper ) establish theoretically that Behavior Cloning does not work in such situations due to the non-iid data generation process in such sequential decision - making settings ( the mistakes grow quadratically in the length of the horizon ) . Their proposed algorithm DAgger fixes this ( the mistakes by the policy are linear in the horizon length ) by using an iterative procedure where the learnt policy from the previous iteration is executed and expert demonstrations on the visited states are recorded , the new data thus generated is added to the previous data and a new policy retrained . Dagger and related methods like Aggrevate provide sample - efficient ways of exploring the environment near where the initial demonstrations were given . WGE is aiming to do the same : explore near demonstration states .
- The problem with putting in the replay buffer only episodes which yield high reward is that extrapolation will inevitably lead the learnt policy towards parts of the state space where there is actually low reward but since no support is present the policy makes such mistakes .
- Therefore would be good to have Dagger or a similar imitation learning algorithm be used as a baseline in the experiments .
- Similar concerns with IRL methods not being used as baselines .

Update : Review score updated after discussion with authors below .


We would like to thank the reviewer for the feedback !

The reviewer suggested further comparisons with inverse reinforcement learning ( IRL ) and DAgger -based methods ( e.g. , DAgger , AggreVaTe ) . In our paper revision , we will address the critical differences between our setting and the settings of these methods , which are summarized below :

- In IRL , the system does not receive rewards from the environment and instead extracts a reward function from demonstrations . In our setting , the system already observes the true reward from the environment , so applying IRL would be redundant . Furthermore , IRL would struggle to learn a good reward function from such a small number of demonstrations ( e.g. , 3 - 10 ) , which we have in our setting .

- DAgger - based methods require access to an expert policy , which is iteratively queried to augment the training data . In our setting , the system gets a small number of demonstrations and can interact with the environment , but does not have access to an expert policy , so these methods can not be directly applied . In addition , while DAgger - based methods do indeed provide an alternative way to explore around a neighborhood of the demonstrations , their goal is different from ours : DAgger addresses compounding errors , while our work addresses finding sparse reward .

Finally , we want to clarify the concern that only high reward episodes are placed in the buffer . The neural policy updates both off - policy from the buffer and on-policy during roll - outs . If the neural policy begins to make mistakes , it will be penalized by receiving low reward during the on-policy rollouts , which will correct these mistakes .

SUMMARY

The paper deals with the problem of training RL algorithms from demonstration and applying them to various web interfaces such as booking flights . Specifically , it is applied to the Mini world of Bids benchmark ( http://alpha.openai.com/miniwob/).

The difference from existing work is that rather than training an agent to directly mimic the demonstrations , it uses demonstrations to constrain exploration . By pruning away bad exploration directions .

The idea is to build a lattice of workflows from demonstration and randomly sample sequence of actions from this lattice that satisfy the current goal . Use the sequences of actions to sample trajectories and use the trajectories to learn the RL policy .



COMMENTS


In effect , the workflow sequences provide more generalization than simply mimicking , but It not obvious , why they do n’t run into overfitting problems . However experimentally the paper performs better than the previous approach .

There is a big literature on learning from demonstrations that the authors could compare with , or explain why their work is different .

In addition , they make general comparison to RL literature such as hierarchy rather than more concrete comparisons with the problem at hand ( learning from demonstrations . )

What does DOM stand for ? The paper is not self - contained . For example , what does DOM stand for ?


In the results of table 1 and Figure 3 . Why more steps mean success ?

In equation 4 there seems to exist an environment model . Why do we need to use this whole approach in the paper then ? Could n’t we just do policy iteration ?


We would like to thank the reviewer for their detailed and thoughtful feedback .

In our revision , we will significantly expand our comparison to related work on learning from demonstrations . The summary is provided below :

Previous work on learning from demonstration fall into two broad categories :
1 ) Using demonstrations to directly update policy parameters ( e.g. , behavioral cloning , IRL , etc . ) .
2 ) Using demonstrations to guide or constrain exploration .

Our method belongs to category ( 2 ) . The core idea is to explore trajectories that lie in a " neighborhood " surrounding an expert demonstration . In our work , the neighborhood is defined by a workflow , which only permits action sequences analogous to the demonstrated actions .

Other methods in category ( 2 ) also explore the neighborhood surrounding a demonstration , using shaping rewards ( Brys et al. 2015 , Hussein et al. 2017 ) or off -policy sampling ( Levine & Koltun , 2013 ) . A key difference is that we define our neighborhood in terms of action - similarity , rather than state-similarity . This distinction is particularly important for the web tasks we study : we can easily and intuitively describe how two actions are analogous ( e.g. , " they both type a username into a textbox " ) , while it is harder to decide if two web page states are analogous ( e.g. , the email inboxes of two different users will have completely different emails , but they could still be analogous , depending on the task . )

Regarding overfitting : our workflow policy does not overfit to demonstrations because the demonstrations are merely used to induce a workflow lattice -- the actual parameters of the workflow policy are learned through trial - and - error reinforcement learning , for which there is infinite data .

The workflow policy maintains a distribution over possible workflows . Some workflows define a very small neighborhood of trajectories surrounding an expert demonstration ( tight ) , while others impose almost no constraints ( loose ) . As the workflow policy is trained , it converges to the tightest workflow that can successfully generalize .

Our final neural policy also does not overfit , because it is trained on a replay buffer of successful episodes discovered by the workflow policy , which is much larger than the original set of demonstrations .

We would like to also quickly address the other questions , which we will be sure to clarify in the paper :
- In Equation 4 , we do not have access to the environment model p( s_t | s_{t - 1 } , a_{t - 1 } ) . We merely state that our
sampling procedure produces episodes e following the distribution p( e | g ) of Equation 4 , which we do not
compute .
- DOM stands for Document Object Model , the standard tree - based representation of a web page .
- The " Steps " column in Table 1 is the number of steps needed to complete the task under the optimal behavior
( e.g. , by a human expert ) . It is a rough measure of task difficulty and is not related to model performance .


This paper introduces a new exploration policy for Reinforcement Learning for agents on the web called " Workflow Guided Exploration " . Workflows are defined through a DSL unique to the domain .

The paper is clear , very well written , and well - motivated . Exploration is still a challenging problem for RL . The workflows remind me of options though in this paper they appear to be hand -crafted . In that sense , I wonder if this has been done before in another domain . The results suggest that WGE sometimes helps but not consistently . While the experiments show that DOMNET improves over Shi et al , that could be explained as not having to train on raw pixels or not enough episodes .

We would like to thank the reviewer for their helpful feedback !

The key distinction between our work and most existing hierarchical RL approaches ( e.g. , options , MAXQ ) is that our hierarchical structures ( workflows ) are inferred from demonstrations , rather than manually crafted or learned from scratch .

We try to keep the constraint language for describing workflow steps as minimal and general as possible . The main part of the language is just an element selector ( element Set ) which selects either ( 1 ) things that share a specified property , or ( 2 ) things that align spatially , both of which are applicable in many typical RL domains ( game playing , robot navigation , etc . )

In our experiments ( Figure 3 ) , WGE ( red ) consistently performs equally or better than behavioral cloning ( green ) . There are some easy tasks in the benchmark ( e.g. , click -button ) , where both WGE and the baselines have perfect performance . But in more difficult tasks ( Table 1 ) , WGE greatly improves over baselines by an average of 42 % absolute success rate .

Regarding the comparison with Shi17 :
- Shi17 used ~ 200 demonstrations per task , whereas we achieve superior performance with only 3 - 10 .
- In addition to pixel - level data , the model of Shi17 actually also uses the DOM tree to compute text alignment
features . Our DOM Net uses the DOM structure more explicitly , which indeed produces better performance .
- Our DOMNet+BC +RL baseline separates the contribution of DOMNet from Workflow - Guided Exploration . Table 1
and Figure 3 illustrate that both are important .


This paper proposes the use of optimistic mirror descent to train Wasserstein Generative Adversarial Networks ( WGANS ) . The authors remark that the current training of GANs , which amounts to solving a zero - sum game between a generator and discriminator , is often unstable , and they argue that one source of instability is due to limit cycles , which can occur for FTRL - based algorithms even in convex - concave zero - sum games . Motivated by recent results that use Optimistic Mirror Descent ( OMD ) to achieve faster convergence rates ( than standard gradient descent ) in convex - concave zero - sum games and normal form games , they suggest using these techniques for WGAN training as well . The authors prove that , using OMD , the last iterate converges to an equilibrium and use this as motivation that OMD methods should be more stable for WGAN training . They then compare OMD against GD on both toy simulations and a DNA sequence task before finally introducing an adaptive generalization of OMD , Optimistic Adam , that they test on CIFAR10 .

This paper is relatively well - written and clear , and the authors do a good job of introducing the problem of GAN training instability as well as the OMD algorithm , in particular highlighting its differences with standard gradient descent as well as discussing existing work that has applied it to zero - sum games . Given the recent work on OMD for zero - sum and normal form games , it is natural to study its effectiveness in training GANs . The issue of last iterate versus average iterate for non convex - concave problems is also presented well .

The theoretical result on last - iterate convergence of OMD for bilinear games is interesting , but somewhat wanting as it does not provide an explicit convergence rate as in Rakhlin and Sridharan , 2013 . Moreover , the result is only at best a motivation for using OMD in WGAN training since the WGAN optimization problem is not a bilinear game .

The experimental results seem to indicate that OMD is at least roughly competitive with GD - based methods , although they seem less compelling than the prior discussion in the paper would suggest . In particular , they are matched by SGD with momentum when evaluated by last epoch performance ( albeit while being less sensitive to learning rates ) . OMD does seem to outperform SGD - based methods when using the lowest discriminator loss , but there does n't seem to be even an attempt at explaining this in the paper .

I found it a bit odd that Adam was not used as a point of comparison in Section 5 , that optimistic Adam was only introduced and tested for CIFAR but not for the DNA sequence problem , and that the discriminator was trained for 5 iterations in Section 5 but only once in Section 6 , despite the fact that the reasoning provided in Section 6 seems like it would have also applied for Section 5 . This gives the impression that the experimental results might have been at least slightly " gamed " .

For the reasons above , I give the paper high marks on clarity , and slightly above average marks on originality , significance , and quality .

Specific comments :
Page 1 , " no - regret dynamics in zero - sum games can very often lead to limit cycles " : I do n't think limit cycles are actually ever formally defined in the entire paper .
Page 3 , " standard results in game theory and no-regret learning " : These results should be either proven or cited .
Page 3 : Do n't the parameter spaces need to be bounded for these convergence results to hold ?
Page 4 , " it is well known that GD is equivalent to the Follow - the- Regularized - Leader algorithm " : For completeness , this should probably either be ( quickly ) proven or a reference should be provided .
Page 5 , " the unique equilibrium of the above game is ... for the discriminator to choose w=0 " : Why is w=0 necessary here ?
Page 6 , " We remark that the set of equilibrium solutions of this minimax problem are pairs ( x , y ) such that x is in the null space of A^T and y is in the null space of A " : Why is this true ? This should either be proven or cited .
Page 6 , Initialization and Theorem 1 : It would be good to discuss the necessity of this particular choice of initialization for the theoretical result . In the Initialization section , it appears simply to be out of convenience .
Page 6 , Theorem 1 : It should be explicitly stated that this result does n't provide a convergence rate , in contrast to the existing OMD results cited in the paper .
Page 7 , " we considered momentum , Nesterov momentum and AdaGrad " : Why is n't Adam used in this section if it is used in later experiments ?
Page 7 - 8 , " When evaluated by .... the lowest discriminator loss on the validation set , WGAN trained with Stochastic OMD ( SOMD ) achieved significantly lower KL divergence than the competing SGD variants . " : Can you explain why SOMD outperforms the other methods when using the lowest discriminator loss on the validation set ? None of the theoretical arguments presented earlier in the paper seem to even hint at this . The only result that one might expect from the earlier discussion and results is that SOMD would outperform the other methods when evaluating by the last epoch . However , this does n't even really hold , since there exist learning rates in which SGD with momentum matches the performance of SOMD .
Page 8 , " Evaluated by the last epoch , SOMD is much less sensitive to the choice of learning rate than the SGD variants " : Learning rate sensitivity does n't seem to be touched upon in the earlier discussion . Can these results be explained by theory ?
Page 8 , " we see that optimistic Adam achieves high numbers of inception scores after very few epochs of training " : These results do n't mean much without error bars .
Page 8 , " we only trained the discriminator once after one iteration of generator training . The latter is inline with the intuition behind the use of optimism .... " : Why did n't this logic apply to the previous section on DNA sequences , where the discriminator was trained multiple times ?


After reading the response of the authors ( in particular their clarification of some technical results and the extra experiments they carried out during the rebuttal period ) , I have decided to upgrade my rating of the paper from a 6 to a 7 . Just as a note , Figure 3 b is now very difficult to read .



We would like to thank you for your comments and suggestions and we explain below how we have addressed your concerns / questions in the updated revision of the paper .

1 ) We have replaced limit cycles in the intro with " limit oscillatory behavior " . We hope that this term is self explanatory , as we want to avoid further notation for formally defining a limit cycle .
2 ) We have added references for the average converging to equilibrium result ( Freund -Ssapire 1999 )
3 ) Indeed regret rates , as is typically , always require some boundedness of the optimization space . We have added a sentence on page 3 ( " theta and w lie in some bounded convex space " ) to address this comment .
4 ) We have added a reference on equivalence between GD and FTRL
5 ) Since we are arguing about an equilibrium , it has to be that theta=v is a best response to w . If w > 0 , then the best response for theta is minus infinity . If w < 0 , then the best response for theta is infinity . If w=0 , then any value for theta is a best response . Similarly , at an equilibrium , w also needs to be a best response to theta . If theta >v , then w=infinity is a best response and if theta < v then w=-infinity is a best response . None of the above can be a simultaneous best-response , and the only unique equilibrium is theta=v and w=0 .
6 ) We will add a quick sentence about this fact , which follows along the exact same lines as the example above : if y is not in the null space of A , then Ay has some non- zero coordinates . Then the best response for x is to set minus infinity on the positive coordinates of Ay and infinity on the negative coordinates . This will lead to a value of minus infinity , which can be avoided by the y player by choosing a y that lives in the null space of A , leading to a value of zero . Hence , at any equilibrium y is such that Ay = \vec{0 } ( i.e. the null space of A ) . Similarly , we can argue for x having to lie in the null space of A^T .
7 ) We have added a convergence rate for Theorem 1 as a function of eta . This result does show that the rate at which Delta_t and consequently the convergence of the solutions goes to the limit value . In particular , this convergence to the limit value of eta gamma^2 Delta_0 , happens at an exponential rate of approximately exp{ - eta^2 * t / gamma^2 } , while this limit value depends linearly with eta . For the regret rates mentioned in section 2 typical values of eta are of the order of 1 / T^{1 / 4} ( see e.g. Syrgkanis et al. 2015 ) . Hence , if one wants both regret rates and convergence to equilibrium , these are reasonable values of eta .
8 ) We have added experiments of Adam and optimistic adam in this section too . We thought that Adam was a method particularly useful for image tasks and hence wanted to compare with simpler and more classical algorithms in this section . However , we do admit that we should have also compared with Adam in this section too and we augmented our experiments to include Adam . Indeed adam and optimistic adam performs better in this task too and not only in the image task . Still optimistic adam outperforms adam in this task too .
9 ) Indeed the theoretical results do not imply that an out - of - sample early stopping would work better under optimism than under other methods . However , we wanted to test performance of OMD with an early stopping criterion , since typically , such criteria are used . We indeed are not explicitly doing an early stopping but rather using out - of - sample performance to choose the best iteration . We found this approach to be interesting in practice and grounded in the observations made in Arjovsky et al ( as we note in the text before the figure ) and we also found that since SOMD was also better performing than other methods other than Adam an interesting finding . Also in terms of last epoch , our updated results show that only Adam has comparable performance with the best performance of optimistic adam or OMD ( i.e. with the best learning rates ) . Also for most learning rates , momentum and nesterov momentum have statistically significant lower performance ( indeed comparable , but strictly worse ) .
[ Continued in the following comment due to character limit ]

10 ) As we explain in the response to reviewer 1 , unfortunately this stability of performance wrt to learning rate was only an artifact of a mistake in our implementation and we have removed this comment from the paper .
11 ) We reran the experiment across 35 runs for 30 epochs ( due to compute restrictions ) for the two top-performing methods ( optimAdam-ratio1 and Adam ) , and plot the results with 10 - 90 error bars in the Appendix of the paper , demonstrating that optim Adam indeed reliably performs better in terms of inception score . Once we can run the experiment 100 times for all 100 epochs , we plan to replace our main - text figure with this style of plot .
12 ) It should indeed apply . We have included such results too . We thought of first comparing with existing proposals and hyperparameter settings in the literature to see the effect of simply adding optimism . However , we agree that we should have included this alternative 1:1 training in this experimental section too . The results in that section are inline again with this intuition and the ratio1 algorithms perform better with their corresponding 5:1 counterparts . ( see figure 3 b )

The paper proposes to use optimistic gradient descent ( OGD ) for GAN training . Optimistic mirror descent is know to yield fast convergence for finding the optimum of zero - sum convex - concave games ( when the players collaborate for fast computation ) , but earlier results concern the performance of the average iterate . This paper extends this result by showing that the last iterate of OGD also provides a good estimate of the value of bilinear games . Based on this new theoretical result ( which is not unexpected but is certainly nice ) , the authors propose to use stochastic OGD in GAN training . Their experiments show that this new approach avoids the cycling behavior observed with SGD and its variants , and provides promising results in GAN training . ( Extensive experiments show the cycling behavior of SGD variants in very simple problems , and some theoretical result is also provided when SGD diverges in solving a simple min-max game ) .

The paper is clearly written and easy to follow ; in fact I quite enjoyed reading it . I have not checked all the details of the proofs , but they seem plausible .
All in all , this is a very nice paper .

Some questions / comments :
- Proposition 1 : Could you show a similar example when you can prove the oscillating behavior ?
- Theorem 1 : It would be interesting to write out the convergence rate of Delta_t , which could be used to optimize eta . Also , my understanding is that you actually avoid computing gamma , hence tuning eta is not straightforward . Alternatively , you could also use an adaptive OGD to automatically tune eta ( see , e.g. , Joulani et al , " A modular analysis of adaptive ( non - ) convex optimization : optimism , composite objectives , and variational Bounds , " ALT 2017 ) . The non- adaptive selection of eta might be the reason that your method does not outperform adagrad SGD in 5 ( b ) , although it is true that the behavior of your method seems quite stable for different learning rates ) .
- LHS of the second line of ( 6 ) should be theta .
- Below ( 6 ) : \ mathcal{R} ( A ) is only defined in the appendix .

We would like to thank you for your comments and suggestions and we explain below how we have addressed your concerns / questions in the updated revision of the paper .

1 ) In these examples that we give that lead to divergence , it is easy to see that if one takes the step size to zero , then you get a limit cycle ( i.e. continuously oscillating behavior ) . For any other non- zero step -size the behavior is still oscillatory but diverging ( i.e. the radius of the cycle is constantly increasing ) . In some sense , the finite step size , makes the dynamics jump from one limit cycle of the continuous limit dynamics ( stepsize=0 ) , to another .

2 ) We have added an explicit form of the convergence of Delta_t as a function of eta and gamma in the theorem . Analyzing the limit dynamics with a non-constant stepsize and extending theorem 1 seems feasible , but would complicate even more the inductive proof with extra notation . Hence , we defer such an extension to the full version . It is true that the condition depends on gamma , albeit only an upper bound on gamma is required . If any such upper bound on gamma is known then an appropriate step size can be chosen . Also in terms of an adaptive step size , we believe that our optimistic Adam algorithm is exactly a way of setting and adaptive step size that adapts to the variance of the problem , hence the reason why it out - performs the fixed step size optimistic mirror descent in both experimental sections . So you are right that adaptive step sizes can lead to improved practical performance even in the presence of optimism . Also we note here that in fact there was a small mistake in our implementation of OMD in the DNA experiment which lead to the stability of the performance of OMD across learning rates . We have fixed this mistake and the stability of the performance across learning rates was only an artifact . Still optimism performs better than most methods and optimistic adam leads to the bet last iterate loss , while OMD leads to best early stopping loss .

This paper proposes a simple modification of standard gradient descent -- called “ Optimistic Mirror Descent ” -- which is claimed to improve the convergence of GANs and other minimax optimization problems . It includes experiments in toy settings which build intuition for the proposed algorithm , as well as in a practical GAN setting demonstrating the potential real - world benefits of the method .


Pros

Section 3 directly compares the learning dynamics of GD vs . OMD for a WGAN in a simple toy setting , showing that the default GD algorithm oscillates around the optimum in the limit while OMD ’s converges to the optimum .

Section 4 demonstrates the convergence of OMD for a linear minimax optimization problem . ( I did not thoroughly verify the proof ’s correctness . )

Section 6 proposes an OMD - like modification of Adam which achieves better results than standard Adam in a practical GAN setting ( WGANs trained on CIFAR10 ) .


Cons/Suggestions

The paper could use a good deal of proofreading / revision for clarity and correctness . A couple examples from section 2 :
- “ If the discriminator is very powerful and learns to accurately classify all samples , then the problem of the generator amounts to solving the Jensen - Shannon divergence between the true distribution and the generators distribution . ” -> It would be clearer to say “ minimizing ” ( rather than “ solving ” ) the JS divergence . ( “ Solving ” sounds more like what the discriminator does . )
- “ Wasserstein GANs ( WGANs ) Arjovsky et al. ( 2017 ) , where the discriminator rather than being treated as a classifier is instead trying to simulate the Wasserstein− 1 or earth - mover metric ” -> Instead of “ simulate ” , “ estimate ” or “ approximate ” would be better word choices . And although the standard GAN discriminator is a binary classifier , when optimized to convergence , it ’s also estimating a divergence -- the JS divergence ( or a shifted and scaled version of it ) . Even though the previous paragraph mentions this , it feels a bit misleading to characterize WGANs as doing something fundamentally different .

Sec 2.1 : There are several non-trivial but uncited mathematical claims hidden behind “ well - known ” or similar descriptors . These results could indeed be well - known in certain circles , but I ’m not familiar with them , and I suspect most readers wo n’t be either . Please add citations . A few examples :
- “ If the loss function L ( θ , w ) ... , then standard results in game theory and no-regret learning imply that … ”
- “ In particular , it is well known that GD is equivalent to the Follow - the-Regularized - Leader algorithm with an L2 regularizer ... ”
- “ It is known that if the learner knew in advance the gradient at the next iteration ... ”

Section 4 : vectors “ b ” and “ c ” are included in the objective written in ( 14 ) , but are later dropped without explanation . ( The constant “ d ” is also dropped but clearly has no effect on the optimization . )


Overall , the paper could use revision but the proposed approach is simple and seems to be theoretically well - motivated with solid analysis and benefits demonstrated in real - world settings .

We would like to thank you for your comments and suggestions and we explain below how we have addressed your concerns / questions in the updated revision of the paper .
1 ) We changed " solving " to " minimizing "
2 ) We changed " simulate " to " approximate " and also added a small comment to stress that traditional GANs are also a form of metric between two distribution
3 ) We have added a reference for the averages of both players converging to an equilibrium in zero - sum games , in particular Freund -Shapire1999
4 ) We have added the Shalev -Swartz survey on online learning and online convex optimization for the claim that gradient descent is equivalent to FTRL with l_2 regularizer
5 ) We have added the follow - the-perturbed - leader paper of Kalai-Vempala and the lecture notes of Philippe Rigollet for the claim that by knowing the gradient in the next iteration you get constant regret ( a consequence of the be-the- leader lemma in these references )
6 ) For vectors b , c , d as we state in the first paragraph of the section we work in the main body only with the simpler game x^ TAy and we point that in Appendix D the analysis easily extends to the more complex games with the b , c and d vectors , hence we omitted these vectors in the theorem presented in the main paper . Indeed d is irrelevant for the optimization of both players .

The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams . This allows the authors to design a matrix that maps bag-of - n-gram embeddings into the LSTM embeddings . They then show that the result matrix satisfies a restricted isometry condition . Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of - n- gram embeddings .

I did n't check all the proof details , but based on my knowledge of compressed sensing theory , the results seem plausible . I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings .

Thank you for the positive review ! We are currently preparing a revision incorporating these comments . We would also like to clarify that our paper concerns LSTM document embeddings , not word embeddings .

The interesting paper provides theoretical support for the low- dimensional vector embeddings computed using LSTMs or simple techniques , using tools from compressed sensing . The paper also provides numerical results to support their theoretical findings . The paper is well presented and organized .

- In theorem 4.1 , the embedding dimension $ d$ is depending on $ T^2 $ , and it may scale poorly with respect to $ T$ .

Thank you for the positive review ! We are currently preparing a revision incorporating these comments .

Comment : “ the embedding dimension $ d$ is depending on $ T^2 $ , and it may scale poorly with respect to $ T$ . ”
Yes the bound may scale poorly with document length . At the moment many tasks in this area use short sentences ( e.g. SST has avg. length < 20 ) , and Fig. 4 indicates convergence of DisC to BonC performance even on the IMDB task ( avg. length > 250 ) so perhaps our bound is too pessimistic . Note that in the unigram ( BoW ) case the scaling is ( provably ) linear in T because then the design matrix is an i.i.d . Rademacher ensemble .

My review reflects more from the compressive sensing perspective , instead that of deep learners .

In general , I find many of the observations in this paper interesting . However , this paper is not strong enough as a theory paper ; rather , the value lies perhaps in its fresh perspective .

The paper studies text embeddings through the lens of compressive sensing theory . The authors proved that , for the proposed embedding scheme , certain LSTMs with random initialization are at least as good as the linear classifiers ; the theorem is almost a direction application of the RIP of random Rademacher matrices . Several simplifying assumptions are introduced , which rendered the implication of the main theorem vague , but it can serve as a good start for the hardcore statistical learning - theoretical analysis to follow .

The second contribution of the paper is the ( empirical ) observation that , in terms of sparse recovery of embedded words , the pretrained embeddings are better than random matrices , the latter being the main focus of compressive sensing theory . Partial explanations are provided , again using results in compressive sensing theory . In my personal opinion , the explanations are opaque and unsatisfactory . An alternative route is suggested in my detailed review .
Finally , extensive experiments are conducted and they are in accordance with the theory .

My most criticism regarding this paper is the narrow scope on compressive sensing , and this really undermines the potential contribution in Section 5 .

Specifically , the authors considered only Basis Pursuit estimators for sparse recovery , and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not . This seems to be somewhat of a tunnel - visioning for me : There are a variety of estimators in sparse recovery problems , and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery .

In particular , in Section 5 , instead of invoking [ Donoho&Tanner 2005 ] , I believe that a more plausible approach is through [ Chandrasekaran et al. 2012 ] . There , a simple deterministic condition ( the null space property ) for successful recovery is proved . It would be of direct interest to check whether such condition holds for a pretrained embedding ( say GloVe ) given some BoWs . Furthermore , it is proved in the same paper that Restricted Strong Convexity ( RSC ) alone is enough to guarantee successful recovery ; RIP is not required at all . While , as the authors argued in Section 5.2 , it is easy to see that pretrained embeddings can never possess RIP , they do not rule out the possibility of RSC .

Exactly the same comments above apply to many other common estimators ( lasso , Dantzig selector , etc. ) in compressive sensing which might be more tolerant to noise .

Several minor comments :

1 . Please avoid the use of “ information theory ” , especially “ classical information theory ” , in the current context . These words should be reserved to studies of Channel Capacity / Source Coding `a la Shannon . I understand that in recent years people are expanding the realm of information theory , but as compressive sensing is a fascinating field that deserves its own name , there ’s no need to mention information theory here .

2 . In Theorem 4.1 , please be specific about how the l2 - regularization is chosen .

3 . In Section 4.1 , please briefly describe why you need to extend previous analysis to the Lipschitz case . I understood the necessity only through reading proofs .

4 . Can the authors briefly comment on the two assumptions in Section 4 , especially the second one ( on n- cooccurrence ) ? Is this practical ?

5 . Page 1 , there is a typo in the sentence preceding [ Radfors et al. , 2017 ] .

6 . Page 2 , first paragraph of related work , the sentence “ Our method also closely related to ... ” is incomplete .

7 . Page 2 , second paragraph of related work , “ Pagliardini also introduce D a linear ... ”

8 . Page 9 , conclusion , the beginning sentence of the second paragraph is erroneous .

[ 1 ] Venkat Chandrasekaran , Benjamin Recht , Pablo A. Parrilo , Alan S. Willsky , “ The Convex Geometry of Linear Inverse Problems ” , Foundations of Computational Mathematics , 2012 .

Thank you for the thorough review ! We ’ll revise incorporating your comments .

Main Responses :

1 ) “ instead of using [ Donoho & Tanner 2005 ] it should be better to use [ Chandrasekaran et al. 2012 ] ’s deterministic condition , the null space property or NSP ” ( paraphrase )

We knew of NSP but turned to Donoho & Tanner ( 2005 ) because NSP is difficult to work with ( no obvious method to check if local NSP holds ; checking global NSP is NP-hard ( Tillmann & Pfetsch , 2014 ) ) . Since NSP is equivalent to exact recovery , our experiments ( Fig. 1 - 2 ) strongly suggest that local NSP holds , but we did not find a way to use it to gain intuition or proofs . While closely related to NSP , the polytope condition of Donoho & Tanner ( 2005 ) implies Corollary 5.1 , which suggests both a nice property of word embeddings and an efficient method to check recovery of nonnegative signals .

2 ) : “ [ the claim that ] certain LSTMs with random initialization are at least as good as the linear classifiers … ... is almost a direction application of the RIP of random Rademacher matrices ”

This is true for the unigram ( BoW ) case . The proof for the n-gram case necessitated constructing a design matrix with correlated entries for which RIP is not as obvious . We agree that the bigger technical contribution is in connecting these ideas to text embeddings .


Other Responses :

Restricted Strong Convexity ( RSC ) : “ it is proved in [ Chandrasekran et al. 2012 ] that Restricted Strong Convexity ( RSC ) alone is enough to guarantee successful recovery . ”

To our knowledge RSC is used mostly for the case of signal / measurement noise ( Negahban et al. , 2010 ; Chandrasekaran et al. , 2012 ) , whereas we are in the noiseless setting . We know of work by Elenberg et al. ( 2016 ) using RSC to guarantee recovery with Orthogonal Matching Pursuit , but we have found that such greedy methods do not work well for pretrained embeddings ( Section 5.1 paragraph 2 ) , indicating that a sufficient RSC condition does not hold .

LASSO / Dantzig Selector : “ the same comments above apply to many other common estimators ( lasso , Dantzig selector , etc. ) in compressive sensing which might be more tolerant to noise . ”

LASSO was in fact the first approach we tried , with similar results as Basis Pursuit ( we refer to it in Section 5.1 paragraph 2 as an “ l_0 - surrogate method ” ) . However , as we are in the noiseless setting we do not need the robustness provided by LASSO ; indeed , experiments show it performs somewhat worse for both pretrained and random vectors . Furthermore , to our knowledge guarantees for LASSO often have analogous results for Basis Pursuit , so the theoretical benefit to studying it is unclear . Although we did not try the Dantzig Selector , it can also be seen as a robust extension of Basis Pursuit and so similarly does not provide a clear advantage in our case .


Minor Points :

1 . We use the phrase “ classical information theory ” only in connection with the scheme in Paskov et al. , ( 2013 ) which is inspired by the Lempel - Ziv compression algorithm ( Ziv & Lempel , 1977 ) ; 40 years old and directly inspired by Shannon ’s works !
2 . In theory the regularization constant C is chosen to minimize the error bound ; in practice it is chosen by cross-validation .
3 . We extend the analysis in order to handle logistic loss as it is commonly used in the NLP community and by supervised LSTMs . We do not need Theorem 4.2 to hold for all Lipschitz functions to get Theorem 4.1 , but the function does need to be Lipschitz to control the error .
4.1 This assumption is without loss of generality and is made to remove a spurious dependence on T in the error bound .
4.2 . There will sometimes be n-cooccurrences that contain a word more than once , e.g. ( as , long , as ) , but they occur infrequently and can be removed by merging words as a preprocessing step . In the SST training corpus only 0.019 % of bigrams and 0.75 % of trigrams have this issue , the latter often due to words between two commas in a list .
5 - 8 . Will be addressed in revision .


V. Chandrasekaran , B. Recht , P. A. Parrilo , and A. S. Willsky . “ The Convex Geometry of Linear Inverse Problems . ” Found. of Comp. Mathematics 2012 .
D. L. Donoho and J. Tanner . “ Sparse nonnegative solution of underdetermined linear equations by linear programming . ” PNAS 2005 .
E. R. Elenberg , R. Khanna , A. G. Dimakis , and S. Negahban . “ Restricted strong convexity implies weak submodularity . ” arXiv 2016 .
S. Negahban , B. Yu , M. J. Wainwright , and P. K. Ravikumar . “ A unified framework for high - dimensional analysis of M-estimators with decomposable regularizers . ” NIPS 2009 .
H. S. Paskov , R. West , J. C. Mitchell , and T. J. Hastie . “ Compressive feature learning . ” NIPS 2013 .
A. M. Tillmann and M. E. Pfetsch . “ The computational complexity of the restricted isometry property , the nullspace property , and related concepts in compressed sensing . ” IEEE Trans. on Info . Theory 2014 .
J. Ziv and A. Lempel . “ A Universal Algorithm for Sequential Data Compression . ” IEEE Trans. on Info . Theory 1977 .

The paper proposes novel techniques for private learning with PATE framework . Two key ideas in the paper include the use of Gaussian noise for the aggregation mechanism in PATE instead of Laplace noise and selective answering strategy by teacher ensemble . In the experiments , the efficacy of the proposed techniques has been demonstrated . I am not familiar with privacy learning but it is interesting to see that more concentrated distribution ( Gaussian ) and clever aggregators provide better utility - privacy tradeoff .

1 . As for noise distribution , I am wondering if the variance of the distribution also plays a role to keep good utility - privacy trade - off . It would be great to discuss and show experimental results for utility - privacy tradeoff with different variances of Laplace and Gaussian noise .

2 . It would be great to have an intuitive explanation about differential privacy and selective aggregation mechanisms with examples .

3 . It would be great if there is an explanation about the privacy cost for selective aggregation . Intuitively , if teacher ensemble does not answer , it seems that it would reveal the fact that teachers do not agree , and thus spend some privacy cost .











We thank the reviewer for their feedback . Below are answers for each of the three points included in your feedback .

1 . You are right that the variance of the distribution plays a fundamental role in the utility - privacy tradeoff . Roughly speaking , larger noise variances typically yield stronger privacy guarantees but reduce the utility of the aggregated label . We updated Figure 2 to illustrate this relationship , separating the effects of the shape of the noise distribution and the number of teachers .

The left chart of Figure 2 plots the utility - privacy tradeoff for the Laplace ( prior work ) and the Gaussian ( ours ) aggregation mechanisms . The measurement points are labelled with the standard deviation of the noise ( sigma ) which ranges from 5 to 200 . As intuition suggests , the accuracy decreases with the variance of the noise . The privacy cost cannot be measured directly ( as it involves considering all possible counterfactuals ) . Rather , it is computed according to Theorem 2 , which is a significant technical contribution of this paper . For some ranges of parameters , privacy costs are a non-monotone function of sigma , which is discussed in the end of Section 4.2.

2 . We acknowledge the reviewer 's comment about giving an intuitive overview of how we achieve differential privacy . The intuitive privacy guarantee of PATE remains the same as that presented in the original PATE paper by Papernot et al . : Partitioning training data ensures that the presence or absence of a single training data point affects at most one teacher ’s vote . By adding noise to the teacher votes , we control the impact of a single teacher vote in the final outcome of the aggregation mechanism , resulting from the plurality of votes in the teacher ensemble . In fact , precisely bounding this impact , in a tighter way , requires the use of data- dependent analysis . The large variability between queries ’ privacy costs motivates the Confident Aggregator , which minimizes total privacy budget by being selective in the student queries it chooses to answer .

To see why the Confident Aggregator is useful , here are a couple of illustrations of cheap and expensive queries in terms of their privacy cost . Consider our ensembles of 5000 teachers with the following votes across classes ( 4900 , 100 , 0 , … 0 ) . You can see that there is an overwhelming consensus and after adding noise , the chance that Class 1 is output by the ensemble is still very high . The resulting privacy cost ( at Gaussian noise with stdev 100 ) is 3.6e-249 .

However , if there ’s poor consensus amongst the teachers , with votes , say , ( 2500 , 2400 , 100 , … ) i.e. , the ensemble is rather confused between Classes 1 and 2 , then the resulting privacy cost is 0.0025 . It is also easy to see intuitively for our choices of thresholds , why the first example would almost surely be selected but the second example would likely not pass the threshold check .

Unfortunately due to space constraints , we could not provide detailed intuition on these aspects . Some of them follow from the original PATE paper , and we try to give some insight into how the Confident Aggregator works through the experiment in Section 4.4.2 . In particular , Figure 6 shows how queries along the lines of the second example above are eliminated .

3 . You are correct : private information may be revealed when the teacher ensemble does not answer because it indicates that teachers do not agree on the prediction . This is why our selective aggregation mechanisms choose in a privacy - preserving way queries that will be answered . This consideration motivates the design of the condition found at line 1 of Algorithm 1 and line 2 of Algorithm 2 in the submission draft , which both add Gaussian noise with variance $ \sigma_1^2 $ before applying the maximum operator and comparing the result to the predefined threshold T. This ensures that the teacher consensus is checked in differentially private manner . In fact , _most_ of the total privacy budget is committed to selecting the set of queries whose answers are going to be revealed . We thank you for bringing this to our attention and updated the introduction of Section 3 , as well as the captions of Algorithms 1 and 2 to emphasize this important consideration .

Summary :
In this work , PATE , an approach for learning with privacy , is modified to scale its application to real - world data sets . This is done by leveraging the synergy between privacy and utility , to make better use of the privacy budget spent when transferring knowledge from teachers to the student . Two aggregation mechanisms are introduced for this reason . It is demonstrated that sampling from a Gaussian distribution ( instead from a Laplacian distribution ) facilitates the aggregation of teacher votes in tasks with large number of output classes .

on the positive side :

Having scalable models is important , especially models that can be applied to data with privacy concerns . The extension of an approach for learning with privacy to make it scalable is of merit . The paper is well written , and the idea of the model is clear .


on the negative side :

In the introduction , the authors introduce the problem by the importance of privacy issues in medical and health care data . This is for sure an important topic . However , in the following paper , the model is applied no neither medical nor healthcare data . The authors mention that the original model PATE was applied to medical record and census data with the UCI diabetes and adult data set . I personally would prefer to see the proposed model applied to this kind of data sets as well .

minor comments :

Figure 2 , legend needs to be outside the Figure , in the current Figure a lot is covered by the legend

We thank you for your feedback , in particular for bringing to our attention the possible improvements to our experimental setup with respect to the datasets considered . In our submission draft , we chose to focus on the Glyph dataset because it presented challenges like class imbalance and mislabeled data . However , we agree that in order to facilitate a comparison with the original PATE publication , it is important to include results on other datasets such as the UCI Adult and Diabetes datasets . As such , we used the resources made publicly available by the authors of the original PATE publication to reproduce their results and measure the performance of our refined aggregation mechanisms on these two datasets .

We also ran our experiments on the Glyph dataset with the aggregator used by Papernot et al. in the original PATE publication to provide an additional point of comparison .

These additional results are now included in our last submission revision and are summarized in Figure 5 . We show that we compare favorably on all of the datasets and models : we either improve student accuracy , strengthen privacy guarantees , or both simultaneously .

We also followed your suggestion of making the legend of Figure 2 less intrusive by splitting the Figure into two , reducing the amount of information hidden by the legend .


This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy . The dataset is partitioned and multiple learning algorithms produce so-called teacher classifiers . The labels produced by the teachers are aggregated in a differentially private manner and the aggregated labels are then used to train a student classifier , which forms the final output . The novelty of this work is a refined aggregation process , which is improved in three ways :
a ) Gaussian instead of Laplace noise is used to achieve differential privacy .
b ) Queries to the aggregator are " filtered " so that the limited privacy budget is only expended on queries where the teachers are confident and the student is uncertain or wrong .
c ) A data-dependent privacy analysis is used to attain sharper bounds on the privacy loss with each query .

I think this is a nice modular framework form private learning , with significant refinements relative to previous work that make the algorithm more practical . On this basis , I think the paper should be accepted . However , I think some clarification is needed with regard to item c above :

Theorem 2 gives a data-dependent privacy guarantee . That is , if there is one label backed by a clear majority of teachers , then the privacy loss ( as measured by Renyi divergence ) is low . This data- dependent privacy guarantee is likely to be much tighter than the data-independent guarantee .
However , since the privacy guarantee now depends on the data , it is itself sensitive information . How is this issue resolved ? If the final privacy guarantee is data-dependent , then this is very different to the way differential privacy is usually applied . This would resemble the " privacy odometer " setting of Rogers -Roth - Ullman -Vadhan [ https://arxiv.org/abs/1605.08294 ].
Another way to resolve this would be to have an output- dependent privacy guarantee . That is , the privacy guarantee would depend only on public information , rather than the private data . The widely - used " sparse vector " technique [ http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf#page=59 ] does this .
In any case , this is an important issue that needs to be clarified , as it is not clear to me how this is resolved .

The algorithm in this work is similar to the so-called median mechanism [ https://www.cis.upenn.edu/~aaroth/Papers/onlineprivacy.pdf ] and private multiplicative weights [ http://mrtz.org/papers/HR10mult.pdf ]. These works also involve a " student " being trained using sensitive data with queries being answered in a differentially private manner . And , in particular , these works also filter out uninformative queries using the sparse vector technique . It would be helpful to add a comparison .


We thank the reviewer for their feedback .

Before we address specific points raised by the reviewer , we offer a short summary of updates to the submission . The introduction was substantially revised and now includes Figure 1 that illustrates our improvements over the original PATE work . Table 1 includes updated parameters that dominate previously reported state - of - the-art utility and privacy on standard datasets . Figures 4 and 5 illustrate savings to privacy costs due to selective answers . The Appendix was expanded to include discussion of smooth sensitivity .

The reviewer correctly notes that the privacy guarantees depend on sensitive information and this should be accounted for . In Section 2.4 , we briefly mention that we handle this using the smooth sensitivity framework ( as in the original PATE paper ) and in our revised draft of the submission , we provide a detailed overview of our smooth sensitivity analysis in Appendix C.3.

The reported numbers in the rest of our work are without this smooth sensitivity noise added to the privacy cost , because we find that the smooth sensitivity is small enough that adding noise to sanitize the privacy cost itself does not have a significant impact on the value of the privacy cost for the cases reported . We will release the code to calculate the smooth sensitivity along with the next version of our paper .

While the work of Rogers et al . deals with related issues , it operates in a different setting . There , the privacy cost is input-independent , but is a function of the output and of when the mechanism is stopped . While it would be useful to have an odometer - like scheme where we can run the mechanism until a certain privacy budget is consumed , we find that running for a fixed number of steps and estimating the privacy spent using the smooth sensitivity framework largely suffices for the current work .

The sparse- vector technique , the works on private multiplicative weight ( PMW ) and the median mechanism are related to our work and we have added more discussion on them in the related work section ( Appendix B ) . Both PMW and the Median mechanism can be thought of as algorithms that first privately select a small number of queries to answer using the database , and then answer them using a differentially private mechanism . In PMW and Median , the selection is drastic and prunes out all but a tiny fraction of queries , and the sparse vector technique allows one to analyze this step . In our case , the selection ends up choosing ( or discarding ) a non-trivial fraction of the queries ( between 30 % and 90 % for parameter settings reported in the submission ) . We explored using the sparse vector technique for this part of the analysis and it did not lead to any reduction in the privacy cost : while we pay only for the selected queries , the additional constant factors in the sparse vector technique wash out this benefit .

For the second part of actually answering the queries , PMW and Median do a traditional data- independent privacy analysis . In our setting , using a data- independent privacy analysis in the second step would require a lot more noise than the learning process can tolerate , and we benefit greatly from using a data- dependent privacy analysis . The selection not only cuts down the number of queries that are answered , but more importantly , selects for queries that are cheap to answer from the privacy point of view . In summary : In PMW , the goal of the selection is to reduce number of answered queries from Q to log Q , and one does a data- independent privacy analysis for answering those . In our case the goal of filtering is to select a constant fraction of queries that will have a clear majority , so that the data- dependent privacy cost is small .

This paper presents the comparison of a list of algorithms for contextual bandit with Thompson sampling subroutine . The authors compared different methods for posterior estimation for Thompson sampling . Experimental comparisons on contextual bandit settings have been performed on a simple simulation and quite a few real datasets .

The main paper + appendix are clearly written and easy to understand . The main paper itself is very incomplete . The experimental results should be summarized and presented in the main context . There is a lack of novelty of this study . Simple comparisons of different posterior estimating methods do not provide insights or guidelines for contextual bandit problem .

What 's the new information provided by running such methods on different datasets ? What are the newly observed advantages and disadvantages of them ? What could be the fundamental reasons for the variety of behaviors on different datasets ? No significant conclusions are made in this work .

Experimental results are not very convincing . There are lots of plots show linear cumulative regrets within the whole time horizon . Linear regrets represent either trivial methods or not long enough time horizon .


We thank the reviewer for their feedback . The reviewer raises several important concerns , which we address below .

Overall , the main concerns were a lack of insightful conclusions / practical guidelines and that the paper relies too heavily on the appendix . Unfortunately , due to poor organization and writing , the insights we gained from the empirical benchmark were not made clear . We plan to significantly revise the paper for clarity . We briefly summarize our contributions and the insights we derived from the empirical results :

Several recent papers claim to innovate on exploration with deep neural networks ( e.g. , two concurrent ICLR submissions : https://openreview.net/forum?id=ByBAl2eAZ, https://openreview.net/forum?id=rywHCPkAW). We argue that such innovations should be benchmarked against existing literature and baselines on simple decision making tasks ( if the methods do n’t improve on contextual bandits , how could they hope to improve in RL ? ) . Our major contribution is this empirical comparison - a series of reproducible benchmarks with baseline implementations ( all of which will be open sourced ) . We hope that the reviewer agrees that this empirical benchmark is a scientifically useful contribution .

From the empirical benchmark , we find that :

1 ) Variational approaches to estimate uncertainty in neural networks are an active area of research , however , to the best of our knowledge , there is no study that systematically benchmarks variational approaches in decision - making scenarios against other state - of - the- art approaches .

From our evaluation , surprisingly , we find that Bayes by Backprop ( BBB ) underperforms even with a linear model . We demonstrate that because the method is simultaneously learning the representation and the uncertainty level , when faced with a limited optimization budget ( for online learning ) , slow convergence becomes a serious concern . In particular , when the fitted model is linear , we evaluate the performance of a mean field model which we we can solve in closed form for the variational objective . We find that as we increase number of training iterations for BBB , it slowly converges to the performance of this exact method ( Fig 25 ) . We also see that the difference can be much larger than the degradation due to using a mean field approximation . We plan to move this experiment to the main text and expand upon the details .

This is not a problem in the supervised learning setting , where we can train until convergence . Unfortunately , in the online learning setting , this is problematic , as we cannot train for an unreasonable number of iterations at each step , so poor uncertainty estimates lead to bad decisions . Additionally , tricks to speed up convergence of BBB , such as initializing the variance parameters to a small value , distort uncertainty estimates and thus are not applicable in the online decision making setting .

We believe that these insights into the problems with variational approaches are of value to the community , and highlight the need for new ways to estimate uncertainty for online scenarios ( i.e. , without requiring great computational power ) .

2 ) We study an algorithm , which we call NeuralLinear , that is remarkably simple , and combines two classic ideas ( NNs and Bayesian linear regression ) . A very similar algorithm was used before in Bayesian optimization [ 1 ] and an independent ICLR submission ( https://openreview.net/forum?id=Bk6qQGWRb) proposes nearly the same algorithm for RL . In our evaluation , NeuralLinear performs well across datasets . Our insight is that , once the learned representation is of decent quality , being able to exactly compute the posterior in closed form with something as simple as a linear model already leads to better decisions than most of the other methods . We believe this simple argument is novel and encourages further development of this promising approach .

3 ) More generally , an interesting observation is that in many cases the stochasticity induced by stochastic gradient descent is enough to perform an implicit Thompson sampling . The greedy approach sometimes suffices ( or conversely is equally bad as approximate inference ) . However , we also proposed the wheel problem , where the need for exploration is smoothly parameterized . In this case , we see that all greedy approaches fail .

While collecting real - world datasets for a benchmark is challenging , the ones that we use are diverse . Some of them are not learnable or solvable ( like Jester ) , while still of interest due to their practical applications ( recommendation systems , in this case ) . For most datasets , we set the horizon to be the full size of the dataset , so it can not be increased . The regret appears linear because these are simply hard problems . Some dataset - dependent conclusions can be drawn : the Gaussian process does well on small datasets where it can handle a large proportion of the data , whereas constant - SGD performs much better on larger data .

[ 1 ] Jasper Snoek , Oren Rippel , Kevin Swersky , Ryan Kiros , Nadathur Satish , Narayanan Sundaram , Mostofa Patwary , Mr Prabhat , and Ryan Adams . Scalable Bayesian optimization using deep neural networks . In International Conference on Machine Learning , 2015 .

If two major questions below are answered affirmatively , I believe this article could be very good contribution to the field and deserve publication in ICLR .

In this article the authors provide a service to the community by comparing the current most used algorithms for Thompson Sampling - based contextual ( parametric ) bandits on clear empirical benchmark . They reimplement the key algorithms , investing time to make up for the lack of published source code for some .

After a clear exposure of the reasons why Thompson Sampling is attractive , they overview concisely the key ideas behind 7 different families of algorithms , with proper literature review . They highlight some of the subtleties of benchmarking bandit problems ( or any active learning algorithms for that matter ) : the lack of counterfactual and hence the difference in observed datasets . They explain their benchmark framework and datasets , then briefly summarise the results for each class of algorithms . Most of the actual measures from the benchmark are provided in a lengthy appendix 12 pages appendix choke - full of graphs and tables .

It is refreshing to see an article that does not boast to offer the new " bestest - ever " algorithm in town , overcrowding a landscape , but instead tries to prune the tree of possibilities and wading through other people 's inflated claims . To the authors : thank you ! It is too easy to dismiss these articles as " pedestrian non- innovative groundwork " : if there were more like it , our field would certainly be more readable and less novelty - prone .

Of course , there is no perfect benchmark , and like every benchmark , the choices made by the authors could be debated to no end . At least , the authors try to explain them , and the tradeoffs they faced , as clearly as possible ( except for two points mentioned below ) , which again is too rare in our field .

Major clarifications needed :

My two key questions are :
* Is the code of good quality , with exact reproducibility and good potential extension in a standard language ( e.g. Python ) ? This benchmark only gets its full interest if the code is publicised and well engineered . The open-sourcing is planned , according to footnote 1 , is planned -- but this should be made clearer in the main text . There is no discussion of the engineering quality , not even of the language used , and this is quite important if the authors want the community to build upon this work . The code was not submitted for review , and as such its accessibility to new contributors is unknown to this reviewer . That could be a make or break feature of this work .
* Is the hyper parameter tuning reproducible ? Hyperparameter tuning should be discussed much more clearly ( in the Appendix ) : while I appreciate the discussion page 8 of how they were frozen across datasets , " they were chosen through careful tuning " is way too short . What kind of tuning ? Was it manual , and hence not reproducible ? Or was it a clear , reproducible grid search or optimiser ? I thoroughly hope for the later , otherwise an unreproducible benchmark would be very

If the answers to the two questions above is " YES " , then brilliant article , I am ready to increase my score . However , if either is a " NO " , I am afraid that would limit to how much this benchmark will serve as a reference ( as opposed to " just one interesting datapoint " ) .


Minor improvements :
* Please proofread some obvious typos :
- page 4 " suggesed " -> " suggested " ,
- page 8 runaway math environment wreaking the end of the sentence .
- reference " Meire Fortunato ( 2017 ) " should be " Fortunato et al. ( 2017 ) " , throughout .
* Improve readability of figures ' legends , e.g. Figure 2 . ( b ) key is un-readable .
* A simple table mapping the name of the algorithm to the corresponding article is missing . Not everyone knows what BBB and BBBN stands for .
* A measure of wall time would be needed : while computational cost is often mentioned ( especially as a drawback to getting proper performance out of variational inference ) , it is nowhere plotted . Of course that would partly depend on the quality of the implementation , but this is somewhat mitigated if all the algorithms have been reimplemented by the authors ( is that the case ? please clarify ) .

We thank the reviewer for carefully reading the manuscript and for their thoughtful feedback .

To address the primary concerns :

1 - The code is written in Python and Tensorflow , and will be committed to a well - known Anonymized open source library . Currently , the code is going through third party code review within our organization and is subject to a high quality standard . We designed the implementation so that adding new algorithms and rerunning the benchmark is straightforward for an external contributor .

2 - We agree that making the hyperparameter selection reproducible is essential . To this end , we will re-run the experiments doing the following : 1 ) we will choose two representative datasets and apply Bayesian optimization to find parameters for each algorithm based on the results from the training datasets . Then , we will freeze these parameters for the remaining datasets and report numbers ( and parameters ) on these heldout datasets . We will update this post when we have revised the manuscript with the new numbers .

Finally , we have fixed the typos and improved the figures ' legends . We added a table mapping algorithm names to their meaning and parameters . We agree that a table showing wall clock time for each algorithm is highly informative , and we plan to add that to the revised manuscript .

We confirm that the authors reimplemented all of the algorithms .


The paper " DEEP BAYESIAN BANDITS SHOWDOWN " proposes a comparative study about bandit approaches using deep neural networks .

While I find that such a study is a good idea , and that I was really interested by the listing of the different possibilities in the algorithms section , I regret that the experimental results given and their analysis do not allow the reader to well understand the advantages and issues of the approaches . The given discussion is not enough connected to the presented results from my point of view and it is difficult to figure out what is the basis of some conclusion .

Also , the considered algorithms are not enough described to allow the reader to have enough insights to fully understand the proposed arguments . Maybe authors should have focused on less algorithms but with more implementation details . Also , what does not help is that it is very hard to conect the names in the result table with the corresponding approaches ( some abbreviations are not defined at all - BBBN or RMS for instances ) .

At last , the experimental protocol should be better described . For instance it is not clear on how the regret is computed : is it based on the best expectation ( as done in most os classical studies ) or on the best actual score of actions ? The wheel bandit protocol is also rather hard to follow ( and where is the results analysis ? ) .

Other remarks :
- It is a pitty that expectation propagation approaches have been left aside since they correspond to an important counterpart to variational ones . It would have been nice to get a comparaison of both ;
- Variational inference decsription in section algorithms is not enough developped w.r.t. the importance of this family of approaches
- Neural Linear is strange to me . Uncertainty does not consider the neural representation of inputs ? How does it work then ?
- That is strange that \ Lambda_0 and \mu_0 do not belong to the stated asumptions in the linear methods part ( ok they correspond to some prior but it should be clearly stated )
- Figure 1 is referenced very late ( after figure 2 )




First , we would like to thank the reviewer for their feedback .

We acknowledge that the submitted version of the paper does not clearly connect the numerical results and our conclusions and claims . For the revision , we are focused on improving clarity . We plan to expand the discussion of the results and to add tables that summarize the relative ranking among algorithms across datasets to make comparison simpler .

Moreover , we plan to extend the sections corresponding to algorithm descriptions and experimental setup . We also now include a table that explains the abbreviated algorithm names and hyperparameter settings ( e.g. , difference between RMS2 and RMS3 , etc . ) .

Regret is computed based on the best expected reward ( as is standard ) . For some real datasets , the rewards were deterministic , in which case , both definitions of regret agree . We reshuffle the order of the contexts , and rerun the experiment a number of times to obtain the cumulative regret distribution and report its statistics . We now clarify this procedure in the experimental setup section .

We agree that the wheel bandit protocol was not clearly explained , and we have expanded the description .

We agree that expectation propagation methods are relevant to this study , so we have implemented the black - box alpha-divergence algorithm [ 1 ] and will add it to the study .

NeuralLinear is based on a standard deep neural network . However , decisions are made according to a Bayesian linear regression applied to the features at the last layer of the network . Note that the last hidden layer representation determines the final output of the network via a linear function , so we can expect a representation that explains the expected value of an action with a linear model . For all the training contexts , their deep representation is computed , and then uncertainty estimates on linear parameters for each action are derived via standard formulas . Thompson sampling will sample from this distribution , say \beta_t , i at time t for action i , and the next context will be pushed through the network until the last layer , leading to its representation c_t . Then , the sampled beta ’s will predict an expected value , and the action with the highest prediction will be taken . Importantly , the algorithm does not use any uncertainty estimates on the representation itself ( as opposed to variational methods , for example ) . On the other hand , the way the algorithm handles uncertainty conditional on the representation and the linear assumption is exact , which seems to be key to its success .

We will add a comment explaining the assumed prior for linear methods .

[ 1 ] Hernández -Lobato , J. M. , Li , Y. , Rowland , M. , Hernández -Lobato , D. , Bui , T. , and Turner , R. E. ( 2016 ) . Black - box α-divergence minimization . In International Conference on Machine Learning .

I think this is a good work that I am sure will have some influence in the near future . I think it should be accepted and my comments are mostly suggestions for improvement or requests for additional information that would be interesting to have .

Generally , my feeling is that this work is a little bit too dense , and would like to encourage the authors in this case to make use of the non-strict ICLR page limit , or move some details to appendix and focus more on more thorough explanations . With increased clarity , I think my rating ( 7 ) would be higher .

Several Figures and Tables are never referenced in the text , making it a little harder to properly follow text . Pointing to them from appropriate places would improve clarity I think .

Algorithm 1 line 14 : You never seem to explain what is sparse ( G ) . Sec 3.1 : What is it exactly that gets communicated ? How do you later calculate the Compression Ratio ? This should surely be explained somewhere .

Sec 3.2 you mention 1 % loss of accuracy . A pointer here would be good , at that point it is not clear if it is in your work later , or in another paper . The efficient momentum correction is great !

As I was reading the paper , I got to the experiments and realized I still do n't understand what is it that you refer to as " deep gradient compression " . Pointer to Table 1 at the end of Sec 3 would probably be ideal along with some summary comments .

I feel the presentation of experimental results is somewhat disorganized . It is not clear what is immediately clear what is the baseline , that should be somewhere stressed . I find it really confusing why you sometimes compare against Gradient Dropping , sometimes against TernGrad , sometimes against neither , sometimes include Gradient Sparsification with momentum correction ( not clear again what is the difference from DGC ) . I recommend reorganizing this and make it more consistent for sake of clarity . Perhaps show here only some highlights , and point to more in the Appendix .

Sec 5 : Here I feel would be good to comment on several other things not mentioned earlier .
Why do you only work with 99.9 % sparsity ? Does 99 % with 64 training nodes lead to almost dense total updates , making it inefficient in your communication model ? If yes , does that suggest a scaling limit in terms of number of training nodes ? If not , how important is the 99.9 % sparsity if you care about communication cost dominating the total runtime ? I would really like to better understand how does this change and what is the point beyond which more sparsity is not practically useful . Put differently , is DGC with 600 x size reduction in total runtime any better than DGC with 60 x reduction ?


Finally , a side remark :
Under eq . ( 2 ) you point to something that I think could be more discussed . When you say what you do has the effect of increasing stepsize , why do n't you just increase the stepsize ?
There has recently been this works on training ImageNet in 1 hour , then in 24 minutes , latest in 15 minutes ... You cite the former , but highlight different part of their work . Broader idea is that this is trend that potentially makes this kind of work less relevant . While I do n't think that makes your work bad or misplaced , I think mentioning this would be useful as an alternative approach to the problems you mention in the introduction and use to motivate your contribution .
... what would be your reason for using DGC as opposed to just increasing the batch size ?

We thank the reviewer for the comments .

- Several Figures and Tables are never referenced in the text , making it a little harder to properly follow text . Pointing to them from appropriate places would improve clarity I think .

We revised our paper . All the figures and tables are referenced properly in the text .

- Algorithm 1 line 14 : You never seem to explain what is sparse ( G ) . Sec 3.1 : What is it exactly that gets communicated ? How do you later calculate the Compression Ratio ?

We have change the name of function to encode ( G ) . The encode ( ) function packs 32 - bit nonzero gradient values and 16 - bit run lengths of zeros in the flattened gradients . The encoded sparse gradients get communicated . These are described in the Sec 3.1 now .
The compression ratio is calculated as follows :
The Gradient Compression Ratio = Size [ encode ( sparse ( G_k ) ) ] / Size [ G_k ]
It is defined in the Sec 4.1 now .

- Sec 3.2 you mention 1 % loss of accuracy . A pointer here would be good , at that point it is not clear if it is in your work later , or in another paper .

We pointed to the Figure 3 ( a ) in the updated draft , and also cite the paper AdaComp [ 1 ] .

- Pointer to Table 1 at the end of Sec 3 would probably be ideal along with some summary comments .

We make a summary at the end of Sec 3 and add Appendix D to show the overall algorithm of DGC in the updated draft .

- I find it really confusing why you sometimes compare against Gradient Dropping , sometimes against TernGrad , sometimes against neither , sometimes include Gradient Sparsification with momentum correction ( not clear again what is the difference from DGC ) .

Because related work did n’t cover them all . Gradient Dropping [ 2 ] only performed experiments on 2 - layer LSTM for NMT , and 3 - layer DNN for MNIST ; TernGrad [ 3 ] only performed experiments on AlexNet , GoogleNet and VGGNet . Therefore , we compared our AlexNet result with TernGrad .

DGC contains not only momentum correction but also momentum factor masking and warm - up training . Momentum correction and Local gradient clipping are proposed to improve local gradient accumulation . Momentum factor masking and warm - up training are proposed to overcome the staleness effect . Comparison between Gradient Sparsification with momentum correction and DGC shows their impact on training respectively .

- Why do you only work with 99.9 % sparsity ? Does 99 % with 64 training nodes lead to almost dense total updates , making it inefficient in your communication model ? If yes , does that suggest a scaling limit in terms of number of training nodes ? If not , how important is the 99.9 % sparsity if you care about communication cost dominating the total runtime ?

Yes , 99 % with 128 training nodes lead to almost dense total updates , making it inefficient in communication . The scaling limit N in terms of number of training nodes depends on the gradient sparsity s : N ≈ 1 / ( 1 - s ) . When the gradient sparsity is 99.9 % , the scaling limit is 1024 training nodes .

- When you say what you do has the effect of increasing stepsize , why do n't you just increase the stepsize ? What would be your reason for using DGC as opposed to just increasing the batch size ?

Since the memory on GPU is limited , the way to increase the stepsize is to increase training nodes . Previous work in increasing the stepsize focus on how to deal with very large mini-batch training , while our work focus on how to reduce the communication consumption among increased nodes under poor network bandwidth . DGC can be considered as increasing the stepsize temporally on top of increasing the actual stepsize spatially .

References :
[ 1 ] Chen , Chia - Yu , et al . " AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training . " arXiv preprint arXiv:1712.02679 ( 2017 ) .
[ 2 ] Aji , Alham Fikri , and Kenneth Heafield . Sparse Communication for Distributed Gradient Descent . In Empirical Methods in Natural Language Processing ( EMNLP ) , 2017 .
[ 3 ] Wen , Wei , et al . TernGrad : Ternary Gradients to Reduce Communication in Distributed Deep Learning . In Advances in Neural Information Processing Systems , 2017 .

The paper is thorough and on the whole clearly presented . However , I think it could be improved by giving the reader more of a road map w.r.t. the guiding principle . The methods proposed are heuristic in nature , and it 's not clear what the guiding principle is . E.g. , " momentum correction " . What exactly is the problem without this correction ? The authors describe it qualitatively , " When the gradient sparsity is high , the interval dramatically increases , and thus the significant momentum effect will harm the model performance " . Can the issue be described more precisely ? Similarly for gradient clipping , " The method proposed by Pascanu et al. ( 2013 ) rescales the gradients whenever the sum of their L2 - norms exceeds a threshold . This step is conventionally executed after gradient aggregation from all nodes . Because we accumulate gradients over iterations on each node independently , we perform the gradient clipping locally before adding the current gradient ... " What exactly is the issue here ? It reads like a story of what the authors did , but it 's not really clear why they did it .

The experiments seem quite thorough , with several methods being compared . What is the expected performance of the 1 - bit SGD method proposed by Seide et al .?

re. page 2 : What exactly is " layer normalization " ?

re. page 4 : What are " drastic gradients " ?

We thank the reviewer for the comments .

- What exactly is the problem without this correction ? Can the issue be described more precisely ?

We already revised our paper , and described the momentum correction more precisely in Section 3.2 . Basically , the momentum correction performs the momentum SGD without update locally and accumulates the velocity u_t locally .

- What exactly is the issue of Gradient clipping ?

When training RNN , people usually use Gradient Clipping to avoid the exploding gradient problem . The hyper-parameter for Gradient Clipping is the threshold thr_G of the gradients L2 -norm . The gradients for optimization is scaled by a coefficient depending on their L2 -norm .

Because we accumulate gradients over iterations on each node independently , we need to scale the gradients before adding them to the previous accumulation , in order to scale the gradients by the correct coefficient . The threshold for local gradient clipping thr_ Gk should be set to N^{ - 1 / 2 } x thr_G . We add Appendix C to explain how N^{ - 1 / 2 } comes .

- What is the expected performance of the 1 - bit SGD method proposed by Seide et al .?

1 - bit SGD [ 1 ] encodes the gradients as 0 or 1 , so the data volume is reduced by 32x . Meanwhile , since 1 - bit SGD quantizes the gradients column - wise , a floating - point scaler per column is required , and thus it cannot yield much speed benefit on convolutional neural networks .

- What exactly is " layer normalization "

“ Layer Normalization ” is similar to batch normalization but computes the mean and variance from the summed inputs in a layer on a single training case . [ 2 ]

- What are " drastic gradients " ?

It means the period when the network weight changes dramatically .

References :
[ 1 ] Frank Seide , Hao Fu , Jasha Droppo , Gang Li , and Dong Yu. 1 - bit stochastic gradient descent and its application to data- parallel distributed training of speech DNNs . In Fifteenth Annual Conference of the International Speech Communication Association , 2014 .
[ 2 ] J. Lei Ba , J. R. Kiros , and G.E.Hinton , Layer Normalization . ArXiv e-prints , July 2016

Experimental results have shown that deep networks ( many hidden layers ) can approximate more complicated functions with less neurons compared to shallow ( single hidden layer ) networks .
This paper gives an explicit proof when the function in question is a sparse polynomial , ie : a polynomial in n variables , which equals a sum J of monomials of degree at most c.
In this setup , Theorem 4.3 says that a shallow network need at least ~ ( 1 + c / n ) ^n many neurons , while the optimal deep network ( whose depth is optimized to approximate this particular input polynomial ) needs at most ~ J*n , that is , linear in the number of terms and the number of variables . The paper also has bounds for neural networks of a specified depth k ( Theorem 5.1 ) , and the authors conjecture this bound to be tight ( Conjecture 5.2 ) .

This is an interesting result , and is an improvement over Lin 2017 ( where a similar bound is presented for monomial approximation ) .
Overall , I like the paper .

Pros : new and interesting result , theoretically sound .
Cons : nothing major .
Comments and clarifications :
* What about the ability of a single neural network to approximate a class of functions ( instead of a single p ) , where the topology is fixed but the network weights are allowed to vary ? Could you comment on this problem ?
* Is the assumption that \sigma has Taylor expansion to order d tight ? ( That is , are there counter examples for relaxations of this assumption ? )
* As noted , the assumptions of your theorems 4.1- 4.3 do not apply to ReLUs , but ReLUs network perform well in practice . Could you provide some further comments on this ?



Thank you for this thoughtful feedback . To respond to the particular comments raised :

- This is a very interesting question . In this work , we have supposed that connections between layers of a network are dense . In this case , the topology is given simply by the number of neurons in each layer , and this architecture is relatively versatile . Architectures of the form described in the proof of Thm. 5.1 ( where the sizes of the hidden layers follow a decreasing geometric progression ) should be especially flexible , able to learn a wide range of monomials and sums of monomials . Intuitively , this network architecture learns well because the initial large hidden layers capture many lower order correlations between input variables , which are then used to calculate higher - order correlations deeper within the network .

- The conditions on the activation function appear to be at least largely tight . As we mention in the text , Thm. 3.4 fails for ReLU activation ( where the Taylor series is not even defined ) , implying that all subsequent theorems also fail for ReLUs . More interestingly , it is possible to multiply d inputs with ( slightly ) fewer than 2 ^d neurons if the constant term in the Taylor series for the activation function is zero . We had previously proven that a less elegant exponential bound still holds as long as the dth Taylor coefficient itself is nonzero ( without any assumptions on the other coefficients ) , and we have included this in our revision .

- In practice , we are rarely concerned with uniform approximation for epsilon truly arbitrarily small . ReLUs can be ( imperfectly ) approximated by Taylor - approximable functions , and the behavior diverges as the desired epsilon decreases . In running our experiments , we observed similar behavior with ReLUs as with Taylor - approximable activation functions , even though the full power of our theoretical results is indeed not applicable .

The paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations . It shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial .

By focusing on polynomials , the paper is able to use of a variety of tools ( e.g. linear algebra ) to investigate the representation question . Results such as Proposition 3.3 relate the representation of a polynomial up to a certain degree , to the approximation question . Here it would be good to be more specific about the domain , however , as approximating the low order terms certainly does not guarantee a global uniform approximation .

Theorem 3.4 makes an interesting claim , that a finite network size is sufficient to achieve the best possible approximation of a polynomial ( the proof building on previous results , e.g. by Lin et al that I did not verify ) . The idea being to construct a superposition of Taylor approximations of the individual monomials . Here it would be good to be more specific about the domain . Also , in the discussion of Taylor series , it would be good to mention the point around which the series is developed , e.g. the origin .

The paper mentions that ``the theorem is false for rectified linear units ( ReLUs ) , which are piecewise linear and do not admit a Taylor series ' ' . However , a ReLU can also be approximated by a smooth function and a Taylor series .

Theorem 4.1 seems to be implied by Theorem 4.2 . Similarly , parts of Section 4.2 seem to follow directly from the previous discussion .

In page 1 ```existence proofs ' without explicit constructions ' ' This is not true , with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation functions .



We are very grateful for this helpful feedback , and have responded below to individual issues raised .

Thank you for the suggestion that we make clearer the domain under which Prop. 3.3 and Thm. 3.4 hold . We have made explicit in our revision that these results hold for any ( fixed ) domain ( - R , R ) ^n , and that Taylor series are constructed around the origin .

While it is indeed true that a ReLU can be approximated by a smooth function with a well - defined Taylor series , any particular choice of such a function would fail our strict requirement of uniform approximation for arbitrarily small \epsilon . Since we have assumed that the choice of nonlinear function \sigma is fixed , we cannot use progressively better approximations to ReLUs . Another way of thinking about this is to note that a neural network with ReLUs is ultimately piecewise linear . For a fixed budget of neurons , the number of linear pieces is bounded . Given a fixed number of linear pieces and a general polynomial to approximate , the approximation cannot be better than some fixed \epsilon ( depending on the polynomial ) , whereas we would like \epsilon to be arbitrarily small .

Theorems 4.1 and 4.2 are in fact independent , with neither implying the other . This is because it is possible for a polynomial to admit a compact uniform approximation without admitting a compact Taylor approximation . We have made this clearer in the text .

We have rephrased our discussion of prior literature to emphasize that “ existence proofs ” are a feature only of * some * of the prior work . There are indeed excellent papers that provide explicit constructions .

Summary and significance : The authors prove that for expressing simple multivariate monomials over n variables , networks of depth 1 require exp ( n ) many neurons , whereas networks of depth n can represent these monomials using only O ( n ) neurons .
The paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks , and quantifying the improvement provided by depth .

+ves :
Explaining the power of depth in NNs is fundamental to an understanding of deep learning . The paper is very easy to follow . and the proofs are clearly written . The theorems provide exponential gaps for very simple polynomial functions .

- ves :
1 . My main concern with the paper is the novelty of the contribution to the techniques . The results in the paper are more general than that of Lin et al. , but the proofs are basically the same , and it 's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas .
2 . The second concern is that the results apply only to non-linear activation functions with sufficiently many non- zero derivatives ( same requirements as for the results of Lin et al . ) .
3 . Finally , in prop 3.3 , reducing from uniform approximations to Taylor approximations , the inequality | E ( δx ) | <= δ^ ( d + 1 ) | N ( x ) - p ( x ) | does not follow from the definition of a Taylor approximation .

Despite these criticisms , I contend that the significance of the problem , and the clean and understandable results in the paper make it a decent paper for ICLR .

We are very grateful for this close reading and constructive comments . Detailed responses follow :

1 . We believe that in addition to presenting more general results than in the literature , we also contribute techniques that are significantly stronger than those in Lin et al . In particular , tighter proof techniques are required in order to prove lower bounds on the number of neurons required for a uniform approximation . One of the more interesting methodological insights resulting from our approach is that even though uniform approximation does not imply Taylor approximation , we can still use the lack of a Taylor approximation as a significant step towards proving the lack of a uniform approximation . To the best of our knowledge , this is the first time that Taylor approximation and uniform approximation of neural networks have been rigorously linked .

2 . The assumptions on the activation function can be weakened somewhat at the expense of less elegant formulations . We had previously proven that an exponential bound still holds as long as the dth Taylor coefficient itself is nonzero ( without any assumptions on the other coefficients ) , and we have included this statement and proof in our revision . As we mention in the text , Thm. 3.4 fails for ReLU activation ( where the Taylor series is not even defined ) , implying that all subsequent theorems also fail for ReLUs . In practice , however , we are rarely concerned with uniform approximation for epsilon truly arbitrarily small . ReLUs can be ( imperfectly ) approximated by Taylor - approximable functions , and the behavior diverges as the desired epsilon decreases . In running our experiments , we observed similar behavior with ReLUs as with Taylor - approximable activation functions , even though the full power of our theoretical results is indeed not applicable .

3 . In our revision , we have rewritten the proof of prop . 3.3 to encompass all cases . Thank you for calling this to our attention .

The paper seems clear enough and original enough . The idea of jointly forming groups of operations to colocate and figure out placement on devices seems to hold merit . Where the paper falls short is motivating the problem setting . Traditionally , for determining optimal execution plans , one may resort to cost - based optimization ( e.g. , database management systems ) . This paper 's introduction provides precisely 1 statement to suggest that may not work for deep learning . Here 's the relevant phrase : " the cost function is typically non-stationary due to the interactions between multiple devices " . Unfortunately , this statement raises more questions than it answers . Why are the cost functions non-stationary ? What exactly makes them dynamic ? Are we talking about a multi-tenancy setting where multiple processes execute on the same device ? Unlikely , because GPUs are involved . Without a proper motivation , its difficult to appreciate the methods devised .

Pros :
- Jointly optimizing forming of groups and placing these seems to have merit
- Experiments show improvements over placement by human " experts "
- Targets an important problem

Cons :
- Related work seems inadequately referenced . There exist other linear / tensor algebra engines / systems that perform such optimization including placing operations on devices in a distributed setting . This paper should at least cite those papers and qualitatively compare against those approaches . Here 's one reference ( others should be easy to find ) : " SystemML 's Optimizer : Plan Generation for Large -Scale Machine Learning Programs " by Boehm et al , IEEE Data Engineering Bulletin , 2014 .
- The methods are not well motivated . There are many approaches to devising optimal execution plans , e.g. , rule - based , cost -based , learning - based . In particular , what makes cost - based optimization inapplicable ? Also , please provide some reasoning behind your hypothesis which seems to be that while costs may be dynamic , optimally forming groups and placing them is learn-able .
- The template seems off . I do n't see the usual two lines under the title ( " Anonymous authors " , " Paper under double - blind review " ) .
- The title seems misleading . " .... Device Placement " seems to suggest that one is placing devices when in fact , the operators are being placed .

Thank you for your constructive feedback !

The reviewer is concerned with the lack of references to previous works and comparison against them . First , we are happy to add more citations to related work ( see Section 1 in the updated submission ) . We believe that related works such as SystemML will be a strong baseline for us if we want to expand this work to memory management , since unlike runtime , memory usage is deterministic . We also compared our approach against cost - based optimization implemented in Scotch library ( see Table 1 ) and showed that our method performs significantly better . The advantage of our method is that it ’s not dependent on the hardware platform because our method can learn runtime information directly through experiments . Whereas to use Scotch , we need to feed information about the hardware platform to it .

The reviewer is asking why the cost is non-stationary and dynamic , and therefore is concerned with motivation of the work . To answer this question we have added a discussion , in Section 1 on why our reward , the runtime of executing a Tensor Flow graph , is non-stationary and also made it more clear that we did compare against cost - based optimizations in Table 1 . In summary , in our distributed environment , we use a shared cluster of CPUs and GPUs , and our CPUs can also serve other jobs at the same time . Furthermore , in next generation of hardware platforms ( such as Cloud TPUs ) , there will be a lot of interferences between the concurrent jobs . Again , the advantage of our method is that it ’s not dependent on the hardware platform because our method can learn runtime information directly through experiments .

Regarding “ The template seems off . I do n't see the usual two lines under the title ( " Anonymous authors " , " Paper under double - blind review " ) . ” and “ The title seems misleading . " .... Device Placement " seems to suggest that one is placing devices when in fact , the operators are being placed . ” Thanks ! We fixed the formatting and will think of new names . We used device placement to be consistent with previous work .


In a previous work [ 1 ] , an auto-placement ( better model partition on multi GPUs ) method was proposed to accelerate a Tensor Flow model ’s runtime . However , this method requires the rule - based co-locating step , in order to resolve this problem , the authors of this paper purposed a fully connect network ( FCN ) to replace the co-location step . In particular , hand - crafted features are fed to the FCN and the output is the prediction of group id of this operation . Then all the embeddings in each group are averaged to serve as the input of a seq2seq encoder .

Overall speaking , this work is quite interesting . However , it also has several limitations , as explained below .

First , the computational cost of the proposed method seems very high . It may take more than one day on 320 - 640 GPUs for training ( I did not find enough details in this paper , but the training complexity will be no less than the in [ 1 ] ) . This makes it very hard to reproduce the experimental results ( in order to verify it ) , and its practical value becomes quite restrictive ( very few organizations can afford such a cost ) .

Second , as the author mentioned , it ’s hard to compare the experimental results in this paper wit those in [ 1 ] because different hardware devices and software versions were used . However , this is not a very sound excuse . I would encourage the authors to implement colocRL [ 1 ] on their own hardware and software systems , and make direct comparison . Otherwise , it is very hard to tell whether there is improvement , and how significant the improvement is . In addition , it would be better to have some analysis on the end - to - end runtime efficiency and the effectiveness of the placements .

[ 1 ] Mirhoseini A , Pham H , Le Q V , et al. Device Placement Optimization with Reinforcement Learning [ J ]. arXiv preprint arXiv:1706.04972 , 2017 . https://arxiv.org/pdf/1706.04972.pdf


Thank you for your constructive feedback !

The reviewer is concerned that the policy training for device placement takes so many resources and quoted “ 320 - 640 GPUs ” being used . In reality , we use 36 GPUs in our experiments ( or 68 GPUs for deep networks ) . We apologize this was not clear in the paper . [ More details can be found in Section 2 under “ Distributed Training ” . ]

Regarding concern about reproducibility , we confirm that it ’s possible to replicate the experiments with only 5 K40 GPUs . We ran an experiment to partition 4 - layer NMT model on 5 devices ( 1 CPU and 4 GPUs ) . We used 5 GPUs , 1 for policy training and 4 for measuring time , and it took roughly 2.5 hours to find a good placement . While this may seem slow , it actually takes around 12.5 GPU - hours to save 265 GPU - hours on training NMT on WMT ’14 En - > Fr for one epoch . [ More details can be found below and in Section 3 including Fig. 3 under “ Overhead of Training Hierarchical Planner ” . ]

The reviewer is concerned with the lack of comparison against ColocRL . We want to emphasize CoLocRL makes a strong assumption that we have a human expert to manually assign operations to groups . Our method does not make this assumption . In addition to being more flexible , our method uses much fewer resources and actually gets better results . For example for NMT ( 2 - layer ) , our improvement over best heuristics is 60.6 % , compared to 19.0 % reported in ColocRL . For NMT ( 4 - layer ) and NMT ( 8 - layer ) , no results were reported for ColocRL , which we suspect is due to the model being unable to handle the large number of operations in these graphs .




This paper proposes a device placement algorithm to place operations of tensorflow on devices .

Pros :

1 . It is a novel approach which trains the placement end to end .
2 . The experiments are solid to demonstrate this method works very well .
3 . The writing is easy to follow .
4 . This would be a very useful tool for the community if open sourced .

Cons :

1 . It is not very clear in the paper whether the training happens for each model yielding separate agents , or a shared agent is trained and used for all kinds of models . The latter would be more exciting . The adjacency matrix varies size for different graphs , so I guess a separate agent is trained for each graph ? However , if the agent is not shared , why not just use integer to represent each operation in the graph , since overfitting would be more desirable in this case .
2 . Averaging the embedding is hard to understand especially for the output sizes and number of outputs .
3 . It is not clear how the adjacency information is used .


Thank you for your positive feedback . We will open -source our code once the paper gets accepted .

The reviewer asks if we are training a policy per model ( which is the case ) and whether it ’s possible to use different embeddings for different ops , because it ’s easier to overfit . While this is true , training the policy network will take longer without the shared embeddings . We actually tried this , and it took longer to train the policy network because the policy network has more parameters to learn .

The reviewer is concerned that “ averaging is hard to understand especially for the output sizes and number of outputs . ” We apologize for this as this is not exactly what we did . We corrected our paper as we are not averaging the operation embeddings , but we are using information about operations assigned to a group to make a new embedding for those groups .

More details are as follows which also include how adjacency information is used ( we also added these details in Section 2 of the submission ) .

First , for creating embedding for each operation , we concatenate 3 vectors :
1 ) A vector that embeds operation type information . We learn this vector similarly to how language model embedding is learned . Our vocabulary is the set of all TF operations and we learn an operation embedding of size 20 .
2 ) A vector that contains output sizes and number of outputs for an operation . We set a fixed threshold ( 6 in our design ) for maximum number of possible output edges for an operation , and for each output edge we set a threshold ( 4 in our design ) for maximum dimension . We fill this vector of size 24 by reading the outputs of an operation one by one and putting in the output operations shapes . We fill the vector with - 1 for non-existing outputs edges or dimensions .
3 ) A vector that contains adjacency information for that operation . We index the graph by traversing it in a BFS manner and set the maximum number of incoming and outgoing edges to 12 ( 6 for each direction ) . We then fill the vector with the index of the incoming and outgoing operations . We fill the vector with - 1 , for non-existing edges .

Second , to create an embedding for each group , we concatenate 3 vectors :
1 ) A vector that counts how many of each operation types are assigned to that group . The size of this vector is the size of vocabulary of of TensorFlow ’s most widely used operations which we limit to 200 .
2 ) A vector that counts the overall output shapes of all the operations in that group . This vector is created by adding all the operation output shape embedding described above ( not including the - 1 ) and is of size 16 .
3 ) A vector that contains group adjacency information . The size of this vector is the number of groups ( 256 in our experiments ) , and its i'th value is 1 if the group has edges to the i'th group and is 0 otherwise .


The paper introduces a new algorithm for training GANs based on the Earth Mover ’s distance . In order to avoid biased gradients , the authors use the dual form of the distance on mini-batches , to make it more robust . To compute the distance between mini batches , they use the Sinkhorn distance . Unlike the original Sinkhorn distance paper , they use the dual form of the distance and do not have biased gradients . Unlike the Cramer GAN formulation , they use a mini-batch distance allowing for a better leverage of the two distributions , and potentially decrease variance in gradients .

Evaluation : the paper shows good results a battery of tasks , including a standard toy example , CIFAR - 10 and conditional image generation , where they obtain better results than StackGAN .

The paper is honest about its shortcomings , in the current set up the model requires a lot of computation , with best results obtained using a high batch size .

Would like to see :
* a numerical comparison with Cramer GAN , to see whether the additional computational cost is worth the gains .
* Cramer GAN shows an increase in diversity , would like to see an analog experiment for conditional generation , like figure 3 in the Cramer GAN paper .


Regarding your two requests :

* The Cramer GAN paper did not report inception scores on the usual data sets , and they do not provide code , so numerical comparison to their work is a bit difficult . I searched for public implementations of Cramer GAN , and the best one I could find is this one : https://github.com/pfnet-research/chainer-gan-lib According to the inception scores reported here , our method performs much better than Cramer GAN .

* We are in correspondence with the authors of Cramer GAN about the details of this conditional generation experiment . So far we have not yet been able to replicate their exact setup . We hope to be able to include this experiment in the paper soon . Related to what you ’re asking for , we have added an additional experiment to the paper investigating sample diversity and mode collapse in OT - GAN as compared to DCGAN ( appendix D ) . What we find here is that DCGAN ( as well as other variants of GAN ) still shows mode collapse when training for longer periods of time . For OT - GAN we see no mode collapse , even when we keep training for a very long time . If we stop training DCGAN when the Inception score is maximal , we see no mode collapse , but the sample diversity is still lower than for OT - GAN ( see figure 6 ) .


The paper presents a variant of GANs in which the distance measure between the generator 's distribution and data distribution is a combination of two recently proposed metrics . In particular , a regularized Sinkhorn loss over a mini-batch is combined with Cramer distance " between " mini-batches . The transport cost ( used by the Sinkhorn ) is learned in an adversarial fashion . Experimental results on CIFAR dataset supports the usefulness of the method .

The paper is well - written and experimental results are supportive ( state -of - the -art ?)

A major practical concern with the proposed method is the size of mini-batch . In the experiment , the size is increased to 8000 instances for stable training . To what extent is this a problem with large models ? The paper does not investigate the effect of small batch -size on the stability of the method . Could you please comment on this ?

Another issue is the adversarial training of the transport cost . Could you please explain why this design choice cannot lead instability ?


Regarding the two issues your raise :

* In section 5.2 we present an experiment on CIFAR - 10 where we vary the batch size used in training OT - GAN . As shown in Figure 4 , small batch sizes are less stable during training , although the results are still on par with previous work on GANs . For large models we can reach the large batch sizes required for optimal results by using more GPUs and / or GPUs with more memory . Many of the other recent SOTA works in GANs use even more time and compute than we do , but it is indeed a limitation of the method as clearly indicated in the paper . In the updated paper we have expanded our discussion of this experiment , the causes behind its result , and its practical importance .

* Our transport cost depends on a critic neural network v , which embeds the images ( generated and real ) into a latent space . As long as this embedding is one - to- one / non - degenerate , the statistical consistency guarantees associated with minimizing energy distance carry over to our setting . When the critic v is learned adversarially , the embedding could potentially degenerate and we could lose these properties . In practice , we manage to avoid this by updating the generator more often than the critic . If the critic maps two distinct inputs to similar embeddings , the generator will take advantage of this , thereby putting pressure on the critic to adapt the embedding . An alternative solution we tried is to parameterize the critic using a RevNet ( Gomez et al. 2017 ) : This way the mapping is always one - to- one by construction . Although this also works , we found it to be unnecessary when updating the generator often enough . The updated paper includes additional discussion on this point , and it also includes a new experiment ( appendix C ) further investigating the importance of adversarially learning the transport cost .


There have recently been a set of interesting papers on adapting optimal transport to GANs . This makes a lot of sense . The paper makes some very good connections to the state of the art and those competing approaches . The proposal makes sense from the generative standpoint and it is clear from the paper that the key contribution is the design of the transport cost . I have two main remarks and questions .

* Regarding the transport cost , the authors say that the Euclidean distance does not work well . Did they try to use normalised vectors with the squared Euclidean distance ? I am asking this question because solving the OT problem with cost defined as in c_eta is equivalent to using a * normalized squared * Euclidean distance in the feature space defined by v_eta . If the answer is yes and it did not work , then there is indeed a real contribution to using the DNN . Otherwise , the contribution has to be balanced . In either case , I would have been happy to see numbers for comparison .

* The square mini batch energy distance looks very much like a maximum mean discrepancy criterion ( see the work of A. Gretton ) , up to the sign , and also to regularised approached to MMD optimisation ( see the paper of Kim , NIPS ' 16 and references therein ) . The MMD is the solution of an optimisation problem which , I suppose , has lots of connections with the dual Wasserstein GAN . The authors should elaborate on the relationships , and eventually discuss regularisation in this context .



Thanks for your review ! We 're currently working on updating the paper , but I wanted to send you a quick reply regarding the two points you have raised :

* We have now run exactly the experiment you suggest , using cosine distance in the pixel space ( or equivalently squared Euclidean distance with normalized vectors ) instead of in the critic-space defined by v_eta . The maximum inception score we were able to achieve using this setup on CIFAR - 10 was 4.93 ( compared to 8.47 achieved using the DNN critic v_eta ) . You can download the corresponding samples here : https://www.dropbox.com/s/e27uqj6ah7j9avq/sample_pixel_space.png?dl=1 We will include the results of this experiment in the upcoming update of the paper .

* There is indeed a close connection between energy distance and MMD . When the energy distance is generalized to other distances between individual samples it becomes equivalent to MMD . This is explained in the following work , among other places : Sejdinovic , Dino , Bharath Sriperumbudur , Arthur Gretton , and Kenji Fukumizu . " Equivalence of distance - based and RKHS - based statistics in hypothesis testing . " The Annals of Statistics ( 2013 ) : 2263-2291.
The novel part of our proposed minibatch energy distance is that it further generalizes the energy distance from individual samples to minibatches . This makes it conceptually different from the existing literature , including Kim , Been , Rajiv Khanna , and Oluwasanmi O. Koyejo . " Examples are not enough , learn to criticize ! Criticism for interpretability . " In Advances in Neural Information Processing Systems , pp. 2280-2288. 2016 . ( please let us know if you were referring to a different paper ) Equivalently we can say our minibatch energy distance generalizes MMD from individual samples to minibatches , but we choose to take the energy distance perspective as it more closely connects to other work in this area ( e.g. Cramer - GAN ) . We will include this discussion + the references in the upcoming update .



The paper proposes a regularizer that encourages a GAN discriminator to focus its capacity in the region around the manifolds of real and generated data points , even when it would be easy to discriminate between these manifolds using only a fraction of its capacity , so that the discriminator provides a more informative signal to the generator . The regularizer rewards high entropy in the signs of discriminator activations . Experiments show that this helps to prevent mode collapse on synthetic Gaussian mixture data and improves Inception scores on CIFAR10 .

The high - level idea of guiding model capacity by rewarding high - entropy activations is interesting and novel to my knowledge ( though I am not an expert in this space ) . Figure ` 1 is a fantastic illustration that presents the core idea very clearly . That said I found the intuitive story a little bit difficult to follow -- it 's true that in Figure 1 b the discriminator wo n't communicate the detailed structure of the data manifold to the generator , but it 's not clear why this would be a problem -- the gradients should still pull the generator * towards * the manifold of real data , and as this happens and the manifolds begin to overlap , the discriminator will naturally be forced to allocate its capacity towards finer - grained details . Is the implicit assumption that for real , high - dimensional data the generator and data manifolds will * never * overlap ? But in that case much of the theoretical story goes out the window . I 'd also appreciate further discussion of the relationship of this approach to Wasserstein GANs , which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap .

More generally I 'd like to better understand what effect we 'd expect this regularizer to have . It appears to be motivated by improving training dynamics , which is understandably a significant concern . Does it also change the location of the Nash equilibria ? ( or equivalently , the optimal generator under the density - ratio-estimator interpretation of discriminators proposed by https://arxiv.org/abs/1610.03483). I 'd expect that it would but the effects of this changed objective are not discussed in the paper .

The experimental results seem promising , although not earthshattering . I would have appreciated a comparison to other methods for guiding discriminator representation capacity , e.g. autoencoding ( I 'd also imagine that learning an inference network ( e.g. BiGAN ) might serve as a useful auxiliary task ? ) .

Overall this feels like an cute hack , supported by plausible intuition but without deep theory or compelling results on real tasks ( yet ) . As such I 'd rate it as borderline ; though perhaps interesting enough to be worth presenting and discussing .

A final note : this paper was difficult to read due to many grammatical errors and unclear or misleading constructions , as well as missing citations ( e.g. sec 2.1 ) . From the second paragraph alone :
" impede their wider applications in new data domain " -> domains
" extreme collapse and heavily oscillation " -> heavy oscillation
" modes of real data distribution " -> modes of the real data distribution
" while D fails to exploit the failure to provide better training signal to G " -> should be " this failure " to refer to the previously - described generator mode collapse , or rewrite entirely
" even when they are their Jensen - Shannon divergence " -> even when their Jensen - Shannon divergence
I 'm sympathetic to the authors who are presumably non- native English speakers ; many good papers contain mistakes , but in my opinion the level in this paper goes beyond what is appropriate for published work . I encourage the authors to have the work proofread by a native speaker ; clearer writing will ultimately increase the reach and impact of the paper .


Thank you for your detailed feedbacks and suggestions .

We have made improvements in the presentation of the paper in its new version . In particular Sec. 2 is rewritten to discuss the effects of the regularizer with better clarity . We also have more compelling and comprehensive experimental results supporting the intuition . Please see the summary of change about the updates in the experiment section .

To address your specific questions / comments :
1 . “ it 's true that in Figure 1 b the discriminator wo n't communicate the detailed structure of the data manifold to the generator , but it 's not clear why this would be a problem -- [ … ] "

Ideally , when GAN training is stable , the min-max game eventually forces D to represent subtle variations in the real data distribution and passes the information to G . But when the internal representation of D is degenerate , two problems happen : 1 ) G under - explores , as all training signals from D could be co-linear if the fake points are in one linear region . It is unclear if G can always recover from collapsed mass caused by this . It is much more desirable for G to better explore the space from the beginning . And even if G could recover , the convergence is slowed down due to the need to correct initial mistakes . 2 ) large linear regions could cause the learning of G to bluntly extrapolate , resulting in large updates , which in turn drop already discovered real data modes and / or lead to oscillations . Both of these intuitions are captured in the updated 2D synthetic plots , as well as more detailed frames in Fig. 12 and 13 . Furthermore , our updated results on the convergence speed for DCGAN confirms that improved initial exploration makes convergence faster even if the dynamics were stable already .


2 . “ location of Nash equilibrium ”

The locations of the Nash equilibria would change . Since D is assigned a different reward objective , there is no reason to believe that D would still have the same values for the Nash equilibria . Annealing the coefficients of the regularizer may be able to maintain these locations ( which depends on the uniform convergence property of the problem and the annealing strategy , which is beyond the scope of this paper . ) . Our preliminary studies for annealing the regularization coefficients produced marginally inferior Inception score . We have not explored different annealing strategies yet in the experiments .

3 “ discussion and comparison to Wasserstein GAN ”

Thank you for the suggestion . We ’ve added a discussion in Sec. \ 2.1 on other WGAN - GP and other methods that regularize gradient norm . We ’ve also added comprehesive comparison in the experiments ( Table 1 . ) showing that BRE outperforms WGAN - GP on all architectures tested .

4 . “ auxiliary tasks ”

Indeed certain auxiliary tasks can regularize GAN training . For example , predicting image classes in semi-supervised learning GAN . BRE regularizer is compatible with semi-supervised GAN as well , and as shown in Sec. 4.3 , can further improve the results .

We tested out reconstructing real data as auxiliary task to regularize D , and found that it worsens results consistently . A brief discussion is added in Sec. 5 ( DISCUSSION AND FUTURE WORK ) , and results are shown in a table in the Appendix . We believe this is an interesting direction worth further experimentations and analysis in future work . There are a few other GAN works that use auto-encoder , such as Energy - Based GAN ( Zhao et al. , 2016 ) and Boundary Equilibrium GAN ( Berthelot et al. , 2017 ) , or learning an inference network as the reviewer suggested , ( Donahue et al. , 2016 ; Dumoulin et al. , 2016 ) . It is unclear if their benefits stem from the regularization effects or the fact that other parts of GAN ( such as the objective ) are modified . We added a disccussion about this in Sec. 2.1.


The paper proposed a novel regularizer that is to be applied to the ( rectifier ) discriminators in GAN in order to encourage a better allocation of the " model capacity " of the discriminators over the ( potentially multi-modal ) generated / real data points , which might in turn helps with learning a more faithful generator .

The paper is in general very well written , with intuitions and technical details well explained and empirical studies carefully designed and executed .

Some detailed comments / questions :

1 . It seems the concept of " binarized activation patterns " , which the proposed regularizer is designed upon , is closely coupled with rectifier nets . I would therefore suggest the authors to highlight this assumption / constraint more clearly e.g. in the abstract .

2 . In order for the paper to be more self - contained , maybe list at least once the formula for " rectifier net " ( sth. like " a^T max ( 0 , wx + b ) + c " ) ? This might also help the readers better understand where the polytopes in Figure 1 come from .

3 . In section 3.1 , when presenting random variables ( U_1 , ... , U_d ) , I find the word " Bernourlli " a bit misleading because typically people would expect U_i to take values from {0 , 1 } whereas here you assume { - 1 , + 1} . This can be made clear with just one sentence yet would greatly help with clearing away confusions for subsequent derivations .
Also , " K " is already used to denote the mini-batch size , so it 's a slight abuse to reuse " k " to denote the " kth marginal " .

4 . In section 3.2 , it may be clearer to explicitly point out the use of the " 3 - sigma " rule for Gaussian distributions here . But I do n't find it justified anywhere why " leave 99.7 % of i , j pairs unpenalized " is sth . to be sought for here ?

5 . In section 3.3 , when presenting Corollary 3.3 of Gavinsky & Pudlak ( 2015 ) , " n " abruptly appears without proper introduction / context .

6 . For the empirical study with 2D MoG , would an imbalanced mixture make it harder for the BRE - regularized GAN to escape from modal collapse ?

7 . Figure 3 is missing the sub-labels ( a ) , ( b ) , ( c ) , ( d ) .

Thank you for your insightful suggestions .

We have made improvements to the presentation of the paper according to the comments .

“ For the empirical study with 2D MoG , would an imbalanced mixture make it harder for the BRE - regularized GAN to escape from modal collapse ? ”

Thank you for the suggestion . We have added one more set of results for imbalanced mixture distributions in the appendix of the revised paper . We find that on imbalanced mixture distributions , BRE - regularized GAN can still discover the support of infrequent modes most of the time , however , sometimes the probability mass assigned to those modes is not correct ( usually under represented ) .

The paper presents a method for improving the diversity of Generative Adversarial Network ( GAN ) by promoting the Gnet 's weights to be as informative as possible . This is achieved by penalizing the correlation between responses of hidden nodes and promoting low entropy intra node . Numerical experiments that demonstrate the diversity increment on the generated samples are shown .

Concerns .

The paper is hard do tear and it is deficit to identify the precise contribution of the authors . Such contribution can , in my opinion , be summarized in a potential of the form

with

$ $
R_BRE = a R_ME + b R_AC = a \sum_k \sum_i s_{ki}^2 + b \sum_{ < k , l >} \sum_i \{ s_{ki} s_{li} \}
$ $
( Note that my version of R_ME is different to the one proposed by the authors , but it could have the same effect )

Where a and b are parameters that weight the relative contribution of each term ( maybe computed as suggested in the paper ) .

In this formulation :

Then R_ME has a high response if the node has saturated responses - 1 ’s or 1``s , as one desire such saturated responses , a should be negative .

The R_AC , penalizes correlation between responses of different nodes .

The point is ,

a ) The second term will introduce low correlation in saturated vectors , then the will be informative .

b ) why the authors use the softsign instead the tanh : $ tahnh \in C^2 $! Meanwhile the derivative id softsign is discontinuous .

c ) It is not clear is the softsign is used besides the activation function : In page 5 is said “ R_BRE can be applied on ant rectified layer before the nolinearity ” . This seems tt the authors propose to add a second activation function ( the softsign ) , why not use the one is in teh layer ?

d ) The authors found hard to regularize the gradient $ \nabla_x D ( x ) $ , even they tray tanh and cosine based activations . It seems that effectively , the introduce their additional softsign in the process .

e ) En the definition of R_AC , I denoted by < k , l> the pair of nodes ( k \ne l ) . However , I think that it should be for pair in the same layer . It is not clear in the paper .

f ) It is supposed that the L_1 regularization motes the weights to be informative , this work is doing something similar . How is it compared the L_1 regularization vs . the proposal ?

Recommendation
I tried to read the paper several times and I accept that it was very hard to me . The most difficult part is the lack of precision on the maths , it is hard to figure out what the authors contribution indeed are . I think there is some merit in the work . However , it is not very well organized and many points are not defined . In my opinion , the paper is in a preliminary stage and should be refined . I recommend a “ SOFT ” REJECT


Thank you for your comments . We have improved the explanation about the motivation of this regularizer , and the math presentation of its formal definition in the revised paper . We believe that there are a few misunderstandings of our method , and we will clarify them and address the reviewer ’s questions below .

** Concerning the use of softsign : **

“ ( b ) why the authors use the softsign instead the tanh : $ tanh \in C^2 $! Meanwhile the derivative id softsign is discontinuous . ”
Using the softsign function to replace the sign function is to prevent null computation when $ $ h=0 $ $ . Theoretically tanh with high temperature could also work . In the revised paper , we have also included the experimental results when using tanh , which shows decreased effectiveness in regularizing the GAN training comparing to softsign . We believe the reason why our version softsign is better empirically than tanh when used in BRE is due to the scale- invariance achieved by the adaptive \epsilon . This is discussed at the end of the first paragraph in Sec. 3.3.
Also , the derivative of the softsign function is continuous ( although its 2nd order derivative is not continuous at a single point , but does not really matter for SGD ) .

“ c ) It is not clear is the softsign is used besides the activation function : In page 5 is said “ R_BRE can be applied on ant rectified layer before the nolinearity ” . This seems tt the authors propose to add a second activation function ( the softsign ) , why not use the one is in teh layer ? ”
No , we compute the value of R_BRE from the immediate pre-nonlinearity layer , and add this value to the objective function . The nonlinearity of the networks are not changed . We have added a figure ( Figure 2 ) in the revised paper to clarify this .

“ d ) The authors found hard to regularize the gradient $ \nabla_x D ( x ) $ , even they tray tanh and cosine based activations . It seems that effectively , the introduce their additional softsign in the process . ”
Regularizing the diversity of $ \nabla_x D ( x ) $ is a straightforward naive approach if we want rich diverse training signal for G. However , this does not work for rectifier nets , for reason analysed in Sec. 5 ( Discussion ) , and hence one of the significance of our contribution . The use of softsign is unrelated to this issue .

** Reviewer ’s interpretation of the regularizer and potential confusion about its effect : **

“ in my opinion , be summarized in a potential of the form with
$ $ R_{BRE} = a R_{ME} + b R_{AC} = a \sum_k \sum_i s_{ki}^2 + b \sum_{ < k , l >} \sum_i \{ s_{ki} s_{li} \} $ $
“

Our formulation of BRE is :
$ $ R_{ME} = \ frac{1}{d}\sum_{k= 1 }^d \ bar{s}_{ ( k ) } ^2 = \frac{1}{d}\sum_{k=1 }^d \ frac{1}{ K^2}( \sum_{i=1}^{K}s_{k , i} ) ^2 $ $
$ $ R_{AC} = \text{avg}_{i \neq j} | s_{i}^{T}s_{j}| / d = \frac{1 } { K ( K - 1 ) } \sum_{i\neq j} | \sum_{l=1 }^d s_{il}s_{jl}| / d. $ $

Let RRME denote the RME term proposed in the reviewer ’s comment , and RME be this term in the paper . Similarly let RRAC and RAC denote the RAC term in the comment and in the paper , respectively . The motivation of our regularizer is to encourage a large entropy of the activation vector on some particular layer of D , so that D would provide informative learning signals for G.

The activation vector s is a binary vector ( each element is either + 1 or - 1 ) , computed from a sign function . So $ $ s_{k , i}^2 $ $ will always be 1 in RRME and this term will be ineffective . On the other hand , RME encourages $ $ \sum_{i=1 }^ { K}s_{k , i} $ $ to be 0 , i.e. zero mean for the k-th hidden unit . We would like to encourage this zero-mean property for the purpose of increasing its entropy ( and thus $ $ a$ $ is set to be positive ) .

For the second term , note that RAC has an absolute value on the correlation of s_i and s_j ( they are approximately zero-mean because of RME ) , which encourage s_i and s_j to be independent . Both positive correlation and negative correlation are penalized . Thus again RAC encourage a large entropy . On the other hand , penalizing RRAC is actually encouraging s_k and s_l to be negatively correlated . The minimal value of the inner product of s_k and s_l is -d . However , it is impossible for all the pairs of ( s_k , s_l ) to simultaneously achieve this minimal value . It seems not easy to analyze when ( s_1 , … , s_K ) would achieve its minimal value on RRAC , which makes it difficult to interpret its effect .
Overall , we do n’t see that the proposed regularizer in the review has a similar effect as the BRE regularizer proposed in the paper .

“ e ) En the definition of R_AC , I denoted by < k , l> the pair of nodes ( k \ne l ) . However , I think that it should be for pair in the same layer . It is not clear in the paper . ”
Yes , that is right . The summation is over all the pairs in the same layer . We have added a footnote for the definition of R_BRE to clarify this point in the new version of the paper .

“ f ) It is supposed that the L_1 regularization motes the weights to be informative , this work is doing something similar . How is it compared the L_1 regularization vs . the proposal ? ”
Our method is to encourage D to have diverse activation patterns , so that G could have more informative signals for learning . The BRE regularizer is computed based on the pre-nonlinearity value of the hidden nodes , while L_1 regularization is applied to the weights of D . We do n’t see the connection between the L_1 regularization and the BRE regularizer at the moment .

The paper presents a method for hierarchical object embedding by Gaussian densities for lexical entailment tasks . Each word is represented by a diagonal Gaussian and the KL divergence is used as a directional distance measure . if D ( f||g ) < gamma then the concept represented by f entails the concept represented by g.

The main technical difference of the present work compared from the main prior work ( Vendrov , 2015 ) is that in addition to mean vector representation they use here also a variance component . The main modeling challenge here to to define a good directional measure that can be suitable for lexical entailment . in Vendrov work they defined a partial ordering . Here , the KL is not symmetric but its directional aspect is not significant .
For example if we set all the variances to be a unit matrix than the KL is collapsed to be a simple symmetrical Euclidean distance . We can also see from Table 1 that if we replace KL by its symmetrical variant we get similar results . Hence , I was not convinced that the propose KL + Gaussian modeling is suitable for directional relations .

The paper also presents several methods for negative samplings and according to table 4 there is a lot of performance variability based on the method that is used for selecting negative sampling . I find this component of the proposed algorithm very heuristic .

To summarize , I do n't think there is enough interesting novelty in this paper . If the focus of the paper is on obtaining good entailment results , maybe an NLP conference can be a more suitable venue .


We thank the reviewer for thoughtful comments .

To address the importance of directionality , we clarify the differences in using symmetric measures to train versus to evaluate . We also highlight and explain the major empirical differences for the purpose of order embeddings .

[ Symmetric Measure for Training ] In Section 3.3.2 : Symmetric Divergence , we describe why symmetric measures can be used to train embeddings that reflect hierarchy :

“ Intuitively , the asymmetric measures should be more successful at training density order embeddings . However , a symmetric measure can result in the encapsulation order as well since a general entity often has to minimize the penalty with many specific elements and consequently ends up having a broad distribution . ”

For further explanation , consider a scenario where we have true relationships a -> c and b -> c and use a symmetric training measure to train the embeddings ( x -> y denotes x entails y ) . The desired outcome is such that the distribution of ‘c ’ encapsulates the distribution of ‘ a ’ as well as ‘ b ’ . To satisfy this , the distribution of ‘c ’ ends up being broad and encompass both ‘ a ’ and ‘ b ’. to lower the average loss .

We emphasize that at evaluation time , if we use symmetric measures to generate entailment scores , we would end up discarding the directionality and thus hurt the performance . This brings us to the next point , where we discuss the empirical differences , underscoring the foundational importance of an asymmetric measure .

[ KL for directionality ]

The directionality of KL is crucial to capture hierarchical information . In our experiments , when our model is trained with the symmetric ELK distance , using ELK to generate encapsulation scores results in poor performance of 0.455 whereas using KL yields 0.532 . This difference is described in section 4.4.

With regards to the performance of ELK in Table 1 , negative log ELK , a symmetric distance metric , can generate meaningful scores for word pairs that do not have directional relationships , such as ‘ animal ’ and ‘ location ’ . This is because these non-relationship density pairs do not significantly overlap and ELK - based metric would yield high distance value . However , this metric is not suitable for capturing directionality of relationships such as ‘ animal ’ and ‘ dog ’ .

For example , Table 2 which illustrates the values of KL divergences between many word pairs , underscores the foundational importance of KL ’s directionality . For instance , KL ( ‘object ’ | ‘ physical_entity ’ ) = 152 , whereas the reverse case KL ( ‘ physical_entity ’ | ‘ object ’ ) = 6618 . Based on the prediction threshold of 1900 ( selected via validation set ) , we correctly predict that ‘ object ’ entails ‘ physical_entity ’ ( since 152 << 1900 ) but ‘ physical_entity ’ does not entail ‘ object ’ ( 6618 >> 1900 ) . The entailment score function ( negative KL ) also nicely assigns high scores to the true hypernym pairs such as ( ‘object ’ -> ‘ physical_entity ’ , with score - 152 ) and low scores for the non-hypernym pairs such as ( ‘ physical_entity ’ -> ‘ object ’ , with score - 6618 ) . This behavior is important to measure the degree of lexical entailment . If the evaluation divergence were symmetric , ( ‘object ’ -> ‘ physical_entity ’ ) and ( ‘ physical_entity ’ -> ‘ object ’ ) would have the same score , which is very undesirable , since these pairs do not have the same degree of entailment .

[ unit variance ] It is true that if the variance components are all the same ( being 1 ) , the KL becomes symmetric . However , our learned distributions tend to have very different covariance matrices . Figure 4 , the log det ( Sigma ) vary markedly among different concepts . The concepts that are more general such as `physical_entity ’ have very high log ( det ( Sigma ) ) of - 219.67 compared to specific concepts such as ‘ city ’ with log ( det ( Sigma ) ) = - 261.89 .



[ novelty ] Aside from the foundational importance of asymmetry in divergences for probabilistic order embeddings , there is much other interesting novelty in the paper too . We propose new training procedures that learn highly effective Gaussian embeddings in the supervised setting . The new changes include ( 1 ) using max-margin loss ( Equation 7 ) instead of rank loss ( Equation 1 ) ; ( 2 ) using a divergence threshold to induce soft partial orders and prevent unnecessary penalization ; ( 3 ) a new scheme to select negative samples for max- margin loss ; ( 4 ) investigating other hyperparameters that are important for density encapsulation such as adjusting variance scale ; ( 5 ) proposing and investigating general alpha divergences as metrics between word densities . This direction is significant — analogous to exploring Renyi divergences instead of variational KL divergences for approximate Bayesian inference .

In general , despite the great promise of probabilistic embeddings , such approaches have not been widely explored , and their use in order embeddings -- where they are perhaps most natural -- is essentially uncharted territory .

The empirical results are generally quite strong . Not all results are positive ( for example , we found alpha divergences to not improve on KL ) , but there is certainly great value in honestly reporting conceptually interesting experiments , especially if there are negative results , which tend to go under-reported . Many of the results ( as discussed above ) are also positive .

[ Effects of new negative sampling ] Note that the traditional approach of Vendrov 2015 uses the sampling scheme S1 . From Table 4 , using S1 alone results in the HyperLex score of at most 0.527 . However , using our proposed approaches ( S1 together with S2 , S3 , S4 with different heuristic combination ) results in a significant increase in performance in most ( if not all ) cases , allowing us to achieve a score of 0.590 ( ~ 12 % increase ) . We would like to point out that this is a strength of our proposed negative sampling methods ( S2 , S3 , S4 ) since most combinations provide an increase in performance .


The paper presents a study on the use of density embedding for modeling hierarchical semantic relations , and in particular on the hypernym one . The goal is to capture hypernyms of some synsets , even if their occurrence is scarce on the training data .
+++pros : 1 ) potentially a good idea , capable of filling an ontology of relations scarcely present in a given repository 2 ) solid theoretical background , even if no methodological novelty has been introduced ( this is also a cons ! )
---cons : 1 ) Badly presented : the writing of the paper fails in let the reader aware of what the paper actually serves

COMMENTS :
The introduction puzzled me : the authors , once they stated the problem ( the scarceness of the hypernyms ' occurrences in the texts w.r.t. their hyponyms ) , proposed a solution which seems not to directly solve this problem . So I suggest the authors to better explain the connection between the told problem and their proposed solution , and how this can solve the problem .

This aspect is also present in the experiments section , since it is not possible to understand how much the problem ( the scarceness of the hypernyms ) is present in the HYPERLEX dataset .

How the 4000 hypernyms have been selected ? Why a diagonal covariance has been estimated , and not a full covariance one ?

n Figure 4 middle , it is not clear whether the location and city concepts are intersecting the other synsets . It should n't be , but the authors should spend a little on this .

Apart from these comments , I found the paper interesting especially for the big amount fo comparisons carried out .

As a final general comment , I would have appreciated a paper more self explanative , without referring to the paper [ Vilnis & McCallum , 2014 ] which makes appear the paper a minor improvement of what it is actually .

Thank you for your thoughtful comments and your interest in the paper . Please see our responses below .

[ Clarification on Introduction ]

Indeed the scarcity of lexical relationships in natural text corpus is not the problem we aim to solve in this paper . We brought this up to emphasize the importance of modeling hierarchical data via * supervised * learning directly on hierarchical data , without relying on word occurrence patterns in the unsupervised approach . Note that in the unsupervised setting , the Gaussian distributions are trained based on word occurrences in natural sentences . In the supervised setting , we model the hierarchical data by minimizing the loss directly on relationship pairs .

The supervised learning of Gaussian densities has not been thoroughly considered in the existing literature . The main goal of our paper is to investigate this highly consequential task . Vilnis ( 2015 ) proposed a Gaussian embedding model that works well for unsupervised task ( semantic word embeddings ) . While the approach can be directly applied to supervised case , the performance is often quite poor based on results reported in Vendrov ( 2015 ) and Vuli\ 'c ( 2016 ) . Our paper , on the other hand , shows the opposite finding that the performance of Gaussian embeddings can be highly competitive so long as we use our different new training approach . We would like to emphasize the significance of this direction : despite their intuitive benefits in providing rich representations for words , probabilistic word embeddings are relatively unexplored . Such embeddings are only considered in a small handful of papers , and in these papers there is no serious consideration of where these embeddings would be most natural , such as in ordered representations .

We would also like to emphasize that we do introduce new methodology in our paper . We propose new training procedures that learn highly effective Gaussian embeddings in the supervised setting . The new changes include ( 1 ) using max-margin loss ( Equation 7 ) instead of rank loss ( Equation 1 ) ; ( 2 ) using a divergence threshold to induce soft partial orders and prevent unnecessary penalization ; ( 3 ) a new scheme to select negative samples for max- margin loss ; ( 4 ) investigating other hyperparameters that are important for density encapsulation such as adjusting variance scale ; ( 5 ) proposing and investigating general alpha divergences as metrics between word densities . This direction is significant — analogous to exploring Renyi divergences instead of variational KL divergences for approximate Bayesian inference .
We thank the reviewer for the questions / comments and we have modified the introduction to make our contributions more clear .

[ Hyperlex ] HyperLex is an evaluation dataset where the instances in HyperLex have some lexical relationships . Instances that have true hypernym relationships have high scores , and instances
without true hypernym relationships have lower scores .

[ Test Set ] 4000 Hypernym pairs are selected randomly from the transitive closure of WordNet . The random split is 4000 for validation set , 4000 for test set , and the rest for training .

[ Diagonal Covariance ] The diagonal covariance enables fast training . The complexity of the objective for
d-dimensional is O( d ) for diagonal case as opposed to O ( d^3 ) in the full covariance case .
This is due to the inverse term in our divergences . Note we use a full diagonal covariance versus a scaled identity . Relative to standard word embeddings , such as word2vec , a probabilistic density , even with a diagonal covariance , is highly flexible .

[ Figure 4 ] Yes , we can see that ‘ location ’ and ‘ living_thing ’ are visually overlapping . However ,
KL ( ‘location ’ | ‘ living_thing ’ ) = 6794 and KL ( ‘ living_thing ’ | ‘ location ’ ) = 4324 , which is higher than the threshold of 1900 ( picked using validation set ) , which means that we do not predict that these words entail one another in either direction . There are some mistakes , however , such as for ‘ location ’ | ‘ object ’ , which can happen when there are not enough negative examples of the pair ( ‘ location ’ ( not ) > ‘ object ’ ) to contrast in the training . Our proposed negative sampling approach helps alleviate this problem .

The paper introduces a novel method for modeling hierarchical data . The work builds on previous approaches , such as Vilnis and McCallum 's Word2 Gauss and Vendrov 's Order Embeddings , to establish a partial order over probability densities via encapsulation , which allows it to model hierarchical information . The aim is to learn embeddings from supervised structured data , such as WordNet . The work also investigates various schemes for selecting negative samples . The evaluation consists of hypernym detection on WordNet and graded lexical entailment , in the shape of HyperLex . This is good work : it is well written , the experiments are thorough and the proposed method is original and works well .

Section 3 could use some more signposting . Especially for 3.3 it would be good to explain ( either at the beginning of section 3 , or the beginning of section 3.3 ) why these measures matter and what is going to be done with them .

It 's good that LEAR is mentioned and compared against , even though it was very recently published . Please do note that the authors ' names are misspelled : Vuli\ 'c not Vulic , Mrk\v{s}i\ 'c instead of Mrksic .

If I am not mistaken , the Vendrov Word Net test set is a set of positive pairs . I would like to see more details on how the evaluation is done here : presumably , the lower I set the threshold , the higher my score ? Or am I missing something ?

It would be useful to describe exactly the extent to which supervision is used - the method only needs positive and negative links , and does not require any additional order information ( i.e. , Word Net strictly contains more information than what is being used ) .

I do n't see what Socher et al. ( 2013 ) has to do with the loss in equation ( 7 ) . Or did they invent the margin loss ?

Word2 gauss also evaluates on similarity and relatedness datasets . Did you consider doing that here too ?

" hypothesis proposed by Santus et al. which says " is not a valid reference .

Thank you for your thoughtful and supportive comments . Please see our responses below .

[ Misspelled names and reference errors ] Thank you for pointing this out . We have corrected the spelling and reference errors .

[ Equation 7 ] Socher et al. 2013 and Vendrov 2015 uses this loss in their tasks .

[ Effects of Divergence Threshold ]

Vendrov ’s Word Net test set is a test of 4000 positive pairs as well as 4000 negative pairs , where the negative pairs are selected using random sampling . We fix the same test set across all experiments .

In the Appendix , Section A.2.1 , we show the effects of the divergence threshold ( gamma ) on the test accuracy . Figure 5 shows that there is an optimal gamma value which yields the best scores on hypernym prediction and lexical entailment . The intuition is as follows : zero gamma corresponds to penalizing any two distributions that do not perfectly overlap ( since D is the lowest if and only if the two distributions are equal ) . This behaviour is undesirable : if a distribution is correctly encapsulated by a parent distribution we should not further penalize the embeddings . High gamma corresponds to low penalization among many distribution pairs — a gamma value that is “ too high ” is lenient , since there might not be sufficient penalization in the loss function to help learn optimal embeddings .

[ Similarity and Relatedness ]

We believe that similarity and relatedness would be more suitable for word embeddings trained on word occurrences in a natural corpus ( word2vec , GloVe , word2 gauss ) , because our embeddings model the hierarchical structure rather than the semantics of concepts . In Section 5 , we discussed the future direction where we plan to use the idea to enhance word embeddings ( Gaussian word distributions ) by incorporating the supervised training ( using labels from WordNet , for instance ) with the unsupervised training on text corpus . The ideal scenario would be that the Gaussian embeddings would have high word similarity scores and also exhibit hierarchical structure that yields good hypernym prediction and graded lexical entailment scores .


NOTE :
I 'm very willing to change my recommendation if I turn out to be wrong
about the issues I 'm addressing and if certain parts of the experiments are fixed .

Having said that , I do ( think I ) have some serious issues :
both with the experimental evaluation and with the theoretical results .
I 'm pretty sure about the experimental evaluation and less sure about the theoretical results .


THEORETICAL CLAIMS :

These are the complaints I 'm not as sure about :

Theorem 1 assumes that L is convex / concave .
This is not generally true for GANs .
That 's fine and it does n't necessarily make the statement useless , but :

If we are willing to assume that L is convex / concave ,
then there already exist other algorithms that will provably converge
to a saddle point ( I think ) . [ 1 ] contains an explanation of this .
Given that there are other algorithms with the same theoretical guarantees ,
and that those algorithms do n't magically make GANs work better ,
I am much less convinced about the value of your theorem .

In [ 0 ] they show that GANs trained with simultaneous gradient descent are locally asymptotically stable ,
even when L is not convex / concave .
This seems like it makes your result a lot less interesting , though perhaps I 'm wrong to think this ?

Finally , I 'm not totally sure you can show that simultaneous gradient descent wo n't converge
as well under the assumptions you made .
If you actually ca n't show that , then the therom * is * useless ,
but it 's also the thing I 've said that I 'm the least sure about .


EXPERIMENTAL EVALUATION :

Regarding the claims of being able to train with a higher learning rate :
I would consider this a useful contribution if it were shown that ( by some measure of GAN ' goodness ' )
a high goodness was achieved faster because a higher learning rate was used .
Your experiments do n't support this claim presently , because you evaluate all the models at the same step .
In fact , it seems like both evaluated Stacked GAN models get worse performance with the higher learning rate .
This calls into question the usefulness of training with a higher learning rate .
The performance is not a huge amount worse though ( based on my understanding of Inception Scores ) ,
so if it turns out that you could get that performance
in 1 /10th the time then that would n't be so bad .

Regarding the experiment with Stacked GANs , the scores you report are lower than what they report [ 2 ] .
Their reported mean score for joint training is 8.59 .
Are the baseline scores you report from an independent reproduction ?
Also , the model they have trained uses label information .
Does your model use label information ?
Given that your reported improvements are small , it would be nice to know what the proposed mechanism is by
which the score is improved .
With a score of 7.9 and a standard deviation of 0.08 , presumably none of the baseline model runs
had ' stability issues ' , so it does n't seem like ' more stable training ' can be the answer .

Finally , papers making claims about fixing GAN stability should support those claims by solving problems
with GANs that people previously had a hard time solving ( due to instability ) .
I do n't believe this is true of CIFAR10 ( especially if you 're using the class information ) .
See [ 3 ] for an example of a paper that does this by generating 128x128 Imagenet samples with a single generator .

I did n't pay as much attention to the non-GAN experiments because
a ) I do n't have as much context for evaluating them , because they are a bit non-standard .
b ) I had a lot of issues with the GAN experiments already and I do n't think the paper should be accepted unless those are addressed .


[ 0 ] https://arxiv.org/abs/1706.04156 ( Gradient Descent GAN Optimization is Locally Stable )

[ 1 ] https://arxiv.org/pdf/1705.07215.pdf ( On Convergence and Stability of GANs )

[ 2 ] https://arxiv.org/abs/1612.04357 ( Stacked GAN )

[ 3 ] https://openreview.net/forum?id=B1QRgziT ( Spectral Regularization for GANs )

EDIT :
As discussed below , I have slightly raised my score .
I would raise it more if more of my suggestions were implemented ( although I 'm aware that the authors do n't have much ( any ? ) time for this - and that I am partially to blame for that , since I did n't respond that quickly ) .
I have also slightly raised my confidence .
This is because now I 've had more time to think about the paper , and because the authors did n't really address a lot of my criticisms ( which to me seems like evidence that some of my criticisms were correct ) .

We agree with the reviewer that theory in this area ( and in deep learning in general ) often requires assumptions that do n’t hold for neural networks . Nonetheless , we think it is worth taking time to explore conditions under which algorithms are guaranteed to work , because this provides a theoretical proof - of - concept , and thinking through theoretical properties of a new algorithm makes it more than just another hack . The purpose of our result is to do just that for our proposed algorithm . We do n’t disagree that analysis exists for other algorithms , but we do n’t think the existence of other algorithms gets us “ off the hook ” from thinking about the theoretical implications of our approach .

That being said , we think the reviewer is overestimating the state of the art in theory for GANs . There is currently no theoretical result that does not make strong assumptions , and many results ( including those referenced by the reviewer ) are quite different from ( and in many ways weaker than ) our own . The result in [ 1 ] shares certain assumptions with our own ( convex - concave assumptions , bounded problem domain , and an ergodic measure of convergence ) . However , the result in [ 1 ] does not prove convergence in the usual sense , but rather that the error will decay to within an o ( 1 ) constant . In contrast , our result shows that the error decays to zero . The result in [ 1 ] also requires simultaneous gradient descent , which is not commonly used in practice ( because it requires more RAM to store [ extremely large ] iterates and it uses a stale iterate when updating the generator and discriminator one - at - a- time ) . In contrast , our result concerns the commonly used alternating direction approach .
The result in [ 0 ] shows stability using a range of assumptions that are different from ( but not necessarily stronger or weaker than ) our own . They require the discriminator to be a linear classifier , and make a strict concavity assumption on the loss function . They also require an assumption ( called Property I ) that is analogous to the “ strict saddle ” assumption in the saddle - point literature ( see , e.g. Lee 2016 , “ Gradient Descent Converges to Minimizers ” ) , which is known not to hold for general neural nets . Also , note that the result in [ 0 ] is only a local stability result ( it only holds once the iterates get close to a saddle satisfying the assumptions ) , whereas our result is a global convergence result that holds for any initialization .
Finally , we emphasize that both [ 0 ] and [ 1 ] are great works that make numerous important contributions to this field and address a host of issues beyond just convergence proofs . Our purpose here is not to make any claims that our result is “ better ” than theirs , but rather to state what differentiates our result from the literature , and why we felt it was worth putting it in the paper .

The purpose of the experiments is not to show that we can train things that are *impossible * to train via other methods ( indeed , almost anything is possible if you tune the hyperparameters and network architecture enough ) , but rather that prediction makes really difficult problems really easy . Compared to simple alternating gradient methods , prediction methods are more stable than other methods , work with a much wider range of hyperparameters than classical schemes , and do n’t suffer from the collapse phenomenon that make it difficult to use other methods .

Below , we address reviewer ’s comments that seem to pertain to specific dataset and architecture :

Regarding DCGAN experiments ( Without using label information ) :
Figure 4 uses the finely tuned learning rate and momentum parameters that come with the pre-packaged DCGAN code distribution . This figure shows that DCGAN collapses frequently ; even with these fine tuned parameters it still requires a carefully chosen stopping time / epoch to avoid collapse . With prediction it does not collapse at all . The purpose of increasing the learning rate is not to show that “ better ” results could be had , but rather to show that prediction methods do n’t require finely tuned parameters . If you have a look at the additional experiments in the appendix ( page 18 ) , we train DCGAN with a litany of different learning rate and momentum parameters . The prediction method succeeds without any collapse events in all of these cases , while non-prediction is unstable as soon as we move away from the carefully tuned parameter choices .

Regarding Stacked GAN ( With using label information ) :
We reproduced this experiment using the Stacked Gan author ’s publicly available code , but were not able to get the same inception scores for Stacked GAN as the original authors . Note the release code did not come with code for computing inception scores , and we used a well - known Tensor Flow implementation that may differ from what the original author ’s used .
We ran all the scenarios for a fixed number of epochs ( 200 epochs , which is default in the Stacked GAN ’s released code ) to ensure a fair comparison . Indeed , prediction method was able to achieve the best inception score of 8.83 at lesser epoch than 200 . Having said that , as per the suggestion , below we also report the performance score measured at the fewer number of epochs for higher learning rates . The quantitative comparison based on the inception score for learning rates of 0.0005 ( 200 / 5 = 40 epochs ) and 0.001 ( 200 / 10 = 20 epochs ) are as follows -

Learning Rate 0.0005 ( epochs =40 ) 0.001 ( epochs = 20 )
Stacked GAN ( joint ) 5.80 +/ - 0.15 1.42 +/ - 0.01
Stacked GAN ( joint ) + Prediction 8.10 +/ - 0.10 7.79 +/ - 0.07

Regarding the absence of problems that are “ hard ” without prediction : In Figure 8 of the appendix , we solve a toy problem that is famously hard : trying to recover all of the modes in a Gaussian mixture model . The prediction method does this easily , while the method without prediction fails to capture all the modes . We also “ turn the dial up ” on this problem by using 100 Gaussian components in Figure 9 , and the non-prediction method produces highly irregular results unless a batch size of over 6000 ( which is very much larger than the number of components ) is used . In contrast , the prediction method represents the distribution well for a wide range of batch sizes and learning rates .

Dear Reviewer ,

We have tried addressing all your concerns in our latest response , please let us know if you still have any remaining concerns ?


This paper proposes a simple modification to the standard alternating stochastic gradient method for GAN training , which stabilizes training , by adding a prediction step .

This is a clever and useful idea , and the paper is very well written . The proposed method is very clearly motivated , both intuitively and mathematically , and the authors also provide theoretical guarantees on its convergence behavior . I particularly liked the analogy with the damped harmonic oscillator .

The experiments are well designed and provide clear evidence in favor of the usefulness of the proposed technique . I believe that the method proposed in this paper will have a significant impact in the area of GAN training .

I have only one minor question : in the prediction step , why not use a step size , say
$ \bar{u}_k + 1 = u_{k + 1 } + \ gamma_k ( u_{k + 1 } − u_k ) $ , such that the " amount of predition " may be adjusted ?


Thanks for the thoughtful comments ! To answer your question : it is indeed possible to generalize this method by adding an extra stepsize parameter for the prediction step , and this is something that we have experimented with extensively . It can be shown that your proposed “ gamma ” parameter method is stable ( under convexity assumptions ) whenever gamma is between 0 and 2 . However , we have not been able to find any worthwhile advantages to choosing any gamma different from 1 . Choosing a smaller gamma weakens the stability benefits of prediction , and choosing a larger gamma seems to slow down convergence a bit . The latter effect can be compensated for by choosing a larger learning rate , but even in this case the method does n’t run noticeably faster than with gamma=1 . For this reason , including this “ gamma ” seemed like unnecessarily added complexity , so we removed it and went with a cleaner presentation .

This work proposes a framework for stabilizing adversarial nets using a prediction step . The prediction step is motivated by primal - dual algorithms in convex optimization where the term having both variables is bi-linear .

The authors prove a convergence result when the function is convex in one variable and concave in the other . This problem is more general than the previous one in convex optimization . Then this prediction step is applied in many recent applications in training adversarial nets and compared with state - of - the- art solvers . The better performance of this simple step is shown in most of the numerical experiments .

Though this work applies one step from the convex optimization to solve a more complicated problem and obtain improved performance , there is more work to be done . Whether there is a better generalization of this prediction step ? There are also other variants of primal - dual algorithms in convex optimization ; can other modification including the accelerated variants be applied ?

We thank the reviewer for the thoughtful comments and suggestions for future work . We think the idea of pursuing accelerated methods is particularly interesting . We have actually already done some experiments with Nesterov-type acceleration ( as described for saddle - point problems by Chambolle and Pock ) , however it seems that the benefits of acceleration vanish when we move from deterministic to stochastic updates . We ’ve made similar observations for standard convex ( non-saddle ) problems . That being said , we ’re still interested in this direction , and are keeping our eyes peeled for possible ways forward .

This paper studies a new architecture DualAC . The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation ( although not new , introducing this as the justification for the architecture design is plausible ) .

There are several drawbacks of the current format of the paper :
1 . The algorithm is vague . Alg 1 line 5 : ' closed form ' : there is no closed form in Eq ( 14 ) . It is just an MC approximation .
line 6 : Decay O ( 1 / t^\beta ) . This is indeed vague albeit easy to understand . The algorithm requires that every step is crystal clear .

2 . Also , there are several format error which may be due to compiling , e.g. , line 2 of Abstract , ' Dual - AC ' ( an extra space ) . There are many format errors like this throughout the paper . The author is suggested to do a careful format check .

3 . The author is suggested to explain more about the necessity of introducing path regularization and SDA . The current justification is reasonable but too brief .

4 . The experimental part is ok to me , but not very impressive .

Overall , this seems to be a nice paper to me .

Thanks for the constructive suggestions .

We modified the stepsize decay form more concretely ( line 6 of Alg 1 ) . It is adjusted based on the theoretical requirement for convergence [ 2 , 3 ]

We fixed the extra space after ` Dual - AC '.

We added more discussion of the benefits and the necessity of the path-regularization and stochastic dual ascent in the updated version in the 2nd paragraph and 3rd paragraph in page 5 , respectively . For better illustrating the necessity of path -regularization and stochastic dual ascent , we also added more empirical experiments in the ablation study part in Figure 1 .

For the experiment parts , we picked the ** best * * implementation of the state - of - the-art TRPO and PPO as our baselines based on the recent comprehensive comparison [ 1 ] . With the best implementations of TRPO and PPO , these two algorithms consistently achieve the best performance in most of the MuJoCo tasks , beating other alternatives , e.g. , DDPG and ACKTR , with significant margins in [ 1 ] . Despite such strong baselines , our Dual - AC algorithm still shows substantial gain in 5 out of 6 domains ( Fig 2 ) , with a tie in the Swimmer - v1 task . In InvertedDoublePendulum - v2 , Dual - AC achieves almost 3 x reward of TRPO and 4 x of PPO .

[ 1 ] , Deep reinforcement learning that matters . Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , David Meger. AAAI 2018 .
[ 2 ] , Robust stochastic approximation approach to stochastic programming . A Nemirovski , A Juditsky , G Lan , A Shapiro . SIAM Journal on optimization 19 ( 4 ) , 1574-1609 .
[ 3 ] , Stochastic first - and zeroth -order methods for nonconvex stochastic programming . S Ghadimi , G Lan. SIAM Journal on Optimization 23 ( 4 ) , 2341-2368.


The paper is well written , and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper . Each extension to the Dual Actor -Critic is well motivated and clear in context . Perhaps the presentation of these extensions could be improved by providing a less formal explanation of what each does in practice ; multi-step updates , regularized against MC returns , stochastic mirror descent .

The practical implementation section losses some of this clear organization , and could certainly be clarified each part tied into Algorithm 1 , and this was itself made less high - level . But these are minor gripes overall .

Turning to the experimental section , I think the authors did a good job of evaluating their approach with the ablation study and comparisons with PPO and TRPO . There were a few things that jumped out to me that I was surprised by . The difference in performance for Dual - AC between Figure 1 and Figure 2 b is significant , but the only difference seems to be a reduce batch size , is this right ? This suggests a fairly significant sensitivity to this hyperparameter if so .

Reproducibility in continuous control is particularly problematic . Nonetheless , in recent work PPO and TRPO performance on the same set of tasks seem to be substantively different than what the authors get in their experiments . I 'm thinking in particular of :

Proximal Policy Optimization Algorithms ( Schulman et. al. , 2017 )
Multi-Batch Experience Replay for Fast Convergence of Continuous Action Control ( Han and Sung , 2017 )

In both these cases the results for PPO and TRPO vary pretty significantly from what we see here , and an important one to look at is the InvertedDoublePendulum - v1 task , which I would think PPO would get closer to 8000 , and TRPO not get off the ground . Part of this could be the notion of an " iteration " , which was not clear to me how this corresponded to actual time steps . Most likely , to my mind , is that the parameterization used ( discussed in the appendix ) is improving TRPO and hurting PPO .

With these in mind I view the comparison results with a bit of uncertainty about the exact amount of gain being achieved , which may beg the question if the algorithmic contributions are buying much for their added complexity ?

Pros :
Well written , thorough treatment of the approaches
Improvements on top of Dual - AC with ablation study show improvement

Cons :
Empirical gains might not be very large


Thanks for the constructive comments .

As suggested by the reviewer , we provided further details to explain the benefits of several important extensions : path regularization ( 2nd paragraph on page 5 ) , stochastic dual ascent ( 1st paragraph of section 4.3 on page 5 ) , practical updates for policy ( the paragraphs surrounding Eqns 16 & 17 on page 7 ) and critic ( 2nd last paragraph on page 6 ) .

The gaps between Figs 1 and 2 are indeed mainly due to the batch size used in the algorithm . As expected , the batch size affects the variance of the gradients , thereby affecting the convergence of the algorithm . Such an effect is not unique to our algorithm and has been observed in the literature ; see for example similar results for the TRPO baseline in a recent empirical study [ 1 ] .

For the comparison between the TRPO and PPO , the recent empirical study [ 1 ] shows that different implementations will affect their performance a lot . Based on the evaluation results in [ 1 ] , we compared our algorithm with the ** best * * implementation of TRPO , i.e. , the original implementation by Schulman , 2015 . From Table 1 and Figure 26 in [ 1 ] , we can see that the best implementation of TRPO may achieve comparable or even better results comparing to PPO on several tasks . On the other hand , we used the same parametrization for all the algorithms , which may be preferable to TRPO . We follow [ 2 ] using the “ iteration ” in the experiments to illustrate the policy behaviors along with the number of updates in the algorithm , rather than the number of data collected for a better understanding of the algorithms in terms of each update .

Re gains of our algorithm : Since the major contribution of our paper is a new algorithm , rather than an alternative parametrization , we conduct the comparison with the baseline using the same parametrizations for fairness . We did not introduce any extra complexity in terms of parameterization . In terms of updates in algorithm , although the update rule for value function needs an extra sample reweighting , the update rule for policy is much simpler than TRPO , which requires extra adjustments for policy and related parameters . Therefore , the gains are ** not * * achieved by added complexity .

[ 1 ] , Deep reinforcement learning that matters . Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , David Meger , AAAI 2018 .
[ 2 ] , Towards generalization and simplicity in continuous control . Aravind Rajeswaran , Kendall Lowrey , Emanuel Todorov , Sham Kakade , NIPS 2017 .

This paper proposes a method , Dual - AC , for optimizing the actor ( policy ) and critic ( value function ) simultaneously which takes the form of a zero - sum game resulting in a principled method for using the critic to optimize the actor . In order to achieve that , they take the linear programming approach of solving the bellman optimality equations , outline the deficiencies of this approach , and propose solutions to mitigate those problems . The discussion on the deficiencies of the naive LP approach is mostly well done . Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regularization . They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements . Lastly , there are empirical experiments done to conclude the superior performance of Dual - AC in contrast to other actor-critic algorithms .

Overall , this paper could be a significant algorithmic contribution , with the caveat for some clarifications on the theory and experiments . Given these clarifications in an author response , I would be willing to increase the score .

For the theory , there are a few steps that need clarification and further clarification on novelty . For novelty , it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results . It looks like Theorem 2 has already been shown in " Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly - Linear Running Time ” . There is a statement that “ Chen & Wang ( 2016 ) ; Wang ( 2017 ) apply stochastic first - order algorithms ( Nemirovski et al. , 2009 ) for the one-step Lagrangian of the LP problem in reinforcement learning setting . However , as we discussed in Section 3 , their algorithm is restricted to tabular parametrization ” . Is you Theorem 2 somehow an extension ? Is Theorem 3 completely new ?

This is particularly called into question due to the lack of assumptions about the function class for value functions . It seems like the value function is required to be able to represent the true value function , which can be almost as restrictive as requiring tabular parameterizations ( which can represent the true value function ) . This assumption seems to be used right at the bottom of Page 17 , where U^{pi*} = V^* . Further , eta_v must be chosen to ensure that it does not affect ( constrain ) the optimal solution , which implies it might need to be very small . More about conditions on eta_v would be illuminating .

There is also one step in the theorem that I can not verify . On Page 18 , how is the squared removed for difference between U and Upi ? The transition from the second line of the proof to the third line is not clear . It would also be good to more clearly state on page 14 how you get the first inequality , for || V^ * ||_{2 , mu}^2.


For the experiments , the following should be addressed .

1 . It would have been better to also show the performance graphs with and without the improvements for multiple domains .

2 . The central contribution is extending the single step LP to a multi-step formulation . It would be beneficial to empirically demonstrate how increasing k ( the multi-step parameter ) affects the performance gains .

3 . Increasing k also comes at a computational cost . I would like to see some discussions on this and how long dual - AC takes to converge in comparison to the other algorithms tested ( PPO and TRPO ) .

4 . The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization . It was also mentioned that increasing the regularization parameter size increases the convergence rate . Empirically , how does changing the regularization parameter affect the performance in terms of reward maximization ? In the experimental section of the appendix , it is mentioned that multiple regularization settings were tried but their performance is not mentioned . Also , for the regularization parameters that were tried , based on hessian inspection , did they all result in local convexity ? A bit more discussion on these choices would be helpful .

Minor comments :
1 . Page 2 : In equation 5 , there should not be a ' ds ' in the dual variable constraint

We appreciate the constructive comments on both theoretical and empirical aspects by the reviewer .

We first emphasize our contributions . The major contributions of this paper are ( 1 ) the ** first * * establishment of the competition between actor and critic in a ** multi-step * * setting ; and ( 2 ) a novel algorithm that is make effective thanks to several critical components we introduce , including path - regularization and stochastic dual ascent , to deal with potential numerical issues that arise when one directly solves the zero - sum game .

Theory Clarification :
1 , Novelty of Theorems : Theorem 2 ( one-step dual form ) is indeed an extension of existing results to continuous state and action MDP . Theorem 3 ( multi-step dual form ) is one of our major contributions and is a novel result . The claim in Theorem 3 may appear natural , but its proof is a highly nontrivial generalization of the one -step case , since the convex - concave structure breaks down in the multi-step setting . We have made this clearer in the revision .

2 , Assumptions on value function class and choice of regularization parameter : We tried to separate the justification of path-regularization ( theory ) from the parametrization of value function ( practice ) .
i ) , Theoretically , regarding Theorem 4 and its proof , we consider the entire value function space , i.e. , the nonparametric limit , without taking into account of parametrization . Hence , as long as the regularization parameter ( i.e. , eta ) is selected appropriately , this does n’t affect the optimality . Note that an implicit condition of eta is provided on Page 18 ; however , finding an explicit condition for the regularization parameter seems to be rather difficult and is beyond the scope of this work .
ii ) , Practically , we always parametrize the value function ( which affects the valid range of eta ) and tune the regularization parameter to achieve the best performance .

3 , Minor gaps in proofs : Yes , there should be a square in the proof on page 18 . This does not jeopardize the rest of the proof as we only need boundedness of this term . We have fixed the issue in our revision . For the first inequality on page 14 about ||V^*||_{2 , .mu}^2 , it comes from the inequality E [ ( X +Y ) ^2 ] <= 2 * ( E [ X^2 ] + E[ Y^2 ] ) , a generalization of ( a + b ) ^2 < = 2 ( a^2 + b^2 ) . We have added more details to the proof . Thanks for pointing out these issues .


Experiments Clarification :
1 , Performance comparisons with and without the improvements : In the ablation experiment part , we compared the proposed dual - AC algorithm with / without the path-regularization , and with / without multi-step on several MuJoCo tasks , including Inverted DoublePendulum , Swimmer , and Hopper . The results suggest that using path -regularization and multi-step significantly improves the performances . Detailed experimental results can be found in Figure 1 .

2 , Effects of the length of multi-steps : We conducted additional experiments to investigate the effects of multi-step lengths . Specifically , we compared the performance with different k = { 1 , 10 , 50 } , and tested on three tasks . Better performances are observed with increasing k , which indicates that reducing the bias is indeed critical . Detailed experimental results can be found in Figure 1 .

3 , Computation overheads of using multi-step : In terms of the computational cost , assuming the length of the trajectories are m , with a simple moving sum algorithm , we calculate all the k length partial reward sums for a trajectory in O ( m ) with a O ( 1 ) amortized cost to calculate each sum of rewards . This method was used in our experiments for our algorithm as well as all competitors . Comparing to TRPO and PPO , the cost for summation is the same for all algorithms and the update costs are constant for each individual algorithm regardless of the choice of k.

4 , Local convexity in practice : In general , using a positive eta_V coefficient with path -regularization always enhances the local convexity . For example , if V is parametrized in a linear form , as long as eta_ V is not zero , local convexity will hold . A larger eta_ V will result in faster convergence , at the cost of extra bias . It is not easy to theoretically / empirically inspect the exact local convexity condition when a complicated parametrization of V is used . In practice , we suggest to simply tune the regularization parameter , and that ’s what we have done in the experiments .


This paper presents a study on the Information Bottleneck ( IB ) theory of deep learning , providing results in contrasts to the main theory claims . According to the authors , the IB theory suggests that the network generalization is mainly due to a ‘ compression phase ’ in the information plane occurring after a ‘ fitting phase ’ and that the ‘ compression phase ’ is due to the stochastic gradient decent ( SDG ) . Instead , the results provided by this paper show that : the generalization can happen even without compression ; that SDG is not the primary factor in compression ; and that the compression does not necessarily occur after the ‘ fitting phase ’ . Overall , the paper tackles the IB theory claims with consistent methodology , thus providing substantial arguments against the IB theory .

The main concern is that the paper is built to argue against another theoretical work , raising a substantial discussion with the authors of the IB theory . This paper should carefully address all the raised arguments in the main text .

There are , moreover , some open questions that are not fully clear in this contribution :
1 ) To evaluate the mutual information in the ReLu networks ( sec. 2 ) the authors discretize the output activity in their range . Should the non-linearity of ReLu be considered as a form of compression ? Do you check the ratio of ReLus that are not active during training or the ratio of inputs that fall into the negative domain of each ReLu ?
2 ) Since one of today common topics is the training of deep neural networks with lower representational precision , could the quantization error due to the low precision be considered as a form of noise inserted in the network layers that influences the generalization performance in deep neural networks ?
3 ) What are the main conclusions or impact of the present study in the theory of neural networks ? Is it the authors aim to just demonstrate that the IB theory is not correct ? Perhaps , the paper should empathize the obtained results not just in contrast to the other theory , but proactively in agreement with a new proposal .

Finally , a small issue comes from the Figures that need some improvement . In most of the cases ( Figure 3 C , D ; Figure 4 A , B , C ; Figure 5 C , D ; Figure 6 ) the axes font is too small to be read . Figure 3C is also very unclear .


Please also see our comments to all reviewers above .

- This paper presents a study on the Information Bottleneck ( IB ) theory of deep learning , providing results in contrasts to the main theory claims . According to the authors , the IB theory suggests that the network generalization is mainly due to a ‘ compression phase ’ in the information plane occurring after a ‘ fitting phase ’ and that the ‘ compression phase ’ is due to the stochastic gradient decent ( SDG ) . Instead , the results provided by this paper show that : the generalization can happen even without compression ; that SDG is not the primary factor in compression ; and that the compression does not necessarily occur after the ‘ fitting phase ’ . Overall , the paper tackles the IB theory claims with consistent methodology , thus providing substantial arguments against the IB theory .

Thank you !

- The main concern is that the paper is built to argue against another theoretical work , raising a substantial discussion with the authors of the IB theory . This paper should carefully address all the raised arguments in the main text .

The revision now addresses these arguments in the main text . We believe the conclusions in our original submission still stand , and are now supported by additional experiments .

- There are , moreover , some open questions that are not fully clear in this contribution :
- 1 ) To evaluate the mutual information in the ReLu networks ( sec. 2 ) the authors discretize the output activity in their range . Should the non-linearity of ReLu be considered as a form of compression ? Do you check the ratio of ReLus that are not active during training or the ratio of inputs that fall into the negative domain of each ReLu ?

Our discretization does consider the nonlinearity of ReLU , which could in principle lead to compression if ReLUs tended to inactivate over the course of training . However they do not seem to in practice , which can be seen from the histograms of activity over training in Fig. 17 . The bottom - most bin contains zero , the ReLU saturation value . There is no consistent trend in the number of saturated ReLU activations over training , with most layers ending up about where they started , with neurons inactive on roughly 50 % of examples .

- 2 ) Since one of today common topics is the training of deep neural networks with lower representational precision , could the quantization error due to the low precision be considered as a form of noise inserted in the network layers that influences the generalization performance in deep neural networks ?

Thank you for the suggestion , we now point to this possibility in the discussion . For networks which explicitly incorporate noise in their architecture ( either through quantization or noise injection ) , the broader information bottleneck theory may apply and yield potentially new training algorithms . Our point in this paper is that the specific claims of the information bottleneck theory of deep learning , which attempt to explain the performance of “ vanilla ” deep networks with no quantization or noise , do not in fact explain the generalization performance of these networks .

- 3 ) What are the main conclusions or impact of the present study in the theory of neural networks ? Is it the authors aim to just demonstrate that the IB theory is not correct ? Perhaps , the paper should empathize the obtained results not just in contrast to the other theory , but proactively in agreement with a new proposal .

There are a variety of theories ( several cited in our introduction ) which may be consistent with all of the results reported in this paper . Most directly , the results in Advani & Saxe , 2017 successfully account for generalization behavior in the linear models we study . However even there , it remains to be seen how those ideas might apply to deep nonlinear networks . It is outside the scope of this paper to provide strong support for any one of these theories , as singling out one theory as better would require experiments designed to specifically test them , which must be left for future work . Our aim rather was to carefully and fairly inspect an exciting and , it seemed to us , promising theory , and the result turned out to be somewhat negative . In our view , negative results are a critical component of a healthy research ecosystem , and on occasion science advances through falsification . The impact of the present study on the theory of neural networks is to help narrow the field of plausible candidate theories . We expect our results to be important to researchers currently building off of the information bottleneck theory of deep learning .

- Finally , a small issue comes from the Figures that need some improvement . In most of the cases ( Figure 3 C , D ; Figure 4 A , B , C ; Figure 5 C , D ; Figure 6 ) the axes font is too small to be read . Figure 3C is also very unclear .

We apologize for this issue , we have increased the size of several figures and are working towards a revision with the rest corrected .


The authors address the issue of whether the information bottleneck ( IB ) theory can provide insight into the working of deep networks . They show , using some counter - examples , that the previous understanding of IB theory and its application to deep networks is limited .

PROS : The paper is very well written and makes its points very clearly . To the extent of my knowledge , the content is original . Since it clearly elucidates the limitations of IB theory in its ability to analyse deep networks , I think it is a significant
contribution worthy of acceptance . The experiments are also well designed and executed .

CONS : On the downside , the limitations exposed are done so empirically , but the underlying theoretical causes are not explored ( although this could be potentially because this is hard to do ) . Also , the paper exposes the limitations of another paper published in a non-peer reviewed location ( arXiv ) which potentially limits its applicability and significance .

Some detailed comments :

In section 2 , the influence of binning on how the mutual information is calculated should be made clear . Since the comparison is between a bounded non-linearity and an unbounded one , it is not self - evident how the binning in the latter case should be done . A justification for the choice made for binning the relu case would be helpful .

In the same section , it is claimed that the dependence of the mutual information I( X ; T ) on the magnitude of the weights of the network explains why a tanh non-linearity shows the compression effect ( non-monotonicity vs I ( X ; T ) ) in the information plane dynamics . But the claim that large weights are required for doing anything useful is unsubstantiated and would benefit from having citations to papaers that discuss this issue . If networks with small weights are able to learn most datasets , the arguments given in this section would n't be applicable in its entirety .

Additionally , figures that show the phase plane dynamics for other non-linearities e.g. relu + or sigmoid , should be added , at
least in the supplementary section . This is important to complete the overall picture of how the compression effect depends on having specific activation functions .

In section 3 , a sentence or two should be added to describe what a " teacher - student setup " is , and how it is relevant / interesting .

Also in section 3 , the cases where batch gradient descent is used and where stochastic gradient descent is used should be
pointed out much more clearly . It is mentioned in the first line of page 7 that batch gradient descent is used , but it is not
clear why SGD could n't have been used to keep things consistent . This applies to figure 4 too .

In section 4 , it seems inconsistent that the comparison of SGD vs BGD is done using linear network as opposed to a relu network which is what 's used in Section 2 . At the least , a comparison using relu should be added to the supplementary section .

Minor comments
The different figure styles using in Fig 4A and C that have the same quantities plotted makes it confusing .
An additional minor comment on the figures : some of the labels are hard to read on the manuscript .

Please also see our comments to all reviewers above .

- PROS : The paper is very well written and makes its points very clearly . To the extent of my knowledge , the content is original . Since it clearly elucidates the limitations of IB theory in its ability to analyse deep networks , I think it is a significant contribution worthy of acceptance . The experiments are also well designed and executed .

Thank you !

- CONS : On the downside , the limitations exposed are done so empirically , but the underlying theoretical causes are not explored ( although this could be potentially because this is hard to do ) . Also , the paper exposes the limitations of another paper published in a non-peer reviewed location ( arXiv ) which potentially limits its applicability and significance .

While we agree that we have not been able to prove theoretically that , for instance , ReLUs will not compress , we do believe we have elucidated some of the theoretical causes : we present a minimal three neuron model that exhibits the compression phenomenon and give an explicit formula for the binning - based MI estimate ; and we give exact calculations of the MI for the linear case , for which the generalization behavior is known . Finally , we now directly discuss the fact that SGD does not necessarily behave like BGD plus additive noise ( and hence there is no stochastic relaxation to a Gibbs distribution ) .

Although the information bottleneck theory of deep learning has appeared only as an arXiv paper , it has achieved attention through video lectures and articles in the popular press . Most importantly from our perspective , researchers are actively attempting to build new methods off of the ideas in the information bottleneck theory , and we believe our results could be significant to those efforts — this , in our view , is the main value in our present work .


- Some detailed comments :

- In section 2 , the influence of binning on how the mutual information is calculated should be made clear . Since the comparison is between a bounded non-linearity and an unbounded one , it is not self - evident how the binning in the latter case should be done . A justification for the choice made for binning the relu case would be helpful .

For ReLU , we simply space bins up to the largest activation value encountered over the course of training ( this method places no a priori assumption on how large the activations might grow , and is equivalent to having bins stretching to infinity since all larger bins would never be used and have probability zero ) . We have added an extended discussion to the appendix which , in addition to these points , shows the results of alternative binning strategies .

- In the same section , it is claimed that the dependence of the mutual information I( X ; T ) on the magnitude of the weights of the network explains why a tanh non-linearity shows the compression effect ( non-monotonicity vs I ( X ; T ) ) in the information plane dynamics . But the claim that large weights are required for doing anything useful is unsubstantiated and would benefit from having citations to papaers that discuss this issue . If networks with small weights are able to learn most datasets , the arguments given in this section would n't be applicable in its entirety .

We have now included an appendix which justifies this claim . First , we note that nonlinearities like tanh are linear near the origin . Hence small weights place activities in this linear regime and the network can only compute a linear function of the input . As essentially all real world tasks are nonlinear , it is a virtual necessity for the weights to increase until the tanh nonlinearities saturate on some examples . More generally , we cite Rademacher complexity bounds which depend on the norm of the weights ( implying that small weight networks can represent only simple functions ) . Finally , as an emiprical matter , we show that for the tanh , ReLU , and linear networks considered in this paper the weight norms increase in every layer over training .

- Additionally , figures that show the phase plane dynamics for other non-linearities e.g. relu + or sigmoid , should be added , at least in the supplementary section . This is important to complete the overall picture of how the compression effect depends on having specific activation functions .

Thank you , we have now added two more nonlinearities ( softplus and softsign ) to the appendix , which also show similar results .

- In section 3 , a sentence or two should be added to describe what a " teacher - student setup " is , and how it is relevant / interesting . Also in section 3 , the cases where batch gradient descent is used and where stochastic gradient descent is used should be pointed out much more clearly . It is mentioned in the first line of page 7 that batch gradient descent is used , but it is not clear why SGD could n't have been used to keep things consistent . This applies to figure 4 too .

We have now more fully described the student - teacher scenario , and more carefully labeled the batch size in our experiments ( though we note that it made little difference on the information plane dynamics in our hands ) .

- In section 4 , it seems inconsistent that the comparison of SGD vs BGD is done using linear network as opposed to a relu network which is what 's used in Section 2 . At the least , a comparison using relu should be added to the supplementary section .

We now use the ReLU network in the main text , and have placed the linear network result in the appendix .

- Minor comments : The different figure styles using in Fig 4A and C that have the same quantities plotted makes it confusing . An additional minor comment on the figures : some of the labels are hard to read on the manuscript .

We apologize for these issues , we intend to submit another revision with larger figure captions and consistent plotting styles .


A thorough investigation on Info Bottleneck and deep learning , nice to read with interesting experiments and references . Even though not all of the approach is uncontroversial ( as the discussion shows ) , the paper contributes to much needed theory of deep learning rather than just another architecture .
Estimating the mutual information could have been handled in a more sophisticated way ( eg using a Kraskov estimator rather than simple binning ) , and given that no noise is usually added the discussion about noise and generalisation does n't seem to make too much sense to me .

It would have been good to see a discussion whether another measurement that would be useful for single - sided saturating nonlinearities that do show a compression ( eg information from a combination of layers ) , from learnt representations that are different to representations learnt using double - sided nonlinearities .

Regarding the finite representation of units ( as in the discussion ) it might be helpful to also consider an implementation of a network with arbitrary precision arithmetic as an additional experiment .

Overall I think it would be nice to see the paper accepted at the very least to continue the discussion .

Please also note our comments to all reviewers above .

-A thorough investigation on Info Bottleneck and deep learning , nice to read with interesting experiments and references . Even though not all of the approach is uncontroversial ( as the discussion shows ) , the paper contributes to much needed theory of deep learning rather than just another architecture .

Thanks for the encouraging comments !

- Estimating the mutual information could have been handled in a more sophisticated way ( eg using a Kraskov estimator rather than simple binning ) , and given that no noise is usually added the discussion about noise and generalisation does n't seem to make too much sense to me .

We now include the Kraskov estimator as well as a nonparametric KDE estimator , which show similar results to the binning - based estimate .

We have revised the text to clarify that the `'' noise ' ' in the student - teacher section on generalization is fundamentally different from the noise added to representations for analysis . It represents approximation error ( i.e. , aspects of the target function which even the best neural network of a given architecture cannot model ) , and is part of generating an interesting dataset based on a teacher . The noise added to representations for analysis , by contrast , is an assumption which affects the student network itself , and is not part of the operation of the student network in practice .

- It would have been good to see a discussion whether another measurement that would be useful for single - sided saturating nonlinearities that do show a compression ( eg information from a combination of layers ) , from learnt representations that are different to representations learnt using double - sided nonlinearities .

So long as hidden activities are continuous , we believe that MI between the input and multiple layers simultaneously should show similar dynamics . Given our results , it seems that single - sided saturating nonlinearities do not in general compress , and this would carry through to measures that combine multiple layers ( because these layers form a Markov chain ) .

- Regarding the finite representation of units ( as in the discussion ) it might be helpful to also consider an implementation of a network with arbitrary precision arithmetic as an additional experiment . Overall I think it would be nice to see the paper accepted at the very least to continue the discussion .

Thank you for the suggestion , we considered doing an experiment with arbitrary precision but were able to rule out this concern through another route : if noise in batch gradient descent from numerical precision causes the weights to converge to a Gibbs distribution , and this in turn causes compression , then we should see compression in ReLU or linear networks trained with BGD . However we do not , as we now show in Fig. 5D , which makes this explanation unlikely in our eyes . Moreover , even the noise in SGD appears insufficient to cause compression for ReLU or linear networks , and hence is unlikely to be the source of compression more generally .

The paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reasonable . The idea presented seems to have merit , however , I found the presentation lacking . Many sentences are poorly written making the paper hard to read , especially when not familiar with the presented methods . The experimental section could be organized better . I did n't like that two types of experiment are now presented in parallel . Finally , the paper stops abruptly without any final discussion and / or conclusion .

We thank you for providing reviews .

We ’ll refine the paper to include more introduction about background , and more detailed explanations about our method .

We ’ll include final discussion / conclusion section .


Let me first note that I am not very familiar with the literature on program generation ,
molecule design or compiler theory , which this paper draws heavily from , so my review is an educated guess .

This paper proposes to include additional constraints into a VAE which generates discrete sequences ,
namely constraints enforcing both semantic and syntactic validity .
This is an extension to the Grammar VAE of Kusner et. al , which includes syntactic constraints but not semantic ones .
These semantic constraints are formalized in the form of an attribute grammar , which is provided in addition to the context - free grammar .
The authors evaluate their methods on two tasks , program generation and molecule generation .

Their method makes use of additional prior knowledge of semantics , which seems task - specific and limits the generality of their model .
They report that their method outperforms the Character VAE ( CVAE ) and Grammar VAE ( GVAE ) of Kusner et. al .
However , it is n't clear whether the comparison is appropriate : the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et. al , whereas Kusner et . al do not make any mention of this .
The baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et. al though .
Can the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format ?

Typos :
- Page 5 : " while in sampling procedure " -> " while in the sampling procedure "
- Page 6 : " a deep convolution neural networks " -> " a deep convolutional neural network "
- Page 6 : " KL - divergence that proposed in " -> " KL - divergence that was proposed in "
- Page 6 : " since in training time " -> " since at training time "
- Page 6 : " can effectively computed " -> " can effectively be computed "
- Page 7 : " reset for training " -> " rest for training "

Thanks for your effort in providing this detailed and useful review !

We present our clarification in the following :

>> Use of data and comparison with baselines :

We would first note that the anonymous accusation was set to “ 17 Nov 2017 ( modified : 28 Nov 2017 ) , readers : ICLR 2018 Conference Reviewers and Higher ” . That ’s why it was not visible to us until Nov 28 , i.e. , the original review release date . This gives us no chance to clarify anything before the review deadline . We have replied to it actively since Nov 28 .
** Note the thread is invisible to us again since Dec 2 . **

1 ) We have experimented both kekulization and non-kekulization for baselines , and have reported the best they can get in all experiments . For example , in Table 2 the GVAE baseline results are improved compared to what was reported in GVAE paper .

2 ) The anonymous commenter is using different kekulization ( RDKIT , rather than our used Marvin ) , different baseline implementation ( custom implementation , rather than the public one in GVAE ’s paper ) and possibly different evaluation code ( since there is no corresponding evaluation online ) . For a reproducible comparision , we released our implementation , data , pretrained model and evaluation code at : https://github.com/anonymous-author-80ee48b2f87/cvae-baseline

3 ) To make further clarification , we ran our method on the vanilla ( non-kekulised ) data . Our performance is actually boosted ( 76.2 % vs 72.8 % reported in the paper ) .
The details of results from these experiments above can be seen in our public reply titled “ We released baseline CVAE code , data and evaluation code for clarification ” and “ Our reconstruction performance without kekulization on Zinc dataset ” .

In either setting still , our method outperforms all baselines on reconstruction . We are sorry that this may have led to some confusions . To avoid further possible misunderstandings , we have extensively rerun all experiments involving ZINC dataset . Though differences are observed , the conclusion in each experiment remains the same . For example , our reconstruction performance is boosted ( 76.2 % vs 72.8 % ) . Since we did n’t address aromaticity semantics by the paper submission deadline , the valid prior fraction drops to 43.5 % , but it is still much higher than baselines ( 7.2 % GVAE , 0.7 % CVAE ) . Please find the updated paper for more details .

>> prior knowledge and limitations

We are targeting on domains where strict syntax and semantics are required . For example , the syntax and semantics are needed to compile a program , or to parse a molecule structure . So such prior knowledge comes naturally with the application . Our contribution is to incorporate such existing syntax and semantics in those compilers , into an on- the- fly generation process of structures .

In general , when numerous amount of data is available , a general seq2seq model would be enough . However , obtaining the useful drug molecules is expensive , and thus data is quite limited . Using knowledges like syntax ( e.g. , in GVAE paper ) , or semantics ( like in our paper ) will greatly reduce the amount of data needed to obtain a good model .

In our paper , we only addressed 2 - 3 semantic constraints , where the improvement is significant . Similarly , in “ Harnessing Deep Neural Networks with Logic Rules ( Hu et.al , ACL 16 ) ” , incorporating several intuitive rules can greatly improve the performance of sentiment analysis , NER , etc . So we believe that , incorporating the knowledge with powerful deep learning achieves a good trade - off between human efforts and model performance .

>> Typos and other writing issue :

We thank you very much for your careful reading and pointing out the typos and writing issues in our manuscript ! We have incorporated your suggested changes in the current revision , and are keeping conducting further detailed proofreading to fix as much as possible the writing issues in the future revisions .

NOTE :

Would the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset ? Fair comparison of the data is a serious concern . I have listed this review as a good for publication due to the novelty of ideas presented , but the accusation of misrepresentation below is a serious one and I would like to know the author 's response .

* Overview *

This paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking . Instead of verifying the semantic correctness offline of a particular discrete structure , the authors propose “ stochastic lazy attributes ” , which amounts to loading semantic constraints into a CFG and using a tailored latent - space decoder algorithm that guarantees both syntactic semantic valid . Using Bayesian Optimization , search over this space can yield decodings with targeted properties .

Many of the ideas presented are novel . The results presented are state - of - the art . As noted in the paper , the generation of syntactically and semantically valid data is still an open problem . This paper presents an interesting and valuable solution , and as such constitutes a large advance in this nascent area of machine learning .

* Remarks on methodology *

By initializing a decoding by “ guessing ” a value , the decoder will focus on high - probability starting regions of the space of possible structures . It is not clear to me immediately how this will affect the output distribution . Since this process on average begins at high - probability region and makes further decoding decisions from that starting point , the output distribution may be biased since it is the output of cuts through high - probability regions of the possible outputs space . Does this sacrifice exploration for exploitation in some quantifiable way ? Some exploration of this issue or commentary would be valuable .

* Nitpicks *

I found the notion of stochastic predetermination somewhat opaque , and section 3 in general introduces much terminology , like lazy linking , that was new to me coming from a machine learning background . In my opinion , this section could benefit from a little more expansion and conceptual definition .

The first 3 sections of the paper are very clearly written , but the remainder has many typos and grammatical errors ( often word omission ) . The draft could use a few more passes before publication .


Thanks for your effort in providing this detailed and constructive review !
We present our clarification in the following :

>> NOTE :

We would first note that the anonymous accusation was set to “ 17 Nov 2017 ( modified : 28 Nov 2017 ) , readers : ICLR 2018 Conference Reviewers and Higher ” . That ’s why it was not visible to us until Nov 28 , i.e. , the original review release date . This gives us no chance to clarify anything before the review deadline . We have replied to it actively since Nov 28 .
** Note the thread is invisible to us again since Dec 2 . **

To summarize our clarification :

>> Use of data

1 ) We have experimented both kekulization and non-kekulization for baselines , and have reported the best they can get in all experiments . For example , in Table 2 the GVAE baseline results are improved compared to what was reported in GVAE paper .

2 ) The anonymous commenter is using different kekulization ( RDKIT , rather than our used Marvin ) , different baseline implementation ( custom implementation , rather than the public one in GVAE ’s paper ) and possibly different evaluation code ( since there is no corresponding evaluation online ) . For a reproducible comparision , we released our implementation , data , pretrained model and evaluation code at : https://github.com/anonymous-author-80ee48b2f87/cvae-baseline

3 ) To make further clarification , we ran our method on the vanilla ( non-kekulised ) data . Our performance is actually boosted ( 76.2 % vs 72.8 % reported in the paper ) .
The details of results from these experiments above can be seen in our public reply titled “ We released baseline CVAE code , data and evaluation code for clarification ” and “ Our reconstruction performance without kekulization on Zinc dataset ” .

In either setting still , our method outperforms all baselines on reconstruction . We are sorry that this may have led to some confusions . To avoid further possible misunderstandings , we have extensively rerun all experiments involving ZINC dataset . Though differences are observed , the conclusion in each experiment remains the same . For example , our reconstruction performance is boosted ( 76.2 % vs 72.8 % ) . Since we did n’t address aromaticity semantics by the paper submission deadline , the valid prior fraction drops to 43.5 % , but it is still much higher than baselines ( 7.2 % GVAE , 0.7 % CVAE ) . Please find the updated paper for more details .

>> sacrifice of exploration

CVAE , GVAE and our SD - VAE are all factorizing the joint probability of entire program / SMILES text in some way . CVAE factorizes in char level , GVAE in Context Free Grammar ( CFG ) tree , while ours factorizes both CFG and non-context free semantics . Since every method is factorizing the entire space , each structure in this space should have the possibility ( despite its magnitude ) of being sampled .

Bias is not always a bad thing . Some bias will help the model quickly concentrate to the correct mode . Definitely , different methods will bias the distribution in a different way . For example , CVAE is biased towards the beginning of the sequence . GVAE is biased by several initial non-terminals .

Our experiments on diversity of generated molecules ( table 3 ) demonstrate that , both GVAE and our method can generate quite diverse molecules . So we think both methods do n’t have noticeable mode collapse problem on this dataset .

>> writings :

Thanks for the suggestions . We are adding more effort in explaining our algorithm and improve writing in revisions . We have revised our experiments sections for clarifying the most important issue , and will keep improving the writing .

To briefly answer the “ lazy linking ” : We do n’t sample the actual value of the attribute at the first encounter ; Instead , later when the actual content is generated , we use bottom - up calculation to fill the value . For example , when generating ringbond attribute , we only sample its existence . The ringbond information ( bond index and bond type ) are filled later .

As a side note , this idea comes from “ lazy evaluation ” in compiler theory where a value is not calculated until it is needed .


The paper adds to the discussion on the question whether Generative Adversarial Nets ( GANs ) learn the target distribution . Recent theoretical analysis of GANs by Arora et al . show that of the discriminator capacity of is bounded , then there is a solution the closely meets the objective but the output distribution has a small support . The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally . The main idea used to estimate the support is the Birthday theorem that says that with probability at least 1 / 2 , a uniform sample ( with replacement ) of size S from a set of N elements will have a duplicate given S > \sqrt{N} . The suggested plan is to manually check for duplicates in a sample of size s and if duplicate exists , then estimate the size of the support to be s^2 . One should note that the birthday theorem assumes uniform sampling . In the revised versions , it has been clarified that the tested distribution is not assumed to be uniform but the distribution has " effectively " small support size using an indistinguishability notion . Given this method to estimate the size of the support , the paper also tries to study the behaviour of estimated support size with the discriminator capacity . Arora et al. showed that the output support size has nearly linear dependence on the discriminator capacity . Experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described above . A result similar to that of Arora et al . is also given for the special case of Encoder - Decoder GAN .

Evaluation :
Significance : The question whether GANs learn the target distribution is important and any significant contribution to this discussion is of value .

Clarity : The paper is written well and the issues raised are well motivated and proper background is given .

Originality : The main idea of trying to estimate the size of the support using a few samples by using birthday theorem seems new .

Quality : The main idea of this work is to give a estimation technique for the support size for the output distribution of GANs .


It is important to note that Theorem 1 and 2 do * not * assume that the tested distribution is uniform . ( The birthday paradox holds even if human birthdays are distributed in a highly nonuniform way . ) This confusion possibly underlies the reviewer ’s score .

Theorem 2 clarifies that if one can consistently see collisions in batches , then the distribution has a major component that has * limited * support size but is almost * indistinguishable * from the full distribution via sampling a small number of samples . ( For example , it could assign very tiny probability to a lot of other images . ) Thus the distribution *effectively * has small support size , which is what one should care about when sampled from . We will try other phrasing of that section to clarify this issue further .

It may help to point out ( as proven in paper [ 1 ] below ) that to correctly estimate support size of a distribution with n modes , at least n / log n samples need to be seen by the human examiner . Since support size is ~ 10^6 in some GANs studied here , examining n / log n is infeasible for a human . Though conceivably some follow - up work could do this via a giant mechanical turk experiment . We will be sure to add these notes to the final version so other readers are not confused .
( Possibly the reviewer is also alluding to the possibility that CelebA dataset is a highly nonuniform distribution of faces . This is possible , but the constructors [ 2 ] tried hard to make it unbiased ( it contains ten thousand identities , each of which has twenty images ) [ 2 ] . Also , we report results on it because it was used in many GANs papers . )

To the best of our knowledge , our birthday paradox test --- which is of course related to classical ideas in statistics --- is more rigorous and quantitative than past tests for mode collapse we are aware of .

Finally , the reviewer appears to have missed the important theoretical contribution showing how encoder - decoder GANs may learn un - informative codes .

[ 1 ] Valiant , Gregory , and Paul Valiant , Estimating the Unseen : An n/ log ( n ) - sample Estimator for Entropy and Support Size , Shown Optimal via New CLTs , STOC 2011
[ 2 ] Liu , Ziwei , Ping Luo , Xiaogang Wang , and Xiaoou Tang . Deep Learning Face Attributes in the Wild
