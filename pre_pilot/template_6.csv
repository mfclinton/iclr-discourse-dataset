Review_index,Review_sentence,Review_affordance,-,Rebuttal_index,Rebuttal_sentence,Related_to,Relation
V0,b'--- Below is based on the original paper ---',,,B0,b'We thank the reviewer for the feedback .',,
V1,,,,B1,"b'We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below .'",,
V2,"b'This paper presents a framework that allows the agent to learn from its observations , but never follows through on the motivation of experimentation --- taking actions mainly for the purpose of learning an improved dynamics model .'",,,B2,"b'We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have .'",,
V3,"b'All of their experiments merely take actions that are best according to the usual model - based or model - free methods , and show that their consistency constraint allows them to learn a better dynamics model , which is not at all surprising .'",,,B3,b'We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places .',,
V4,"b'They do not even allow for the type of experimentation that has been done in reinforcement learning for as long as it has been around , which is to allow exploration by artificially increasing the reward for the first few times that each state is visited .'",,,B4,"b""We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions .""",,
V5,b'That would be a good baseline against which to compare their method .',,,B5,b'We briefly summarize the key idea of the paper and then address the specific concerns .',,
V6,,,,B6,,,
V7,,,,B7,,,
V8,b'Overall :',,,B8,b'What is the idea ?',,
V9,,,,B9,,,
V10,b'Pros :',,,B10,b'========',,
V11,,,,B11,,,
V12,b'1 .',,,B12,,,
V13,b'Clear writing',,,B13,b'Our goal is to provide a mechanism for the agent to lean a better dynamics model as well as more powerful policy by ensuring the consistency in their predictions ( such that predictions from the model are grounded in the real environment ) .',,
V14,,,,B14,,,
V15,b'2 . Good motivation description .',,,B15,,,
V16,,,,B16,b'This mechanism enables the agent to have a direct \xe2\x80\x9c interaction \xe2\x80\x9d b/w the agent \xe2\x80\x99s policy and its dynamics model .',,
V17,,,,B17,"b'This interaction is different from the standard approaches in reinforcement learning where the agent uses the policy to sample trajectories over which the agent is trained , and then use these sampled trajectories to learn the dynamics model .'",,
V18,b'Cons :',,,B18,"b'In those cases , there is no ( direct ) mechanism for the dynamics model to affect the policy , and hence there is no \xe2\x80\x9c direct interaction \xe2\x80\x9d between the policy and the dynamics model .'",,
V19,,,,B19,"b'In our case , both the policy and the model are trained jointly while making sure that the predictions from the dynamics model are consistent with the observation from the environment .'",,
V20,b'1 .',,,B20,b'This provides a mechanism where learning a model can itself change the policy ( thus \xe2\x80\x9c interacting \xe2\x80\x9d with the environment ) instead of just training on the data coming from a policy which is trained independently of how well the model performs on the collected .',,
V21,b'Failed to connect presented work with the motivation .',,,B21,,,
V22,,,,B22,,,
V23,b'2 .',,,B23,b'A practical instantiation of this idea is the consistency loss where we ensure consistency between the predictions from the dynamics model and the actual observations from the environment and this simple baseline works surprisingly well compared to the state of the art methods ( as demonstrated by our experiments ) and that others have not tried it before .',,
V24,b'No comparison against known methods for exploration .',,,B24,b'Applying consistency constraint means we have two learning signals for the policy :',,
V25,,,,B25,b'The one from the reinforcement learning loss ( i.e maximize return ) and the other due to consistency constraint .',,
V26,,,,B26,b'We show that adding the proposed consistency constraint helps the agent to learn better dynamics model and as well as better policy for both observation space models and state space models .',,
V27,,,,B27,b'We compare against strong baselines :',,
V28,b'---- Below is based on the revision ---',,,B28,,,
V29,,,,B29,,,
V30,,,,B30,b'Hybrid model - based and model - free ( Mb - Mf ) algorithm ( Nagabandi et al [ 1 ] )',,
V31,b'Thanks to the reviewers for making the paper much clearer .',,,B31,,,
V32,b'I have no particular issues on the items that are in the paper .',,,B32,b'Learning and Querying Fast Generative Models for Reinforcement Learning ( Buesing et al [ 2 ] ) - This is a state - of - the - art model for state space models .',,
V33,"b'However , subsections 7.2.1 and 7.2.2 are missing .'",,,B33,,,
V34,,,,B34,,,
,,,,B35,b'Our evaluation protocol considers a total of 7 environments and we show that using the consistency constraint leads to better generative models ( in terms of log likelihood ) and more powerful policy ( average return ) for all the cases .',,
,,,,B36,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
,,,,B37,,,
,,,,B38,,,
,,,,B39,b'Our key contribution is the proposal of using the consistency loss which helps to learn more powerful policy and better dynamics model ( as demonstrated over different tasks ) while being very easy to integrate with existing model - based RL approaches .',,
,,,,B40,"b'While our method is relatively simple , we are not aware of prior works that show something similar , and we believe such a simple baseline would be useful for anyone who \xe2\x80\x99s working on model - based RL.'",,
,,,,B41,"b'Further , our experiment demonstrates the effectiveness of the approach .'",,
,,,,B42,"b'If we are mistaken regarding prior works , please let us know !'",,
,,,,B43,,,
,,,,B44,,,
,,,,B45,"b'We would like to emphasize that our work presents an extensive comparative evaluation , and we believe that these results should be taken into consideration in evaluating our work .'",,
,,,,B46,b'We compare multiple approaches across more than 5 simulated tasks to the state of the art methods .',,
,,,,B47,"b'Hopefully , our clarifications are convincing in terms of explaining why the evaluation is fair and rigorous , and we would , of course , be happy to modify it as needed .'",,
,,,,B48,"b'But at a higher level , the fact that such simple model - based approaches work better than somewhat complex model - free approaches actually is the point of the paper to me .'",,
,,,,B49,,,
,,,,B50,,,
,,,,B51,b'Continued ...',,
,,,,B52,,,
,,,,B53,,,
,,,,B54,b'How is it different from just learning a model based on k-step prediction ?',,
,,,,B55,,,
,,,,B56,b'========',,
,,,,B57,,,
,,,,B58,,,
,,,,B59,"b""Our approach is different from just learning a k-step prediction model as in our case , the agent \xe2\x80\x99s behavior ( i.e the agent 's policy ) is dependent on its internal model too .""",,
,,,,B60,"b'In the standard case , the policy is optimized only using the RL gradient i.e maximizing expected reward and the state transition pairs ( collected as the agent acts in the environment ) become the supervising dataset for learning the model , and hence the policy is not affected when the model is being updated and there is no feedback from the model learning process to the policy .'",,
,,,,B61,"b'Hence , the data used for training the model is coming from a policy which is trained independently of how well the model performs on the collected trajectories .'",,
,,,,B62,b'So the process of learning the model has no control over what kind of data is produced for its training .',,
,,,,B63,,,
,,,,B64,,,
,,,,B65,b'We propose to train both the policy and the model during the open loop .',,
,,,,B66,b'Hence the k-step predictions are used for training both the model and the policy simultaneously .',,
,,,,B67,"b'Training the policy on both the RL loss and the consistency loss provides a mechanism where learning a model , can itself change the policy thus leading to a much closer interplay between the dynamics model and the policy .'",,
,,,,B68,b'We show that this relatively simple approach leads to much better performance when compared to very strong baselines for both observation space and state space models for all the 7 environments we considered .',,
,,,,B69,,,
,,,,B70,,,
,,,,B71,,,
,,,,B72,,,
,,,,B73,b'===================================================================================================',,
,,,,B74,,,
,,,,B75,,,
,,,,B76,,,
,,,,B77,b'We now refer to the specific aspects of the reviews :',,
,,,,B78,,,
,,,,B79,,,
,,,,B80,,,
,,,,B81,"b'"" but never follows through on the motivation of experimentation --- taking actions mainly for the purpose of learning an improved dynamics model .'",,
,,,,B82,"b'AND Failed to connect presented work with the motivation . ""'",,
,,,,B83,,,
,,,,B84,,,
,,,,B85,b'Our goal and motivation is to provide a mechanism for the agent to learn a better dynamics model as well as more powerful policy by ensuring the consistency in their predictions ( such that predictions from the model are grounded in the real environment ) .',,
,,,,B86,,,
,,,,B87,,,
,,,,B88,b'This mechanism enables the agent to have a direct \xe2\x80\x9c interaction \xe2\x80\x9d b/w the agent \xe2\x80\x99s policy and its dynamics model .',,
,,,,B89,"b'This interaction is different from the standard approaches in reinforcement learning where the agent uses the policy to sample trajectories over which the agent is trained , and then use these sampled trajectories to learn the dynamics model .'",,
,,,,B90,"b'In those cases , there is no ( direct ) mechanism for the dynamics model to affect the policy , and hence there is no \xe2\x80\x9c direct interaction \xe2\x80\x9d between the policy and the dynamics model .'",,
,,,,B91,"b'In our case , both the policy and the model are trained jointly while making sure that the predictions from the dynamics model are consistent with the observation from the environment .'",,
,,,,B92,b'This provides a mechanism where learning a model can itself change the policy ( thus \xe2\x80\x9c interacting \xe2\x80\x9d with the environment ) instead of just training on the data coming from a policy which is trained independently of how well the model performs on the collected .',,
,,,,B93,,,
,,,,B94,,,
,,,,B95,b'A practical instantiation of this idea is the consistency loss where we ensure consistency between the predictions from the dynamics model and the actual observations from the environment and this simple baseline works surprisingly well compared to the state of the art methods ( as demonstrated by our experiments ) and that others have not tried it before .',,
,,,,B96,b'Applying consistency constraint means we have two learning signals for the policy :',,
,,,,B97,b'The one from the reinforcement learning loss ( i.e maximize return ) and the other due to consistency constraint .',,
,,,,B98,b'We show that adding the proposed consistency constraint helps the agent to learn better dynamics model and as well as better policy for both observation space models and state space models .',,
,,,,B99,,,
,,,,B100,,,
,,,,B101,"b'Our evaluation protocol consists of 7 environments ( Ant , Half Cheetah , Humanoid etc ) and both observation space and state space models .'",,
,,,,B102,"b'For the observation space model , we use the \xe2\x80\x9c Hybrid model - based and model - free ( Mb - Mf ) algorithm \xe2\x80\x9d ( Nagabandi et al [ 1 ] ) which is a very strong baseline and for the state space models , we use the \xe2\x80\x9c Learning and Querying Fast Generative Models for Reinforcement Learning \xe2\x80\x9d ( Buesing et al [ 2 ] ) as the baseline .'",,
,,,,B103,b'This model is a state - of - the - art model for state space models .',,
,,,,B104,b'We show that adding the consistency constraint to the baseline models results in improvements to both the dynamics models and the policy for all the environments that we consider .',,
,,,,B105,,,
,,,,B106,,,
,,,,B107,b'Continued ...',,
,,,,B108,,,
,,,,B109,,,
,,,,B110,,,
,,,,B111,"b'"" All of their experiments merely take actions that are best according to the usual model - based or model - free methods and show that their consistency constraint allows them to learn a better dynamics model , which is not at all surprising . ""'",,
,,,,B112,,,
,,,,B113,,,
,,,,B114,b'Our key contribution is the proposal of using the consistency loss which helps to learn more powerful policy AND better dynamics model ( as demonstrated over different tasks ) while being very easy to integrate with existing model - based RL approaches .',,
,,,,B115,b'It is important to note that our proposed approach improves over the state of the art results despite being relatively simple .',,
,,,,B116,b'We are not aware of work in RL which describes and validates the benefits of imposing the consistency constraint and would be happy to include references to such work .',,
,,,,B117,,,
,,,,B118,,,
,,,,B119,"b'We would like to highlight that our evaluation shows that the agent learns both better dynamics models AND more powerful policy ( figure 2 , 3 , 5 ) .'",,
,,,,B120,b'There seems to be some confusion about our evaluation protocol .',,
,,,,B121,b'We have updated the paper to improve that .',,
,,,,B122,b'Section 3.1 describes the different loss components and how the consistency constraint can be applied in the general .',,
,,,,B123,b'Section 3.2 and 3.3 describes the baselines and how these baselines were modified to support the consistency constraint for the observation space and the state space models respectively .',,
,,,,B124,,,
,,,,B125,,,
,,,,B126,,,
,,,,B127,b'We summarize the baselines and the evaluation protocol here :',,
,,,,B128,,,
,,,,B129,,,
,,,,B130,"b'Our evaluation protocol consists of 7 environments ( Ant , Half Cheetah , Humanoid etc ) and both observation space and state space models .'",,
,,,,B131,"b'Solving Half Cheetah environment , when observations are in the pixel space ( images ) , is very challenging as useful information like velocity is not available .'",,
,,,,B132,,,
,,,,B133,,,
,,,,B134,"b'For the observation space model , we use the \xe2\x80\x9c Hybrid model - based and model - free ( Mb - Mf ) algorithm \xe2\x80\x9d ( Nagabandi et al [ 1 ] ) .'",,
,,,,B135,"b'It is a strong baseline where the authors proposed to use a trained , deep neural network based dynamics model to initialize a model - free learning agent to combine the sample efficiency of model - based approaches with the high task - specific performance of model - free methods .'",,
,,,,B136,"b'For the state space models , we use the \xe2\x80\x9c Learning and Querying Fast Generative Models for Reinforcement Learning \xe2\x80\x9d ( Buesing et al [ 2 ] ) as the baseline .'",,
,,,,B137,b'This is a state - of - the - art model for state space models .',,
,,,,B138,"b'As shown by our experiments ( section 5 ) , by having this consistency constraint we outperform both these baselines .'",,
,,,,B139,,,
,,,,B140,,,
,,,,B141,"b'We focus on evaluating the agent for both dynamics models ( in terms of imagination log likelihood ) and policy ( in terms of average episodic returns and loss , figure 2 , 3 , 5 ) .'",,
,,,,B142,b'We show that adding the consistency constraint to the baseline models results in improvements to both the dynamics models and the policy for all the environments that we consider .',,
,,,,B143,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
,,,,B144,,,
,,,,B145,,,
,,,,B146,b'Our key contribution is the proposal of using the consistency loss which helps to learn more powerful policy and better dynamics model ( as demonstrated over different tasks ) while being very easy to integrate with existing model - based RL approaches .',,
,,,,B147,"b'While the proposed approach looks relatively simple , we are not aware of work in RL which describes and validates the benefits of imposing the consistency constraint .'",,
,,,,B148,,,
,,,,B149,,,
,,,,B150,"b""We would appreciate it if the reviewer could take another look at our changes and additional results , and let us know if the reviewer has request for additional changes that would alleviate the reviewer 's concerns .""",,
,,,,B151,,,
,,,,B152,,,
,,,,B153,b'[ 1 ] : Neural Network Dynamics for Model - Based Deep Reinforcement Learning with Model - Free Fine-Tuning - https://arxiv.org/pdf/1708.02596.pdf',,
,,,,B154,,,
,,,,B155,,,
,,,,B156,b'[ 2 ] : Learning and Querying Fast Generative Models for Reinforcement Learning - https://arxiv.org/pdf/1802.03006.pdf',,
,,,,B157,,,
,,,,B158,,,
,,,,B159,"b'We have updated the paper , and the basic motivation is that now ( as the reviewer 2 points out ) the policy is updated using two learning signals - the RL loss , and the loss from the consistency constraint .'",,
,,,,B160,"b'The dynamics model is not used for action selection , but only as an additional learning signal for the policy .'",,
,,,,B161,"b'Learning a good dynamics model is a nice side product , but this model is not used at test time .'",,
,,,,B162,,,
,,,,B163,,,
,,,,B164,"b'We would appreciate it if the reviewer could take another look at our changes and additional results , and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns .'",,
,,,,B165,"b""We hope that our updates to the manuscript address the reviewer 's concerns about clarity , and we hope that the discussion above addresses the reviewer 's concerns about empirical significance .""",,
,,,,B166,b'We once again thank the reviewer for the feedback of our work .',,
,,,,B167,,,
,,,,B168,,,
,,,,B169,,,
,,,,B170,b'Dear Reviewer',,
,,,,B171,,,
,,,,B172,,,
,,,,B173,b'We have added new evaluation results to investigate the robustness of the proposed approach in terms of compounding errors .',,
,,,,B174,"b'When we use the recurrent dynamics model for prediction , the ground - truth sequence is not available for conditioning .'",,
,,,,B175,b'This leads to problems during sampling as even small prediction errors can compound when sampling for a large number of steps .',,
,,,,B176,b'We evaluate the proposed model for robustness by predicting the future for much longer timesteps ( 50 timesteps ) than it was trained on ( 10 timesteps ) .',,
,,,,B177,"b'More generally , in figure 9 ( section 7.3 in appendix ) , we demonstrate that this auxiliary cost helps to learn a better model with improved long - term dependencies by using a training objective that is not solely focused on predicting the next observation , one step at a time .'",,
,,,,B178,,,
,,,,B179,,,
,,,,B180,b'Thank you for your time !',,
,,,,B181,b'The authors appreciate the time reviewers have taken for providing feedback .',,
,,,,B182,b'which resulted in improving the presentation of our paper .',,
,,,,B183,"b'Hence , we would appreciate it if the reviewers could take a look at our changes and additional results , and let us know if they would like to either revise their rating of the paper or request additional changes that would alleviate their concerns .'",,
,,,,B184,,,
,,,,B185,,,
,,,,B186,b'Thank you again for the thoughtful review .',,
,,,,B187,b'We would like to know if our rebuttal adequately addressed your concerns .',,
,,,,B188,b'We would also appreciate any additional feedback on the revised paper .',,
,,,,B189,b'Are there any other aspects of the paper that you think could be improved ?',,
,,,,B190,,,
