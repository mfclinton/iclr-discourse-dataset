Review_index,Review_sentence,Review_affordance,-,Rebuttal_index,Rebuttal_sentence,Related_to,Relation
V0,b'# 1 . Summary',,,B0,b'We would like to thank the reviewer1 for the detailed comments and suggestions for the manuscript .',,
V1,,,,B1,b'We have updated the paper and highlight the major changes with red color .',,
V2,b'This paper presents a novel spatio-temporal attention mechanism .',,,B2,,,
V3,"b'The spatial attention is decomposed from the temporal attention and acts on each frame independently , while the temporal attention is applied on top of it on the temporal domain .'",,,B3,,,
V4,b'The main contribution of the paper is the introduction of regularisers that improve performance and interpretability of the model .',,,B4,b'1 : Novelty',,
V5,,,,B5,,,
V6,,,,B6,b'( 1 ) comparison with [ 1 ] :',,
V7,b'Strengths :',,,B7,,,
V8,,,,B8,b'We use a different attention mechanism .',,
V9,b'*',,,B9,"b'For temporal attention , we use attention mechanism based on Convolutional LSTM , which to our knowledge is novel and has not been used before .'",,
V10,"b'Quality of the paper , although some points need to clarified and expanded a bit more ( see # 2 )'",,,B10,"b'In contrast , [ 1 ] uses cluster attention .'",,
V11,,,,B11,,,
V12,b'*',,,B12,b'The temporal cue in [ 1 ] assumes that temporal information is not important .',,
V13,"b'Nice diversity of experiments , datasets and tasks that the method is tested on ( see # 4 )'",,,B13,"b'I think it depends on the dataset , for some dataset , shuffling / reversing the image sequence may have very small or no influence on the final performance , for instance , something - something dataset is highly influenced by temporal information while UCF101 is less influenced by shuffling [ 3 ] .'",,
V14,,,,B14,"b'Kinetics is also influenced by temporal ordering , the performance drops a lot when reversing the image sequences [ 4 ] .'",,
V15,,,,B15,b'At least temporal information has never been proved harmful for the video action recognition .',,
V16,b'Weaknesses :',,,B16,,,
V17,,,,B17,b'The result reported in [ 1 ] for HMDB51 and UCF101 are obtained using a model with both optical flow and RGB streams .',,
V18,b'*',,,B18,"b'Our method does not use the optical flow stream , therefore , we did not compare with this method directly .'",,
V19,b'The paper do not present substantial novelty compared to previous work ( see # 3 )',,,B19,,,
V20,,,,B20,"b'( 2 ) Comparison with Youtube 8M : Although our temporal attention is also based on LSTM , the convolutional LSTM used in our work is different from the Youtube8M workshop , and our temporal attention also considered the spatial information , especially the spatial attention which learned from our previous spatial attention module .'",,
V21,,,,B21,,,
V22,,,,B22,b'( 3 ) We expanded the related work section in the revised and updated version of the paper .',,
V23,b'# 2 . Clarity and Motivation',,,B23,b'Changes include adding [ 8 ] and other related works mentioned by the reviewer .',,
V24,,,,B24,,,
V25,"b'The paper is in general clear and well motivated , however there are few points that need to be improved :'",,,B25,,,
V26,,,,B26,b'2 . Clarity and Motivation',,
V27,b'*',,,B27,,,
V28,b'How is the importance mask ( Eq. 1 ) is defined ?',,,B28,b'( 1 ) Importance mask : we listed the detailed network structure for the importance mask in appendix B.2 in the newly updated version .',,
V29,"b'The authors said \xe2\x80\x9c we simply use three convolutional layers to learn the importance mask \xe2\x80\x9d , however the convolutional output should be somehow processed to get out the importance map , in order to match the same sizes of X_i .'",,,B29,,,
V30,b'The details of this network are missing to be able to reproduce the model .',,,B30,"b'\\phi( H ) and \\phi( W ) are two fully - connected networks used for generating temporal attention weights , the input \\phi( H_{t - 1 } ) is the hidden layer feature map , and the input \\phi ( X{t} ) is the current feature map .'",,
V31,,,,B31,,,
V32,b'*',,,B32,b'( 2 ) The contrast loss is to make the action foreground and background separable for attention map .',,
V33,"b'The authors introduced \\ phi( H ) and \\phi( X ) which are feedforward networks , but their definition and specifics are not mentioned in the paper .'",,,B33,"b'According to Eq.( 9 ) , the first term encourages the mask value of the foreground region ( M_i>0.5 ) to be 1 , and the second term encourages the mask value of background region ( M_i<0.5 ) to be 0 .'",,
V34,,,,B34,,,
V35,b'*',,,B35,b'( 3 ) We agree with the reviewer that because of the nature of dataset as currently all the datasets we are using contain only a single action and usually this action happens in a sequence of frames .',,
V36,b'It is not clear how Eq. 9 performs regularization of the mask .',,,B36,b'Here we do not consider a video which has more than one action class as we only have one label for each video .',,
V37,b'Can the authors give an intuition about the definition of L_{contrast} ?',,,B37,"b'We agree with the reviewer that each video has one label have some limitations , it will be interesting to explore one video with multiple labels for future work .'",,
V38,b'What does it encourages ?',,,B38,,,
V39,b'In which cases might it be useful ?',,,B39,,,
V40,,,,B40,b'3 .',,
V41,b'*',,,B41,b'Experiments',,
V42,b'Why does L_{unimodal} need to encourage the temporal attention weights to be unimodal ?',,,B42,,,
V43,"b'It seems that the assumption is valid because of the nature of the dataset , i.e. , the video clips contain only a single action with some \xe2\x80\x9c background \xe2\x80\x9d frames in the beginning and the end .'",,,B43,b'( 1 ) .',,
V44,b'This is not valid in general .',,,B44,b'Comparison with previous work [ 5 ] for temporal localization :',,
V45,b'Can the authors discuss about this maybe with an example ?',,,B45,,,
V46,,,,B46,"b'It is important to highlight that our method is weakly supervised , i.e. , only classification labels are used during training .'",,
V47,,,,B47,"b'In other words , no temporal labels are used .'",,
V48,,,,B48,b'While [ 5 ] is a full supervised method .',,
V49,b'# 3 . Novelty',,,B49,"b'Therefore , we did not compare with this method but only compare with the reinforcement learning based method [ 6 ] and weakly supervised method [ 7 ] which share our setting and inputs .'",,
V50,,,,B50,,,
V51,b'The main concern of the proposal in this paper is its novelty .',,,B51,,,
V52,b'Temporal attention pooling have been explored in other papers ; just to cite a popular one among others :',,,B52,b'( 2 ) The performance of video action recognition attention model depends on both the attention model and the backbone architecture .',,
V53,,,,B53,b'A stronger backbone will generally improve the performance of our method .',,
V54,"b'* Long , Xiang , et al .'",,,B54,,,
V55,"b'"" Attention clusters : Purely attention based local feature integration for video classification . ""'",,,B55,"b'For instance , if the base network is ResNet50 , the accuracy is 47.78 % without attention ; with our spatial - temporal attention , the acc is 49.93 % .'",,
V56,b'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition .',,,B56,b'We also run new experiments using base network ResNet101 and ResNet152 .',,
V57,b'2018 .',,,B57,"b'For ResNet101 , the base network accuracy is 49.73 % ; with our spatial - temporal attention , the accuracy reaches 53.07 % .'",,
V58,,,,B58,"b'For ResNet152 , the base network accuracy is 50.04 % ; with our spatial - temporal attention , the accuracy reaches 54.44 % .'",,
V59,b'*',,,B59,,,
V60,b'Other paper from the youtube8m workshops explore the same ideas : https://research.google.com/youtube8m/workshop2017/',,,B60,,,
V61,,,,B61,b'[ 1 ]',,
V62,b'Sec. 2.2 should be expanded by including papers and discuss how the presented temporal attention differs from that .',,,B62,"b'Attention clusters : Purely attention based local feature integration for video classification . ""'",,
V63,,,,B63,b'CVPR 2018 .',,
V64,,,,B64,,,
V65,b'Moreover spatio-temporal attention has been previously explored .',,,B65,b'[ 2 ]',,
V66,"b'For example , the following paper also decouple the spatial and temporal component as the proposal :'",,,B66,b'Other paper from the youtube8m workshops explore the same ideas : https://research.google.com/youtube8m/workshop2017/',,
V67,,,,B67,,,
V68,b'*',,,B68,b'[ 3 ]',,
V69,"b'Song , Sijie , et al .'",,,B69,"b'Zhou , Bolei , Alex Andonian , and Antonio Torralba .'",,
V70,"b'"" An End-to - End Spatio- Temporal Attention Model for Human Action Recognition from Skeleton Data . "" AAAI .'",,,B70,"b'"" Temporal relational reasoning in videos . ""'",,
V71,b'Vol. 1 . No. 2. 2017 .',,,B71,"b'ECCV , 2018 .'",,
V72,,,,B72,,,
V73,"b'This is just an example , but there are there are other papers that model the spatio-temporal extent of videos without attention for action recognition .'",,,B73,b'[ 4 ]',,
V74,b'The authors should expand Sec. 2 by including such relevant literature .',,,B74,"b'Xie , Saining , et al .'",,
V75,,,,B75,"b'"" Rethinking spatiotemporal feature learning : Speed -accuracy trade - offs in video classification . ""'",,
V76,,,,B76,b'ECCV . 2018 .',,
V77,,,,B77,,,
V78,b'# 4 . Experimentation',,,B78,b'[ 5 ]',,
V79,,,,B79,"b'G. Singh , S Saha , M. Sapienza , P. H. S. Torr and F Cuzzolin .'",,
V80,"b'The experiments are carried on video action recognition task on three public available datasets , including HMDB51 , UCF101 and Moments in Time .'",,,B80,"b'"" Online Real time Multiple Spatiotemporal Action Localisation and Prediction . ""'",,
V81,b'The authors show a nice ablation study by removing the main components of the proposed method and show nice improvements with respect to some baseline ( Table 1 ) .',,,B81,"b'ICCV , 2017 .'",,
V82,"b'Although the results are not too close to the state of the art for video action recognition on HMDB51 and UCF101 , the authors first show nice accuracy on Moments in Time ( Table 2 ) .'",,,B82,,,
V83,,,,B83,b'[ 6 ]',,
V84,,,,B84,"b'Yeung , Serena , et al .'",,
V85,"b'Moreover the authors show that the model can be useful on the more challenging task of weakly supervised action localization ( UCF101 - 24 , THUMOS ) .'",,,B85,"b'"" End - to - end learning of action detection from frame glimpses in videos . ""'",,
V86,"b'Specifically , spatial attention is used to localize the action in each frame by thresholding , showing competitive results ( Table 3 ) .'",,,B86,b'CVPR . 2016 .',,
V87,"b'Although some more recent references are missing , see the following paper for example :'",,,B87,,,
V88,,,,B88,b'[ 7 ]',,
V89,"b'* G. Singh , S Saha , M. Sapienza , P. H. S. Torr and F Cuzzolin .'",,,B89,"b'Wang , Limin , et al .'",,
V90,"b'"" Online Real time Multiple Spatiotemporal Action Localisation and Prediction . ""'",,,B90,"b'"" Untrimmednets for weakly supervised action recognition and detection . ""'",,
V91,"b'ICCV , 2017 .'",,,B91,b'ICCV . 2017 .',,
V92,,,,B92,,,
V93,b'Then the authors tested also for temporal action localization ( Table 4 ) .',,,B93,b'[ 8 ]',,
V94,,,,B94,"b'Song , Sijie , et al .'",,
V95,,,,B95,"b'"" An End-to - End Spatio- Temporal Attention Model for Human Action Recognition from Skeleton Data . "" AAAI . 2017 .'",,
V96,"b'In general , the paper is not showing state - of - the- art results , however the diversity of experiments , datasets and tasks that are presented makes it pretty solid and interesting .'",,,B96,,,
V97,,,,,,,
V98,,,,,,,
V99,,,,,,,
