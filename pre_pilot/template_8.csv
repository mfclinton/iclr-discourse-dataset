Review_index,Review_sentence,Review_affordance,-,Rebuttal_index,Rebuttal_sentence,Related_to,Relation
V0,,,,B0,b'We thank the reviewer for the feedback .',,
V1,b'Summary :',,,B1,"b'We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below .'",,
V2,,,,B2,"b'We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have .'",,
V3,,,,B3,b'We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places .',,
V4,b'This paper presents a simple auxiliary loss term for model - based RL that attempts to enforce consistency between observed experience trajectories and hallucinated rollouts .',,,B4,"b""We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions .""",,
V5,b'Simple experiments demonstrate that the constraint slightly improves performance .',,,B5,b'We briefly summarize the key idea of the paper and then address the specific concerns .',,
V6,,,,B6,,,
V7,,,,B7,,,
V8,b'Quality :',,,B8,b'What is the idea ?',,
V9,,,,B9,,,
V10,,,,B10,b'=====================================',,
V11,"b'While I think the idea of a consistency constraint is probably reasonable , I consider this a poorly executed exploration of the idea .'",,,B11,,,
V12,b'The paper makes no serious effort to compare and contrast this idea with other efforts at model - based RL .',,,B12,,,
V13,"b'The most glaring omission is comparison to very old ideas ( such as dyna ) and new ideas ( such as imagination agents ) , both of which they cite .'",,,B13,b'Our goal is to provide a mechanism for the agent to lean a better dynamics model as well as more powerful policy by ensuring the consistency in their predictions ( such that predictions from the model are grounded in the real environment ) .',,
V14,,,,B14,,,
V15,,,,B15,,,
V16,b'Clarity :',,,B16,b'This mechanism enables the agent to have a direct \xe2\x80\x9c interaction \xe2\x80\x9d b/w the agent \xe2\x80\x99s policy and its dynamics model .',,
V17,,,,B17,"b'This interaction is different from the standard approaches in reinforcement learning where the agent uses the policy to sample trajectories over which the agent is trained , and then use these sampled trajectories to learn the dynamics model .'",,
V18,,,,B18,"b'In those cases , there is no ( direct ) mechanism for the dynamics model to affect the policy , and hence there is no \xe2\x80\x9c direct interaction \xe2\x80\x9d between the policy and the dynamics model .'",,
V19,"b'The paper is reasonably clear , although there are some holes .'",,,B19,"b'In our case , both the policy and the model are trained jointly while making sure that the predictions from the dynamics model are consistent with the observation from the environment .'",,
V20,"b'For example , in the experimental section , it is unclear what model - based RL algorithm is being used , and how it was modified to support the consistency constraint .'",,,B20,b'This provides a mechanism where learning a model can itself change the policy ( thus \xe2\x80\x9c interacting \xe2\x80\x9d with the environment ) instead of just training on the data coming from a policy which is trained independently of how well the model performs on the collected .',,
V21,b'( I did not read the appendix ) .',,,B21,,,
V22,,,,B22,,,
V23,,,,B23,b'A practical instantiation of this idea is the consistency loss where we ensure consistency between the predictions from the dynamics model and the actual observations from the environment and this simple baseline works surprisingly well compared to the state of the art methods ( as demonstrated by our experiments ) and that others have not tried it before .',,
V24,b'Originality :',,,B24,b'Applying consistency constraint means we have two learning signals for the policy :',,
V25,,,,B25,b'The one from the reinforcement learning loss ( i.e maximize return ) and the other due to consistency constraint .',,
V26,,,,B26,b'We show that adding the proposed consistency constraint helps the agent to learn better dynamics model and as well as better policy for both observation space models and state space models .',,
V27,b'It is not clear how novel the central idea is .',,,B27,b'We compare against strong baselines :',,
V28,,,,B28,,,
V29,,,,B29,,,
V30,b'Significance :',,,B30,b'Hybrid model - based and model - free ( Mb - Mf ) algorithm ( Nagabandi et al [ 1 ] )',,
V31,,,,B31,,,
V32,,,,B32,b'Learning and Querying Fast Generative Models for Reinforcement Learning ( Buesing et al [ 2 ] ) - This is a state - of - the - art model for state space models .',,
V33,b'This idea is not significant .',,,B33,,,
V34,,,,B34,,,
V35,,,,B35,b'Our evaluation protocol considers a total of 7 environments and we show that using the consistency constraint leads to better generative models ( in terms of log likelihood ) and more powerful policy ( average return ) for all the cases .',,
V36,b'Pros :',,,B36,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
V37,,,,B37,,,
V38,"b'+ A simple , straightforward idea'",,,B38,,,
V39,,,,B39,b'Our key contribution is the proposal of using the consistency loss which helps to learn more powerful policy and better dynamics model ( as demonstrated over different tasks ) while being very easy to integrate with existing model - based RL approaches .',,
V40,b'+ A good topic - progress in model - based RL is always welcome',,,B40,"b'While our method is relatively simple , we are not aware of prior works that show something similar , and we believe such a simple baseline would be useful for anyone who \xe2\x80\x99s working on model - based RL.'",,
V41,,,,B41,"b'Further , our experiment demonstrates the effectiveness of the approach .'",,
V42,,,,B42,"b'If we are mistaken regarding prior works , please let us know !'",,
V43,b'Cons :',,,B43,,,
V44,,,,B44,,,
V45,b'- Unclear how this is significantly different from other related work ( such as imagination agents )',,,B45,"b'We would like to emphasize that our work presents an extensive comparative evaluation , and we believe that these results should be taken into consideration in evaluating our work .'",,
V46,,,,B46,b'We compare multiple approaches across more than 5 simulated tasks to the state of the art methods .',,
V47,b'- Experimental setup is poorly executed .',,,B47,"b'Hopefully , our clarifications are convincing in terms of explaining why the evaluation is fair and rigorous , and we would , of course , be happy to modify it as needed .'",,
V48,,,,B48,"b'But at a higher level , the fact that such simple model - based approaches work better than somewhat complex model - free approaches actually is the point of the paper to me .'",,
V49,b'- Statistical significance of improvements is unclear',,,B49,,,
V50,,,,B50,,,
V51,b'-',,,B51,b'Continued ...',,
V52,b'No attempt to relate to any other method in the field',,,B52,,,
V53,,,,B53,,,
V54,b'-',,,B54,,,
V55,b'No explanation of what algorithms are being used',,,B55,b'What are the empirical results ?',,
V56,,,,B56,,,
V57,,,,B57,b'=====================================',,
,,,,B58,,,
,,,,B59,,,
,,,,B60,"b'Our evaluation protocol consists of 7 environments ( Ant , Half Cheetah , Humanoid etc ) and both observation space and state space models .'",,
,,,,B61,"b'Solving Half Cheetah environment , when observations are in the pixel space ( images ) , is very challenging as useful information like velocity is not available .'",,
,,,,B62,,,
,,,,B63,,,
,,,,B64,"b'For the observation space model , we use the \xe2\x80\x9c Hybrid model - based and model - free ( Mb - Mf ) algorithm \xe2\x80\x9d ( Nagabandi et al [ 1 ] ) .'",,
,,,,B65,"b'It is a strong baseline where the authors proposed to use a trained , deep neural network based dynamics model to initialize a model - free learning agent to combine the sample efficiency of model - based approaches with the high task - specific performance of model - free methods .'",,
,,,,B66,"b'For the state space models , we use the \xe2\x80\x9c Learning and Querying Fast Generative Models for Reinforcement Learning \xe2\x80\x9d ( Buesing et al [ 2 ] ) as the baseline .'",,
,,,,B67,b'This is a state - of - the - art model for state space models .',,
,,,,B68,"b'As shown by our experiments ( section 5 ) , by having this consistency constraint we outperform both these baselines .'",,
,,,,B69,,,
,,,,B70,,,
,,,,B71,"b'We focus on evaluating the agent for both dynamics models ( in terms of imagination log likelihood , figure 4 ) and policy ( in terms of average episodic returns and loss , figure 2 , 3 , 5 ) .'",,
,,,,B72,b'We show that adding the consistency constraint to the baseline models results in improvements to both the dynamics models and the policy for all the environments that we consider .',,
,,,,B73,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
,,,,B74,,,
,,,,B75,,,
,,,,B76,b'==================================================================================================',,
,,,,B77,,,
,,,,B78,,,
,,,,B79,b'We now refer to the specific aspects of the reviews :',,
,,,,B80,,,
,,,,B81,,,
,,,,B82,"b'"" This paper presents a simple auxiliary loss term for model - based RL that attempts to enforce consistency between observed experience trajectories and hallucinated rollouts .'",,
,,,,B83,"b'Simple experiments demonstrate that the constraint slightly improves performance . ""'",,
,,,,B84,,,
,,,,B85,,,
,,,,B86,b'Thanks for the very useful feedback .',,
,,,,B87,"b'We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below .'",,
,,,,B88,"b'We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have .'",,
,,,,B89,,,
,,,,B90,,,
,,,,B91,,,
,,,,B92,b'=====================================',,
,,,,B93,,,
,,,,B94,,,
,,,,B95,"b'"" Why is different from just learning a model based on k-step prediction ? ""'",,
,,,,B96,,,
,,,,B97,"b'"" Unclear how this is significantly different from other related work ( such as imagination agents ) ""'",,
,,,,B98,,,
,,,,B99,,,
,,,,B100,"b""Our approach is different from just learning a k-step prediction model as in our case , the agent \xe2\x80\x99s behavior ( i.e the agent 's policy ) is dependent on its internal model too .""",,
,,,,B101,"b'In the standard case , the policy is optimized only using the RL gradient i.e maximizing expected reward and the state transition pairs ( collected as the agent acts in the environment ) become the supervising dataset for learning the model , and hence the policy is not affected when the model is being updated and there is no feedback from the model learning process to the policy .'",,
,,,,B102,"b'Hence , the data used for training the model is coming from a policy which is trained independently of how well the model performs on the collected trajectories .'",,
,,,,B103,b'So the process of learning the model has no control over what kind of data is produced for its training .',,
,,,,B104,,,
,,,,B105,,,
,,,,B106,b'We propose to train both the policy and the model during the open loop .',,
,,,,B107,b'Hence the k-step predictions are used for training both the model and the policy simultaneously .',,
,,,,B108,"b'Training the policy on both the RL loss and the consistency loss provides a mechanism where learning a model , can itself change the policy thus leading to a much closer interplay between the dynamics model and the policy .'",,
,,,,B109,b'We show that this relatively simple approach leads to much better performance when compared to very strong baselines for both observation space and state space models for all the 7 environments we considered .',,
,,,,B110,,,
,,,,B111,,,
,,,,B112,b'We have updated the paper ( section 3 ) to describe the baselines and how to modifiy the baselines for applying the consistency constraint for both the observation space models ( section 3.2 ) and the state space models ( section 3.3 ) .',,
,,,,B113,,,
,,,,B114,"b'Experiments ( Section 5 ) shows the improvement that result by the use of consistency constaint for both observation space models ( figure 2 , 3 ) and state space models ( figure 4 , 5 )'",,
,,,,B115,,,
,,,,B116,,,
,,,,B117,b'Continued',,
,,,,B118,,,
,,,,B119,,,
,,,,B120,,,
,,,,B121,,,
,,,,B122,"b'"" no serious effort to compare and contrast this idea with other efforts at model - based RL. \xe2\x80\xa6 it is unclear what model - based RL algorithm is being used , and how it was modified to support the consistency constraint . ""'",,
,,,,B123,,,
,,,,B124,,,
,,,,B125,b'We have updated the paper to address the concern about the baselines and the proposed approach not being described in detail .',,
,,,,B126,b'Section 3.1 describes the different loss components and how the consistency constraint can be applied in the general .',,
,,,,B127,b'Section 3.2 and 3.3 describes the baselines and how these baselines were modified to support the consistency constraint for the observation space and the state space models respectively .',,
,,,,B128,,,
,,,,B129,,,
,,,,B130,b'We summarize the baselines and the evaluation protocol here :',,
,,,,B131,,,
,,,,B132,,,
,,,,B133,"b'Our evaluation protocol consists of 7 environments ( Ant , Half Cheetah , Humanoid etc ) and both observation space and state space models .'",,
,,,,B134,"b'For the observation space model , we use the \xe2\x80\x9c Hybrid model - based and model - free ( Mb - Mf ) algorithm \xe2\x80\x9d ( Nagabandi et al [ 1 ] ) which is a very strong baseline and for the state space models , we use the \xe2\x80\x9c Learning and Querying Fast Generative Models for Reinforcement Learning \xe2\x80\x9d ( Buesing et al [ 2 ] ) as the baseline .'",,
,,,,B135,b'This model is a state - of - the - art model for state space models .',,
,,,,B136,,,
,,,,B137,,,
,,,,B138,"b'We focus on evaluating the agent for both dynamics models ( in terms of imagination log likelihood , figure 4 ) and policy ( in terms of average episodic returns and loss , figure 2 , 3 , 5 ) .'",,
,,,,B139,b'We show that adding the consistency constraint to the baseline models results in improvements to both the dynamics models and the policy for all the environments that we consider .',,
,,,,B140,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
,,,,B141,,,
,,,,B142,,,
,,,,B143,b'=====================================',,
,,,,B144,,,
,,,,B145,,,
,,,,B146,,,
,,,,B147,"b'"" It is not clear how novel the central idea is .'",,
,,,,B148,"b'""'",,
,,,,B149,,,
,,,,B150,,,
,,,,B151,b'Our key contribution is the proposal of using the consistency loss which helps to learn more powerful policy and better dynamics model ( as demonstrated over different tasks ) while being very easy to integrate with existing model - based RL approaches .',,
,,,,B152,"b'While our method is relatively simple , we are not aware of prior works that show something similar , and we believe such a simple baseline would be useful for anyone who \xe2\x80\x99s working on model - based RL.'",,
,,,,B153,"b'Further , our experiment demonstrates the effectiveness of the approach .'",,
,,,,B154,"b'If we are mistaken regarding prior works , please let us know !'",,
,,,,B155,,,
,,,,B156,,,
,,,,B157,"b'We would like to emphasize that our work presents an extensive comparative evaluation , and we believe that these results should be taken into consideration in evaluating our work .'",,
,,,,B158,b'We compare multiple approaches across more than 5 simulated tasks to the state of the art methods .',,
,,,,B159,"b'Hopefully , our clarifications are convincing in terms of explaining why the evaluation is fair and rigorous , and we would , of course , be happy to modify it as needed .'",,
,,,,B160,"b'But at a higher level , the fact that such simple model - based approaches work better than somewhat complex model - free approaches actually is the point of the paper to me .'",,
,,,,B161,,,
,,,,B162,,,
,,,,B163,,,
,,,,B164,b'=====================================',,
,,,,B165,,,
,,,,B166,,,
,,,,B167,,,
,,,,B168,"b'"" Statistical significance of improvements is unclear ""'",,
,,,,B169,,,
,,,,B170,,,
,,,,B171,"b'Our evaluation protocol ( section 5 ) consists of 7 environments ( Ant , Half Cheetah , Humanoid etc ) and both observation space and state space models .'",,
,,,,B172,"b'Solving Half Cheetah environment , when observations are in the pixel space ( images ) , is very challenging as useful information like velocity is not available .'",,
,,,,B173,,,
,,,,B174,,,
,,,,B175,,,
,,,,B176,"b'For the observation space model ( section 5.1 ) , we use the \xe2\x80\x9c Hybrid model - based and model - free ( Mb - Mf ) algorithm \xe2\x80\x9d ( Nagabandi et al [ 1 ] ) .'",,
,,,,B177,"b'It is a very strong baseline where the authors proposed to use a trained , deep neural network based dynamics model to initialize a model - free learning agent to combine the sample efficiency of model - based approaches with the high task - specific performance of model - free methods .'",,
,,,,B178,"b'For the state space models ( section 5.2 ) , we use the \xe2\x80\x9c Learning and Querying Fast Generative Models for Reinforcement Learning \xe2\x80\x9d ( Buesing et al [ 2 ] ) as the baseline .'",,
,,,,B179,b'This is a state - of - the - art model for state space models .',,
,,,,B180,,,
,,,,B181,,,
,,,,B182,"b'We focus on evaluating the agent for both dynamics models ( in terms of imagination log likelihood ) ( figure 4 ) and policy ( in terms of average episodic returns ) ( figure 2 , 3 , 5 ) .'",,
,,,,B183,b'We show that adding the consistency constraint to the baseline models results in improvements to both the dynamics models and the policy for all the environments that we consider .',,
,,,,B184,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
,,,,B185,,,
,,,,B186,,,
,,,,B187,"b""We would appreciate it if the reviewer could take another look at our changes and additional results , and let us know if the reviewer has a request for additional changes that would alleviate the reviewer 's concerns .""",,
,,,,B188,,,
,,,,B189,,,
,,,,B190,b'[ 1 ] : Neural Network Dynamics for Model - Based Deep Reinforcement Learning with Model - Free Fine-Tuning - https://arxiv.org/pdf/1708.02596.pdf',,
,,,,B191,,,
,,,,B192,,,
,,,,B193,b'[ 2 ] : Learning and Querying Fast Generative Models for Reinforcement Learning - https://arxiv.org/pdf/1802.03006.pdf',,
,,,,B194,,,
,,,,B195,,,
,,,,B196,b'Dear Reviewer',,
,,,,B197,,,
,,,,B198,,,
,,,,B199,b'We have added new evaluation results to investigate the robustness of the proposed approach in terms of compounding errors .',,
,,,,B200,"b'When we use the recurrent dynamics model for prediction , the ground - truth sequence is not available for conditioning .'",,
,,,,B201,b'This leads to problems during sampling as even small prediction errors can compound when sampling for a large number of steps .',,
,,,,B202,b'We evaluate the proposed model for robustness by predicting the future for much longer timesteps ( 50 timesteps ) than it was trained on ( 10 timesteps ) .',,
,,,,B203,"b'More generally , in figure 9 ( section 7.3 in appendix ) , we demonstrate that this auxiliary cost helps to learn a better model with improved long - term dependencies by using a training objective that is not solely focused on predicting the next observation , one step at a time .'",,
,,,,B204,,,
,,,,B205,,,
,,,,B206,b'Thank you for your time !',,
,,,,B207,b'The authors appreciate the time reviewers have taken for providing feedback .',,
,,,,B208,b'which resulted in improving the presentation of our paper .',,
,,,,B209,"b'Hence , we would appreciate it if the reviewers could take a look at our changes and additional results , and let us know if they would like to either revise their rating of the paper or request additional changes that would alleviate their concerns .'",,
,,,,B210,,,
,,,,B211,,,
,,,,B212,b'Thank you again for the thoughtful review .',,
,,,,B213,b'We would like to know if our rebuttal adequately addressed your concerns .',,
,,,,B214,b'We would also appreciate any additional feedback on the revised paper .',,
,,,,B215,b'Are there any other aspects of the paper that you think could be improved ?',,
,,,,B216,,,
,,,,B217,,,
