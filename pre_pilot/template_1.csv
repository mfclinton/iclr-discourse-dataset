Review_index,Review_sentence,Review_affordance,-,Rebuttal_index,Rebuttal_sentence,Related_to,Relation
V0,b'Summary :',,,B0,"b'Thank you for your review , especially the comments regarding the clarity of the method description and experimental setting , which are helping us to revise the text to depend less on familiarity with meta-learning approaches and few - shot learning setups .'",Thanks,
V1,,,,B1,"b'In the meantime we offer clarifications here , and in particular address the problem setting , architecture and optimization novelty , and experimental comparisons .'",summary,
V2,b'This paper proposed a few - shot learning approach for interactive segmentation .',,,B2,b'We will make a follow - up post once the revision is uploaded .',Planning,
V3,"b'Given a set of user- annotated points , the proposed model learns to generate dense segmentation masks of objects .'",,,B3,"b""Please let us know if the method and experiments are now clear , and how these details impact your evaluation of the submission 's originality , significance , and experiments .""",Follow up,
V4,"b'To incorporate the point- wise annotation , the guidance network is introduced .'",,,B4,,,
V5,"b'The proposed idea is applied to guided image segmentation , semantic segmentation , and video segmentation .'",,,B5,,,
V6,,,,B6,b'>',,
V7,,,,B7,b'This paper proposed a few - shot learning approach for interactive segmentation',summary,
V8,b'Clarity :',,,B8,,,
V9,,,,B9,,,
V10,"b'Overall , the presentation of the paper can be significantly improved .'",,,B10,b'We would like to clarify that our work is an extension of interactive segmentation .',,
V11,"b'First of all , it is not clear what the problem setting of this paper is , as it seems to have two sets of training data of fully - annotated images ( for training ) and the combined set of point - wise annotated images and unannotated images ( guidance images T in the first equation ) ;'",,,B11,"b'Our meta-learning learning approach , guided segmentation , generalizes the usual problem statement of interactive segmentation .'",,
V12,"b'It is not clear whether authors generate the second dataset out of the first one , or they have separate datasets for these two .'",,,B12,"b'Given an image with partial annotations , an interactive segmentor fully segments that image , but it cannot segment a new image without any annotations .'",,
V13,"b'Also , it is not clear how the authors incorporate the unannotated images for training .'",,,B13,"b'That is , for an interactive segmentor , annotations on one image do not inform the segmentation of another image .'",,
V14,,,,B14,"b'On the other hand , our guided segmentor extracts a latent representation of the pixel - wise annotations and conditions on it to inform the segmentation of all images , and additional annotations on any image affect the segmentation of all of them .'",,
V15,,,,B15,,,
V16,"b'The descriptions on model architecture are also not quite clear , as it involves two components ( g and f ) but start discussing with g without providing a clear overview of the combined model ( I would suggest changing the order of Section 4.1 and Section 4.2 to make it clearer ) .'",,,B16,,,
V17,"b'The loss functions are introduced in the last part of the method , which makes it also very difficult to understand .'",,,B17,b'>',,
V18,,,,B18,b'I do not see many novel contributions in terms of both network architecture and learning perspective .',Quote,
V19,,,,B19,,,
V20,b'Originality and significance :',,,B20,,,
V21,,,,B21,,,
V22,b'The technical contribution of the paper is very limited .',,,B22,"b'Prior work is limited to binary segmentation of a single image ( interactive segmentation by Xu et al. 2016 ) , two -class tasks supervised by dense annotations from a single image ( one - shot semantic segmentation by Shaban et al. 2017 ) , and slow optimization that fails for sparse annotations ( video object segmentation through fine-tuning by Caelles et al. 2017 ) .'",Disagreement (premise),
V23,b'I do not see many novel contributions in terms of both network architecture and learning perspective .',,,B23,b'Our novel choices for architecture and optimization are key to addressing these issues :',Disagreement (premise),
V24,,,,B24,,,
V25,,,,B25,,,
V26,b'Experiment :',,,B26,"b'- Our novel late-fusion architecture ( Section 4.1 and Figure 3 ) is necessary for efficient representation and segmentation from annotations that are multi-shot ( multi-image , multi-pixel ) and multi-way ( multi-class ) .'",Disagreement (premise),
V27,,,,B27,"b'Xu et al. and Shaban et al. , with their early fusion architectures , are limited to one image and two classes at a time .'",Disagreement (premise),
V28,"b'Overall , I am not quite convinced with the experiment results .'",,,B28,"b'When annotations change , they must re-compute the entire network as the annotations are fused early at the input , while we update in constant time w.r.t. the full network time since only the late stage is re-computed .'",Disagreement (premise),
V29,"b'The method is compared against only a few ( not popular ) interactive segmentation methods , although there exist many recent works addressing the same task ( e.g. Xu et al. 2016 ) .'",,,B29,"b'For multi-class segmentation , our model simply and efficiently fuses shared image features with the annotations for each class ( end of Section 4.1 ) , while Xu et al. and Shaban et al .'",Disagreement (premise),
V30,,,,B30,b'inefficiently have to do a forward pass for each class .',Disagreement (premise),
V31,,,,B31,,Disagreement (premise),
V32,b'The experiment settings are also not clearly presented .',,,B32,"b'- With optimization by meta-learning , our model learns to handle sparse annotations that the Caelles et al .'",Disagreement (premise),
V33,"b'For instance , what is the dataset used for the evaluation of the first paragraph in section 5.1 ?'",,,B33,b'approach of optimization by fine-tuning fails on .',Disagreement (premise),
V34,b'How do you split the Pascal VOC data to exclusive sets ?',,,B34,"b'While Shaban et al . likewise optimize by meta-learning , they require dense annotations , and we show more than 50 % relative improvement for accuracy in the sparse regime .'",Disagreement (premise),
V35,b'How do you sample point- wise annotation from dense mask labels ?',,,B35,,,
V36,b'How does the sampling procedure affect the performance ?',,,B36,"b'- Our novel contributions to meta-learning optimization ( Section 4.3 ) are ( 1 ) sampling tasks with different shot ( number of labels ) and way ( number of classes ) per episode of optimization for better generalization to different amounts of supervision and ( 2 ) investigating transfer learning when meta-learning one kind of task , instances , then meta-testing on a different kind of task , semantics .'",Summary (Accomplishments),
V37,,,,B37,,,
V38,,,,B38,,,
V39,"b'The performance of the guided semantic segmentation is also quite low , limiting the practical usefulness of the method .'",,,B39,"b'For novelty in experiments , our work is the first to show results on this set of tasks with a unified model .'",Disagreement (premise),
V40,"b'Finally , the paper does not present qualitative results , which are essential to understanding the performance of the segmentation system .'",,,B40,,,
V41,,,,B41,,,
V42,,,,B42,b'>',,
V43,b'Minor comments :',,,B43,"b'The method is compared against only a few ( not popular ) interactive segmentation methods , although there exist many recent works addressing the same task ( e.g. Xu et al. 2016 )'",,
V44,,,,B44,,,
V45,b'1 .',,,B45,,,
V46,b'There are a lot of grammar issues .',,,B46,"b'For comparison we chose popular , state - of - the - art at publication methods : DIOS ( Xu et al. 2016 ) for interactive segmentation and OSVOS ( Caelles et al. ) for video object segmentation .'",Comparison,
V47,b'Please revise your draft .',,,B47,b'To the best of our knowledge Shaban et al .',,
V48,,,,B48,b'2017 is the first and only few - shot semantic segmentor prior to our work .',,
V49,b'2 .',,,B49,"b'Furthermore , these methods were chosen for fair comparison since their architectures and ours are all derived from a VGG - 16 backbone and are free from confounding differences in post - processing , data augmentation , and so forth .'",Disagreement (premise),
V50,b'Please revise the notations in equations .',,,B50,"b'Our work shows results on few - shot semantic segmentation , video object segmentation , and interactive instance segmentation ( as mentioned above , guided segmentation is not simply interactive segmentation , as evidenced by this set of tasks ) .'",Summary,
V51,"b'For instance ,'",,,B51,,,
V52,,,,B52,,,
V53,"b'T = {{ ( x_1 , L_1 ) , ...} \\cup {\\bar{x} _1 , ...}'",,,B53,b'We ask that the reviewer please be specific about alternative comparisons .',Follow up,
V54,,,,B54,,,
V55,"b'L_s = { ( p_j , l_j ) : j\\in{1 , ... , P} , l\\in{1 , ... , K}\\cup{\\emptyset}}'",,,B55,,,
V56,,,,B56,,,
V57,"b'Also , in the next equation , j\\in\\bar{x}_q} -> p_ j\\in\\bar{x}_q} ( j is an index of pixel )'",,,B57,"b'> it is not clear what the problem setting of this paper is , as it seems to have two sets of training data of fully - annotated images ( for training ) and the combined set of point - wise annotated images and unannotated images ( guidance images )'",,
V58,,,,B58,,,
V59,,,,B59,,,
,,,,B60,b'Our problem setting is meta-learning for segmentation .',,
,,,,B61,"b'Meta-learning seeks to learn a learning algorithm that can learn a new task , often from little supervision .'",Answer Elaboration,
,,,,B62,"b'In our case , a task consists of a support set of ( sparsely ) labeled images and a query set of unlabeled images to be segmented .'",Answer Elaboration,
,,,,B63,"b'In the standard terminology of few - shot learning , the "" point - wise annotated images "" are the labeled supports and the "" unannotated images "" are the queries to be segmented according to the labeled support .'",Answer Elaboration,
,,,,B64,,,
,,,,B65,,,
,,,,B66,b'We divide the set of tasks into sets for meta-training and meta-testing .',Arugment,
,,,,B67,"b'We optimize the parameters of our model to perform learning on tasks drawn from the meta-training set , and evaluate on tasks drawn from meta-test .'",Arugment,
,,,,B68,"b'For our guided nets , learning a task corresponds to inference in the model , which we call guidance : extracting the task representation from the supports and guided inference to segment the queries .'",Arugment,
,,,,B69,"b'Meta-training optimizes the model parameters to improve guidance , and once meta-training is complete the model parameters are fixed and only the task representation changes as a function of the support .'",Arugment,
,,,,B70,"b'For meta-testing we evaluate on heldout instances , classes , or videos in our interactive , semantic , and video object segmentation results respectively .'",,
,,,,B71,,,
,,,,B72,,,
,,,,B73,b'>',,
,,,,B74,"b'It is not clear whether authors generate the second dataset out of the first one , or they have separate datasets for these two .'",,
,,,,B75,,,
,,,,B76,,,
,,,,B77,"b'This kind of dataset division is a common approach to few - shot learning for image classification ( e.g. , Omniglot from Lake et al. 2015 , miniImage'",Answer Elaboration,
,,,,B78,b'Net from Vinyals et al. 2016 ) that we adapt to pixel - wise tasks .',,
,,,,B79,,,
,,,,B80,,,
,,,,B81,"b'We generate our sparse meta-learning datasets from the standard , fully - annotated segmentation datasets by sampling different tasks ( e.g. , segment a particular bear in all the frames of the video ) and subsampling the annotations .'",Answer Elaboration,
,,,,B82,b'A task consists of a support set of ( sparsely ) labeled images and a query set of unlabeled images to be segmented .',Answer Elaboration,
,,,,B83,"b'Tasks are synthesized from a densely labeled dataset such as PASCAL by binarizing and sparsifying dense masks , as illustrated in Section 4.3 Figure 4 .'",Answer Elaboration,
,,,,B84,"b'During training , the query set is given as input to the model without labels , and the dense ground truth labels for the query set used to define the loss .'",Answer Elaboration,
,,,,B85,b'We are revising section 4.3 to clearly explain this process .',,
,,,,B86,,,
,,,,B87,,,
,,,,B88,b'> it is not clear how the authors incorporate the unannotated images for training ( guidance images )',,
,,,,B89,,,
,,,,B90,,,
,,,,B91,"b'Our method is trained by meta-learning through episodic optimization : during meta-training , the unnannotated images are given as queries to be segmented by the model , the model infers an output segmentation , and these are compared against the true segmentation of the queries ( known only during meta-training ) .'",,
,,,,B92,b'Please see figure 4 and section 4.3 .',,
,,,,B93,b'Are queries what was meant by guidance images ?',Follow up,
,,,,B94,,,
,,,,B95,,,
,,,,B96,b'> what is the dataset used for the evaluation of the first paragraph in section 5.1 ?',,
,,,,B97,b'How do you split the Pascal VOC data to exclusive sets ?',,
,,,,B98,,,
,,,,B99,,,
,,,,B100,"b'The dataset used in the first paragraph of Section 5.1 is PASCAL VOC / SBD , as used in Xu et al. , which we compare against ( we are correcting this omission in a revision of the text \xe2\x80\x94 thank you for noticing it ) .'",,
,,,,B101,"b'For few - shot semantic segmentation , we follow the experimental protocol of Shaban et al. , as stated in the second to last paragraph of Section 5.1 , which tests few - shot performance on held - out classes by dividing the 20 classes of PASCAL into 4 sets of 5 , then reports the average performance across these sets for the 5 held - out classes after training on the remaining 15 .'",,
,,,,B102,b'Images that contain both held - out and training classes are placed in the held - out set .',,
,,,,B103,,,
,,,,B104,,,
,,,,B105,b'>',,
,,,,B106,b'How do you sample point- wise annotation from dense mask labels ?',,
,,,,B107,b'How does the sampling procedure affect the performance ?',,
,,,,B108,,,
,,,,B109,,,
,,,,B110,b'The dense ground truth labels are sparsified via uniform random sampling .',,
,,,,B111,"b'We found random sampling to perform about equal to more complex sampling strategies explored in previous work , such as Xu et al. 2016 .'",,
,,,,B112,b'We are adding these details to the paper appendix .',,
,,,,B113,,,
,,,,B114,,,
,,,,B115,b'>',,
,,,,B116,b'The performance of the guided semantic segmentation is also quite low',,
,,,,B117,,,
,,,,B118,,,
,,,,B119,b'The performance of our method is in some cases lower than the performance of task - specific methods ( video object segmentation and 5 - shot semantic segmentation ) .',,
,,,,B120,"b'However , a main contribution of our work is to present a first general meta-learning framework for structured output tasks .'",,
,,,,B121,"b'A compensating advantage of our proposed late fusion architecture is that it is quicker to update than Shaban et al. and Caelles et al. , making it more practical for interactive use .'",,
,,,,B122,,,
,,,,B123,,,
,,,,B124,b'>',,
,,,,B125,b'Please revise the notations in equations .',,
,,,,B126,,,
,,,,B127,,,
,,,,B128,b'Thank you for noticing these typsetting errors !',,
,,,,B129,b'We are correcting them in a revision of the text .',,
,,,,B130,,,
