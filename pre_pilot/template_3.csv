Review_index,Review_sentence,Review_affordance,-,Rebuttal_index,Rebuttal_sentence,Related_to,Relation
V0,b'The paper propose an end - to - end technique that applies both spatial and temporal attention .',,,B0,b'We would like to thank the reviewer for the detailed comments and suggestions for the manuscript .',,
V1,"b'The spatial attention is done by training a mask - filter , while the temporal - attention use a soft-attention mechanism .'",,,B1,b'We have updated the paper and highlight the major changes with red colour .',,
V2,b'In addition the authors propose several regularization terms to directly improve attention .',,,B2,b'The following replies are used to address the concerns .',,
V3,"b'The evaluated datasets are action recognition datasets , such as HMDB51 , UCF10 , Moments in Time , THUMOS \xe2\x80\x99 14 .'",,,B3,,,
V4,b'The paper reports SOTA on all three datasets .',,,B4,,,
V5,,,,B5,b'1 .',,
V6,,,,B6,b'So sorry for the confusing numbers in HMDB51 / UCF101 .',,
V7,,,,B7,"b'We have already updated the results and add more baselines in Table 1 of the paper , please refer to the updated version .'",,
V8,,,,B8,"b'For these two datasets , we mainly used them as ablation study and compare with state- the- art attention based methods with the same RGB inputs , rather than competing the state - of - the- art accuracy with other highly complicated and computational expensive models , such as I3D .'",,
V9,b'Strengths :',,,B9,"b'For HMDB51 / UCF101 , our baselines are other state - of - the-art attention models ( such as papers from NIPS2017 [ 1 ] and CVIU2018 [ 2 ] ) with RGB images and RGB images using the same base network without attention mechanism .'",,
V10,,,,B10,b'We achieve better results than baseline methods .',,
V11,,,,B11,,,
V12,"b'The paper is well written : easy to follow , and describe the importance of spatial - temporal attention .'",,,B12,,,
V13,,,,B13,b'2 .',,
V14,,,,B14,"b'For the spatial localization on Thumos 14 dataset , we have added the results of R-C3D [ 3 ] as one of our baseline method in our updated version .'",,
V15,"b'The model is simple , and propose novel attention regularization terms .'",,,B15,"b'But notable to say that R-C3D is a full supervised learning method which needs bounding boxes during training , which is very expensive as bounding boxes are needed for each frame .'",,
V16,,,,B16,b'While our method is a weakly supervised learning method which is trained only with class labels but without bounding boxes .',,
V17,,,,B17,,,
V18,"b'The authors evaluates on several tasks , and shows good qualitative behavior .'",,,B18,,,
V19,,,,B19,b'3 .',,
V20,,,,B20,"b'The spatial attention is just simply using several layers of convolution , which is not very computationally demanding .'",,
V21,,,,B21,"b'For UCF101 and HMDB51 , we also tested on a smaller number of frames , the results are very similar when using 25 and 50 frames .'",,
V22,b'Weaknesses :',,,B22,"b'Many the video action recognition literatures , for instance , attentional pooling [ 1 ] , videoLSTM [ 2 ] , and the two -stream network [ 4 ] , use 25 frames of RGB images .'",,
V23,,,,B23,"b'I think the bottleneck of using optical flow for large datasets is the optical flow extraction time and storage , especially for large datasets , such as Moments in Time dataset .'",,
V24,,,,B24,"b'If using 25 RGB frames , motion stream also use 25 optical flow frames .'",,
V25,b'The reported number on UCF101 and HMDB51 are confusing / misleading .',,,B25,,,
V26,"b'Even with only RGB , the evaluation miss numbers of models like ActionVLAD with 50 % on HMDB51 or Res3D with 88 % on UCF101 .'",,,B26,,,
V27,"b'I \xe2\x80\x99ll also add that there are available models nowadays that achieve over 94 % accuracy on UCF101 , and over 72 % on HMDB51 .'",,,B27,b'4 .',,
V28,b'The paper should at least have better discussion on those years of progress .',,,B28,"b'For the comparison with VideoLSTM [ 2 ] ,'",,
V29,"b'The mis-information also continues in THUMOS14 , for instance R-C3D beats the proposed model .'",,,B29,,,
V30,,,,B30,"b'( 1 ) The spatial - temporal attention mechanisms are different : they used LSTM for spatial attention , while we are using ConvLSTM for temporal attention .'",,
V31,,,,B31,"b'We also use different mechanisms for spatial attention : they are using ConvLSTM for spatial attention , while we are using several layers of ConvNet to learn an attention mask .'",,
V32,b'In my opinion the paper should include a flow variant .',,,B32,,,
V33,"b'It is a common setup in action recognition , and a good model should take advantage of these features .'",,,B33,"b'( 2 ) For temporal attention , [ 2 ] means impose similar visual attention to motion stream with optical flow input .'",,
V34,"b'Especially for spatial - temporal attention , e.g. , VideoLSTM paper by Li.'",,,B34,b'But our temporal attention uses different mechanism with RGB frames as input .',,
V35,,,,B35,,,
V36,,,,B36,"b'( 3 ) For localization , the VideoLSTM can only do spatial localization , our spatial - temporal attention can do both spatial and temporal localization .'",,
V37,b'In general spatial attention over each frame is extremely demanding .',,,B37,,,
V38,"b'The original image features are now multiplied by 49 factor , this is more demanding in terms of memory consumption than the flow features they chose to ignore .'",,,B38,"b'( 4 ) Compared with their RGB stream results , our results are much better and please refer to Table 1 of our newly updated paper .'",,
V39,b'The authors reports on 15 - frames datasets for those short videos .',,,B39,,,
V40,"b'But it will be interesting to see if the model is still useable on longer videos , for instance on Charades dataset .'",,,B40,,,
V41,,,,B41,"b'[ 1 ] Girdhar , Rohit , and Deva Ramanan .'",,
V42,,,,B42,"b'"" Attentional pooling for action recognition . ""'",,
V43,b'Can you please explain why you chose a regularized making instead of Soft-attention for spatial attention ?',,,B43,b'Advances in Neural Information Processing Systems ( NIPS ) .',,
V44,,,,B44,b'2017 .',,
V45,,,,B45,,,
V46,b'To conclude :',,,B46,b'[ 2 ]',,
V47,,,,B47,"b'Li , Zhenyang , et al . "" VideoLSTM convolves , attends and flows for action recognition . ""'",,
V48,"b'The goal of spatial - temporal attention is important , and the proposed approach behaves well .'",,,B48,b'Computer Vision and Image Understanding ( CVIU ) .',,
V49,"b'Yet the model is an extension of known techniques for image attention , which are not trivial to apply on long - videos with many frames .'",,,B49,b'2018 .',,
V50,b'Evaluating only on rgb features is not enough for an action recognition model .',,,B50,,,
V51,"b'Importantly , even when considering only rgb models , the paper still missed many popular stronger baselines .'",,,B51,b'[ 3 ]',,
V52,,,,B52,"b'Xu , Huijuan , Abir Das , and Kate Saenko .'",,
V53,,,,B53,"b'"" R-C3D : region convolutional 3d network for temporal activity detection . ""'",,
V54,,,,B54,b'IEEE Int. Conf. on Computer Vision ( ICCV ) . 2017 .',,
,,,,B55,,,
,,,,B56,b'[ 4 ]',,
,,,,B57,"b'Simonyan , Karen , and Andrew Zisserman .'",,
,,,,B58,"b'"" Two -stream convolutional networks for action recognition in videos . ""'",,
,,,,B59,b'Advances in neural information processing systems ( NIPS ) .',,
,,,,B60,b'2014 .',,
,,,,B61,,,
,,,,B62,,,
