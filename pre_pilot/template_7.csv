Review_index,Review_sentence,Review_affordance,-,Rebuttal_index,Rebuttal_sentence,Related_to,Relation
V0,,,,B0,b'We thank the reviewer for such a detailed feedback .',,
V1,b'-------------',,,B1,"b'We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below .'",,
V2,,,,B2,"b'We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have .'",,
V3,b'Summary',,,B3,b'We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places .',,
V4,,,,B4,"b""We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions .""",,
V5,b'-------------',,,B5,b'We briefly summarize the key idea of the paper and then address the specific concerns .',,
V6,,,,B6,,,
V7,b'The authors propose to train a policy while concurrently learning a dynamics model .',,,B7,,,
V8,"b'In particular , the policy is updated using both the RL loss ( rewards from the environment ) and the "" consistency constraint "" , which the authors introduce .'",,,B8,b'What is the idea ?',,
V9,"b'This consistency constraint is a supervised learning signal , which compares trajectories in the environment with trajectories in the imagined world ( produced with the dynamics model ) .'",,,B9,,,
V10,,,,B10,b'========',,
V11,,,,B11,,,
V12,b'---------------------',,,B12,,,
V13,,,,B13,b'Our goal is to provide a mechanism for the agent to lean a better dynamics model as well as more powerful policy by ensuring the consistency in their predictions ( such that predictions from the model are grounded in the real environment ) .',,
V14,b'Main Feedback',,,B14,,,
V15,,,,B15,,,
V16,b'---------------------',,,B16,b'This mechanism enables the agent to have a direct \xe2\x80\x9c interaction \xe2\x80\x9d b/w the agent \xe2\x80\x99s policy and its dynamics model .',,
V17,,,,B17,"b'This interaction is different from the standard approaches in reinforcement learning where the agent uses the policy to sample trajectories over which the agent is trained , and then use these sampled trajectories to learn the dynamics model .'",,
V18,"b'I feel like there might be some interesting ideas in this work , and the results suggest that this approach performs well .'",,,B18,"b'In those cases , there is no ( direct ) mechanism for the dynamics model to affect the policy , and hence there is no \xe2\x80\x9c direct interaction \xe2\x80\x9d between the policy and the dynamics model .'",,
V19,"b'However , I had a difficult time understanding how exactly the method works , and what its advantages are .'",,,B19,"b'In our case , both the policy and the model are trained jointly while making sure that the predictions from the dynamics model are consistent with the observation from the environment .'",,
V20,b'These are my main questions :',,,B20,b'This provides a mechanism where learning a model can itself change the policy ( thus \xe2\x80\x9c interacting \xe2\x80\x9d with the environment ) instead of just training on the data coming from a policy which is trained independently of how well the model performs on the collected .',,
V21,,,,B21,,,
V22,,,,B22,,,
V23,"b'1 ) At the beginning of Section 4 the authors write "" The learning agent has two pathways for improving its behaviour : ( ... ) ( ii ) the open loop path , where it imagines taking actions and hallucinates the state transitions that could happen "" .'",,,B23,b'A practical instantiation of this idea is the consistency loss where we ensure consistency between the predictions from the dynamics model and the actual observations from the environment and this simple baseline works surprisingly well compared to the state of the art methods ( as demonstrated by our experiments ) and that others have not tried it before .',,
V24,b'Do you actually do this ?',,,B24,b'Applying consistency constraint means we have two learning signals for the policy :',,
V25,b'This is not mentioned in anywhere .',,,B25,b'The one from the reinforcement learning loss ( i.e maximize return ) and the other due to consistency constraint .',,
V26,"b'And as far as I understand , the reward function is not learned - hence there will be no training signal in the open loop path .'",,,B26,b'We show that adding the proposed consistency constraint helps the agent to learn better dynamics model and as well as better policy for both observation space models and state space models .',,
V27,b'Does the reward signal always come from the true environment ?',,,B27,b'We compare against strong baselines :',,
V28,,,,B28,,,
V29,b'2 ) Is the dynamics model used for anything else than action-selection during training ?',,,B29,,,
V30,b'Planning ?',,,B30,b'Hybrid model - based and model - free ( Mb - Mf ) algorithm ( Nagabandi et al [ 1 ] )',,
V31,"b""If not , I do n't really understand the results and why this works at all ( k=20 being better than k=5 , for example ) .""",,,B31,,,
V32,,,,B32,b'Learning and Querying Fast Generative Models for Reinforcement Learning ( Buesing et al [ 2 ] ) - This is a state - of - the - art model for state space models .',,
V33,b'3 ) Is the dynamics model pre-trained in any way ?',,,B33,,,
V34,b'I find it surprising that the model - free method and the proposed method perform similar at the beginning ( Figure 3 ) .',,,B34,,,
V35,"b""If the agent chooses its actions based on the state that is predicted by the dynamics model , this should throw off the learning of the policy at the beginning ( when the dynamics model has n't learned anything sensible yet ) .""",,,B35,b'Our evaluation protocol considers a total of 7 environments and we show that using the consistency constraint leads to better generative models ( in terms of log likelihood ) and more powerful policy ( average return ) for all the cases .',,
V36,,,,B36,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
V37,,,,B37,,,
V38,b'-----------------------',,,B38,,,
V39,,,,B39,b'Our key contribution is the proposal of using the consistency loss which helps to learn more powerful policy and better dynamics model ( as demonstrated over different tasks ) while being very easy to integrate with existing model - based RL approaches .',,
V40,b'Other Questions',,,B40,"b'While our method is relatively simple , we are not aware of prior works that show something similar , and we believe such a simple baseline would be useful for anyone who \xe2\x80\x99s working on model - based RL.'",,
V41,,,,B41,"b'Further , our experiment demonstrates the effectiveness of the approach .'",,
V42,b'-----------------------',,,B42,"b'If we are mistaken regarding prior works , please let us know !'",,
V43,,,,B43,,,
V44,b'4 ) How exactly does training without the consistency constraint look ?',,,B44,,,
V45,b'Is this the same as k=1 ?',,,B45,"b'We would like to emphasize that our work presents an extensive comparative evaluation , and we believe that these results should be taken into consideration in evaluating our work .'",,
V46,,,,B46,b'We compare multiple approaches across more than 5 simulated tasks to the state of the art methods .',,
V47,b'5 ) Could the authors comment on the evaluation protocol in the experimental section ?',,,B47,"b'Hopefully , our clarifications are convincing in terms of explaining why the evaluation is fair and rigorous , and we would , of course , be happy to modify it as needed .'",,
V48,b'Are the results averages over multiple runs ?',,,B48,"b'But at a higher level , the fact that such simple model - based approaches work better than somewhat complex model - free approaches actually is the point of the paper to me .'",,
V49,"b'If so , it would help to see confidence intervals to make a fair assessment of the results .'",,,B49,,,
V50,,,,B50,,,
V51,"b'6 ) For the swimmer in Figure 2 , the two lines ( with consistency and without consistency ) start at different initial returns , why is that so ?'",,,B51,b'Continued ...',,
V52,"b""If the same architecture and seed was used , should n't this be the same ( or can you just not see it in the graph ) ?""",,,B52,,,
V53,,,,B53,,,
V54,,,,B54,,,
V55,b'---------',,,B55,,,
V56,,,,B56,,,
V57,b'Clarity',,,B57,b'Why is different from just learning a model based on k-step prediction ?',,
V58,,,,B58,,,
V59,b'---------',,,B59,b'========',,
V60,,,,B60,,,
V61,"b'The title and introduction initially gave me a slightly wrong impression on what the paper is going to be about , and several things were not followed up on later in the paper .'",,,B61,,,
V62,,,,B62,"b""Our approach is different from just learning a k-step prediction model as in our case , the agent \xe2\x80\x99s behavior ( i.e the agent 's policy ) is dependent on its internal model too .""",,
V63,b'Title :',,,B63,"b'In the standard case , the policy is optimized only using the RL gradient i.e maximizing expected reward and the state transition pairs ( collected as the agent acts in the environment ) become the supervising dataset for learning the model , and hence the policy is not affected when the model is being updated and there is no feedback from the model learning process to the policy .'",,
V64,,,,B64,"b'Hence , the data used for training the model is coming from a policy which is trained independently of how well the model performs on the collected trajectories .'",,
V65,"b'8 ) "" generative models "" reminds of things like a VAE or GAN ; however , I believe the authors mean "" dynamics models "" instead'",,,B65,b'So the process of learning the model has no control over what kind of data is produced for its training .',,
V66,,,,B66,,,
V67,"b'9 ) "" by interaction "" is a bit vague as to what the contribution is ( are n\'t policies and dynamic models in general trained by interacting with the environment ? ) ; the main idea of the paper is the consistency constraint'",,,B67,,,
V68,,,,B68,b'We propose to train both the policy and the model during the open loop .',,
V69,b'Abstract / Introduction :',,,B69,b'Hence the k-step predictions are used for training both the model and the policy simultaneously .',,
V70,,,,B70,"b'Training the policy on both the RL loss and the consistency loss provides a mechanism where learning a model , can itself change the policy thus leading to a much closer interplay between the dynamics model and the policy .'",,
V71,b'10 )',,,B71,b'We show that this relatively simple approach leads to much better performance when compared to very strong baselines for both observation space and state space models for all the 7 environments we considered .',,
V72,"b'The authors talk about humans carrying out "" experiments via interaction "" to help uncover "" true causal relationships "" .'",,,B72,,,
V73,"b""This idea is not brought up again in the methods section , and I do n't see evidence that with the proposed approach , the policy does targeted experiments to uncover causal relationships .""",,,B73,,,
V74,b'It is not clear to me why this is the intuition that motivates the consistency constraint .',,,B74,,,
V75,,,,B75,b'What are the empirical results ?',,
V76,"b'11 ) As the authors state in the introduction , the hope of model - based RL is better sample complexity .'",,,B76,,,
V77,"b'This is usually achieved by using the model in some way , for example by planning several steps ahead when choosing the current action .'",,,B77,b'========',,
V78,b'Could the authors comment on where they would place their proposed method - how does it address sample complexity ?',,,B78,,,
V79,,,,B79,,,
V80,b'12 )',,,B80,"b'Our evaluation protocol consists of 7 environments ( Ant , Half Cheetah , Humanoid etc ) and both observation space and state space models .'",,
V81,"b'In the introduction , the authors discuss the problem of compounding errors .'",,,B81,"b'Solving Half Cheetah environment , when observations are in the pixel space ( images ) , is very challenging as useful information like velocity is not available .'",,
V82,"b'These must be a problem in the proposed method as well , especially as k grows .'",,,B82,,,
V83,b'Could the authors comment on that ?',,,B83,,,
V84,b'How come that the performance is so good for k= 20 ?',,,B84,"b'For the observation space model , we use the \xe2\x80\x9c Hybrid model - based and model - free ( Mb - Mf ) algorithm \xe2\x80\x9d ( Nagabandi et al [ 1 ] ) .'",,
V85,,,,B85,"b'It is a strong baseline where the authors proposed to use a trained , deep neural network based dynamics model to initialize a model - free learning agent to combine the sample efficiency of model - based approaches with the high task - specific performance of model - free methods .'",,
V86,b'13 )',,,B86,"b'For the state space models , we use the \xe2\x80\x9c Learning and Querying Fast Generative Models for Reinforcement Learning \xe2\x80\x9d ( Buesing et al [ 2 ] ) as the baseline .'",,
V87,"b'The authors write that in most model - based approaches , the dynamics model is "" learned with supervised learning techniques , i.e. , just by observing the data "" and not via interaction .'",,,B87,b'This is a state - of - the - art model for state space models .',,
V88,"b""There 's two things I do n't understand : ( 1 ) in the existing model - based approaches the authors refer to , the policy also interacts with the world to get the data to do supervised learning - what exactly is the difference ? ( 2 )""",,,B88,"b'As shown by our experiments ( section 5 ) , by having this consistency constraint we outperform both these baselines .'",,
V89,"b'The auxiliary loss "" which explicitly seeks to match the generative behaviour to the observed behaviour "" is just a supervised learning loss as well , so how is this different ?'",,,B89,,,
V90,,,,B90,,,
V91,,,,B91,"b'We focus on evaluating the agent for both dynamics models ( in terms of imagination log likelihood , figure 4 ) and policy ( in terms of average episodic returns and loss , figure 2 , 3 , 5 ) .'",,
V92,"b'For me , it would help the readability and understanding of the paper if some concepts were introduced more formally .'",,,B92,b'We show that adding the consistency constraint to the baseline models results in improvements to both the dynamics models and the policy for all the environments that we consider .',,
V93,,,,B93,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
V94,"b'14 ) In Section 2 , it would help me to see a formal definition of the MDP and what exactly is optimised .'",,,B94,,,
V95,"b'The authors write "" optimise a reward signal "" and "" maximise its expected reward "" , however I believe it should be the expected cumulative reward ( i.e. , return ) .'",,,B95,,,
V96,,,,B96,b'===============================================================================================',,
V97,b'15 )',,,B97,,,
V98,b'The loss function for the dynamics model is not explicitly stated .',,,B98,,,
V99,"b'From the text I assume that it is the mean squared error for the per-step loss , and a GAN loss for the trajectory - wise loss .'",,,B99,b'We now refer to the specific aspects of the reviews .',,
V100,,,,B100,,,
V101,"b'16 ) Could the authors explicitly state what the overall loss function is , and how the RL and supervised objective are combined ?'",,,B101,,,
V102,"b'Is the dynamics model f trained only on the supervised loss , and the policy pi only on the RL loss ?'",,,B102,,,
V103,,,,B103,"b'"" The authors ... as an input .'",,
V104,b'17 ) In 2.3 the variable z_t is not formally introduced .',,,B104,,,
V105,b'What does it represent ?',,,B105,,,
V106,,,,B106,b'There seems to be a small discrepancy in the summary .',,
V107,,,,B107,b'The actions are always selected using the true state of the environment .',,
V108,b'------------------------',,,B108,"b'When the agent is performing the open-loop , the agent transitions from one \xe2\x80\x9c imagined \xe2\x80\x9d state to another \xe2\x80\x9c imagined \xe2\x80\x9d state , unlike the closed loop state where the agent transitions between actually observed states ( coming from the environment ) .'",,
V109,,,,B109,b'The consistency loss ensures that the sequence of imagined states behaves similarly to the sequence of actually observed states .',,
V110,b'Other Comments',,,B110,b'This aspect has been clarified in the paper in section 3.1 which talks about consistency constraint in general and describes how the consistency loss is to be computed ( eq 1 ) .',,
V111,,,,B111,b'Section 3.2 and section 3.3 go into the specific cases of observation space models and state space models respectively .',,
V112,b'------------------------',,,B112,,,
V113,,,,B113,,,
V114,"b'18 ) I find it problematic to use words such as "" hallucination "" and "" imagination "" when talking about learning algorithms .'",,,B114,b'==============================',,
V115,"b'I would much prefer to see formal / factual language ( like saying that the dynamics model is used to do make predictions / do planning , rather than that the agent is hallucinating ) .'",,,B115,,,
V116,,,,B116,,,
V117,,,,B117,"b'"" 1 ) At the beginning ... environment ? ""'",,
V118,b'-- edit ( 19.11 . )',,,B118,,,
V119,b'---',,,B119,,,
V120,,,,B120,b'Thank you for pointing out this out .',,
V121,b'- updated score to 5',,,B121,b'We have improved the writing in the paper to make it more explicit ( equation 1 ) .',,
V122,,,,B122,b'We briefly summarise this aspect here for completion :',,
V123,b'- corrected summary',,,B123,,,
V124,,,,B124,,,
,,,,B125,"b'Let us say that at time t , the agent is in some state s_t while it \xe2\x80\x9c imagines \xe2\x80\x9d to be in state s_t ^'",,
,,,,B126,b'I .',,
,,,,B127,"b'In the closed loop , it samples an action a_t using the policy \\pi and transitions to a new state s_{t + 1 } by performing the action in the environment .'",,
,,,,B128,"b'In the open loop , the agent performs the action a_t in its dynamic model and transitions from state s_t ^'",,
,,,,B129,b'I to s_{t + 1 } ^ I .',,
,,,,B130,,,
,,,,B131,,,
,,,,B132,"b'For the closed loop , the loss comes from the reward signal .'",,
,,,,B133,"b'For the open loop , the loss comes in form of consistency constraint imposed on the sequence of actual state transitions and the predicted state transitions .'",,
,,,,B134,b'This is described by equation 1 in section 3.1 .',,
,,,,B135,"b'During the open loop , both the policy and the model are updated using the consistency loss .'",,
,,,,B136,,,
,,,,B137,,,
,,,,B138,b'Cont ..',,
,,,,B139,,,
,,,,B140,,,
,,,,B141,"b'"" 2 ) Is the dynamics ... for example ) ""'",,
,,,,B142,,,
,,,,B143,,,
,,,,B144,,,
,,,,B145,b'The dynamics model is indeed being used like in other model - based approaches .',,
,,,,B146,b'k=20 works better than k=10 because now the model \xe2\x80\x99s predictions are being grounded in \xe2\x80\x9c real observations \xe2\x80\x9d for a much longer time span .',,
,,,,B147,,,
,,,,B148,,,
,,,,B149,"b'"" 3 ) Is the dynamics ... sensible yet ) ""'",,
,,,,B150,,,
,,,,B151,,,
,,,,B152,b'We clarify that the agent is not using the dynamics model for action selection .',,
,,,,B153,b'The role of the dynamics model is the following -',,
,,,,B154,b'The policy is trained using both the RL loss as well as the loss from the dynamics model .',,
,,,,B155,,,
,,,,B156,,,
,,,,B157,,,
,,,,B158,"b'"" 4 ) How exactly ... k= 1 ? ""'",,
,,,,B159,,,
,,,,B160,,,
,,,,B161,"b'The case of training without the consistency loss is the standard reward - based training of RL agents , without any consistency constraint .'",,
,,,,B162,b'K=1 would correspond to the case where the consistency loss is applied on per step predictions .',,
,,,,B163,,,
,,,,B164,,,
,,,,B165,"b'"" 5 ) Could the ... results . ""'",,
,,,,B166,,,
,,,,B167,,,
,,,,B168,b'We have updated the paper to improve the experimental section - both in terms of description of baselines and in terms of the evaluation protocol .',,
,,,,B169,"b'Further , Section 3.1 describes the different loss components and how the consistency constraint can be applied in the general .'",,
,,,,B170,b'Section 3.2 and 3.3 describes the baselines and how these baselines were modified to support the consistency constraint for the observation space and the state space models respectively .',,
,,,,B171,b'All the experiments are averaged over 3 random seeds ( along with 1 standard deviation interval ) are plotted .',,
,,,,B172,,,
,,,,B173,,,
,,,,B174,b'We summarize the baselines and the evaluation protocol here :',,
,,,,B175,,,
,,,,B176,,,
,,,,B177,"b'Our evaluation protocol consists of 7 environments ( Ant , Half Cheetah , Humanoid etc ) and both observation space and state space models .'",,
,,,,B178,"b'For the observation space model , we use the \xe2\x80\x9c Hybrid model - based and model - free ( Mb - Mf ) algorithm \xe2\x80\x9d ( Nagabandi et al [ 1 ] ) which is a very strong baseline and for the state space models , we use the \xe2\x80\x9c Learning and Querying Fast Generative Models for Reinforcement Learning \xe2\x80\x9d ( Buesing et al [ 2 ] ) as the baseline .'",,
,,,,B179,b'This model is a state - of - the - art model for state space models .',,
,,,,B180,,,
,,,,B181,,,
,,,,B182,b'We focus on evaluating the agent for both dynamics models ( in terms of imagination log likelihood ) and policy ( in terms of average episodic returns and loss ) .',,
,,,,B183,b'We show that adding the consistency constraint to the baseline models results in improvements to both the dynamics models and the policy for all the environments that we consider .',,
,,,,B184,b'All the experiments are averaged over 3 random seeds and are plotted with 1 standard deviation interval .',,
,,,,B185,,,
,,,,B186,,,
,,,,B187,b'Our key contribution is the proposal of using the consistency loss which helps to learn more powerful policy and better dynamics model ( as demonstrated over different tasks ) while being very easy to integrate with existing model - based RL approaches .',,
,,,,B188,"b'While the proposed approach looks relatively simple , we are not aware of work in RL which describes and validates the benefits of imposing the consistency constraint .'",,
,,,,B189,,,
,,,,B190,,,
,,,,B191,,,
,,,,B192,"b'"" For the ... it in the graph ) ? "" ""'",,
,,,,B193,,,
,,,,B194,,,
,,,,B195,,,
,,,,B196,b'We believe that the reason is the swimmer plot is averaged over 10 batches .',,
,,,,B197,b'We have added this information in the caption of the plot .',,
,,,,B198,,,
,,,,B199,,,
,,,,B200,,,
,,,,B201,"b'"" 8 ) ... instead ""'",,
,,,,B202,,,
,,,,B203,,,
,,,,B204,b'We agree that the title could sound a little misleading .',,
,,,,B205,"b'Based on the suggestion , we have updated the title to \xe2\x80\x9c Learning powerful policies and better dynamics models by encouraging consistency \xe2\x80\x9d'",,
,,,,B206,,,
,,,,B207,,,
,,,,B208,"b'"" 9 ) by interaction ... consistency constraint ""'",,
,,,,B209,,,
,,,,B210,,,
,,,,B211,b'We acknowledge that the use of \xe2\x80\x9c by interaction \xe2\x80\x9d sounds a little vague and have incorporated this feedback into the draft .',,
,,,,B212,,,
,,,,B213,,,
,,,,B214,"b'"" 10 ) The authors ... constraint ""'",,
,,,,B215,,,
,,,,B216,,,
,,,,B217,b'Our broad goal is to provide a mechanism for the agent to interact with the environment while it is learning the dynamics model as this could be helpful in learning a more powerful policy and better dynamics model .',,
,,,,B218,b'We discuss several possible manifestations of this idea in the introduction / motivation and focus on one specific instantiation - ensuring consistency between the predictions from the dynamics model and the actual observations from the environment .',,
,,,,B219,b'We show that adding the proposed consistency constraint helps the agent to learn better dynamics model and better policy for both observation space models and state space models .',,
,,,,B220,b'It is both interesting and surprising to see that our proposed approach improves over the state of the art results despite being relatively simple thus highlighting the usefulness of the \xe2\x80\x98 interaction \xe2\x80\x9d with the environment .',,
,,,,B221,,,
,,,,B222,,,
,,,,B223,b'Continue',,
,,,,B224,,,
,,,,B225,,,
,,,,B226,"b'"" 11 ) As the ... sample complexity ? ""'",,
,,,,B227,,,
,,,,B228,,,
,,,,B229,b'We address this issue from two perspectives :',,
,,,,B230,,,
,,,,B231,,,
,,,,B232,b'Qualitatively -',,
,,,,B233,b'We propose to train both the policy and the model during the open loop .',,
,,,,B234,b'Hence the k-step predictions are used for training both the model and the policy simultaneously .',,
,,,,B235,"b'Training the policy on both the RL loss and the consistency loss provides a mechanism where learning a model , can itself change the policy thus leading to a much closer interplay between the dynamics model and the policy .'",,
,,,,B236,b'This approach is different from other works focusing on learning k-step prediction models .',,
,,,,B237,"b'In those cases , the policy is learned solely focussing on the reward structure and the state transition trajectories ( collected as the agent acts in the environment ) become the supervising dataset for learning the model .'",,
,,,,B238,b'There is no feedback from the model learning process to the policy learning process .',,
,,,,B239,b'So the process of learning the model has no control over what kind of data is produced ( by the policy ) for its training .',,
,,,,B240,,,
,,,,B241,,,
,,,,B242,,,
,,,,B243,b'Empirical Evaluation -',,
,,,,B244,b'We show that this relatively simple approach improves the performance for both the dynamics model and the policy when compared to very strong baselines for both observation space and state space models for all the 7 environments we considered .',,
,,,,B245,"b'Our evaluation protocol consists of 7 environments ( Ant , Half Cheetah , Humanoid etc ) and both observation space and state space models .'",,
,,,,B246,"b'For the observation space model , we use the \xe2\x80\x9c Hybrid model - based and model - free ( Mb - Mf ) algorithm \xe2\x80\x9d ( Nagabandi et al [ 1 ] ) which is a very strong baseline and for the state space models , we use the \xe2\x80\x9c Learning and Querying Fast Generative Models for Reinforcement Learning \xe2\x80\x9d ( Buesing et al [ 2 ] ) as the baseline .'",,
,,,,B247,b'This model is a state - of - the - art model for state space models .',,
,,,,B248,,,
,,,,B249,,,
,,,,B250,,,
,,,,B251,,,
,,,,B252,"b'"" 12 ) In the ... k= 20 ? ""'",,
,,,,B253,,,
,,,,B254,,,
,,,,B255,b'This comment refers to figure 3 .',,
,,,,B256,"b'Here the proposed agents are trained with 2 different values of k , that is 5 and 20 .'",,
,,,,B257,"b'Since the agent with k=20 is trained for longer sequences , it performs better than the other agent .'",,
,,,,B258,,,
,,,,B259,,,
,,,,B260,,,
,,,,B261,"b'"" 13 ) The authors ... different ? ""'",,
,,,,B262,,,
,,,,B263,,,
,,,,B264,"b'The key difference between our approach and existing approaches for learning the dynamics model is that in our case , the process of learning the model can change the policy .'",,
,,,,B265,,,
,,,,B266,,,
,,,,B267,"b'In the standard cases , the policy is learned solely focussing on the reward structure and the state transition trajectories ( collected as the agent acts in the environment ) become the supervising dataset for learning the model .'",,
,,,,B268,"b'In that setup , the policy is not updated when the model is being updated and there is no feedback from the model learning process to the policy learning process .'",,
,,,,B269,"b'Hence , the data used for training the model is coming from a policy which is trained independently of how well the model performs on the collected trajectories .'",,
,,,,B270,b'So the process of learning the model has no control over what kind of data is produced for its training .',,
,,,,B271,b'This is what we mean by \xe2\x80\x9c learning the dynamics model by just observing the data \xe2\x80\x9d .',,
,,,,B272,,,
,,,,B273,,,
,,,,B274,b'We propose to train both the policy and the model during the open loop .',,
,,,,B275,b'Hence the k-step predictions are used for training both the model and the policy simultaneously .',,
,,,,B276,"b'Training the policy on both the RL loss and the consistency loss provides a mechanism where learning a model , can itself change the policy .'",,
,,,,B277,b'This is what we mean by \xe2\x80\x9c learning the dynamics model via interaction \xe2\x80\x9d .',,
,,,,B278,b'This close interplay between the dynamics model and the policy provides a pathway to the model to interact with the environment instead of just using the sampled trajectories .',,
,,,,B279,b'The resulting consistency loss helps to learn more powerful policy and better dynamics model ( as demonstrated over different tasks ) while being very easy to integrate with existing model - based RL approaches .',,
,,,,B280,b'It is important to note that our proposed approach improves over the state of the art results despite being relatively simple .',,
,,,,B281,b'We are not aware of work in RL which describes and validates the benefits of imposing the consistency constraint and would be happy to include references to such work .',,
,,,,B282,,,
,,,,B283,,,
,,,,B284,,,
,,,,B285,,,
,,,,B286,"b'"" 14 ) In Section ... return ) ""'",,
,,,,B287,,,
,,,,B288,,,
,,,,B289,b'We apologize for the mistake .',,
,,,,B290,b'Thanks for pointing it out .',,
,,,,B291,b'We are indeed optimizing the expected return .',,
,,,,B292,b'We have also updated section 2 to describe the MDP and the related terms in a formal manner .',,
,,,,B293,,,
,,,,B294,,,
,,,,B295,,,
,,,,B296,"b'"" 15 ) The loss ... trajectory - wise loss ""'",,
,,,,B297,,,
,,,,B298,,,
,,,,B299,b'We have improved the section on consistency constraint ( Section 3.1 ) to describe the consistency loss in detail .',,
,,,,B300,b'We do not use any stepwise loss .',,
,,,,B301,b'A recurrent model is used to encode the trajectory into a fixed - sized vector and the l2 loss is applied between the encoding for the trajectory of observed states and the imagined states .',,
,,,,B302,b'This has been formalized in equation 1 .',,
,,,,B303,"b'Further , Section 3.2 and 3.3 describe how to modify the baselines to support the consistency constraint for observation space and state space models respectively .'",,
,,,,B304,,,
,,,,B305,,,
,,,,B306,b'Continue',,
,,,,B307,,,
,,,,B308,,,
,,,,B309,"b'"" 16 ) Could the ... RL loss ? ""'",,
,,,,B310,,,
,,,,B311,,,
,,,,B312,,,
,,,,B313,b'We have updated the section on consistency constraint ( 3.1 ) to include an equation describing the different components of the loss function .',,
,,,,B314,b'Both the dynamics model and the policy pi are trained on the total loss ( which is a combination of the RL loss and the consistency loss )',,
,,,,B315,,,
,,,,B316,,,
,,,,B317,"b'"" 17 ) In 2.3 ... represent ? ""'",,
,,,,B318,,,
,,,,B319,,,
,,,,B320,b'We have updated the relevant section to define z_t .',,
,,,,B321,b'It refers to the latent variable introduced per timestep to introduce stochasticity in state transition function .',,
,,,,B322,,,
,,,,B323,,,
,,,,B324,"b'"" 18 ) I find ... hallucinating . ""'",,
,,,,B325,,,
,,,,B326,,,
,,,,B327,b'We have addressed this point by replacing the word hallucination with \xe2\x80\x9c imagination \xe2\x80\x9d and \xe2\x80\x9c prediction \xe2\x80\x9d as per the context .',,
,,,,B328,,,
,,,,B329,,,
,,,,B330,,,
,,,,B331,"b""We would appreciate it if the reviewer could take another look at our changes and additional results , and let us know if the reviewer has request for additional changes that would alleviate the reviewer 's concerns .""",,
,,,,B332,,,
,,,,B333,,,
,,,,B334,b'[ 1 ] : Neural Network Dynamics for Model - Based Deep Reinforcement Learning with Model - Free Fine-Tuning - https://arxiv.org/pdf/1708.02596.pdf',,
,,,,B335,,,
,,,,B336,,,
,,,,B337,b'[ 2 ] : Learning and Querying Fast Generative Models for Reinforcement Learning - https://arxiv.org/pdf/1802.03006.pdf',,
,,,,B338,,,
,,,,B339,,,
,,,,B340,,,
,,,,B341,,,
,,,,B342,b'Dear Reviewer',,
,,,,B343,,,
,,,,B344,,,
,,,,B345,b'We have added new evaluation results to investigate the robustness of the proposed approach in terms of compounding errors .',,
,,,,B346,"b'When we use the recurrent dynamics model for prediction , the ground - truth sequence is not available for conditioning .'",,
,,,,B347,b'This leads to problems during sampling as even small prediction errors can compound when sampling for a large number of steps .',,
,,,,B348,b'We evaluate the proposed model for robustness by predicting the future for much longer timesteps ( 50 timesteps ) than it was trained on ( 10 timesteps ) .',,
,,,,B349,"b'More generally , in figure 9 ( section 7.3 in appendix ) , we demonstrate that this auxiliary cost helps to learn a better model with improved long - term dependencies by using a training objective that is not solely focused on predicting the next observation , one step at a time .'",,
,,,,B350,,,
,,,,B351,,,
,,,,B352,b'Thank you for your time !',,
,,,,B353,b'The authors appreciate the time reviewers have taken for providing feedback .',,
,,,,B354,b'which resulted in improving the presentation of our paper .',,
,,,,B355,"b'Hence , we would appreciate it if the reviewers could take a look at our changes and additional results , and let us know if they would like to either revise their rating of the paper or request additional changes that would alleviate their concerns .'",,
,,,,B356,,,
