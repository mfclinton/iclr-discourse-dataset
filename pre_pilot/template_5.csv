Review_index,Review_sentence,Review_affordance,-,Rebuttal_index,Rebuttal_sentence,Related_to,Relation
V0,"b'A method for activity recognition in videos is presented , which uses spatial soft attention combined with temporal soft attention .'",,,B0,b'We would like to thank the reviewer for the detailed comments and suggestions on the manuscript .',,
V1,"b'In a nutshell , a pixelwise mask is output and elementwise combined with feature maps for spatial attention , and temporal attention is a distribution over frames .'",,,B1,b'We have updated the paper and highlight the major changes with red colour .',,
V2,b'The method is tested on several datasets .',,,B2,b'The following replies are used to address the concerns .',,
V3,,,,B3,,,
V4,,,,B4,,,
V5,"b'My biggest concern with the paper is novelty , which is rather low .'",,,B5,b'1 .',,
V6,"b'Attention models are one of the most highly impactful discoveries in deep learning , which have been widely and extensively studied in computer vision , and also in activity recognition .'",,,B6,b'Novelty :',,
V7,b'Spatial and temporal attention mechanisms are now widely used by the community .',,,B7,,,
V8,"b'I am not sure to see the exact novelty of the proposed , it seems to be very classic : soft attention over feature maps and frames is not new .'",,,B8,b'We agree with the reviewer that the attention model has been widely used in many different tasks ( please refer to our introduction and related work part ) .',,
V9,b'Using attention distributions for localization has also been shown in the past .',,,B9,b'The novelty of each work usually lies on using different attention mechanisms and their applications are for different tasks .',,
V10,,,,B10,b'Our focus is on attention mechanism for video action recognition .',,
V11,,,,B11,"b'Different from the LSTM based soft-attention is only used for spatial localization [ 1 , 2 , 3 ] , but we used it for temporal localization .'",,
V12,"b'This also shows in the related works section , which contains only 3 references for spatial attention and only 2 references for temporal attention out of a vast body of known work .'",,,B12,"b'For spatial attention , we propose a new method which just uses several convolutional layers to learn an attention mask , which is novel and has not been used before .'",,
V13,,,,B13,b'We also introduced two regularizers for spatial attention .',,
V14,,,,B14,"b'For temporal attention , we introduce the convolutional LSTM based mechanism and a regularizer .'",,
V15,"b'The unimodality prior ( implemented as log concave prior ) is interesting , but uni-modality is a very strong assumption .'",,,B15,b'This temporal mechanism also considers the salient spatial information which learned by our spatial attention in the previous step .',,
V16,"b'While it could be argued that spurior attention should be avoided , unimodality is much less clear .'",,,B16,,,
V17,"b'For this reason , the prior should be compared with even simpler priors , like total variation over time ( similar to what has been done over space ) .'",,,B17,,,
V18,,,,B18,b'2 .',,
V19,,,,B19,b'The purpose of the unimodality prior is fundamentally different from the total variation ( TV ) regularization .',,
V20,"b'The ablation study in the experimental section shows , that the different mechanisms only marginally contribute to the performance of the method : + 0.7 p on HMDB51 , slightly more on UCF101 .'",,,B20,b'The TV regularization encourages attention weights to remain the same in consecutive frames .',,
V21,"b'Similarly , the different loss functions only very marginally contribute to the performance .'",,,B21,"b'However , it does not encourage sparsity and it is not sufficient on the action recognition dataset .'",,
V22,,,,B22,"b'On the contrary , we found empirical evidence that salient information in most videos is contained only in a few consecutive frames .'",,
V23,,,,B23,b'Therefore it is sensible to make the unimodality assumption on the importance of frames .',,
V24,"b'The method is only compared to Sharma 2015 on these datasets , which starts to be dated and is not state of the art anymore .'",,,B24,,,
V25,"b'Activity recognition has recently very much benefitted from optimization of convolutional backbones , like I3D and variants .'",,,B25,,,
V26,,,,B26,b'3 .',,
V27,,,,B27,b'We added two state - of- the- art visual attention video action recognition baselines [ 2 ] [ 3 ] which also use attention mechanism with the RGB images as input .',,
V28,b'The LSTM equations at the end of page are unnecessary because widely known .',,,B28,b'Our results are better than these two methods .',,
V29,,,,B29,b'We did not compare with I3D and variants as they are using 3D convolutions and too computationally expensive .',,
V30,,,,B30,"b'The attention model is not a totally independent model , but a plug - in model .'",,
,,,,B31,"b'The performance of the entire action recognition network not only depends on the attention model , but also the backbone model .'",,
,,,,B32,,,
,,,,B33,"b'For instance , the accuracy for the base network ResNet50 is 47.78 % , with our spatial - temporal attention , the accuracy is 49.93 % .'",,
,,,,B34,"b'For ResNet101 , the base network accuracy is 49.73 % , with our spatial - temporal attention , the accuracy achieves 53.07 % .'",,
,,,,B35,"b'For ResNet152 , the base network accuracy is 50.04 % , with our spatial - temporal attention , the accuracy achieves 54.44 % .'",,
,,,,B36,,,
,,,,B37,"b'The I3D network is very computationally expensive and data hungry , and need to pretrain on large datasets , such as Kinetics .'",,
,,,,B38,"b'Currently we are not using I3D network due to computational limitations , as pretraining needs 8 GPUs and train 2 weeks .'",,
,,,,B39,b'Our attention model could extend to spatial - temporal attention based on 3D networks by learning a 3D spatial mask and the frame temporal attention with Convolutional LSTM .',,
,,,,B40,b'It will be an interesting direction if computational resources are limited .',,
,,,,B41,,,
,,,,B42,,,
,,,,B43,b'[ 1 ]',,
,,,,B44,"b'Sharma , Shikhar , Ryan Kiros , and Ruslan Salakhutdinov .'",,
,,,,B45,"b'"" Action recognition using visual attention . ""'",,
,,,,B46,b'arXiv preprint arXiv:1511.04119 ( 2015 ) .',,
,,,,B47,,,
,,,,B48,b'[ 2 ]',,
,,,,B49,"b'Li , Zhenyang , et al . "" VideoLSTM convolves , attends and flows for action recognition . ""'",,
,,,,B50,"b'Computer Vision and Image Understanding ( CVIU ) , 2018 .'",,
,,,,B51,,,
,,,,B52,b'[ 3 ]',,
,,,,B53,"b'Girdhar , Rohit , and Deva Ramanan .'",,
,,,,B54,"b'"" Attentional pooling for action recognition . ""'",,
,,,,B55,b'Advances in Neural Information Processing Systems ( NIPS ) .',,
,,,,B56,b'2017 .',,
,,,,B57,,,
,,,,B58,,,
