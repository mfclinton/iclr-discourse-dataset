This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples . The main goal is to quantify mode collapse in state - of - the- art generative models . The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse .
Using the birthday paradox test , the experiments show that GANs can learn and consistently reproduce the same examples , which are not necessarily exactly the same as training data ( eg. the triplets in Figure 1 ) .
The results are interpreted to mean that mode collapse is strong in a number of state - of - the - art generative models .
Bidirectional models ( ALI , BiGANs ) however demonstrate significantly higher diversity that DCGANs and MIX + DCGANs .
Finally , the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator .

This is a very interesting area and exciting work . The main idea behind the proposed test is very insightful . The main theoretical contribution stimulates and motivates much needed further research in the area . In my opinion both contributions suffer from some significant limitations . However , given how little we know about the behavior of modern generative models , it is a good step in the right direction .


1 . The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity . The authors do mention this issue , but do not put much effort into evaluating its implications in practice , or parsing Theorems 1 and 2 . My current understanding is that , in practice , when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal , or because my generative model has bad diversity . Anecdotally , real - life distributions are far from uniform , so this should be a common issue . I would still use the test as a part of a suite of measurements , but I would not solely rely on it . I feel that the authors should give a more prominent disclaimer to potential users of the test .

2 . Also , given how mode collapse is the main concern , it seems to me that a discussion on coverage is missing . The proposed test is a measure of diversity , not coverage , so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribution . As long as they yield collisions at the same rate , these two generative models are ‘ equally diverse ’ . Is n’t coverage of equal importance ?

3 . The other main contribution of the paper is Theorem 3 , which shows —via a very particular construction on the generator and encoder — that bidirectional GANs can also suffer from serious mode collapse . I welcome and are grateful for any theory in the area . This theorem might very well capture the underlying behavior of bidirectional GANs , however , being constructive , it guarantees nothing in practice . In light of this , the statement in the introduction that “ encoder - decoder training objectives cannot avoid mode collapse ” might need to be qualified . In particular , the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of Theorem 3.

Thank you for your positive and detailed comment ! We ’ll address your concerns point by point .

“ Conflates mode collapse with non-uniformity ” “ coverage ”
It is important to clarify --- though very likely the reviewer understood - -- that Theorem 1 and 2 hold without the assumption of uniformity . ( The birthday paradox holds even though human birthdays are not uniformly distributed . ) That said , the reviewer is correct that our test does not test for coverage and we will add a disclaimer to this effect . We note that testing coverage of n in general requires at least n/ log n samples . ( See our response to 3rd reviewer . )

Indeed , we are assuming that CelebA dataset is reasonably well - balanced ( it contains ten thousand identities , each of which has twenty images ) [ 2 ] , and therefore a GAN that produces a highly non-uniform distribution of faces is some kind of failure mode . It is conceivable that CelebA is not well - constructed for the reasons mentioned by the reviewer , but it has been used in most previous GANs papers , so it was natural to report our findings on that . In the final version we ’ll put a suitable disclaimer about this issue .


“ Practical implication of Theorem 3 ? ”
The reviewer is correct that we have only shown *existence * of a bad equilibrium , not proved that SGD or other algorithms * find * it . ( Analysing SGD ’s behavior for deep learning is of course an open problem . ) But note that some of the problems raised by Theorem 3 are observed in practice too ; see e.g. empirical studies ( [ 1 ] , [ 2 ] ) which suggest that BiGANs / ALI can learn un - informative codes . We ’ll rewrite to make these issues clearer .

[ 1 ] Chunyuan Li , Hao Liu , Changyou Chen , Yunchen Pu , Liqun Chen , Ricardo Henao and Lawrence Carin , ALICE : Towards Understanding Adversarial Learning for Joint Distribution Matching , NIPS 2017
[ 2 ] Jun - Yan Zhu* Taesung Park* Phillip Isola Alexei A. Efros Unpaired image - to-image translation using cycle-consistent adversarial networks . ICCV , 2017 .


The article " Do GANs Learn the Distribution ? Some Theory and Empirics " considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images . The authors argue that GANs in fact generate the distributions with fairly low support .

The proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching ( or very similar ) pairs in the generated sample . This test is expected to experimentally support the previous theoretical analysis by Arora et al . ( 2017 ) . The further theoretical analysis is also performed showing that for encoder - decoder GAN architectures the distributions with low support can be very close to the optimum of the specific ( BiGAN ) objective .

The experimental part of the paper considers the CelebA and CIFAR - 10 datasets . We definitely see many very similar images in fairly small sample generated . So , the general claim is supported . However , if you look closely at some pictures , you can see that they are very different though reported as similar . For example , some deer or truck pictures . That 's why I would recommend to reevaluate the results visually , which may lead to some change in the number of near duplicates and consequently the final support estimates .

To sum up , I think that the general idea looks very natural and the results are supportive . On theoretical side , the results seem fair ( though I did n't check the proofs ) and , being partly based on the previous results of Arora et al. ( 2017 ) , clearly make a step further .

Thank you for the positive and careful review ! We agree that a judgement call needs to be made for assessing whether two images are “ essentially ” the same . For the final version of the paper we will utilize a second human examiner and report collisions only if both examiners judge the image to be same . It is correct that this may slightly affect the estimate of support size , though we expect the conclusions to not change too much .

This paper proposes a new theoretically - motivated method for combining reinforcement learning and imitation learning for acquiring policies that are as good as or superior to the expert . The method assumes access to an expert value function ( which could be trained using expert roll - outs ) and uses the value function to shape the reward function and allow for truncated - horizon policy search . The algorithm can gracefully handle suboptimal demonstrations / value functions , since the demonstrations are only used for reward shaping , and the experiments demonstrate faster convergence and better performance compared to RL and AggreVaTeD on a range of simulated control domains . The paper is well - written and easy to understand .

My main feedback is with regard to the experiments :
I appreciate that the experiments used 25 random seeds ! This provides a convincing evaluation .
It would be nice to see experimental results on even higher dimensional domains such as the ant , humanoid , or vision - based tasks , since the experiments seem to suggest that the benefit of the proposed method is diminished in the swimmer and hopper domains compared to the simpler settings .
Since the method uses demonstrations , it would be nice to see three additional comparisons : ( a ) training with supervised learning on the expert roll - outs , ( b ) initializing THOR and AggreVaTeD ( k=1 ) with a policy trained with supervised learning , and ( c ) initializing TRPO with a policy trained with supervised learning . There does n't seem to be any reason not to initialize in such a way , when expert demonstrations are available , and such an initialization should likely provide a significant speed boost in training for all methods .
How many demonstrations were used for training the value function in each domain ? I did not see this information in the paper .

With regard to the method and discussion :
The paper discusses the connection between the proposed method and short -horizon imitation and long-horizon RL , describing the method as a midway point . It would also be interesting to see a discussion of the relation to inverse RL , which considers long - term outcomes from expert demonstrations . For example , MacGlashn & Littman propose a midway point between imitation and inverse RL [ 1 ] .
Theoretically , would it make sense to anneal k from small to large ? ( to learn the most effectively from the smallest amount of experience )

[ 1 ] https://www.ijcai.org/Proceedings/15/Papers/519.pdf


Minor feedback :
- The RHS of the first inequality in the proof of Thm 3.3 seems to have an error in the indexing of i and exponent , which differs from the line before and line after

** Edit after rebuttal ** : I have read the other reviews and the authors ' responses . My score remains the same .

We thank the reviewer for constructive feedback . Below R stands for Reviewer and A stands for our answers .

R : “ Experiment on higher - dimensional domains ” :

A : In our experiments , we investigated the option of using a V^e pre-trained from expert demonstrations . In higher - dimensional tasks , training a globally accurate value function purely from a batch of expert demonstrations is difficult due to the large state space and the fact that the expert demonstrations only cover a tiny part of the state space . This is the main reason we think the Swimmer and Hopper experiment did not show the clear advantage of our approach . For higher - dimensional tasks , we believe we need a stronger assumption on the availability of the expert . For example , we may require oracles to exist in the training loop ( similar to the assumptions used in previous IL work such as DAgger ( Ross et.al , 2011 ) ) so that we can estimate and query V^e ( s ) on the fly . This kind of oracle can exist in practice : for example access to a simulator , a search algorithm , and in real robotics applications ( e.g. , see Choudhury et.al , 17 , ICRA , Pan et.al , 17 , where optimal controllers were used as oracles ) , but also in natural language processing , where the ground truth label information can be leveraged to construct experts ( Chang et al , 15 , ICML , Sun et al , 17 ICML ) . In these applications , we can not guarantee the constructed “ expert ” is globally optimal , hence we believe our work can be directly applied and result in improved performance . Experiments like these are left to future work .

R : Compare to Supervised Training :

A : We thank the reviewer for this suggestion , we are working on this and will include a comparison to a simple supervised learning approach , although previous work has demonstrated that supervised learning wo n’t work as well as the interactive learning setting ( Ross & Bagnell , 2011 , AISTATS ) in both theory and in practice .

R : “ how many demonstrations : ”

A : We used a number of demonstration trajectories ranging from 10 to 100 ( almost the same as the number of trajectories we use in each batch during learning ) , depending on the task . For higher - dimensional tasks , a larger number of state-action pairs collected from demonstrations are needed . This is simply due to the fact that the feature space has a higher dimension .

R : “ Regarding to previous work on Imitation learning and inverse RL ”

A : We thank the reviewer for pointing out this paper . This paper is related and this paper also shows the advantage of using a truncated horizon ( e.g. , less computational complexity ) , although the context is different : interpolating between behaviour cloning and inverse RL ( or “ intention learning , ” in the paper ’s words ) by learning a reward functions and then planning with the known dynamics . Our work interpolates between a different imitation learning approach --- interactive IL and RL . We think this work and our work together show the advantage of using a truncated horizon : less computational complexity ( what this paper showed ) , and better performance than an imperfect expert ( what we showed in our work ) .


=== SUMMARY ===

The paper considers a combination of Reinforcement Learning ( RL ) and Imitation Learning ( IL ) , in the infinite horizon discounted MDP setting .
The IL part is in the form of an oracle that returns a value function V^e , which is an approximation of the optimal value function . The paper defines a new cost ( or reward ) function based on V^e , through shaping ( Eq . 1 ) . It is known that shaping does not change the optimal policy .

A key aspect of this paper is to consider a truncated horizon problem ( say horizon k ) with the reshaped cost function , instead of an infinite horizon MDP .
For this truncated problem , one can write the ( dis ) advantage function as a k-step sum of reward plus the value returned by the oracle at the k-th step ( cf. Eq. 5 ) .
Theorem 3.3 shows that the value of the optimal policy of the truncated MDP w.r.t. the original MDP is only O ( gamma^k eps ) worse than the optimal policy of the original problem ( gamma is the discount factor and eps is the error between V^e and V * ) .

This suggests two things :
1 ) Having an oracle that is accurate ( small eps ) leads to good performance . If oracle is the same as the optimal value function , we do not need to plan more than a single step ahead .
2 ) By planning for k steps ahead , one can decrease the error in the oracle geometrically fast . In the limit of k — > inf , the error in the oracle does not matter .

Based on this insight , the paper suggests an actor-critic-like algorithm called THOR ( Truncated HORizon policy search ) that minimizes the total cost over a truncated horizon with a modified cost function .

Through a series of experiments on several benchmark problems ( inverted pendulum , swimmer , etc. ) , the paper shows the effect of planning horizon k.



=== EVALUATION & COMMENTS ===

I like the main idea of this paper . The paper is also well - written . But one of the main ideas of this paper ( truncating the planning horizon and replacing it with approximation of the optimal value function ) is not new and has been studied before , but has not been properly cited and discussed .

There are a few papers that discuss truncated planning . Most closely is the following paper :

Farahmand , Nikovski , Igarashi , and Konaka , “ Truncated Approximate Dynamic Programming With Task - Dependent Terminal Value , ” AAAI , 2016 .

The motivation of AAAI 2016 paper is different from this work . The goal there is to speedup the computation of finite , but large , horizon problem with a truncated horizon planning . The setting there is not the combination of RL and IL , but multi-task RL . An approximation of optimal value function for each task is learned off - line and then used as the terminal cost .
The important point is that the learned function there plays the same role as the value provided by the oracle V^e in this work . They both are used to shorten the planning horizon . That paper theoretically shows the effect of various error terms , including terms related to the approximation in the planning process ( this paper does not do that ) .

Nonetheless , the resulting algorithms are quite different . The result of this work is an actor-critic type of algorithm . AAAI 2016 paper is an approximate dynamic programming type of algorithm .

There are some other papers that have ideas similar to this work in relation to truncating the horizon . For example , the multi-step lookahead policies and the use of approximate value function as the terminal cost in the following paper :

Bertsekas , “ Dynamic Programming and Suboptimal Control : A Survey from ADP to MPC , ” European Journal of Control , 2005 .

The use of learned value function to truncate the rollout trajectory in a classification - based approximate policy iteration method has been studied by

Gabillon , Lazaric , Ghavamzadeh , and Scherrer , “ Classification - based Policy Iteration with a Critic , ” ICML , 2011 .

Or in the context of Monte Carlo Tree Search planning , the following paper is relevant :

Silver et al. , “ Mastering the game of Go with deep neural networks and tree search , ” Nature , 2016 .

Their “ value network ” has a similar role to V^e . It provides an estimate of the states at the truncated horizon to shorten the planning depth .

Note that even though these aforementioned papers are not about IL , this paper ’s stringent requirement of having access to V^e essentially make it similar to those papers .


In short , a significant part of this work ’s novelty has been explored before . Even though not being completely novel is totally acceptable , it is important that the paper better position itself compared to the prior art .


Aside this main issue , there are some other comments :


- Theorem 3.1 is not stated clearly and may suggest more than what is actually shown in the proof . The problem is that it is not clear about the fact the choice of eps is not arbitrary .
The proof works only for eps that is larger than 0.5 . With the construction of the proof , if eps is smaller than 0.5 , there would not be any error , i.e. , J( \hat{pi}^ * ) = J( pi^ * ) .

The theorem basically states that if the error is very large ( half of the range of value function ) , the agent does not not perform well . Is this an interesting case ?


- In addition to the papers I mentioned earlier , there are some results suggesting that shorter horizons might be beneficial and / or sufficient under certain conditions . A related work is a theorem in the PhD dissertation of Ng :

Andrew Ng , Shaping and Policy Search in Reinforcement Learning , PhD Dissertation , 2003 .
( Theorem 5 in Appendix 3.B : Learning with a smaller horizon ) .

It is shown that if the error between Phi ( equivalent to V^e here ) and V * is small , one may choose a discount factor gamma ’ that is smaller than gamma of the original MDP , and still have some guarantees . As the discount factor has an interpretation of the effective planning horizon , this result is relevant . The result , however , is not directly comparable to this work as the planning horizon appears implicitly in the form of 1 / ( 1 - gamma’ ) instead of k , but I believe it is worth to mention and possibly compare .

- The IL setting in this work is that an oracle provides V^e , which is the same as ( Ross & Bagnell , 2014 ) . I believe this setting is relatively restrictive as in many problems we only have access to ( state , action ) pairs , or sequence thereof , and not the associated value function . For example , if a human is showing how a robot or a car should move , we do not easily have access to V^e ( unless the reward function is known and we estimate the value with rollouts ; which requires us having a long trajectory ) . This is not a deal breaker , and I would not consider this as a weakness of the work , but the paper should be more clear and upfront about this .


- The use of differential operator nabla instead of gradient of a function ( a vector field ) in Equations ( 10 ) , ( 14 ) , ( 15 ) is non-standard .

- Figures are difficult to read , as the colors corresponding to confidence regions of different curves are all mixed up . Maybe it is better to use standard error instead of standard deviation .


===
After Rebuttal : Thank you for your answer . The revised paper has been improved . I increase my score accordingly .


We thank the reviewer for constructive feedback , below R stands for Reviewer and A stands for our answer

R : “ Regarding to some previous work using truncated horizon ” :

A : we gratefully thank the reviewer for pointing out all of these related previous works . We agree with the reviewer that the idea of using truncated horizon has been explored before . Following the suggestions from the reviewer , we have revised the paper to better position our work . Please see Sec 1.1 in the revision for a discussion on related work and our contributions .

Some of the previous work that uses the idea of truncating horizon mainly focuses on using V^{\pi} as the termination value for bias-variance tradeoff . We used V^e , instead of V^{\pi} . We think using V^{e} with truncated horizon allows us to interpolate between pure IL and full RL and trades between sample complexity and the optimality : with k=1 , the reshaped MDP is easy to solve as it 's one - step greedy and we reveal IL algorithm AggreVaTeD , but at the cost of only learning a policy that has similar performance as the expert . When k > 1 , we face a MDP that is between the one-step greedy MDP and the original full horizon MDP . Solving a truncated horizon MDP with k > 1 is harder than the one-step greedy MDP , but at the benefit of outperforming the expert and getting closer to optimality . Another contribution of the paper is an efficient actor-critic like algorithm for continuous MDPs , which is not available in previous work ( e.g. , Ng 's thesis work )

R : " Theorem 5 in Appendix 3. B in Ng 's PhD dissertation " :

A : Again , we gratefully thank the reviewer for pointing out this theorem that we were not aware of ! We agree with the reviewer , a smaller discount factor in theory is equivalent to a shorter planning horizon . One of the advantages of explicitly using a truncated horizon is for reducing computational complexity , as mentioned by some other previous work . Though we agree that our main theorem is similar to Theorem 5 in the appendix of Ng 's dissertation , we would like to emphasize that one of the contributions of our work is that we interpret expert 's value function as a potential function and this could help us explain and generalize previous imitation learning and bridge the gap between IL and RL.

R : the choice of \eps in the proof of theorem 3.1

A : We believe we can construct similar MDPs to make \eps smaller by increasing the number of actions . We believe we can make \eps small around 1 / |A| , where | A| is the number of actions . The main idea is that we want to construct a MDP where for each state s , the value A^e ( s , a ) itself is close to the value A * ( s , a ) , but the order on actions induced by A^e is different from the order of actions induced by A*( s , a ) , forcing the greedy policy with respect to A^e to make mistakes .



This work proposes to use the value function V^e of some expert policy \pi^e in order to speed up learning of an RL agent which should eventually do better than the expert . The emphasis is put on using k-steps ( with k> 1 ) Bellman updates using bootstrapping from V^e .

It is claimed that the case k=1 does not allow the agent to outperform the expert policy , whereas k>1 does ( Section 3.1 , paragraph before Lemma 3.2 ) .

I disagree with this claim . Indeed a policy gradient algorithm ( similar to ( 10 ) ) with a 1 - step advantage c( s , a ) + gamma V^e ( s_{t + 1 } ) - V^e( s_t ) will converge ( say in the tabular case , or in the case you consider of a rich enough policy space \ Pi ) to the greedy policy with respect to V^e , which is strictly better than V^e ( if V^e is not optimal ) . So you do n’t need to use k>1 to improve the expert policy . Now it ’s true that this will not converge to the optimal policy ( since you keep bootstrapping with V^e instead of the current value function ) , but neither the k-step advantage will .

So I do n’t see any fundamental difference between k=1 and k> 1 . The only difference being that the k-step bootstrapping will implement a k-step Bellman operator which contracts faster ( as gamma^k ) when k is large . But the best choice of k has to be discussed in light of a bias-variance discussion , which is missing here . So I find that the main motivation for this work is not well supported .

Algorithmic suggestion :
Instead of bootstrapping with V^e , why not bootstrap with min( V^e , V ) , where V is your current approximation of the value function . In that way you would benefit from ( 1 ) fast initialization with V^e at the beginning of learning , ( 2 ) continual improvement once you ’ve reached the performance of the expert .

Other comments :
Requiring that we know the value function of the expert on the whole state space is a very strong assumption that we do not usually make in Imitation learning . Instead we assume we have trajectories from expert ( from which we can compute value function along those trajectories only ) . Generalization of the value function to other states is a hard problem in RL and is the topic of important research .

The overall writing lacks rigor and the contribution is poor . Indeed the lower bound ( Theorem 3.1 ) is not novel ( btw , the constant hidden in the \ Omega notation is 1 / ( 1 - gamma ) ) . Theorems 3.2 and 3.3 are not novel either . Please read [ Bertsekas and Tsitsiklis , 96 ] as an introduction to dynamic programming with approximation .

The writing could be improved , and there are many typos , such as :
- J is not defined ( Equation ( 2 ) )
- Why do you call A a disadvantage function whereas this quantity is usually called an advantage ?
- You are considering a finite ( ie , k ) horizon setting , so the value function depend on time . For example the value functions defined in ( 11 ) depend on time .
- All derivations in Section 4 , before subsection 4.1 are very approximate and lack rigor .
- Last sentence of Proof of theorem 3.1 . I do n’t understand H -> 2H epsilon . H is fixed , right ? Also your example does not seem to be a discounted problem .


We thank the reviewer for constructive feedback , below R stands for Reviewer and A stands for our answer

R : K=1 VS K > 1 :

A : We do not agree with the reviewer on this point . First , when k=infinity , we can find the optimal policy ( under the assumption that we can fully optimize the MDP , of course ) which will be significantly better than an imperfect expert pi^e . While we agree that when K=1 , the greedy policy with respect to V^e can outperform the expert , the greedy policy is only a * one -step * deviation improvement of V^e . If pi^e is far away from optimality , the one-step improvement greedy policy will likely be far away as well . This is shown in the lower bound analysis . Our main theorem clearly shows that as k increases , the learned policy is getting closer and closer to optimality . Combine the lower bound on the performance of the one-step greedy policy , and the upper bound on the performance of the learned policy with k> 1 , and it is clear that the learned policy under k>1 is closer to optimality . In summary , we are emphasizing that with k>1 , we can learn a policy that is even better than the greedy policy with respect Q^e ( not just pi^e ) , which is the best one can learn if one uses previous algorithm such as AggreVaTe .

We have clarified this point in the revised version of the paper .

R : Regarding bias -variance tradeoff

Unlike previous work ( e.g. , TD learning with k-step look ahead ) , we are using V^e as the termination value instead of V^pi . The main argument of our work is that by reasoning k steps into the future with V^e as the termination value , we are trading between learning complexity and the optimality . When k = 1 , we are greedy with respect to the one -step cost , but the downside is that we can only hope that the learned policy is , at best , a one -step deviation improvement over the expert . When k > 1 , but less than infinity , we are essentially solving a MDP that is easier than the original MDP due to the shorter horizon , but with the benefit of learning a policy that is moving closer to the optimality than the greedy policy with respect to Q^e . In our theorem , gamma^k does not merely serve as a contraction factor as it did in , for example , TD learning . gamma^k here serves as a measure on how close the learned policy is to the optimal policy .

R : “ Requiring know the value function of the expert on the whole state space is a very strong assumption ... ”

We agree with the reviewer . In our experiments , we learned V^e from a set of demonstrations and then used the learned V^e . As we showed , previous work AggreVaTe ( D ) performs poorly in this setting , as V^e may only be an accurate oracle in the state space of the expert ’s demonstrations . For challenging problems with very large state action spaces , we need to assume that \pi^e exists so that V^e can be estimated by rollouts ( the same assumption DAgger used ( Ross & Bagnell 11 , AISTATS ) ) . Luckily , this kind of oracles do exist in practice via access to a simulator , a search algorithm , even in real robotics applications ( e.g. , see Choudhury et.al , 17 , ICRA , Pan et.al , 17 ) , and in natural language processing , where the ground - truth label information can be leveraged to construct experts ( Chang et al , 15 , ICML , Sun et al , 17 ICML ) . In these applications , we cannot guarantee the constructed “ expert ” is globally optimal , hence our results can be directly applied .

R : " J is not defined .. " :

We thank the reviewer for pointing this out . We have revised the draft accordingly .

R : proof of theorem 3.1

We thank the reviewer for pointing out the confusion . Although the general proof strategy is not changed , we have revised the proof and also changed it to the setting with discount factor and infinite horizon to make it consistent with the main setting in the paper .


Neal ( 1994 ) showed that a one hidden layer Bayesian neural network , under certain conditions , converges to a Gaussian process as the number of hidden units approaches infinity . Neal ( 1994 ) and Williams ( 1997 ) derive the resulting kernel functions for such Gaussian processes when the neural networks have certain transfer functions .

Similarly , the authors show an analogous result for deep neural networks with multiple hidden layers and an infinite number of hidden units per layer , and show the form of the resulting kernel functions . For certain transfer functions , the authors perform a numerical integration to compute the resulting kernels . They perform experiments on MNIST and CIFAR - 10 , doing classification by scaled regression .

Overall , the work is an interesting read , and a nice follow - up to Neal ’s earlier observations about 1 hidden layer neural networks . It combines several insights into a nice narrative about infinite Bayesian deep networks . However , the practical utility , significance , and novelty of this work -- in its current form -- are questionable , and the related work sections , analysis , and experiments should be significantly extended .


In detail :

( 1 ) This paper misses some obvious connections and references , such as
* Krauth et. al ( 2017 ) : “ Exploring the capabilities and limitations of Gaussian process models ” for recursive kernels with GPs .
* Hazzan & Jakkola ( 2015 ) : “ Steps Toward Deep Kernel Methods from Infinite Neural Networks ” for GPs corresponding to NNs with more than one hidden layer .
* The growing body of work on deep kernel learning , which “ combines the inductive biases and representation learning abilities of deep neural networks with the non-parametric flexibility of Gaussian processes ” . E.g. : ( i ) “ Deep Kernel Learning ” ( AISTATS 2016 ) ; ( ii ) “ Stochastic Variational Deep Kernel Learning ” ( NIPS 2016 ) ; ( iii ) “ Learning Scalable Deep Kernels with Recurrent Structure ” ( JMLR 2017 ) .

These works should be discussed in the text .

( 2 ) Moreover , as the authors rightly point out , covariance functions of the form used in ( 4 ) have already been proposed . It seems the novelty here is mainly the empirical exploration ( will return to this later ) , and numerical integration for various activation functions . That is perfectly fine -- and this work is still valuable . However , the statement “ recently , kernel functions for multi-layer random neural networks have been developed , but only outside of a Bayesian framework ” is incorrect . For example , Hazzan & Jakkola ( 2015 ) in “ Steps Toward Deep Kernel Methods from Infinite Neural Networks ” consider GP constructions with more than one hidden layer . Thus the novelty of this aspect of the paper is overstated .

See also comment [ *] later on the presentation . In any case , the derivation for computing the covariance function ( 4 ) of a multi-layer network is a very simple reapplication of the procedure in Neal ( 1994 ) . What is less trivial is estimating ( 4 ) for various activations , and that seems to the major methodological contribution .

Also note that multidimensional CLT here is glossed over . It ’s actually really unclear whether the final limit will converge to a multidimensional Gaussian with that kernel without stronger conditions . This derivation should be treated more thoroughly and carefully .

( 3 ) Most importantly , in this derivation , we see that the kernels lose the interesting representations that come from depth in deep neural networks . Indeed , Neal himself says that in the multi-output settings , all the outputs become uncorrelated . Multi-layer representations are mostly interesting because each layer shares hidden basis functions . Here , the sharing is essentially meaningless , because the variance of the weights in this derivation shrinks to zero .
In Neal ’s case , the method was explored for single output regression , where the fact that we lose this sharing of basis functions may not be so restrictive . However , these assumptions are very constraining for multi-output classification and also interesting multi-output regressions .

[ * ] : Generally , in reading the abstract and introduction , we get the impression that this work somehow allows us to use really deep and infinitely wide neural networks as Gaussian processes , and even without the pain of training these networks . “ Deep neural networks without training deep networks ” . This is not an accurate portrayal . The very title “ Deep neural networks as Gaussian processes ” is misleading , since it ’s not really the deep neural networks that we know and love . In fact , you lose valuable structure when you take these limits , and what you get is very different than a standard deep neural network . In this sense , the presentation should be re-worked .

( 4 ) Moreover , neural networks are mostly interesting because they learn the representation . To do something similar with GPs , we would need to learn the kernel . But here , essentially no kernel learning is happening . The kernel is fixed .

( 5 ) Given the above considerations , there is great importance in understanding the practical utility of the proposed approach through a detailed empirical evaluation . In other words , how structured is this prior and does it really give us some of the interesting properties of deep neural networks , or is it mostly a cute mathematical trick ?

Unfortunately , the empirical evaluation is very preliminary , and provides no reassurance that this approach will have any practical relevance :
( i ) Directly performing regression on classification problems is very heuristic and unnecessary .
( ii ) Given the loss of dependence between neurons in this approach , it makes sense to first explore this method on single output regression , where we will likely get the best idea of its useful properties and advantages .
( iii ) The results on CIFAR10 are very poor . We do n’t need to see SOTA performance to get some useful insights in comparing for example parametric vs non- parametric , but 40 % more error than SOTA makes it very hard to say whether any of the observed patterns hold weight for more competitive architectural choices .

A few more minor comments :
( i ) How are you training a GP exactly on 50 k training points ? Even storing a 50 k x 50 k matrix requires about 20 GB of RAM . Even with the best hardware , computing the marginal likelihood dozens of times to learn hyperparameters would be near impossible . What are the runtimes ?
( ii ) " One benefit in using the GP is due to its Bayesian nature , so that predictions have uncertainty estimates ( Equation ( 9 ) ) . ” The main benefit of the GP is not the uncertainty in the predictions , but the marginal likelihood which is useful for kernel learning .

With regards to the comments on empirical results :

-- “ ( i ) … regression on classification problems is very heuristic and unnecessary . ”

We do make clear that these experiments using regression for classification are less principled , in the main text . However , we ’d like to note that least - squares classification is widely used and effective [ 3 ] . Moreover , it allows us to compare exact inference via a GP to prediction by a trained neural network on well - studied tasks ( e.g. MNIST and CIFAR - 10 ) .

-- “ 3 ) Most importantly , in this derivation , we see that the kernels lose the interesting representations that come from depth in deep neural networks . Indeed , Neal himself says that in the multi-output settings , all the outputs become uncorrelated . Multi-layer representations are mostly interesting because each layer shares hidden basis functions . Here , the sharing is essentially meaningless , because the variance of the weights in this derivation shrinks to zero .
In Neal ’s case , the method was explored for single output regression , where the fact that we lose this sharing of basis functions may not be so restrictive . However , these assumptions are very constraining for multi-output classification and also interesting multi-output regressions . ”

“ ( ii ) Given the loss of dependence between neurons in this approach , it makes sense to first explore this method on single output regression , where we will likely get the best idea of its useful properties and advantages . ”

This is an excellent point , which applies to almost all GP work . Based on your recommendation , we are looking into single - output regression tasks .

However , we would like to emphasize that despite the NNGP being unable to explicitly capture dependencies between classes it still could outperform neural networks on multi-class regression . We believe this provides stronger , rather than weaker , evidence for the utility of the NNGP formulation .

-- “ ( iii ) The results on CIFAR10 are very poor . ”

First we would like to emphasize that the purpose of experiments was to show that the NNGP is the limiting behaviour for a specified neural network architecture . Because the GP equivalence was derived for vanilla fully - connected networks , all experiments were performed using that architecture . Achieving state -of- the-art on CIFAR - 10 typically involves using a convolutional architecture , as well as data augmentation , batch - norm , residual connection , dropout , etc .

Restricting to vanilla multi-layer fully - connected networks with ReLU activation , the performance quoted in [ 4 ] is actually slightly lower than our GP results ( 53 - 55 % accuracy , Figure 4 ( a ) of [ 4 ] ) . So our baseline and results are not poor for the class of models we examine . Our experiments show that , for the given class of neural network architecture , as width increases the behaviour more closely resembles that of the NNGP , which is competitive or better than that of the given neural network class .

We note that introducing linear bottleneck layer structure in [ 4 ] seem to achieve SOTA in permutation invariant ( without convolutional layers ) CIFAR - 10 which is higher than ours . It is an interesting question how this type of model relates to the GP limit but it is outside the scope of this work .

Regards to the other comments :

( i ) Exact GP computation in the large data regime can be costly . We used a machine with 150 GB of RAM ( with some inefficiencies in memory usage , e.g. stemming from use of float64 , and TensorFlow retaining intermediate state in memory ) , and 64 CPU cores , to run the full MNIST / CIFAR - 10 experiments . We utilized parallel linear algebra computations available through Tensor Flow to speed up computations . For a typical run , constructing the kernel per layer took 90 - 140 seconds , and solving the linear equations ( via Cholesky decomposition ) took 180 - 220 seconds for 1000 test points .

( ii ) We agree with the reviewer that one strength of Bayesian methods is providing marginal likelihood and using that for model selection . Although we propose this possibility for future work in the text , greater emphasis could have been made . With that said , we believe that providing uncertainty estimates is another important benefit of a Bayesian approach , that we explore experimentally in our text , and that the GP perspective on neural networks is beneficial in this regard as well .


[ 3 ] Ryan Rifkin and Aldebaro Klautau . In defense of one - vs- all classification . Journal of machine learning research , 5 ( Jan ) :101–141 , 2004 .
Ryan Rifkin , Gene Yeo , Tomaso Poggio , et al . Regularized least-squares classification . Nato Science Series Sub Series III Computer and Systems Sciences , 190:131–154 , 2003 .

[ 4 ] Zhouhan Lin , Roland Memisevic , Kishore Konda , How far can we go without convolution : Improving fully - connected networks , arXiv 1511.02580 .


We thank the reviewer for their time and constructive feedback on the submission .

-- references

We thank the reviewer for suggesting related works . In the revised version , we will add Krauth et al. ( 2017 ) as well as additional comparisons with the deep kernel learning literature .

-- novelty with regards to Hazan & Jaakola ( 2015 )

We do not believe that the work in H-J significantly detracts from the novelty of our paper .

H- J is also interested in constructing kernels equivalent to infinitely wide deep neural networks . Theorem 1 in H- J is a good stepping stone for our construction . However the H-J construction does not go beyond two hidden layers with nonlinearities . They state :
“ We present our framework with only two intermediate layers ... It can be extended to any depth but the higher layers may not use nonlinearities . " H- J

We believe that the fact that H-J approached the same problem , and only derived a GP kernel for up to two layers , despite making use of the same random kernel literature we do , is illustrative of the non-obvious nature of the equivalence between infinitely wide networks of arbitrary depth and GPs .

We will expand our existing discussion of H- J in the text , and state that previous work has proposed GP kernels for networks with up to two hidden layers .

--“ In any case , the derivation for computing the covariance function ( 4 ) of a multi-layer network is a very simple reapplication of the procedure in Neal ( 1994 ) . ”

We agree that the derivation is simple . We believe that this , combined with the fact that it has gone unpublished for more than two decades , increases rather than detracts from its significance .

--“ Also note that multidimensional CLT here is glossed over . It ’s actually really unclear whether the final limit will converge to a multidimensional Gaussian with that kernel without stronger conditions . This derivation should be treated more thoroughly and carefully . ”

Thank you for sharing your concerns . We would very much like to address them . Could you be more specific about the ways in which you are concerned the CLT may fail in this case ? If we take the infinite - width limit layer - by-layer , the application of the CLT seems appropriate without additional subtlety .

-- “ In fact , you lose valuable structure when you take these limits , and what you get is very different than a standard deep neural network . In this sense , the presentation should be re-worked . ”

We agree that the qualitative behavior of infinitely wide neural networks may be different than that of narrow networks . We will update the text to more clearly discuss this .

We note though that finite width network performance often increases with increasing network width , as they become closer to the GP limit . For example , see [ 1 ] , [ 2 ] . In fact in our Figure 1 , we found that the performance of finite width networks increases , and more closely resembles that of the NNGP , as the network is made wider .

To more thoroughly address this concern and support this observation , we performed an additional experiment where we trained 5 layer fully connected networks with Tanh and ReLU nonlinearities on CIFAR10 , with random optimization and initialization hyperparameters . We then filtered for training runs which achieved 100 % classification accuracy on the training set , resulting in 125 Tanh and 55 ReLU networks . We then examined the performance of these networks vs . network width . We found that the best performing networks are in fact the widest . See the following figures , where each point shows the width and corresponding generalization gap of a single trained 5 layer network , with 100 % training accuracy :
https://www.dropbox.com/s/np4myfzy1a3ts46/relu_depth_5_gap_to_width_cifar10.pdf
https://www.dropbox.com/s/f1cd73hvpesm8n2/tanh_depth_5_gap_to_width_cifar10.pdf

-- “ Moreover , neural networks are mostly interesting because they learn the representation . To do something similar with GPs , we would need to learn the kernel . But here , essentially no kernel learning is happening . The kernel is fixed . ”

We agree that the learned representations are one important aspect of deep networks , and we agree that no explicit representation learning happens in our GP approach .

However , we emphasize that in many situations deep networks are chosen not for their interpretable representations , but rather because of the high accuracy of their predictions . We believe that work that reproduces the predictions made by deep networks using an alternative procedure is useful even if it does not also reproduce the internal representations of deep networks .

Ways to sample deep representations from the corresponding NNGP would be a fascinating avenue for future research .

[ 1 ] Neyshabur B , Tomioka R , Srebro N . In search of the real inductive bias : On the role of implicit regularization in deep learning . arXiv:1412.6614 . 2014 .

[ 2 ] Zagoruyko S , Komodakis N. Wide residual networks . arXiv:1605.07146 . 2016 .

This paper leverages how deep Bayesian NNs , in the limit of infinite width , are Gaussian processes ( GPs ) . After characterizing the kernel function , this allows us to use the GP framework for prediction , model selection , uncertainty estimation , etc .


- Pros of this work

The paper provides a specific method to efficiently compute the covariance matrix of the equivalent GP and shows experimentally on CIFAR and MNIST the benefits of using the this GP as opposed to a finite- width non -Bayesian NN .

The provided phase analysis and its relation to the depth of the network is also very interesting .

Both are useful contributions as long as deep wide Bayesian NNs are concerned . A different question is whether that regime is actually useful .


- Cons of this work

Although this work introduces a new GP covariance function inspired by deep wide NNs , I am unconvinced of the usefulness of this regime for the cases in which deep learning is useful .

For instance , looking at the experiments , we can see that on MNIST - 50 k ( the one with most data , and therefore , the one that best informs about the " true " underlying NN structure ) the inferred depth is 1 for the GP and 2 for the NN , i.e. , not deep . Similarly for CIFAR , where only up to depth 3 is used . None of these results beat state - of - the - art deep NNs .

Also , the results about the phase structure show how increased depth makes the parameter regime in which these networks work more and more constrained .

In [ 1 ] , it is argued that kernel machines with fixed kernels do not learn a hierarchical representation . And such representation is generally regarded as essential for the success of deep learning .

My impression is that the present line of work will not be relevant for deep learning and will not beat state - of - the- art results because of the lack of a structured prior . In that sense , to me this work is more of a negative result informing that to be successful , deep Bayesian NNs should not be wide and should have more structure to avoid reaching the GP regime .


- Other comments :

In Fig. 5 , use a consistent naming for the axes ( bias and variances ) .

In Fig. 1 , I did n't find the meaning of the acronym NN with no specified width .

Does the unit norm normalization used to construct the covariance disallow ARD input selection ?


[ 1 ] Yoshua Bengio , Olivier Delalleau , and Nicolas Le Roux . The Curse of Dimensionality for Local Kernel Machines . 2005 .

-- Fixed Kernel machines vs representation learning of deep neural networks

While the functional form of our GP kernel is fixed , and no kernel learning is happening in the sense of Deep Kernel Learning [ 3 ] , we do learn hyper - parameters ( induced by neural network architecture ) for kernels in our experiments by grid search . Using GP marginal likelihood , one could learn hyper - parameters for the equivalent neural network by end - to - end gradient descent as well .

Although our NNGP does not admit explicit hierarchical representation learning , we note that our experiments showing that an NNGP can perform better than its finite width counterpart suggest interesting scientific question on the role of learned representations . Exploring ways to sample intermediate representations from the posterior implied by the NNGP would be a fascinating direction for future work .

Regards to the other comments :

-- In Fig. 5 , use a consistent naming for the axes ( bias and variances ) .

Thank you for noticing this . We will update the figures in the revised version .

-- In Fig. 1 , I did n't find the meaning of the acronym NN with no specified width .

We will include the description in the revised version . The acronym NN in the figure denotes the best performing ( on the validation set ) neural network across all width and trials . Often this is the neural network with the largest width .

-- “ Does the unit norm normalization used to construct the covariance disallow ARD input selection ? ”

Thank you for bringing up the point about ARD . With extra computational and memory cost , unit normalization for inputs could be avoided by separately tiling the variance of each input when constructing the lookup table in Section 2.5 . Also note , input pre-processing in general can change ARD scores , and scaling inputs to have a constant norm is not an uncommon form of pre-processing .


Thank you again for your careful review ! We believe we have effectively addressed your primary concern about the relevance of the wide network limit , and we hope you will consider raising your score as a result .

[ 3 ] Andrew Gordon Wilson , Zhiting Hu , Ruslan Salakhutdinov , Eric P. Xing , Deep Kernel Learning . AISTATS 2016 .

We thank the reviewer for their time and constructive feedback on the submission .

-- Usefulness of the regime .

As noted , the best performing depth for the NNGP for full datasets was shallow ( depth 1 in MNIST and depth 3 in CIFAR - 10 ) . A few points about this :
i ) For these datasets , the best performing neural network is also shallow ( depth 2 in MNIST and depth 3 or 2 in CIFAR - 10 ) . As our NNGP construction is the limiting behaviour of wide neural networks , we believe that the GP performing best with a shallow depth is consistent with this equivalence .
ii ) For the small data-regime , we note that there were benefits from increased depth , both for the NN and the NNGP .
iii ) We also note that when the dataset became more complex ( MNIST to CIFAR - 10 ) the GP and NN both benefited from additional depth .
iv ) All experiments in our paper were performed in the fully connected case , where the evidence for the benefits of hierarchy + depth is weaker than for convolutional networks .
v ) Lastly , although the best performing depth are shallow , the deep NNGPs perform competitively with the shallow ones . For example , with RELU the depth 10 NNGP for MNIST - 50 k has test accuracy of 0.987 , and for CIFAR - 45 k with RELU has test accuracy 0.5573 . The best performing accuracy for those cases was 0.9875 and 0.5566 respectively . ( Note that for CIFAR depth 10 test accuracy is actually * higher * than depth - 3 , this is due to model selection based on the validation set rather than the test set . ) The performance loss from depth in NNs is much larger , possibly due to harder optimization .

-- “ In [ Bengio , Delalleau , and Le Roux ] , it is argued that kernel machines with fixed kernels do not learn a hierarchical representation . And such representation is generally regarded as essential for the success of deep learning .

My impression is that the present line of work will not be relevant for deep learning and will not beat state - of - the- art results because of the lack of a structured prior . In that sense , to me this work is more of a negative result informing that to be successful , deep Bayesian NNs should not be wide and should have more structure to avoid reaching the GP regime . ”

First , we would like to note that finite width network performance often increases monotonically with increasing network width , as the networks become closer to a GP limit . For example , see [ 1 ] , [ 2 ] . In fact in our Figure 1 , we found that the performance of finite width networks increases , and more closely resembles that of the NNGP , as the network is made wider .

To more thoroughly address this concern and support this observation , we performed an additional experiment where we trained 5 layer fully connected networks with tanh and ReLU nonlinearities on CIFAR10 , with random optimization and initialization hyperparameters . We then filtered for training runs which achieved 100 % classification accuracy on the training set , resulting in 125 Tanh and 55 ReLU networks . We then examined the performance of these networks vs . network width . We found that the best performing networks are in fact the widest . See the following figures , where each point shows the width and corresponding generalization gap of a single trained 5 layer network , with 100 % training accuracy :
https://www.dropbox.com/s/np4myfzy1a3ts46/relu_depth_5_gap_to_width_cifar10.pdf
https://www.dropbox.com/s/f1cd73hvpesm8n2/tanh_depth_5_gap_to_width_cifar10.pdf

Second , we would like to address the concerns about kernel methods which the reviewer cites from Bengio , Delalleau , and Le Roux ( BDL ) . The analysis in BDL assumes a local kernel ( e.g. an RBF kernel ) . The NNGP kernel is non-local and heavy tailed , as can be seen in the Figure showing its angular structure in Appendix B of our paper . Specifically , Equation 10 in BDL , which demands that the kernel approaches a constant with increasing distance between points , does not hold for the NNGP kernel : As discussed in our paper , inputs are scaled to have constant norm -- i.e. all inputs live on the surface of a hypersphere . There is no angular separation between points on the hypersphere after which the NNGP kernel goes to a constant ( again see Appendix B figure ) .

Finally , we are not sure if we have fully understood your concern about the lack of a structured prior . If the above responses do not address your concern , could you be more specific about what structure is required in a prior of functions which a GP is unable to capture ?

[ 1 ] Neyshabur B , Tomioka R , Srebro N . In search of the real inductive bias : On the role of implicit regularization in deep learning . arXiv:1412.6614 . 2014 .

[ 2 ] Zagoruyko S , Komodakis N. Wide residual networks . arXiv:1605.07146 . 2016 .

This paper presents a new covariance function for Gaussian processes ( GPs ) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width . As a result , exact Bayesian inference with a deep neural network can be solved with the standard GP machinery .


Pros :

The result highlights an interesting relationship between deep nets and Gaussian processes . ( Although I am unsure about how much of the kernel design had already appeared outside of the GP literature . )

The paper is clear and very well written .

The analysis of the phases in the hyperparameter space is interesting and insightful . On the other hand , one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work !


Cons :

Although the computational complexity of computing the covariance matrix is given , no actual computational times are reported in the article .

I suggest using the same axis limits for all subplots in Figure 3.

Thank you for your time and constructive suggestions on the submission .

-- “ Although the computational complexity of computing the covariance matrix is given , no actual computational times are reported in the article . ”

We are grateful the suggestion . In the revised version we will add computation time for full MNIST for reference . As one datapoint , when constructing the 50 k x 50 k covariance matrix , the amortized computations for each layer take 90 - 140s ( depending on CPU generation and network depth ) , running on 64 CPUs .

--” I suggest using the same axis limits for all subplots in Figure 3 . ”

We will update the figures accordingly in the revised version .


This work proposes non-autoregressive decoder for the encoder - decoder framework in which the decision of generating a word does not depends on the prior decision of generated words . The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part , not the generated target words as inputs . To achieve the goal , authors investigated various techniques : For inference , sample fertility space for generating multiple possible translations . For training , apply knowledge distilation for better training followed by fine tuning by reinforce . Experiments for English / German and English / Romanian show comparable translation qualities with speedup by non-autoregressive decoding .

The motivation is clear and proposed methods are very sound . Experiments are carried out very carefully .

I have only minor concerns to this paper :

- The experiments are designed to achieve comparable BLEU with improved latency . I 'd like to know whether any BLUE improvement might be possible under similar latency , for instance , by increasing the model size given that inference is already fast enough .

- It 'd also like to see other language pairs with distorted word alignment , e.g. , Chinese / English , to further strengthen this work , though it might have little impact given that attention already capture sort of alignment .

- What is the impact of the external word aligner quality ? For instance , it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner .

- The positional attention is rather unclear and it would be better to revise it . Note that equation 4 is simply mentioning attention computation , not the proposed positional attention .

As explained in the response to Reviewer 2 , we decided to standardize on a single model size for the WMT experiments , but acknowledge that an evaluation of comparative performance at different sizes would be a worthwhile follow - up . However , direct comparison of the NAT at one model size to an autoregressive Transformer at a different model size may not be especially informative , because our NAT relies on an autoregressive teacher with the same model size in order to initialize the encoder .

The presence of the model distillation step also suggests that a meaningful BLEU improvement from larger NAT model size is unlikely , since only the NPD step allows the NAT to outperform its autoregressive teacher , even in the best case .

We believe that the NAT 's gap in performance between the English / German language pair and the English / Romanian language pair suggests that it is sensitive to the degree of reordering ; we agree that it would be worthwhile to follow up on this hypothesis with pairs like Japanese / English or Turkish / English that exhibit even more reordering than German / English does .

The external aligner we used produces fairly noisy results ; our experiments with using the attention weights from an autoregressive Transformer as a ( potentially more powerful ) alignment model resulted in somewhat worse performance , suggesting that the dependence on alignment quality may not be straightforward .

We can revise our description of the positional attention layer .

This paper describes an approach to decode non-autoregressively for neural machine translation ( and other tasks that can be solved via seq2seq models ) . The advantage is the possibility of more parallel decoding which can result in a significant speed - up ( up to a factor of 16 in the experiments described ) . The disadvantage is that it is more complicated than a standard beam search as auto-regressive teacher models are needed for training and the results do not reach ( yet ) the same BLEU scores as standard beam search .

Overall , this is an interesting paper . It would have been good to see a speed - accuracy curve which plots decoding speed for different sized models versus the achieved BLUE score on one of the standard benchmarks ( like WMT14 en- fr or en-de ) to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same BLEU scores . Table 1 gives a hint of that but it is not clear whether much smaller models with standard beam search are possibly as good and fast as NAT -- losing 2-5 BLEU points on WMT14 is significant . While the Ro-> En results are good , this particular language pair has not been used much by others ; it would have been more interesting to stay with a single well - used language pair and benchmark and analyze why WMT14 en - >de and de - >en are not improving more . Finally it would have been good to address total computation in the comparison as well -- it seems while total decoding time is smaller total computation for NAT + NPD is actually higher depending on the choice of s.


The reviewer has brought up an interesting point about comparison of models at different sizes .

We agree that the gap on English / German WMT14 is large enough that a relatively smaller autoregressive Transformer , especially on short sentences and with highly optimized inference kernels , might achieve similar latency and accuracy to the NAT . But no amount of kernel optimization or model size reduction can change the sequential nature of autoregressive translation ; the autoregressive latency will always be proportional to the sentence length and the NAT will be faster when sequences are sufficiently long . The non- autoregressive Transformer can also benefit from low - level optimizations like quantization ; we believe we compared the two on an even footing by using similar implementations for both .

Also , while the original Transformer paper provided a strong set of baseline hyperparameters for the autoregressive architecture given a particular model size , we would need to conduct a significant amount of additional search to identify the right parameter settings for other model sizes . Instead we chose to focus our computational resources on the ablation study and more language pairs .

We think the difference between the EN < –>DE and EN < –> RO results may be the result of a greater need for long - distance ( clause - level ) reordering between English and German ( which are closely related languages with significant differences in sentence structure ) than between English and Romanian ( which , while less closely related , have more similarities in word order ) ; this is an interesting direction for future research .

As for computation time , we are making the assumption that a significant amount of parallelism is available and the primary metric is the latency on the critical path . This is not necessarily the case for every deployment of machine translation in practice , but it is a metric on which existing neural MT systems perform particularly poorly . Given that assumption , the additional computation needed for NPD , while potentially significant in terms of throughput , would not result in more than a doubling of latency .

This paper can be seen as an extension of the paper " attention is all you need " that will be published at nips in a few weeks ( at the time I write this review ) .

The goal here is to make the target sentence generation non auto regressive . The authors propose to introduce a set of latent variables to represent the fertility of each source words . The number of target words can be then derived and they 're all predicted in parallel .

The idea is interesting and trendy . However , the paper is not really stand alone . A lot of tricks are stacked to reduce the performance degradation . However , they 're sometimes to briefly described to be understood by most readers .

The training process looks highly elaborate with a lot of hyper parameters . Maybe you could comment on this .

For instance , the use fertility supervision during training could be better motivated and explained . Your choice of IBM 2 is wired since it does n't include fertility . Why not IBM 4 , for instance ? How you use IBM model for supervision . This a simple example , but a lot of things in this paper is too briefly described and their impact not really evaluated .

We agree that the methodology presented in this paper contains several moving parts and a few techniques , such as external alignment supervision , sequence - level knowledge distillation , and fine-tuning using reinforcement learning , that could be considered “ tricks . ” All of these techniques are targeted at solving the multi-modality problem introduced when performing non- autoregressive translation , but if there ’s a particular part of the pipeline that you feel is unclear , we would be happy to improve the description and explanation .

Also , we would argue that our approach introduces relatively few additional hyperparameters over the original Transformer , primarily the inclusion of the various fine - tuning losses and the number of fertility samples used at inference time . While we did not conduct an exhaustive search over e.g. a range of possible values for the weights on each fine - tuning loss , we tried to present a reasonably comprehensive set of ablations to identify the effect of each part of our methodology .

We also agree that IBM 2 might not be the best possible choice of fertility inference model , since the model itself only considers fertility implicitly ( as part of the alignment process ) and not explicitly like IBM 3 + . Our decision to use IBM 2 was based on the availability , performance , and ease of integration of a popular existing implementation ( fast_align ) of that particular alignment model . Meanwhile , the use of fertility supervision in the first place can be justified from two perspectives : from the variational inference perspective , an external alignment model provides a very simple and tractable proposal distribution ; at a higher level , fertility supervision simply turns a difficult , unsupervised learning problem into an easier , supervised one .

Pros :
- new module
- good performances ( not state - of - the - art )
Cons :
- additional experiments

The paper is well motivated , and is purely experimental and proposes a new architecture . However , I believe that more experiments should be performed and the explanations could be more concise .

The section 3 is difficult to read because the notations of the different formula are a little bit heavy . They were nicely summarised on the Figure 1 : each of the formula ' block could be replaced by a figure , which would make this section faster to read and understand .

I would have enjoyed a parameter comparison in Table 3 as it is claimed this architecture has less parameters and additional experiments would be welcome . As it does not reach the state - of - the -art , " super separable convolutions " could be compared on other tasks ?

minor :
" In contrast , regular convolutional layers break
this creed by learning filters that must simultaneously perform the extraction of spatial features and
their merger into channel dimensions ; an inefficient and ineffective use of parameters . " - a verb is missing ?


I think there is a kind of consensus about the reviews of this paper . I would like to kindly encourage the authors to modify the sections 2&3 , in order to incorporate the changes we requested , and to give some additional numerical results with a little bit more comments . In this case and if the modifications are relevant , I would be happy to raise my rating .

We are very grateful for helping us improve the paper .

The reviewer wrote : " section 3 is difficult to read because the notations of the different formula are a little bit heavy . They were nicely summarised on the Figure 1 : each of the formula ' block could be replaced by a figure , which would make this section faster to read and understand . " We took this very seriously and we have re-arranged the whole presentation of equations in Section 3 . In the new revision , every set of equations comes together with the corresponding Figure , and the figures were slightly re-drawn to match the equations closer . We hope that this addressed the main concern about presentation .

As for Table 3 , we found it hard to get the parameter count for every one of the earlier models presented in the table . But we asked the authors of the other papers and we will try to add one more revision with parameter counts .

As for not reaching state - of - the -art , we believe that it is due to the lack of self - attention in the decoder . Please see the comment to Review 1 above where we discuss this .

This paper presents the SliceNet architecture , an sequence - to- sequence model based on super- dilated convolutions , which allow to reduce the computational cost of the model compared to standard convolution . The proposed model is then evaluated on machine translation and yields competitive performance compared to state - of - the- art approaches .

In terms of clarity , the paper is overall easy to follow , however I am a bit confused by Section 2 about what is related work and what is a novel contribution , although the section is called “ Our Contribution ” . For instance , it seems that the separable convolution presented in Section 2.1 were introduced by ( Chollet , 2016 ) and are not part of the contribution of this paper . The authors should thus clarify the contributions of the paper .

In terms of significance , the SliceNet architecture is interesting and is a solid contribution for reducing computation cost of sequence - to- sequence models . The experiments on NMT are convincing and gives interesting insights , although I would like to see some pointers about why in Table 3 the Transformer approach ( Vaswani et al. 2017 ) outperforms SliceNet .

I wonder if the proposed approach could be applied to other sequence - to- sequence tasks in NLP or even in speech recognition ?

Minor comment :
* The equations are not easy to follow , they should be numbered . The three equations just before Section 2.2 should also be adapted as they seem redundant with Table 1 .


We are very grateful for the review .

As for the suggestion to improve presentation and equations , we have uploaded a new revision with diagrams put together with equations in a new way ( inspired by another review ) . We hope this makes it easier to understand .

As for this point : " I would like to see some pointers about why in Table 3 the Transformer approach ( Vaswani et al. 2017 ) outperforms SliceNet . " -- let us explain how Transformer has crucial architectural parts missing from SliceNet . One key part of Transformer is self - attention in the decoder : an attention layer that allows the decoder to attend to previously generated ( decoded ) words . This is a main innovation of the Transformer architecture and it is missing from SliceNet ( as we started working on SliceNet before the Transformer paper ) . We only have the encoder - decoder attention known from previous sequence - to-sequence models ( i.e. , the decoder can attend to the encoder , but not to previously decoded words ) . We believe that this difference is responsible for the difference in results : as far as we know , no architecture without self - attention in the decoder has shown better results than SliceNet . It should also be possible to combine the decoder self - attention with SliceNet -- some results ( with non-separable convolutions ) are already coming up as SNAIL for image generation ( https://arxiv.org/abs/1712.09763). We believe that the techniques we present in this paper can be used to extend SNAIL to get additional improvements , and it should also work for tasks like image generation , parsing , summarization and others .

A DeepRL algorithm is presented that represents distributions over Q values , as applied to DDPG ,
and in conjunction with distributed evaluation across multiple actors , prioritized experience replay , and
N-step look -aheads . The algorithm is called Distributed Distributional Deep Deterministic Policy Gradient algorithm , D4PG .
SOTA results are generated for a number of challenging continuous domain learning problems ,
as compared to benchmarks that include DDPG and PPO , in terms of wall - clock time , and also ( most often ) in terms
of sample efficiency .

pros/cons
+ the paper provides a thorough investigation of the distributional approach , as applied to difficult continuous
action problems , and in conjunction with a set of other improvements ( with ablation tests )
- the story is a bit mixed in terms of the benefits , as compared to the non-distributional approach , D3PG
- it is not clear which of the baselines are covered in detail in the cited paper :
" Anonymous . Distributed prioritized experience replay . In submission , 2017 . " ,
i.e. , should readers assume that D3 PG already exists and is attributable to this other submission ?

Overall , I believe that the community will find this to be interesting work .

Is a video of the results available ?

It seems that the distributional model often does not make much of a difference ,
as compared to D3 PG non-prioritized . However , sometimes it does make a big difference , i.e. , 3D parkour ; acrobot .
Do the examples where it yields the largest payoff share a particular characteristic ?

The benefit of the distributional models is quite different between the 1 - step and 5 - step versions . Any ideas why ?

Occasionally , D4PG with N=1 fails very badly , e.g. , fish , manipulator ( bring ball ) , swimmer .
Why would that be ? Should n't it do at least as well as D3 PG in general ?

How many atoms are used for the categorical representation ?
As many as [ Bellemare et al . ] , i.e. , 51 ?
How much " resolution " is necessary here in order to gain most of the benefits of the distributional representation ?

As far as I understand , V_min and V_max are not the global values , but are specific to the current distribution .
Hence the need for the projection . Is that correct ?

Would increasing the exploration noise result in a larger benefit for the distributional approach ?

Figure 2 : DDPG performs suprisingly poorly in most examples . Any comments on this ,
or is DDPG best avoided in normal circumstances for continuous problems ? :-)

Is the humanoid stand so easy because of large ( or unlimited ) torque limits ?

The wall - clock times are for a cluster with K=32 cores for Figure 1 ?

" we utilize a network architecture as specified in Figure 1 which processes the terrain info in order to reduce its dimensionality "
Figure 1 provides no information about the reduced dimensionality of the terrain representation , unless I am somehow failing to see this .

" the full critic architecture is completed by attaching a critic head as defined in Section A "
I could find no further documenation in the paper with regard to the " head " or a separate critic for the " head " .
It is not clear to me why multiple critics are needed .

Do you have an intuition as to why prioritized replay might be reducing performance in many cases ?


Thank you !

As to the baselines , we use the same framework for distributing computation and prioritization as in the cited paper ( Anonymous . Distributed prioritized experience replay , also submitted to ICLR ) . However this other work focuses primarily on discrete-action tasks .

Videos of the parkour performance can be found at https://www.youtube.com/playlist?list=PLFU7BiIwAjPDqsIL9OLm1z7_RXZA1Jyfj.

We have also found that the distributional model helps most in harder , higher - dimensional tasks . The main characteristic these tasks seem to share is the time / data required with which the solve the task . Potentially due to the complexity of learning the Q-function .

We found in general , for both distributional and non-distributional , that the 5 - step version provided better results . Although this is not fully corrected for ( see answers to the above reviewers ) we found this to experimentally provide quite a bit of benefit to all variations of the algorithm .

We used 51 atoms across all tasks except for the humanoid parkour task which used 101 . The level of resolution necessary will depend on the problem under consideration , and controlled by the combination of the number of atoms as well as the V_{min , max} values , however we found this to be relatively robust . Here we changed the number of atoms for the humanoid task in order to keep the resolution roughly consistent with other tasks .

The V_min and V_max values are global values that bound the support of the distribution . However , you are correct that this is what requires the projection . When applying the Bellman operator to a distribution it will more than likely lie outside the bounds given by V_min / V_max , so we project in order to ensure that our distributions are always within these bounds . Again , we also found these values to be relatively robust and we generally set these given knowledge of the maximum immediate reward of the system .

We did not extensively experiment with increasing the exploration noise , but from preliminary experiments we saw that the algorithm was fairly robust to this value . Deviating from the values we used did not significantly hurt nor hinder the algorithm ’s performance .

The poor performance of DDPG in these experiments is primarily due to the fact that DDPG is quite slow to learn . For the easier control suite tasks DDPG is actually a feasible algorithm if given enough time . However for the harder tasks ( any of the humanoid tasks , manipulator , and parkour tasks ) DDPG would take much too long to work effectively . Finally , one of the bigger problems DDPG has is that it can exhibit quite unstable learning which is not exhibited by D4PG .

The easy - ness of the humanoid stand task is more due to the fact that it has less complicated motions to make than any of the other humanoid tasks .

The wall - clock times are for 32 cores on separate machines . We found communication across machines to be fast enough that having them all be on the same machine was not a requirement .

We apologize that the description of the network architecture was poorly explained and will correct it . The networks have two branches , one of which process the the terrain info to produce a lower- dimensional hidden state before combining it with the proprioceptive information . Utilizing this second branch to process the proprioceptive information and reduce it to a smaller number of hidden units is what we refer to as “ reducing its dimensionality ” however we will explain this better .

We will also explain critic architecture and what we refer to as “ heads ” further . Here we refer to the “ distributional output ” component of the network as a head . In this way we can replace the Categorical output with a Mixture of Gaussians output as described in section A . By “ head ” we only mean this final component which takes the last set of hidden units , passes them through a linear layer , and outputs the parameters of a distribution .


The paper investigates a number of additions to DDPG algorithm and their effect on performance . The additions investigated are distributional Bellman updates , N-step returns , and prioritized experience replay .

The paper does a good job of analyzing these effects on a wide range of continuous control tasks , from the standard benchmark suite , to hand manipulation , to complex terrain locomotion and I believe these results are valuable to the community .

However , I have a concern about the soundness of using N-step returns in DDPG setting . When a sequence of length N is sampled from the replay buffer and used to calculate N-step return , this sequence is generated according a particular policy . As a result , experience is non-stationary - for the same state-action pair , early iterations of the algorithm will produce structurally different ( not just due to stochasticity ) N-step returns because the policy to generate those N steps has changed between algorithm iterations . So it seems to me the authors are using off - policy updates where strictly on - policy updates should be used . I would like some clarification from the authors on this point , and if it is indeed the case to bring attention to this point in the final manuscript .

It would also be useful to evaluate the effect of N for values other than 1 and 5 , especially given the significance this addition has on performance . I can believe N-step returns are useful , possibly due to effectively enlarging simulation timestep , but it would be good to know at which point it becomes detrimental .

I also believe " Distributional Policy Gradients " is an overly broad title for this submission as this work still relies on off - policy updates and does not tackle the problem of marrying distributional updates with on-policy methods . " Distributional DDPG " or " Distributional Actor -Critic " or variant perhaps could be more fair title choices ?

Aside from these concerns , lack of originality of contributions makes it difficult to highly recommend the paper . Nonetheless , I do believe the experimental evaluation if well - conducted and would be of interest to the ICLR community .

Thanks for the helpful review !

The reason for our use of N-step returns is it allows us to compute the returns as soon as they are collected and insert into replay without storing full sequences . This is done for efficiency reasons . For N>1 this ignores the difference between the behavior and target policies . This could be corrected using an off-policy correction such as Retrace ( Safe and Efficient Off -Policy Reinforcement Learning , Munos et al. , 2016 ) but that would require storing full trajectories .

However , for reasonably small N this difference is not great , which is what we show in our experiments . With N much larger than the value of 5 , we see a degradation in performance for exactly this reason . We include further discussion of exactly this point .


Comment : The paper proposes a simple extension to DDPG that uses a distributional Bellman operator for critic updates , and introduces two simple modifications which are the use of N-step returns and parallelizing evaluations . The method is evaluated on a wide variety of many control and robotic talks .

In general , the paper is well written and organised . However I have some following major concerns regarding the quality of the paper :

- The proposal , D4 PG , is quite straightforward which is simply use the idea of distributional value function by Bellemare et al. ( previously used in DQN ) . Two modifications are also simple and well - known techniques . It would be nicer if the description in Section 3 is less straightforward by giving more justifications and analysis why and how distributional updates are necessary in the context of policy search methods like DDPG .

- A positive side of the paper is a large set of evaluations on many different control and robotic tasks . For many tasks , D4 PG performs better than the variant that does not use distributional updates ( D3PG ) , however by not much . There are some tasks showing no-difference . On the other hand , the choice of N=5 in comparisons is hard to understand and lacks further experimental justifications . Different setting and new performance metrics ( e.g. data efficiency , number of episodes in total ) might also reveal more properties of the proposed methods .



* Other minor comments :

- Algorithm 1 consists of two parts but there are connection between them . It might be confused for ones who are not familiar with the actor-critic framework .

- It would be nicer if all expectation operators in Section 3 comes with corresponding distributions .

- page 2 , second paragraph : typos in " hence my require less samples to learn "

- it might be better if the reference on arXiv should be changed to relevant publication conferences with archival proceeding : work by Marc G. Bellemare at ICML 2017

Thank you for the review !

As to the necessity of the distributional updates , the DPG algorithm relies heavily on the accuracy of the value function estimate due to the fact that the gradient computed under the DPG theorem is based only on gradients of the policy pi and gradients of the Q-function . By better estimating the Q-function we directly impact the accuracy of the policy gradient . We will include further discussion of this .

It is true that the distributional version ( D4 PG ) does not always out - perform the non-distributional version ( D3PG ) . However this is typically on easier tasks . In the control suite of tasks the distributional version significantly out - performs on the acrobot , humanoid , and swimmer set of tasks . For manipulation tasks this holds for the hardest pickup and orient task . And finally for all parkour tasks . So for tasks that are already somewhat easy to solve there are limited immediate gains , but for harder tasks this update tends to help ( and help significantly for the parkour tasks ) .

The choice of a higher N is suggested by algorithms such as A3C and Rainbow , among others . Note that the Rainbow algorithm ( Rainbow : Combining Improvements in Deep Reinforcement Learning , Hessel et al , 2017 ) utilizes an off-policy Q-learning update with uncorrected n-step returns , in a very similar way to that used by D4PG . In order to fully correct for this we should be using an off-policy correction , which we have not used for reasons of efficiency ( see our response to the next reviewer ) . However , experimentally we have shown that this minor modification helps quite significantly and can be used directly in any off- policy algorithm . In all of our experiments and across both distributional and non-distributional updates it tends to be better to use the higher N . We did find that increasing N much higher than N>5 tended to degrade performance , which makes sense as this would be more off-policy . We will include further discussion of this aspect of the algorithm .


This paper extends and speeds up PROSE , a programming by example system , by posing the selection of the next production rule in the grammar as a supervised learning problem .

This paper requires a large amount of background knowledge as it depends on understanding program synthesis as it is done in the programming languages community . Moreover the work mentions a neurally - guided search , but little time is spent on that portion of their contribution . I am not even clear how their system is trained .

The experimental results do show the programs can be faster but only if the user is willing to suffer a loss in accuracy . It is difficult to conclude overall if the technique helps in synthesis .

> Q : Please clarify how the system is trained .

1 ) We use the industrially collected set of 375 string transformation tasks . Each task is a single input-output examples and 2 - 10 unseen inputs for evaluating generalization . Further , we split the 375 tasks into 65 % train , 15 % validation , and 20 % test ones .
2 ) We run PROSE on each of those tasks and collect the ( symbol , production , spec input , spec output -> best program score after learning ) information on all nodes of the search tree . As mentioned in the introduction , such traces provide a rich description of the synthesis problem thanks to the Markovian nature of deductive search in PROSE and enabling the creation of large datasets required for learning deep models . As a result , we obtain a dataset of ~ 450,000 search outcomes from mere 375 tasks .
3 ) We further split all the search outcomes by the used symbol or its depth in the grammar . In our final evaluation , we present the results for the models trained on the decisions on the `transform` ( depth=1 ) , `pp` , `pos` symbols . We have also trained other symbol models as well as a single common model for all symbols / depths , but they did n’t perform as well .
4 ) We employ Adam ( Kingma and Ba , 2014 ) to optimize the objective . We use a batch size of 32 and a learning rate of 0.01 and use early stopping to pick the final model . The model architecture and the corresponding loss function ( squared error ) are discussed in Section 3.1 . We will add the specific training details in the next revision of the paper .
5 ) As discussed in Section 3.3 , the learned models are integrated in the corresponding PROSE controller when the current search tree node matches the model 's conditions ( i.e. it is on the same respective symbol or depth ) .

> Q : Is the approach useful for synthesis when there is a loss in program accuracy ?

In fact , NGDS achieves higher average test accuracy than baseline PROSE ( 68.49 % vs. 67.12 % ) , although with slightly lower validation accuracy ( 63.83 % vs. 70.21 % ) which effectively corresponds to 4 tasks .

However , this is not the most important factor : PBE is bound to often fail in synthesizing the _intended_ program from a single input-output example . Even a machine - learned ranking function picks the wrong program 20 % of the time ( Ellis & Gulwani , IJCAI 2017 ) .

Thus , the main goal of this work is speeding up the synthesis process on difficult scenarios without sacrificing the generalization accuracy too much . As a result , we achieve on average 50 % faster synthesis time , with 10 x speed - ups for many difficult tasks that require multiple seconds while still retaining competitive accuracy . Appendix C shows the breakdown of time and accuracy : out of 120 validation / test tasks , there are :
- 76 tasks where both systems are correct ,
- 7 tasks where PROSE learns a correct program and NGDS learns a wrong one ,
- 4 tasks where PROSE learns a wrong program and NGDS learns a correct one ,
- 33 tasks where both systems are wrong .

The paper presents a branch - and -bound approach to learn good programs
( consistent with data , expected to generalise well ) , where an LSTM is
used to predict which branches in the search tree should lead to good
programs ( at the leaves of the search tree ) . The LSTM learns from
inputs of program spec + candidate branch ( given by a grammar
production rule ) and ouputs of quality scores for programms . The issue
of how greedy to be in this search is addressed .

In the authors ' set up we simply assume we are given a ' ranking
function ' h as an input ( which we treat as black - box ) . In practice
this will simply be a guess ( perhaps a good educated one ) on which
programs will perform correctly on future data . As the authors
indicate , a more ambitious paper would consider learning h , rather
than assuming it as a given .

The paper has a number of positive features . It is clearly written
( without typo or grammatical problems ) . The empirical evaluation
against PROSE is properly done and shows the presented method working
as hoped . This was a competent approach to an interesting ( real )
problem . However , the ' deep learning ' aspect of the paper is not
prominent : an LSTM is used as a plug - in and that is about it . Also ,
although the search method chosen was reasonable , the only real
innovation here is to use the LSTM to learn a search heuristic .


The authors do not explain what " without attention " means .


I think the authors should mention the existence of ( logic ) program
synthesis using inductive logic programming . There are also ( closely
related ) methods developed by the LOPSTR ( logic -based program
synthesis and transformation ) community . Many of the ideas here are
reminiscent of methods existing in those communities ( e.g. top - down search
with heuristics ) . The use of a grammar to define the space of programs
is similar to the " DLAB " formalism developed by researchers at KU
Leuven .

ADDED AFTER REVISIONS / DISCUSSIONS

The revised paper has a number of improvements which had led me to give it slightly higher rating .



Thank you for the related work suggestions -- we will update this discussion in the next draft . We address your concerns below :

> Q : Limited innovation in terms of deep learning :

Rather than being a pure contribution to deep learning , this work applies deep learning to the important field of program synthesis , where statistical approaches are still underexplored . Our main contribution is a hybrid approach to program synthesis that utilizes the best of both neural and symbolic synthesis techniques . Combining insights from both worlds in this way achieves a new milestone in program synthesis performance : from a single example it generates programs that generalize better than prior state - of - the-art ( including neural RobustFill , symbolic PROSE , and hybrid DeepCoder ) , the generated program is provably correct , and the generation is 50 % faster on average

DeepCoder ( Balog et al. , ICLR 2017 ) first explored a hybrid approach last year by first predicting the likelihood of various operators and then using it to guide an external symbolic synthesis engine . Since deep networks are data-hungry , Balog et al. obtain training data by randomly sampling programs from the DSL and generating satisfying random strings as input-output examples . As noted in Section 1 and as evidenced by its inferior performance against our method , the generated programs tend to be unnatural leading to poor generalization . In contrast , NGDS closely integrates neural models at each step of the synthesis and so , it is possible to obtain large amounts of training data while utilizing a relatively small number of real - world examples .

> Q : Learning the ranking function instead of taking it as a given :

While related , this problem is orthogonal to our work : a ranking function evaluates whether a given full program generalizes well , whereas we aim to predict the generalization of the best program produced from a given partial search state .

Importantly , the proposed technique , NGDS is independent of the ranking function and can be trivially integrated with any high - quality ranking function . For instance , the manually written ranking function of FlashFill in PROSE that we use is a result of 7 years of engineering and heavy fine- tuning for industrial applications . An even better-quality learned ranking function would only improve the accuracy of predictions , which are already on par with baseline PROSE ( 68.49 % vs 67.12 % ) .

In fact , a lot of recent prior work focuses on learning a ranking function for program induction , see ( Singh & Gulwani , CAV 2015 ) and ( Ellis & Gulwani , IJCAI 2017 ) . For comparison , we are currently performing a set of experiments with an ML -learned ranking function ; we 'll update with the new results once it 's done .

> Q : What does " without attention " mean ?

All the models we explore encode input and output examples using ( possibly multi-layered , bi-directional ) LSTMs with or without an attention mechanism ( Bahdanau et al. , ICLR 2015 ) . As mentioned in Section 8 , the most accurate predictions arise when we attend to the input string while encoding the output string similar to the attention - based models proposed by Devlin et al. , 2017 . We will make this clearer in the next version of the paper .

Such an attention mechanism allows the network to learn complex features like " whether the output is a substring of the input " . Unfortunately , such accuracy comes at a cost of increasing the network evaluation time to quadratic instead of linear . As a result , prediction time at every node of the search tree dominates the search time , and NGDS is slower than PROSE even when its predictions are accurate . Therefore , we only use LSTM models without any attention mechanism in our evaluations .


This is a strong paper . It focuses on an important problem ( speeding up program synthesis ) , it ’s generally very well - written , and it features thorough evaluation . The results are impressive : the proposed system synthesizes programs from a single example that generalize better than prior state - of - the -art , and it does so ~ 50 % faster on average .

In Appendix C , for over half of the tasks , NGDS is slower than PROSE ( by up to a factor of 20 , in the worst case ) . What types of tasks are these ? In the results , you highlight a couple of specific cases where NGDS is significantly * faster * than PROSE — I would like to see some analysis of the cases were it is slower , as well . I do recognize that in all of these cases , PROSE is already quite fast ( less than 1 second , often much less ) so these large relative slowdowns likely do n’t lead to a noticeable absolute difference in speed . Still , it would be nice to know what is going on here .

Overall , this is a strong paper , and I would advocate for accepting it .


A few more specific comments :


Page 2 , “ Neural - Guided Deductive Search ” paragraph : use of the word “ imbibes ” - while technically accurate , this use does n’t reflect the most common usage of the word ( “ to drink ” ) . I found it very jarring .

The paper is very well - written overall , but I found the introduction to be unsatisfyingly vague — it was hard for me to evaluate your “ key observations ” when I could n’t quite yet tell what the system you ’re proposing actually does . The paragraph about “ key observation III ” finally reveals some of these details — I would suggest moving this much earlier in the introduction .

Page 4 , “ Appendix A shows the resulting search DAG ” - As this is a figure accompanying a specific illustrative example , it belongs in this section , rather than forcing the reader to hunt for it in the Appendix .



Thank you for the constructive feedback ! We ’ll add more details and clarify the introduction in the next revision .

Q : Which factors lead to NGDS being slower than PROSE on some tasks ?
Our method is slower than PROSE when the predictions do not satisfy the requirements of the controller i.e. all the predicted scores are within the threshold or they violate the actual scores in branch and bound exploration . This leads to NGDS evaluating the LSTM for branches that were previously pruned . This can be especially harmful when branches that got pruned out at the very beginning of the search need to be reconsidered -- as it could lead to evaluating the network many times . While evaluating the network leads to minor additions in run-time , there are many such additions , and since PROSE performance is already << 1s for such cases , this results in considerable relative slowdown .

Why do the predictions violate the controller 's requirements ? This happens when the neural network is either indecisive ( its predicted scores for all branches are too close ) or wrong ( its predicted scores have exactly the opposite order of the actual program scores ) .
We will update the draft with this discussion and present some examples below

Some examples :
A ) " 41.711483001709,-91.4123382568359,41.6076278686523,-91.6373901367188 " ==> " 41.711483001709 "
The intended program is a simple substring extraction . However , at depth 1 , the predicted score of Concat is much higher than the predicted score of Atom , and thus we end up exploring only the Concat branch . The found Concat program is incorrect because it uses absolute position indexes and does not generalize to other similar extraction tasks with different floating - point values in the input strings .
We found this scenario relatively common when the output string contains punctuation - the model considers it a strong signal for Concat .
B ) " type size = 36 : Bartok.Analysis.CallGraphNode type size = 32 : Bartok.Analysis.CallGraphNode CallGraphNode " == > " 36->32 "
We correctly explore only the Concat branch , but the slowdown happens at the level of the `pos` symbol . There are many different logics to extract the “ 36 ” and “ 32 ” substrings . NGDS explores RelativePosition branch first , but the score of the resulting program is less then the prediction for RegexPositionRelative . Thus , the B&B controller explores both branches anyway and we end up with a relative slowdown caused by the network inference time .

In this paper , ( previous states , action ) pairs and task ids are embedded into the same latent space with the goal of generalizing and sharing across skill variations . Once the embedding space is learned , policies can be modified by passing in sampled or learned embeddings .

Novelty and Significance : To my knowledge , using a variational approach to embedding robot skills is novel . Significantly , the embedding is learned from off - policy trajectories , indicating feasibility on a real - world setting . The manipulation experiments show nice results on non-trivial tasks . However , no comparisons are shown against prior work in multitask or transfer learning . Additionally , the tasks used to train the embedding space were tailored exactly to the target task , making it unclear that this method will work generally .

Questions :
- I am not sure how to interpret Figure 3 . Do you use Bernoulli in the experiments ?
- How many task IDs are used for each experiment ? 2 ?
- Are the manipulation experiments learned with the off-policy variant ?
- Figure 4b needs the colors to be labeled . Video clips of the samples would be a plus .
- ( Major ) For the experiments , only exactly the useful set of tasks is used to train the embedding . What happens if a single latent space is learned from all the tasks , and Spring - wall , L-wall , and Rail - push are each learned from the same embedding .

I find the method to be theoretically interesting and valuable to the learning community . However , the experiments are not entirely convincing .


I would really like to see an experiment where an embedding space is trained on a wider variety of tasks rather than just what is needed to generalize to the target task . However , I find that this paper is a valuable contribution to ICLR , and I think that it should be accepted .

As ICLR allows the authors to upload a new pdf , I do not understand why the author response only said they would make changes in the final version ( especially for things like labeling a figure ) .

We are grateful for the insightful comments and suggestions .

Please find the answers to the inline questions below , we will clarify all of these points in the final version of the paper .
- I am not sure how to interpret Figure 3 . Do you use Bernoulli in the experiments ?
- A Bernoulli distribution is only used for for Figure 3 to demonstrate that our method can work with other distributions .

- How many task IDs are used for each experiment ? 2 ?
- Yes , T was set to 2 for the manipulation experiments .

- Are the manipulation experiments learned with the off-policy variant ?
- That is correct . All experiments were performed in an off-policy setting . This decision was made due to the higher sample - efficiency of the off-policy methods .

- Figure 4b needs the colors to be labeled . Video clips of the samples would be a plus .
- We will add the labels and address this problem in the final version of the paper

Regarding the last question on training the embedding space on all of the tasks ; we are currently working on this experiment and are planning to include it in the final version of the paper . It is worth noting that the multi-task RL training can be challenging ( especially with poorly scaled rewards ) and it maintains as an open problem that is beyond the scope of this work . Our method presents a solution to a problem of finding an embedding space that enables re-using , interpolating and sequencing previously learned skills , with the assumption that the RL agent was able to learn them in the first place . However , we strongly believe that the off-policy setup presented in this work has much more flexibility that its on - policy equivalents as to how to address the multi-task RL problem .


The submission tackles an important problem of learning and transferring multiple motor skills . The approach relies on using an embedding space defined by latent variables and entropy - regularized policy gradient / variational inference formulation that encourages diversity and identifiability in latent space .

The exposition is clear and the method is well - motivated . I see no issues with the mathematical correctness of the claims made in the paper . The experimental results are both instructive of how the algorithm operates ( in the particle example ) , and contain impressive robotic results . I appreciated the experiments that investigated cases where true number of tasks and the parameter T differ , showing that the approach is robust to choice of T.

The submission focuses particularly on discrete tasks and learning to sequence discrete tasks ( as training requires a one - hot task ID input ) . I would like a bit of discussion on whether parameterized skills ( that have continuous space of target location , or environment parameters , for example ) can be supported in the current formulation , and what would be necessary if not .

Overall , I believe this is in interesting piece of work at a fruitful intersection of reinforcement learning and variational inference , and I believe would be of interest to ICLR community .

We thank the reviewer for their comments and suggestions .

Our method does indeed support parameterized skills as suggested by the reviewer . For instance , the low - level policy could receive an embedding conditioned on a continuous target location instead of the task ID ( given a suitable embedding space ) . It is also not limited to the multi-task setting , i.e. , the number of tasks T used for training can be set to 1 ( as explored in the point- mass experiments ) . We will add this to the discussion to the paper .

The paper presents a new approach for hierarchical reinforcement learning which aims at learning a versatile set of skills . The paper uses a variational bound for entropy regularized RL to learn a versatile latent space which represents the skill to execute . The variational bound is used to diversify the learned skills as well as to make the skills identifyable from their state trajectories . The algorithm is tested on a simple point mass task and on simulated robot manipulation tasks .

This is a very intersting paper which is also very well written . I like the presented approach of learning the skill embeddings using the variational lower bound . It represents one of the most principled approches for hierarchical RL.

Pros :
- Interesting new approach for hiearchical reinforcement learning that focuses on skill versatility
- The variational lower bound is one of the most principled formulations for hierarchical RL that I have seen so far
- The results are convincing

Cons :
- More comparisons against other DRL algorithms such as TRPO and PPO would be useful

Summary : This is an interesting deep reinforcement learning paper that introduces a new principled framework for learning versatile skills . This is a good paper .

More comments :
- There are several papers that focus on learning versatile skills in the context of movement primitive libraries , see [ 1 ] , [ 2 ] , [ 3 ] . These papers should be discussed .

[ 1 ] Daniel , C. ; Neumann , G. ; Kroemer , O. ; Peters , J. ( 2016 ) . Hierarchical Relative Entropy Policy Search , Journal of Machine Learning Research ( JMLR ) ,
[ 2 ] End , F. ; Akrour , R. ; Peters , J. ; Neumann , G. ( 2017 ) . Layered Direct Policy Search for Learning Hierarchical Skills , Proceedings of the International Conference on Robotics and Automation ( ICRA ) .
[ 3 ] Gabriel , A. ; Akrour , R. ; Peters , J. ; Neumann , G. ( 2017 ) . Empowered Skills , Proceedings of the International Conference on Robotics and Automation ( ICRA ) .


We very much appreciate the reviewer ’s comments and suggestions .

Regarding the comparison to other on - policy methods such as TRPO or PPO , we would like to emphasize that the presented approach is mostly independent of the underlying RL learning algorithm . In fact , it will be easier to implement our approach in the on-policy setup . The off - policy setup with experience replay that we are considering requires additional care due to the embedding variable which we also maintain in the replay buffer . In Section 5 , we present all the modifications necessary to running our method in the more data-efficient off - policy setup , which we believe is crucial to running it on the real robots in the future .

We would also like to thank the reviewer for pointing out the additional references - we will be very happy to include them . While some of the high - level ideas are related , there are differences both in the formulation and the algorithmic framework . An important aspect of our work is that we show how to apply entropy - regularized RL with latent variables when working with neural networks and in an off-policy setting , avoiding both the burden of using a limited number of hand - crafted features and allowing for data-efficient learning .


The authors present a scalable model for questioning answering that is able to train on long documents . On the TriviaQA dataset , the proposed model achieves state of the art results on both domains ( wikipedia and web ) . The formulation of the model is straight - forward , however I am skeptical about whether the results prove the premise of the paper ( e.g. multi-mention reasoning is necessary ) . Furthermore , I am slightly unconvinced about the authors ' claim of efficiency . Nevertheless , I think this work is important given its performance on the task .

1 . Why is this model successful ? Multi-mention reasoning or more document context ?
I am not convinced of the necessity of multi-mention reasoning , which the authors use as motivation , as shown in the examples in the paper . For example , in Figure 1 , the answer is solely obtained using the second last passage . The other mentions provide signal , but does not provide conclusive evidence . Perhaps I am mistaken , but it seems to me that the proposed model can not seem to handle negation , can the authors confirm / deny this ? I am also skeptical about the computation efficiency of a model that scores all spans in a document ( which is O ( N^2 ) , where N is the document length ) . Can you show some analysis of your model results that confirm / deny this hypothesis ?

2 . Why is the computational complexity not a function of the number of spans ?
It seems like the derivations presents several equations that score a given span . Perhaps I am mistaken , but there seems to be n^2 spans in the document that one has to score . Should n't the computational complexity then be at least O ( n^2 ) , which makes it actually much slower than , say , SQuAD models that do greedy decoding O ( 2n + nm ) ?

Some minor notes
- 3.3.1 seems like an attention computation in which the attention context over the question and span is computed using the question . Explicitly mentioning this may help the reading grasp the formulation .
- Same for 3.4 , which seems like the biattention ( Seo 2017 ) or coattention ( Xiong 2017 ) from previous squad work .
- The sentence " We define ... to be the embeddings of the l words of the sentence that contains s. " is not very clear . Do you mean that the sentence contains l words ? It could be interpreted that the span has l words .
- There is a typo in your 3.7 " level 1 complexity " : there is an extra O inside the big O notation .

Thank you for your comments ! We will revise the paper based on your feedback but we would like to clarify some aspects beforehand :

Success of the model :
Our model benefits from both multi-mention reasoning and more document context . The ablation in Table 2 shows that without the Level 3 multi-mention aggregation , model performance drops from 52.18 % to 46.52 % . The only purpose of the level 3 model is to do aggregation across multiple mentions , and therefore this shows that multi-mention reasoning helps our model significantly . More document context allows the upper bound of the dev EM under our approach to be 92 % , compared to the 83 % in the baseline method from Joshi et. al. ( 2017 ) . We can make this more clear in our revision .

Computational efficiency :
We only allow spans up to a length l ( where l=5 ) . Therefore our computational complexity is O ( nl ) and not O ( n^2 ) . Moreover , our method trivially parallelizes across the length of the document , unlike recurrent network based approaches .

Negation :
Our model does not handle negation specifically but negation is not typically a key aspect of existing reading comprehension tasks ( as it is in sentiment analysis for instance ) .

Attention :
We will clarify the attention section and cite ( Seo et al. 2017 , Xiong et al. 2017 ) . Unlike their approach , we use the word embeddings as input to the attention , not the LSTM states as they do .


This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches . The model also does not consider the first mention of the answer span as gold , instead formulating its loss function to incorporate multiple mentions of the answer within the evidence . The reported results were state - of - the-art ( * ) on the TriviaQA dataset at the time of the submission deadline . It 's interesting that such a simple model , relying mainly on ( weighted ) word embedding averages , can outperform more complex architectures ; however , these improvements are likely due to decreased truncation as opposed to bag-of - words architectures being superior to RNNs .

Overall , I found the paper interesting to read , and scaling QA up to larger documents is definitely an important research direction . On the other hand , I 'm not quite convinced by its experimental results ( more below ) and the paper is lacking an analysis of what the different sub-models are learning . As such , I am borderline on its acceptance .

* The TriviaQA leaderboard shows a submission from 9/24/17 ( by " chrisc " ) that has significantly higher EM / F1 scores than the proposed model . Why is this result not compared to in Table 1 ?

Detailed comments :
- Did you consider pruning spans as in the end - to - end coreference paper of Lee et al. , EMNLP 2017 ? This may allow you to avoid truncation altogether . Perhaps this pruning could occur at level 1 , making subsequent levels would be much more efficient .
- How long do you estimate training would take if instead of bag-of - words , level 1 used a biLSTM encoder for spans / questions ?
- What is the average number of sentences per document ? It 's hard to get an idea of how reasonable the chosen truncation thresholds are without this .
- In Figure 3 , it looks like the exact match score is still increasing as the maximum tokens in document is increased . Did the authors try truncating after more words ( e.g. , 10 k ) ?
- I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1 , for example , to give some intuition as to how each level works .
- " Krasner " misspelled multiple times as " Kramer "

Thank you for your comments ! We will revise our paper based on your feedback , particularly discussing more the contribution of each submodel and addressing your detailed comments . However , we would like to clarify some main points below :

The “ chrisc ” leaderboard submission :
TriviaQA allows someone to submit privately and then make their result public later . Therefore while the web result for the chrisc model might have been submitted earlier , it was not publicly visible before the ICLR Oct 27 deadline and therefore was not included in our table . None of the other ICLR submissions we are aware of report this result either e.g. ( https://openreview.net/forum?id=rJl3yM-Ab, https://openreview.net/forum?id=HJRV1ZZAW, https://openreview.net/pdf?id=B1twdMCab ) The paper itself was posted on arXiv on 29th Oct ( https://arxiv.org/abs/1710.10723) and only contained Web ( and not Wikipedia ) results . We would also like to point out that the chrisc model involves a two -stage pipeline , many layers of recurrent neural nets and is procedurally more involved than ours .

Pruning spans :
We did try pruning spans , based on the levels , i.e. level 1 considered all spans in the document ( up to truncation ) and level 2 considered the top K spans from level 1 , and so on . However , we found this decreased the accuracy by ~ 4 - 5 points . This could be attributed to the lower levels pruning away good candidates , because they did not have access to more information , such as sentence context , and attention with the question . We will revise the paper to include these results .

Using biLSTMs :
Running a biLSTM over the entire document of length n to obtain span representations is not parallelizable over the document length and therefore would be much slower ( unlike our approach which trivially parallelizes the attention computation over the O ( nl ) spans ) .

Truncation stats :
Truncating documents to contain at most 6000 tokens gives us an upper bound of 92 % on dev EM in the Wikipedia domain ( avg number of sentences being 141 ) . 87 % of documents in the Wikipedia dataset are fully covered under this truncation limit .

We do not make any claims about the expressive power of BOW models vs RNNs . Our model performance can be attributed to the scalability of BOW architectures which can take advantage of longer documents , which RNN architectures are not well suited for . Furthermore , our other contributions ( multi-loss + multi-mention learning ) significantly boost the performance of these simple architectures as explored in the ablations in Table 2 .


This paper proposes a lightweight neural network architecture for reading comprehension , which 1 ) only consists of feed - forward nets ; 2 ) aggregates information from different occurrences of candidate answers , and demonstrates good performance on TriviaQA ( where documents are generally pretty long ) .

Overall , I think it is a nice demonstration that non-recurrent models can work so well , but I also do n’t find the results strikingly surprising . It is also a bit hard to get the main takeaway messages . It seems that multi-loss is important ( highlight that ! ) , summing up multiple mentions of the same candidate answers seems to be important ( This paper should be cited : Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547). But all the other components seem to have been demonstrated previously in other papers .

An important feature of this model is it is easier to parallelize and speed up the training / testing processes . However , I do n’t see any demonstration of this in the experiments section .

Also , I am a bit disappointed by how “ cascades ” are actually implemented . I was expecting some sophisticated ways of combining information in a cascaded way ( finding the most relevant piece of information , and then based on what it is obtained so far trying to find the next piece of relevant information and so on ) . The proposed model just simply sums up all the occurrences of candidate answers throughout the full document . 3 - layer cascade is really just more like stacking several layers where each layer captures information of different granularity .

I am wondering if the authors can also add results on other RC datasets ( e.g. , SQuAD ) and see if the model can generalize or not .


Thank you very much for your comments , we will revise the paper over the next few weeks based on your feedback . Below , we make some clarifications .

Primary contributions of our work :
Our work presents a novel and more scalable approach to question answering that is considerably different than the existing literature that is dominated by monolithic LSTM - based architectures . The takeaway messages regarding our approach are :

1 . Multi-loss formulation , which to our knowledge has not been used in question answering , before . Empirically , this factors to a 10pt difference in the dev EM , as demonstrated in Table 2 .
2 . Aggregating multiple mentions of candidates at the representation level . We found this strategy to allow us to obtain high accuracy with simpler models when the multiple- mention assumption holds . Empirically , removing the aggregation level drops accuracy by 5.5 points in dev EM as shown in Table 2 .
3 . Unlike existing approaches , our model is trivially parallelizable in that we can process all the O ( nl ) spans in the document in parallel , allowing it to handle larger documents .

We believe that since our approach can scale to much longer document spans with only 1 GPU attests to its scalability . We also provide asymptotic analysis of our runtime complexity .

Cascades :
We would like to point out in contrast to previous approaches that aggregate answers ( e.g. Attention Sum Reader Network , which we will add a reference for ) , our method aggregates positions at the * representation * level ( by adding vector representations of mentions ) , not at the score level . While we agree that there could be more complex ways of realizing cascades , we chose the simplest approach that would show the efficacy of such an idea . More sophisticated ways to combine information may not be compatible with the trivial parallelizability in our model .

SQuAD dataset :
While it could run on SQuAD , our model was specifically designed for a case that is different from SQuAD . In SQuAD , the evidence only consists of a paragraph ( avg length 122 tokens , TriviaQA evidence is more than 20X longer ) so scalability is not a concern . Furthermore , answers to SQuAD questions are almost always unique spans in the passage , hence many of our intuitions of multi-mention learning might not be relevant for this task .



Summary of the paper
-------------------------------

The authors propose to add 4 elements to the ' Federated Averaging ' algorithm to provide a user- level differential privacy guarantee . The impact of those 4 elements on the model 'a accuracy and privacy is then carefully analysed .

Clarity , Significance and Correctness
--------------------------------------------------

Clarity : Excellent

Significance : I 'm not familiar with the literature of differential privacy , so I 'll let more knowledgeable reviewers evaluate this point .

Correctness : The paper is technically correct .

Questions
--------------

1 . Figure 1 : Adding some noise to the updates could be view as some form of regularization , so I have trouble understand why the models with noise are less efficient than the baseline .
2 . Clipping is supposed to help with the exploding gradients problem . Do you have an idea why a low threshold hurts the performances ? Is it because it reduces the amplitude of the updates ( and thus simply slows down the training ) ?
3 . Is your method compatible with other optimizers , such as RMSprop or ADAM ( which are commonly used to train RNNs ) ?

Pros
------

1 . Nice extensions to Federated Averaging to provide privacy guarantee .
2 . Strong experimental setup that analyses in details the proposed extensions .
3 . Experiments performed on public datasets .

Cons
-------

None

Typos
--------

1 . Section 2 , paragraph 3 : " is given in Figure 1 " -> " is given in Algorithm 1 "

Note
-------

Since I 'm not familiar with the differential privacy literature , I 'm flexible with my evaluation based on what other reviewers with more expertise have to say .

We thank the reviewer for the thoughtful review and good questions , which we address below :

1 . Figure 1 : Adding some noise to the updates could be view as some form of regularization , so I have trouble understand why the models with noise are less efficient than the baseline .

Indeed , we were hoping to see some regularization benefit from noise , but there does not appear to be a significant effect , at least for these models . In Figure 3 , which isolates the noise addition , we do see a slight improvement with a modest amount of noise early in training ( blue line , noise around 0.012 ) , but otherwise the Gaussian noise we add does not appear to help . We did not do training set evaluation on these models , it is possible ( and likely based on results from " Deep learning with differential privacy " , Figs. 3 and 6 ) that the addition of noise decreases the gap between test and training accuracy . Other work has also observed that adding noise may not work well as a regularizer for LSTMs , see the " negative results " paragraph in Sec 4 of https://openreview.net/pdf?id=rkjZ2Pcxe

2 . Clipping is supposed to help with the exploding gradients problem . Do you have an idea why a low threshold hurts the performances ? Is it because it reduces the amplitude of the updates ( and thus simply slows down the training ) ?

This is an important direction for future work , but we have some preliminary thoughts . First , to clarify , note we are clipping each user 's update before averaging across users , whereas traditional clipping is applied to a single minibatch update after averaging over examples , and so it is possible that these two types of clipping behave differently .

We suspect two primary reasons for the drop in performance with over - aggressive clipping : ( 1 ) reduction in the amplitude of the updates , as you suggest ; and ( 2 ) clipping introduces bias into the way updates from different users are weighted , essentially changing the loss function being optimized . Some preliminary subsequent experiments indicate that both effects are significant , and that the effect of ( 1 ) can be somewhat offset by rescaling the updates on the server . Nevertheless , we emphasize that our primary result is that despite these effects , it is possible to set the clipping parameter large enough that we can still train high - accuracy models .

3 . Is your method compatible with other optimizers , such as RMSprop or ADAM ( which are commonly used to train RNNs ) ?

There are multiple ways these optimizers could be extended to the federated setting . Either algorithm could be applied locally on each client ( that is , inside UserUpdateFedAvg ) to compute the update , and our approach would work without modification . Running these algorithms across clients while combining them with the additional local computation done by Federated Averging ( which we found to be important for achieving DP ) would essentially mean designing a new optimization procedure --- certainly an interesting direction , but beyond the scope of the current work .


This paper extends the previous results on differentially private SGD to user - level differentially private recurrent language models . It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model .

The idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community . This work makes a pretty significant contribution to such topic . It adapts techniques from some previous work to address the difficulties in training language model and providing user- level privacy . The experiment shows good privacy and utility .

The presentation of the paper can be improved a bit . For example , it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping , the original FedAvg and FedSGD , and moments accountant as well as privacy amplification ; otherwise , it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper . Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work .


We thank the reviewer for the thoughtful review , and will attempt to improve the presentation in the final version . While it will be difficult to fit a complete introduction of all the topics mentioned into the page limit , we will add additional coverage of this material .

= Quality =
Overall , the authors do a good job of placing their work in the context of related research , and employ a variety of non-trivial technical details to get their methods to work well .

= Clarity =

Overall , the exposition regarding the method is good . I found the setup for the sequence tagging experiments confusing , tough . See more comments below .

= Originality / Significance =

The paper presents a clever idea that could help make SPENs more practical . The paper 's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compression .

= Major Comment =

I 'm concerned by the quality of your results and the overall setup of your experiments . In particular , the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper .

Most of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed - forward inference network . You have have very few experiments using a SPEN energy function parametrization that does n't correspond to a CRF , even though you could have used an arbitrary convnet , RNN , etc . The one exception is when you use the tag language model . This is a good idea , but it is pretrained , not trained using the saddle - point objective you introduce . In fact , you do n't have any results demonstrating that the saddle - point approach is better than simpler alternatives .

It seems that you could have written a very different paper about model compression with CRFs that would have been very interesting and you could 've have used many of the same experiments . It 's unclear why SPENs are so important . The idea of amortizing inference is perhaps more general . My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine - grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep - network - based energy functions .


= Minor Comments =

* You should mention ' Energy Based GANs "

* I do n't understand " This approach performs backpropagation through each step of gradient descent , permitting more stable training but also evidently more overfitting . " Why would it overfit more ? Simply because training was more stable ? Could n't you prevent overfitting by regularizing more ?

* You spend too much space talking about specific hyperparameter ranges , etc . This should be moved to the appendix . You should also add a short summary of the TLM architecture to the main paper body .

* Regarding your footnote discussing using a positive vs . negative sign on the entropy regularization term , I recommend checking out " Regularizing neural networks by penalizing confident output distributions . "

* You should add citations for the statement " In these and related settings , gradient descent has started to be replaced by inference networks . "

* I did n't find Table 1 particularly illuminating . All of the approaches seem to perform about the same . What conclusions should I make from it ?

* Why not use KL divergence as your \ Delta function ?

* Why are the results in Table 5 on the dev data ?

* I was confused by Table 4 . First of all , it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained CRF energy and amortizing inference by training an inference network . This idea of training with a standard loss ( conditional log lik. ) and then amortizing inference post-hoc was not explicitly introduced as an alternative to the saddle point objective you put forth earlier in the paper . Second , I was very surprised that the inference network outperformed Viterbi ( 89.7 vs. 89.1 for the same CRF energy ) . Why is this ?

* I 'm confused by the difference between Table 6 and Table 4 ? Why not just include the TLM results in Table 4 ?








The new experiments sections is substantially better . It does a good job of providing separate analyses of the various contributions of the paper . Overall , there is definitely a wealth of follow - on work to be done in this area , and the ICLR community will appreciate this paper .

Thanks very much for the thoughtful review !

Regarding your major comment , we will first mention that the revised version includes additional experimental results when using our framework to train a SPEN with a global energy that includes the tag language model ( TLM ) energy . These results are described in Sec. 7.2.4.

We agree that the original submission suffered from a bit of an identity crisis . As you mentioned , “ The idea of amortizing inference is perhaps more general ” and we intend to develop this direction in future work . Also , in the revised version , we restructured the sequence labeling section so as to more cleanly separate the discussion of training SPENs ( Sec. 7.2.3 ) and exploring richer energy functions ( Sec. 7.2.4 ) from the discussion of amortizing inference for pretrained structured predictors ( Sec. 7.2.5 ) .

Replies to your minor comments are below :

“ Energy Based GANs ”
Thanks -- we added a mention and citation to the Related Work section .

“ Why would it overfit more ? Simply because training was more stable ? Could n't you prevent overfitting by regularizing more ? ”

We should have provided a citation for this . The github page hosting the SPEN code includes the claim : “ the end - to - end approach fitting the training data much better is that it is more prone to overfitting ” . As that method is from prior work , we do not know exactly what the cause is of the observed overfitting . It may be that it is caused by the increased capability of calculating precise gradients obtained by unrolling gradient descent into a computation graph , rather than merely performing gradient descent for inference in an offline manner . We clarified the above in Sec. 7.1.

“ Move hyperparameter ranges , etc to the appendix . Add summary of TLM architecture to the main paper ”

We moved several tuning details to the appendix and moved the TLM description to the main body ( Sec. 7.2.4 ) .

“ footnote on using positive vs . negative sign on entropy regularization term ”

Thanks for the pointer ! We added a citation .

“ add citations for ‘ gradient descent has started to be replaced by inference networks . ’”

Good point . We added relevant citations to that claim .

“ Table 1 not particularly illuminating . All of the approaches seem to perform about the same . ”

Usually , using cost -augmented inference for testing ( with an SVM ) gives really bad predictions . We were surprised to see that the final cost - augmented inference network performs well as a test - time inference network . This suggests that by the end of training , the cost - augmented network may be approaching the argmin . Nonetheless , since the differences are small there , we moved this table and discussion of this to the appendix .

“ Why are results in Table 5 on dev ? ”
We often reported results only on dev so as to avoid reporting too many configurations on the test set , in order to prevent us ( and the community ) from learning too much about what works best on the test set .

“ Why not use KL divergence as your \ Delta function ? ”

In classic max - margin structured prediction , \ Delta is a symmetric function , so we did n’t consider using KL divergence . But we could use JS divergence and we think an exploration of the choices here would be interesting future work . ( Also , we could try using asymmetric \ Delta functions as there does not appear to be any strong theoretical motivation to use symmetric \ Delta functions ( in our view ) ; it appears to be mostly just a convention . )

“ confused by Table 4 ”

Thanks to the comments by you and the other reviewers , we heavily modified Table 4 , splitting it into multiple simpler tables ( see the new tables 4 , 6 , and 9 ) .

“ very surprised that the inference network outperformed Viterbi ( 89.7 vs. 89.1 for the same CRF energy ) . Why is this ? ”

This is a good question . We added some speculation to Sec. 7.2.3 that we think is relevant to this question as well . In particular , the stabilization terms used when training the inference network may be providing a regularizing effect for the model .

“ confused by difference between Table 6 and Table 4 ”

Yes , we agree that was confusing . We restructured both tables . Please see the new tables 4 , 5 , and 6 .

The paper proposes training ``inference networks , '' which are neural network structured predictors . The setup is analogous to generative adversarial networks , where the role of the discriminator is played by a structured prediction energy network ( SPEN ) and the generator is played by an inference network .

The idea is interesting . It could be viewed as a type of adversarial training for large - margin structured predictors , where counterexamples , i.e. , structures with high loss and low energy , can not be found by direct optimization . However , it remains unclear why SPENs are the right choice for an energy function .

Experiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent . However , the experimental results are not clearly presented . The clarity is poor enough that the paper might not be ready for publication .

Comments and questions :

1 ) It is unclear whether this paper is motivated by training SPENs or by training structured predictors . The setup focuses on using SPENs as an inference network , but this seems inessential . Experiments with simpler energy functions seem to be absent , though the experiments are unclear ( see below ) .

2 ) The confusion over the motivation is confounded by the fact that the experiments are very unclear . Sometimes predictions are described as the output of SPENs ( Tables 2 , 3 , 4 , and 7 ) , sometimes as inference networks ( Table 5 ) , and sometimes as a CRF ( Tables 4 and 6 ) . In 7.2.2 it says that a BiLSTM is used for the inference network in Twitter POS tagging , but Tables 4 and 6 indicate both CRFs and BiLSTMS ? It is also unclear when a model , e.g. , BiLSTM or CRF is the energy function ( discriminator ) or inference network ( generator ) .

3 ) The third and fourth columns of Table 5 are identical . The presentation should be made consistent , either with dev / test or - retuning / + retuning as the top level headers .

4 ) It is also unclear how to compare Tables 4 and 5 . The second to bottom row of Table 5 seems to correspond with the first row of Table 5 , but other methods like slack rescaling have higher performance . What is the takeaway from these two tables supposed to be ?

5 ) Part of the motivation for the work is said to be the increasing interest in inference networks : " In these and related settings , gradient descent has started to be replaced by inference networks . Our results below provide more evidence for making this transition . " However , no other work on inference networks is directly cited .

Thanks for the questions . We agree with you that the experiment description was unclear in many places and we think the revised version is much improved in this regard . Specific answers to your numbered questions are below :

1 ) The primary goal of the paper is to propose a framework to do training and inference with SPENs . We have rewritten the experimental results section to focus on evaluating this SPEN training / inference framework . It turns out that the framework can also be applied to simpler families of structured prediction models , so we also include experimental results for applying inference network training to CRFs ( see Sec. 7.2.5 in the revised version ) . In the new version , we have tried to more cleanly separate the contributions for SPENs from the contributions to structured prediction more generally , by relegating the latter results to Sec. 7.2.5 only .

2 ) All good points . We hope the revised version will help resolve all of these confusions . Please let us know if anything is still unclear .

3 ) We restructured this table to remove redundant columns and make the presentation simpler .

4 ) Thanks to the comments by you and the other reviewers , we heavily modified Table 4 , splitting it into multiple simpler tables ( see the new tables 4 , 6 , and 9 ) .

5 ) Good point . We added relevant citations to that claim .


This paper proposes an improvement in the speed of training / inference with structured prediction energy networks ( SPENs ) by replacing the inner optimization loop with a network trained to predict its outputs .

SPENs are an energy - based structured prediction method , where the final prediction is obtained by optimizing min_y E_theta ( f_phi ( x ) , y ) , i.e. , finding the label set y with the least energy , as computed by the energy function E ( ) , using a set of computed features f_phi( x ) which comes from a neural network . The key innovation in SPENs was representing the energy function E ( ) as an arbitrary neural network which takes the features f( x ) and candidate labels y and outputs a value for the energy . At inference time y can be optimized by gradient descent steps . SPENs are trained using maximum - margin loss functions , so the final optimization problem is max - loss ( y , y ' ) where y' = argmin_y E ( f ( x ) , y ) .

The key idea of this paper is to replace the minimization of the energy function min_y E( f ( x ) , y ) with a neural network which is trained to predict the resulting output of this minimization . The resulting formulation is a min-max problem at training time with a striking similarity to the GAN min-max problem , where the y-predicting network learns to predict labels with low energy ( according to the E-computing network ) and high loss while the energy network learns to assign a high energy to predicted labels which have a higher loss than true labels ( i.e. the y-predicting network acts as a generator and the E-predicting network acts as a discriminator ) .

The paper explores multiple loss functions and techniques to train these models . They seem rather finnicky , and the experimental results are n't particularly strong when it comes to improving the quality over SPENs but they have essentially the same test - time complexity as simple feedforward models while having accuracy comparable to full inference - requiring energy - based models . The improved understanding of SPENs and potential for further work justify accepting this paper .

Thank you for the comments and the support !

The authors describe a variant of the negotiation game in which agents of different type , selfish or prosocial , and with different preferences . The central feature is the consideration of a secondary communication ( linguistic ) channel for the purpose of cheap talk , i.e. talk whose semantics are not laid out a priori .

The essential findings include that prosociality is a prerequisite for effective communication ( i.e. formation of meaningful communication on the linguistic channel ) , and furthermore , that the secondary channel helps improve the negotiation outcomes .

The paper is well - structured and incrementally introduces the added features and includes staged evaluations for the individual additions , starting with the differentiation of agent characteristics , explored with combination of linguistic and proposal channel . Finally , agent societies are represented by injecting individuals ' ID into the input representation .

The positive :
- The authors attack the challenging task of given agents a means to develop communication patterns without apriori knowledge .
- The paper presents the problem in a well - structured manner and sufficient clarity to retrace the essential contribution ( minor points for improvement ) .
- The quality of the text is very high and error-free .
- The background and results are well - contextualised with relevant related work .

The problematic :
- By the very nature of the employed learning mechanisms , the provided solution provides little insight into what the emerging communication is really about . In my view , the lack of interpretable semantics hardly warrants a reference to ' cheap talk ' . As such the expectations set by the well - developed introduction and background sections are moderated over the course of the paper .
- The goal of providing agents with richer communicative ability without providing prior grounding is challenging , since agents need to learn about communication partners at runtime . But it appears as of the main contribution of the paper can be reduced to the decomposition of the learnable feature space into two communication channels . The implicit relationship of linguistic channel on proposal channel input based on the time information ( Page 4 , top ) provides agents with extended inputs , thus enabling a more nuanced learning based on the relationship of proposal and linguistic channel . As such the well - defined semantics of the proposal channel effectively act as the grounding for the linguistic channel . This , then , could have been equally achieved by providing agents with a richer input structure mediated by a single channel . From this perspective , the solution offers limited surprises . The improvement of accuracy in the context of agent societies based on provided ID follows the same pattern of extending the input features .
- One of the motivating factors of using cheap talk is the exploitation of lying on the part of the agents . However , apart from this initial statement , this feature is not explicitly picked up . In combination with the previous point , the necessity / value of the additional communication channel is unclear .

Concrete suggestions for improvement :

- Providing exemplified communication traces would help the reader appreciate the complexity of the problem addressed by the paper .
- Figure 3 is really hard to read / interpret . The same applies to Figure 4 ( although less critical in this case ) .
- Input parameters could have been made explicit in order to facilitate a more comprehensive understanding of technicalities ( e.g. in appendix ) .
- Emergent communication is effectively unidirectional , with one agent as listener . Have you observed other outcomes in your evaluation ?

In summary , the paper presents an interesting approach to combine unsupervised learning with multiple communication channels to improve learning of preferences in a well - established negotiation game . The problem is addressed systematically and well - presented , but can leave the reader with the impression that the secondary channel , apart from decomposing the model , does not provide conceptual benefit over introducing a richer feature space that can be exploited by the learning mechanisms . Combined with the lack of specific cheap talk features , the use of actual cheap talk is rather abstract . Those aspects warrant justification .

We would like to emphasize that the linguistic / utterance channel and the proposal channel are completely separate , and there is no a priori link in the messages in the proposal channel and the messages in the linguistic channel . When the agents are forced to use the linguistic channel exclusively , they must learn from scratch how to use the linguistic channel to communicate effectively to solve the negotiation task and thus the proposal channel is never communicated to the opponent . In short , LINGUISTIC refers to negotiating using ONLY the linguistic channel , PROPOSAL only the preference and BOTH using the combination .

> By the very nature of the employed learning mechanisms , the provided solution provides little insight into what the emerging communication is really about .

It is difficult to quantify precisely what communication is about , especially in our bottom - up approach starting from arbitrary symbols . Despite this , in our post - processing analyses of the communication analyses , we show : ( i ) agents partition themselves into speaker and listener , ( ii ) elements of natural language are found in the protocols that emerged , and ( iii ) the content of the messages indicate that the agents are encoding their utilities in the language channel .

> In my view , the lack of interpretable semantics hardly warrants a reference to ' cheap talk ' . As such the expectations set by the well - developed introduction and background sections are moderated over the course of the paper .

We did not intend to suggest that the lack of interpretable semantics warrants a reference to cheap talk . We refer to cheap talk due to the fact that the exchanges in the linguistic channel have no effect on the resulting payoff , which follows directly from the definition .
The lack of interpretable semantics is orthogonal to any references to cheap talk : it simply motivates the research question of whether communication can emerge among learning agents . We will clarify this .

> Providing exemplified communication traces would help the reader appreciate the complexity of the problem addressed by the paper .

In our most recent revision , we have added an appendix showing what sample games with each of the communication channels open look like .

> Figure 3 is really hard to read / interpret . The same applies to Figure 4 ( although less critical in this case ) .

We have made the figures larger , and added more explanation in the text .

> Input parameters could have been made explicit in order to facilitate a more comprehensive understanding of technicalities ( e.g. in appendix ) .

We have added an appendix showing the values of the hyperparameters we used . We would also like to thank the public comments that acted as additional motivation to help reproducibility .

> Emergent communication is effectively unidirectional , with one agent as listener . Have you observed other outcomes in your evaluation ?

In our experiments , we consistently see the agents separating into speaker - listener roles , as mentioned in the paper .


This paper explores how agents can learn to communicate to solve a negotiation task . They explore several settings : grounded vs. ungrounded communication , and self - interested vs . prosocial agents . The main findings are that prosocial agents are able to learn to ground symbols using RL , but self - interested agents are not . The work is interesting and clearly described , and I think this is an interesting setting for studying emergent communication .

My only major comment is that I ’m a bit skeptical about the claim that “ self - interested agents cannot ground cheap talk to exchange meaningful information ” . Given that the agents ’ rewards would be improved if they were able to make agreements , and humans can use ‘ cheap talk ’ to negotiate , surely the inability to do so here shows a failure of the learning algorithm ( rather than a general property of self - interested agents ) ?

I am also concerned about the dangers posed by robots inventing their own language , perhap the authors should shut this down :-)


> My only major comment is that I ’m a bit skeptical about the claim that “ self - interested agents cannot ground cheap talk to exchange meaningful information ” . Given that the agents ’ rewards would be improved if they were able to make agreements , and humans can use ‘ cheap talk ’ to negotiate , surely the inability to do so here shows a failure of the learning algorithm ( rather than a general property of self - interested agents ) ?

We agree ; we believe this is the the main reason that the bottom - up approach is particularly challenging . Humans ( and to a lesser extent , the demonstration data in top -down approaches ) benefit by having a priori semantics on the symbols in the linguistic channel . We used the term ‘ self - interested agents ’ mainly to separate them from the prosocial ones , but we do indeed mean in the context of the standard RL learning algorithms used in the paper , not more generally to mean ‘ any possible self - interested agent ’ . We will clarify this .
In future work , we will explore more sophisticated RL learning techniques that allow self - interested to negotiate using a ‘ cheap - talk ’ channel ( which due to its unbinding and unverifiable nature poses a challenge for the current RL algorithms ) .


The experimental setup is clear , although the length of the utterances and the number of symbols in them is not explicitly stated in the text ( only the diagrams ) .

Experiment 1 confirms that agents who seek only to maximise their own rewards fail to coordinate over a non-binding communication channel . The exposition of the experiments , however , is unclear .
In Fig 1 , it is not clear what Agent 1 and Agent 2 are . Do they correspond to arbitrary labels or the turns that the agent takes in the game ?
Why is Agent 1 the one who triumphs in the no-communication channel game ? Is there any advantage to going first generally ? Where are the tests of robustness on the curves demonstrated in Figure 2a ?
Has figure 2 b been cherry picked ? This should be demonstrated over many different negotiations with error bars .
In the discussion of the agents being unable to ground cheap talk , the symbolic nature of the linguistic channel clouds the fact that it is not the symbolic , ungrounded aspect but the non-binding nature of communication on this channel . This would be more clearly demonstrated and parsimonious by using a non-binding version of the proposal channel and saving the linguistic discussion for later .

Experiment 2 shows that by making the agents prosocial , they are able to learn to communicate on the linguistic channel to achieve pretty much optimal rewards , a very nice result .
The agents are not able to reach the same levels of cooperation on the proposal channel , in fact performing worse than the no-communication baseline . Protocols could be designed that would allow the agents to communicate their utilities over this channel ( within 4 turns ) , so the fact they do n't suggests it is the learning procedure that is not able to find this optimum . Presenting this as a result about the superiority of communication over the linguistic channel is not well supported .
Why do they do worse with random termination than 10 turns in the proposal channel ? 4 proposals should contain enough information to determine the utilities .
Why are the 10 turn games even included in this table ? It seems that this was dismissed in the environment setup section , due to the first mover advantage .
Why do no - communication baselines change so much between random termination and 10 turns in the prosocial case ?
Why do self - interested agents for 10 turns on the linguistic channel terminate early ?
Table 1 might be better represented using the median and quartiles , since the data is skewed .

Analysis of the communication , i.e. what is actually sent , is interesting and the division into speaker and listener suggests that this is a simple protocol that is easy for agents to learn .

Experiment 3 aims to determine whether an agent is able to negotiate against a community of other agents with mixed levels of prosociality . It is shown that if the fixed agent is able to identify who they are playing against they can do better than not knowing , in the case where the fixed agent is self interested .
The pca plot of agent id embeddings related is good .
Both Figure 4 and Table 3 use Agent 1 and Agent 2 rather than Agent A and Agent B and is not clear whether this is a mistake or Agent 1 is different from Agent A.
The no -communication baseline is referred to in the text but the results are not shown in the table .
There are no estimates of the uncertainty of the results in table 3 , how robust are these results to different initial conditions ?
This section seems like a bit of an add - on to address criticisms that might arise about the initial experiment being only two agents .

Overall , the paper has some nice results and an interesting ideas but could do with some tightening up of the results to make it really good .


> In Fig 1 , it is not clear what Agent 1 and Agent 2 are . Do they correspond to arbitrary labels or the turns that the agent takes in the game ?

We have clarified that Agent 1 is the agent who consistently goes first in negotiation .

> Why is Agent 1 the one who triumphs in the no-communication channel game ? Is there any advantage to going first generally ?

Actually , Agent 2 typically triumphs in the no-communication setup . We do not believe there is any significant advantages to either going first or second , as demonstrated by the fact that self - interested agents seem to be able to negotiate fairly in this environment .

> Where are the tests of robustness on the curves demonstrated in Figure 2a ?

We have re-run the experiments with 20 different random seeds , and have updated Figure 2a to show their averaged results . The uncertainty estimates are unfortunately only visible in the linguistic communication setup , as the other communication protocols seem to give rise to very stable training .

> Has figure 2 b been cherry picked ? This should be demonstrated over many different negotiations with error bars .

Figure 2 b shows the average reward per turn over 1000 different negotiations . We present results from training 20 seeds ( 1280 test negotiations per seed ) . We have added interquartile ranges at every timestep , and clarified that they are averaged results in the text .

> Protocols could be designed that would allow the agents to communicate their utilities over this channel ( within 4 turns ) , so the fact they do n't suggests it is the learning procedure that is not able to find this optimum .

We agree . However , discovering the optimal protocol through self - play RL is a significantly harder problem than designing one with knowledge of the optimal structure . For example , the pre-grounded nature of the proposal channel , combined with its lower information bandwidth , means that random exploration is less likely to find the optimal communication protocol .

> Why are the 10 turn games even included in this table ? It seems that this was dismissed in the environment setup section , due to the first mover advantage .

We include the 10 turn results so that we can include the key self - interested and prosocial results in the same table . We also wanted to demonstrate how strong the first mover advantage was . We have moved these results to the appendix however in the new draft .
We have also changed both tables to return mean and interquartile range as suggested .

> Why do self - interested agents for 10 turns on the linguistic channel terminate early ?
We believe that when enough information is exchanged , the agents make effective proposals and thus do not need to negotiate further .

> Both Figure 4 and Table 3 use Agent 1 and Agent 2 rather than Agent A and Agent B and is not clear whether this is a mistake or Agent 1 is different from Agent A.

We have corrected this .

> The no - communication baseline is referred to in the text but the results are not shown in the table .

We have added the no-communication baseline figure to the text .

> There are no estimates of the uncertainty of the results in table 3 , how robust are these results to different initial conditions ?

These results are averaged across 10 batches of 128 games in each batch . We have clarified this , and added standard deviations to the table .

This paper proposes ensemble adversarial training , in which adversarial examples crafted on other static pre-trained models are used in the training phase . Their method makes deep networks robust to black - box attacks , which was empirically demonstrated .

This is an empirical paper . The ideas are simple and not surprising but seem reasonable and practically useful .
Empirical results look natural .

[ Strong points ]
* Proposed randomized white - box attacks are empirically shown to be stronger than original ones .
* Proposed ensemble adversarial training empirically achieves smaller error rate for black - box attacks .

[ Weak points ]
* no theoretical guarantee for proposed methods .
* Robustness of their ensemble adversarial training depends on what pre-trained models and attacks are used in the training phase .


Thank you for the constructive review .

> Robustness of their ensemble adversarial training depends on what pre-trained models and attacks are used in the training phase

While we agree that the robustness of ensemble adversarial training may depend on the choices of model architectures and attacks used , this is not fundamentally different from the meta-parameter choices faced with " regular " adversarial training , or even non- adversarial training . For instance , it has been shown that the choice of model architecture has a strong influence on how well regular adversarial training performs ( e.g. , see our MNIST experiments in Appendix C.2 of the revised manuscript ) . For the Inception v3 architecture , we find that ensemble adversarial training with two different sets of pre-trained models yield very similar results .

Regarding the diversity of models used , we note that the main goal of ensemble adversarial training is to decouple the attack from the model being trained , in order to prevent gradient masking . Our MNIST experiments ( Appendix C.2 of the revised manuscript ) show that using a single pre-trained model with the same architecture than the model being trained is often a very effective form of ensemble adversarial training . We have emphasized the importance of decoupling gradients in our paper .

> no theoretical guarantee for proposed methods .

We thank you for raising the question of formal guarantees for future attacks ( indeed our models remain vulnerable to white - box l-infinity attacks ) . Following your suggestion , we draw a connection between Ensemble Adversarial Training and the formal generalization guarantees obtained for Domain Adaptation , wherein a model is trained on multiple source distributions and evaluated on a different target distribution ( Section 3.4 and Appendix B in our revised manuscript ) . While the resulting bounds may not necessarily be meaningful in practice , they do show that Ensemble Adversarial Training can provide formal guarantees for future adversaries of “ similar power ” than the ones considered during training . Some works manage to provide stronger guarantees than ours for small datasets ( e.g. , against all bounded l-infinity attacks ) , using techniques that appear out of reach for ImageNet - scale tasks . Yet , even extending these guarantees to arbitrary adversaries is a daunting task , given that we do not know how to define or enumerate the right sets of adversarial metrics . We believe that this connection to Domain Adaptation will be interesting to the community , as the resulting bounds are independent of the noise model ( e.g. , l-infinity perturbations ) being considered .


This paper describes computationally efficient methods for training adversarially robust deep neural networks for image classification . ( These methods may extend to other machine learning models and domains as well , but that 's beyond the scope of this paper . )

The former standard method for generating adversarially images quickly and using them in training was to do a single gradient step to increase the loss of the true label or decrease the loss of an alternate label . This paper shows that such training methods only lead to robustness against these " weak " adversarial examples , leaving the adversarially - trained models vulnerable to multi-step white - box attacks and black - box attacks ( adversarial examples generated to attack alternate models ) .

There are two proposed solutions . The first is to generate additional adversarial examples from other models and use them in training . This seems to yield robustness against black - box attacks from held - out models as well . Of course , it requires that you have a somewhat diverse group of models to choose from . If that 's the case , why not directly build an ensemble of all the models ? An ensemble of neural networks can still be represented as a neural network , although a more computationally costly one . Thus , while this heuristic appears to be useful with current models against current attacks , I do n't know how well it will hold up in the future .

The second solution is to add random noise before taking the gradient step . This yields more effective adversarial examples , both for attacking models and for training , because it relies less on the local gradient . This is another simple idea that appears to be effective . However , I would be interested to see a comparison to a 2 - step gradient - based attack . R+ Step -LL can be viewed as a 2 - step attack : a random step followed by a gradient step . What if both steps were gradient steps instead ? This interpolates between Step -LL and I- Step -LL , with an intermediate computational cost . It would be very interesting to know if R+ Step -LL is more or less effective than 2 + Step - LL , and how large the difference is .

I like that this paper demonstrates the weakness of previous methods , including extensive experiments and a very nice visualization of the loss landscape in two adversarial dimensions . The proposed heuristics seem effective in practice , but they 're somewhat ad hoc and there is no analysis of how these heuristics might or might not be vulnerable to future attacks .

Thank you for the constructive review .

> Of course , it requires that you have a somewhat diverse group of models to choose from . If that 's the case , why not directly build an ensemble of all the models

A large diversity of pre-trained models is not necessary for ensemble adversarial training . The main goal of our approach is to decouple the attack ( the method used to produce adversarial examples ) from the defense ( the model being trained ) so as to avoid the gradient masking issue . In this sense , even using a single pre-trained model is valuable and we indeed found this to be very effective on MNIST ( Appendix C.2 in our revised manuscript ) .
Of course , using multiple models will only increase the diversity of adversarial examples encountered during training . As shown by Liu et al . ( ICLR ’ 17 ) , applying the FGSM to different Image Net models generates very diverse perturbations ( the gradients of different models are often close to orthogonal ) but these perturbations still transfer between the models . Thus , although different models can produce very diverse attacks , simply ensembling these models is not necessarily a good defense strategy , as the same adversarial examples will fool most of the models in the ensemble . For instance , if we ensemble all the pre-trained Image Net models we used , except for Inception v4 , and then use a black - box FGSM attack computed on Inception v4 , the ensemble 's robustness is only marginally better than that of a single undefended model .
When using Ensemble Adversarial Training with the Inception v3 architecture , we found that the marginal benefit of adding more pre-trained models is relatively low , thus also corroborating the fact that the main benefit of Ensemble Adversarial Training is in decoupling the attack procedure from the model being trained . We have clarified this point in our paper .

> It would be very interesting to know if R+ Step -LL is more or less effective than 2 + Step - LL , and how large the difference is .

We thank the reviewer for the question about the 2 - step Iter - LL attack , as it yields another nice illustration of the gradient masking effect . It turns out that for non- defended models and ensemble - adversarially trained models , a 2 - step iterative attack is stronger than R+Step - LL , as is to be expected ( the difference is roughly 10 % top1 / top5 accuracy ) . However , for standard adversarial training on Inception v3 , R+Step -LL is stronger than the 2 - step Iter -LL attack ( by about 7 % top1 / top5 accuracy ) . Thus , this shows that the local gradient of the adversarially trained model is worse than a random direction from an optimization perspective . We added these results to Table 2 in our paper .

> The proposed heuristics seem effective in practice , but they 're somewhat ad hoc and there is no analysis of how these heuristics might or might not be vulnerable to future attacks

We thank you for raising the question of formal guarantees for future attacks ( indeed our models remain vulnerable to white - box l-infinity attacks ) . Following your suggestion , we draw a connection between Ensemble Adversarial Training and the formal generalization guarantees obtained for Domain Adaptation , wherein a model is trained on multiple source distributions and evaluated on a different target distribution ( Section 3.4 and Appendix B in our revised manuscript ) . While the resulting bounds may not necessarily be meaningful in practice , they do show that Ensemble Adversarial Training can provide formal guarantees for future adversaries of “ similar power ” than the ones considered during training . Some works manage to provide stronger guarantees than ours for small datasets ( e.g. , against all bounded l-infinity attacks ) , using techniques that appear out of reach for ImageNet - scale tasks . Yet , even extending these guarantees to arbitrary adversaries is a daunting task , given that we do not know how to define or enumerate the right sets of adversarial metrics . We believe that this connection to Domain Adaptation will be interesting to the community , as the resulting bounds are independent of the noise model ( e.g. , l-infinity perturbations ) being considered .

There is also an independent submission ( https://openreview.net/forum?id=HknbyQbC-) that proposes a different type of black - box attacks based on GANs , that we did not consider in our paper . The authors evaluate their attack against ensemble adversarially trained models and find that our defense outperforms both standard adversarial training , as well as approaches with strong guarantees against white - box robustness , on both MNIST and CIFAR10 . This provides further evidence that our defense generalizes to attacks unseen during training ( and also that it works well on CIFAR10 ) .

The paper proposes a modification to adversarial training . Instead of alternating between clean and examples generated on - the-fly by the fast gradient sign during training , the model training is performed by alternating clean examples and adversarial examples generated from pre-trained models . The motivation behind this change is that one-step method to generate adversarial examples fail at generating good adversarial examples when applied to models trained in the adversarial setting . In contrast , one - step methods applied to models trained only on natural data generate adversarial examples that transfer reasonably well , even on models trained with usual adversarial training . The authors also propose a slight modification to the fast gradient sign method , in which an adversarial example is created using a random perturbation and the current model 's gradient , which seems to work better than the fast gradient sign method . Experiments with inception models on Image Net show increased robustness both against " black - box " attacks using held - out models not used in ensemble adversarial training .

One advantage of the method is that it is extremely simple . It uses pre-trained models that are readily available , and gains robustness against several well - known adversaries widely considered in the state of the art . The experiments are carried out on ImageNet and are seriously conducted .

On the negative side , there is a significant loss in accuracy , and the models are more vulnerable to white - box attacks than using standard adversarial training . As the authors discuss in the conclusion , this leaves open the question as to whether the models are indeed more robust , or whether it is an artifact of the static black - box attack schemes that are considered in the paper , which measures how much a single model is robust to adversarial examples for other models that were trained independently . For instance , there are no experiments against what is called adaptive black - box adversaries ; one could also imagine finding adversarial examples that are trained to fool all models in a predefined collection of models . In the end , while the work presented in the paper found its use in the recent NIPS competition on defending against adversarial examples , it is still unclear whether this kind of defence would make a difference in critical applications .



Thank you for the constructive review .

> On the negative side , there is a significant loss in accuracy

While the drop in accuracy ( on ImageNet ) is not zero , it is small for the Inception ResNet v2 model ( 0.6 % top1 and 0.3 % top5 ) and somewhat larger for Inception v3 ( 1.6-2.2 % top1/top5 ) . However , there are few other defenses proposed on ImageNet to compare these numbers against . One concurrent submission ( https://openreview.net/forum?id=SyJ7ClWCb) also aims at increasing white - box robustness at the cost of a much larger decrease in clean accuracy ( 10 - 15 % top1 ) .

> the models are more vulnerable to white - box attacks than using standard adversarial training

We would like to clarify that our models are not more vulnerable to white - box attacks than those learned with standard adversarial training . While this appears to be the case on single - step attacks , it is due to the absence of a gradient masking effect , which is an intended consequence of ensemble adversarial training .
For iterative white - box attacks on ImageNet , we find that the robustness increase with adversarial training ( whether the standard version or our ensemble variant ) is only marginal compared to standard training . This was already observed in the " Adversarial Training at Scale " paper of Kurakin et al. , ICLR '17 .
While there have been some recent successes in hardening models against white - box attacks , the techniques required are expensive and not currently applicable to large - scale problems such as ImageNet . Incidentally , an independent submission ( https://openreview.net/forum?id=HyydRMZC-) shows that ensemble adversarial training is more robust than other adversarial training variants against white - box “ spatially transformed ” adversarial examples .

> one could also imagine finding adversarial examples that are trained to fool all models in a predefined collection of models

Thank you for bringing this to our attention , we have updated our manuscript to clarify that we evaluated our models against adversarial examples that evade a collection of models . Specifically , we applied various attacks ( including multi-step attacks like Step-LL , Iter -LL , PGD , etc. ) to an ensemble of all of our holdout models on ImageNet ( Inception V4 , ResNet v 1 and ResNet v 2 ) and then transferred these examples to the adversarially trained models . We did not find this to produce a stronger attack and have clarified this point in our paper .

> there are no experiments against what is called adaptive black - box adversaries

For adaptive attacks , there are few baselines to evaluate defenses against . The “ substitute model ” attack of Papernot et al. ( https://arxiv.org/abs/1602.02697) is hard to scale to ImageNet ( this was attempted in https://arxiv.org/abs/1708.03999). For MNIST , Papernot et al. report that their attack is mostly ineffective against an adversarially trained model .
There is also a concurrent submission that proposes an adaptive attack that attempts to “ crawl ” the model ’s decision boundary ( https://openreview.net/forum?id=SyZI0GWCZ). The attack requires a large number of calls to the black - box model ( ~ 10,000 per image ) and is optimized for the l2 metric . We have attempted to transpose this attack to the l-infinity metric considered in our paper , but have not yet been able to find a set of hyperparameters that produces adversarial examples with small perturbations ( even for undefended models ) . Further work in this direction will be very valuable for the community , and we believe our ensemble adversarially trained models can serve as a good baseline for evaluating new attacks .

For instance , ensemble adversarial training is used a baseline in an independent submission ( https://openreview.net/forum?id=HknbyQbC-) which considers black - box attacks based on GANs . The authors find that our defense outperforms both standard adversarial training , as well as approaches with strong guarantees against white - box robustness , on both MNIST and CIFAR10 . This provides further evidence that our defense generalizes to unseen attacks ( and also that it works well on CIFAR10 ) .

Finally , from a formal perspective , we discovered a natural connection between Ensemble Adversarial Training and Domain Adaptation , wherein a model is trained on multiple source distributions and evaluated on a different target distribution . Generalization bounds obtained in that litterature transfer to our setting , and allow us to express some formal guarantees for future adversaries that are not significantly more powerful than the ones considered during training ( Section 3.4 and Appendix B in our revised manuscript ) .
Although these bounds are not as strong as some of the formal guarantees obtained for simpler tasks , we believe these results and this connection will be interesting to the community , as they are independent of the noise model ( e.g. , l-infinity perturbations ) being considered .

The paper presents a new algorithm for inference - based reinforcement learning for deep RL . The algorithm decomposes the policy update in two steps , an E and an M-step . In the E-step , the algorithm estimates a variational distribution q which is subsequentially used for the M-step to obtain a new policy . Two versions of the algorithm are presented , using a parametric or a non-parametric ( sample -based ) distribution for q . The algorithm is used in combination with the retrace algorithm to estimate the q-function , which is also needed in the policy update .

This is a well written paper presenting an interesting algorithm . The algorithm is similar to other inference - based RL algorithm , but is the first application of inference based RL to deep reinforcement learning . The results look very promising and define a new state of the art or deep reinforcement learning in continuous control , which is a very active topic right now . Hence , I think the paper should be accepted .


I do have a few comments / corrections / questions about the paper :

- There are several approaches that already use the a combination of the KL - constraint with reverse KL on a non-parametric distribution and subsequently an M-projection to obtain again a parametric distribution , see HiREPS , non-parametric REPS [ Hoof2017 , JMLR ] or AC - REPS [ Wirth2016 , AAAI ] . These algorithms do not use the inference - based view but the trust region justification . As in the non-parametric case , the asymptotic performance guarantees from the EM framework are gone , why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward ?

- It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1 . First , alpha changes every iteration of the algorithm while the objective assumes that alpha is constant . This means that we change the objective all the time which is theoretically a bit weird . Moreover , the presented algorithm also changes the prior all the time ( in order to introduce the 2nd trust region ) in the M-step . Again , this changes the objective , so it is unclear to me what exactly is maximised in the end . Would it not be cleaner to start with the average reward objective ( no prior or alpha ) and then introduce both trust regions just out of the motivation that we need trust regions in policy search ? Then the objective is clearly defined .

- I did not get whether the additional " one - step KL regularisation " is obtained from the lower bound or just added as additional regularisation ? Could you explain ?

- The algorithm has now 2 KL constraints , for E and M step . Is the epsilon for both the same or can we achieve better performance by using different epsilons ?

- I think the following experiments would be very informative :

- MPO without trust region in M-step

- MPO without retrace algorithm for getting the Q-value

- test different epsilons for E and M step





I do have a few comments / corrections / questions about the paper :

- There are several approaches that already use the a combination of the KL - constraint with reverse KL on a non-parametric distribution and subsequently an M-projection to obtain again a parametric distribution , see HiREPS , non-parametric REPS [ Hoof2017 , JMLR ] or AC - REPS [ Wirth2016 , AAAI ] . These algorithms do not use the inference - based view but the trust region justification . As in the non-parametric case , the asymptotic performance guarantees from the EM framework are gone , why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward ?

- It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1 . First , alpha changes every iteration of the algorithm while the objective assumes that alpha is constant . This means that we change the objective all the time which is theoretically a bit weird . Moreover , the presented algorithm also changes the prior all the time ( in order to introduce the 2nd trust region ) in the M-step . Again , this changes the objective , so it is unclear to me what exactly is maximised in the end . Would it not be cleaner to start with the average reward objective ( no prior or alpha ) and then introduce both trust regions just out of the motivation that we need trust regions in policy search ? Then the objective is clearly defined .

- I did not get whether the additional " one - step KL regularisation " is obtained from the lower bound or just added as additional regularisation ? Could you explain ?

- The algorithm has now 2 KL constraints , for E and M step . Is the epsilon for both the same or can we achieve better performance by using different epsilons ?

- I think the following experiments would be very informative :

- MPO without trust region in M-step

- MPO without retrace algorithm for getting the Q-value

- test different epsilons for E and M step



We thank you for your questions and insightful comments .

> There are several approaches that already use the a combination of the KL - constraint with reverse KL on a non -
parametric distribution and subsequently an M-projection to obtain again a parametric distribution , see HiREPS , non-parametric REPS [ Hoof2017 , JMLR ] or AC - REPS [ Wirth2016 , AAAI ] .
> These algorithms do not use the inference - based view but the trust region justification . As in the non-parametric case , the asymptotic performance guarantees from the EM framework are gone , why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward ?

Thank you for pointing out the additional related work . We will include it in the paper . Regarding the EM vs. trust-region question : The benefit of deriving the algorithm from the perspective of an EM - like coordinate ascent is that it motivates and provides a convenient means for theoretical analysis of the two-step procedure used in our approach . See the added a theoretical analysis that was added to the appendix of the paper .

> It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1.
> First , alpha changes every iteration of the algorithm while the objective assumes that alpha is constant .
> This means that we change the objective all the time which is theoretically a bit weird .
> Moreover , the presented algorithm also changes the prior all the time ( in order to introduce the 2nd trust region ) in the M-step .
> Again , this changes the objective , so it is unclear to me what exactly is maximised in the end .
> Would it not be cleaner to start with the average reward objective ( no prior or alpha ) and then introduce both trust regions
> just out of the motivation that we need trust regions in policy search ? Then the objective is clearly defined .

The reviewers point is well taken . While we think the unconstrained ( soft-regularized ) is instructive and useful for theoretical analysis the hard - constrained version can indeed be understood as proposed by the reviewer and equally provides important insights . We will clarify this in the paper and also include an experimental comparison between the soft and hard - regularized cases .
Regarding your two concerns : For our theoretical guarantee ( that we have now derived in the appendix ) to hold we have to fix alpha . However , in practice it changes slowly during optimization and converges to a stable value . One can indeed think of the second trust -region as a simple regularizer that prevents overfitting / too large changes in the ( sample - based ) M-step ( similar small changes in the policy are also required by our proof ) .

- Regarding the additional experiments you asked for :

We agree and have carried out additional experiments that will be included in the final version , preliminary results are as follows :

1 ) MPO without trust region in M-step :
Also works well for low- dimensional problems but is less robust for high - dimensional problems such as the humanoid .

2 ) MPO without retrace algorithm for getting the Q-value
Is significantly slower to reach the same level of performance in the majority of the control suite tasks ( retrace + MPO is never worse in any of the control suite tasks ) .

3 ) test different epsilons for E and M step
The algorithm seems to be robust to settings of epsilon - as long as it is set roughly to the right order of magnitude ( 10^ - 3 to 10 ^ - 2 for the E-step , 10^ - 4 to 10 ^ - 1 for the M-step ) . A very small epsilon will , of course , slow down convergence .

This is an interesting policy - as-inference approach , presented in a reasonably clear and well - motivated way . I have a couple questions which somewhat echo questions of other commenters here . Unfortunately , I am not sufficiently familiar with the relevant recent policy learning literature to judge novelty . However , as best I am aware the empirical results presented here seem quite impressive for off - policy learning .

- When is it possible to normalize the non-parametric q( a|s ) in equation ( 6 ) ? It seems to me this will be challenging in most any situation where the action space is continuous . Is this guaranteed to be Gaussian ? If so , I do n’t understand why .

– In equations ( 5 ) and ( 10 ) , a KL divergence regularizer is replaced by a “ hard ” constraint . However , for optimization purposes , in C.3 the hard constraint is then replaced by a soft constraint ( with Lagrange multipliers ) , which depend on values of epsilon . Are these values of epsilon easy to pick in practice ? If so , why are they easier to pick than e.g. the lambda value in eq ( 10 ) ?



We thank the reviewer for comments and thoughtful questions . We reply to your main concerns in turn below .

> When is it possible to normalize the non-parametric q( a|s ) in equation ( 6 ) ? It seems to me this will be challenging in most any situation where the action space is continuous .
> Is this guaranteed to be Gaussian ? If so , I do n’t understand why .

Please see appendix , section C.2 . In the parametric case the solution for q( a|s ) is trivially normalized when we impose a parametric form that allows analytic evaluation of the normalization function ( such as a Gaussian distribution ) . .
For the non- parametric case note that the normalizer is given by
Z ( s ) = \int \pi_old ( a|s ) exp ( Q ( s , a ) / eta ) da ,
i.e. it is an expectation with respect to our old policy for which we can obtain a MC estimate : \hat { Z} ( s ) = 1 / N \sum_i exp ( Q ( s , a_i ) / eta ) with a_i \sim \pi_old ( \cdot | s ) .
Thus we can empirically normalize the density for those state-action samples that we use to estimate pi_new in the M-step .

> In equations ( 5 ) and ( 10 ) , a KL divergence regularizer is replaced by a “ hard ” constraint .
> However , for optimization purposes , in C.3 the hard constraint is then replaced by a soft constraint ( with Lagrange multipliers ) , which depend on values of epsilon .
> Are these values of epsilon easy to pick in practice ? If so , why are they easier to pick than e.g. the lambda value in eq ( 10 ) ?

Thank you for pointing out that the reasoning behind this was not entirely easy to follow . We will improve the presentation in the paper . Indeed we found that choosing epsilon can be easier than choosing a multiplier for the KL regularizer . This is due to the fact that the scale of the rewards is unknown a-priori and hence the multiplier that trades of maximizing expected reward and minimizing KL can be expected to change for different RL environments . In contrast to this , when we put a hard constraint on the KL we can explicitly force the policy to stay " epsilon - close " to the last solution - independent of the reward scale . This allows for an easier transfer of hyperparameters across tasks .

This paper studies new off - policy policy optimization algorithm using relative entropy objective and use EM algorithm to solve it . The general idea is not new , aka , formulating the MDP problem as a probabilistic inference problem .

There are some technical questions :
1 . For parametric EM case , there is asymptotic convergence guarantee to local optima case ; However , for nonparametric EM case , there is no guarantee for that . This is the biggest concern I have for the theoretical justification of the paper .

2 . In section 4 , it is said that Retrace algorithm from Munos et al. ( 2016 ) is used for policy evaluation . This is not true . The Retrace algorithm , is per se , a value iteration algorithm . I think the author could say using the policy evaluation version of Retrace , or use the truncated importance weights technique as used in Retrace algorithm , which is more accurate .

Besides , a minor point : Retrace algorithm is not off - policy stable with function approximation , as shown in several recent papers , such as
“ Convergent Tree -Backup and Retrace with Function Approximation ” . But this is a minor point if the author does n’t emphasize too much about off - policy stability .

3 . The shifting between the unconstrained multiplier formulation in Eq.9 to the constrained optimization formulation in Eq.10 should be clarified . Usually , an in - depth analysis between the choice of \lambda in multiplier formulation and the \epsilon in the constraint should be discussed , which is necessary for further theoretical analysis .

4 . The experimental conclusions are conducted without sound evidence . For example , the author claims the method to be ' highly data efficient ' compared with existing approaches , however , there is no strong evidence supporting this claim .


Overall , although the motivation of this paper is interesting , I think there is still a lot of details to improve .

We appreciate the detailed comments and questions regarding the connection between our method and EM methods . We have addressed your main concern with an additional theoretical analysis of the algorithm , strengthening the paper .

> 1 . For parametric EM case , there is asymptotic convergence guarantee to local optima case ; However , for nonparametric
> EM case , there is no guarantee for that . This is the biggest concern I have for the theoretical justification of the paper .

We have derived a proof that gives a monotonic improvement guarantee for the nonparametric variant of the algorithm under certain circumstances . We will include this proof in the paper . To summarize : Assuming Q can be represented and estimated , the " partial " E-step in combination with an appropriate gradient - based M-step leads to an improvement of the KL regularized objective and guarantees monotonic improvement of the overall procedure under certain circumstances . See also our response to the Anonymous question below .

> 2 . In section 4 , it is said that Retrace algorithm from Munos et al. ( 2016 ) is used for policy evaluation . This is not true .
> The Retrace algorithm , is per se , a value iteration algorithm . I think the author could say using the policy evaluation version of Retrace ,
> or use the truncated importance weights technique as used in Retrace algorithm , which is more accurate .

We will clarify that we are using the Retrace operator for policy evaluation only ( This use case was indeed also analyzed in Munos et al . ( 2016 ) ) .

> Besides , a minor point : Retrace algorithm is not off - policy stable with function approximation , as shown in several recent papers , such as
> “ Convergent Tree -Backup and Retrace with Function Approximation ” . But this is a minor point if the author does n’t emphasize too much about off - policy stability .

We agree that off -policy stability with function approximation is an important open problem that deserves additional attention but not one specific to this method ( i.e. any existing DeepRL algorithm shares these concerns ) . We will add a short note .

> 3 . The shifting between the unconstrained multiplier formulation in Eq.9 to the constrained optimization formulation in Eq.10 should be clarified .
> Usually , an in - depth analysis between the choice of \lambda in multiplier formulation and the \epsilon in the constraint should be discussed , which is necessary for further theoretical analysis .

We now have a detailed analysis of the unconstrained multiplier formulation ( see comment above ) of our algorithm . In practice we found that implementing updates according to both hard - constraints and using a fixed regularizer worked well for individual domains . Both \lambda and \epsilon can be found via a small hyperparameter search in this case . When applying the algorithm to many different domains ( with widely different reward scales ) with the same set of hyperparameters we found it easier to use the hard - constrained version ; which is why we placed a focus on it . We will include these experimental results in an updated version of the paper . We believe these observations are in - line with research on hard - constrained / KL - regularized on -policy learning algorithms such as PPO / TRPO ( for which explicit connections between the two settings are also ) .

> 4 . The experimental conclusions are conducted without sound evidence . For example , the author claims the method to be ' highly data efficient ' compared with existing approaches , however , there is no strong evidence supporting this claim .

We believe that the large set of experiments we conducted in the experimental section gives evidence for this . Figure 4 e.g. clearly shows the improved data- efficiency MPO gives over our implementations of state - of - the-art RL algorithms for both on -policy ( PPO ) and off-policy learning ( DDPG , policy gradient + Retrace ) . Further , when looking at the results for the parkour domain we observe an order of magnitude improvement over the reference experiment . We have started additional experiments for parkour with a full humanoid body - leading to similar speedups over PPO - which will be included in the final version and further solidify the claim on a more difficult benchmark .

The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks . The paper is an extension of Kawaguchi ' 16 . It also provides some sufficient conditions for the non-linear cases .

I think the main technical concerns with the paper is that the technique only applies to a linear model , and it does n't sound the techniques are much beyond Kawaguchi ' 16 . I am happy to see more papers on linear models , but I would expect there are more conceptual or technical ingredients in it . As far as I can see , the same technique here will fail for non- linear models for the same reason as Kawaguchi 's technique . Also , I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum . This wo n't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it 's unclear whether one can avoid those saddle points .

We appreciate your efforts for reviewing our paper . We admit that the key results of our paper are for the linear case , and global optimality conditions do not directly apply to nonlinear models that are used in practice .

But we believe that there is strong value in trying to fully understand the linear case , as this offers building blocks towards investigating nonlinear models , for instance in helping identify structures and settings that help us in quest for understanding realistic architectures .

We note that our results extend previous papers such as Kawaguchi ’ 16 and Hardt and Ma’17 in a substantial manner : our results have direct computational implications and provide a “ complete ” picture of the landscape of optimal for the deep linear case .

More concretely , previous works on this topic only show that there are only global minima or saddle points ; these results are “ existence ” results and there is little computational gain one can get from them . In contrast , we present *efficiently checkable * conditions for distinguishing the two different types of critical points ( global min or saddle ) : one can even use these conditions while running optimization algorithms to check whether the critical points we encounter are saddle points or not , if desired .

More broadly , we would like to emphasize again that since deep linear networks is itself a nonconvex problem , having a checkable necessary and sufficient global optimality condition is quite interesting , because in general for nonconvex problems , not only global but merely verifying even local optimality can be computationally intractable .

Developing a provably convergent algorithm based on our results is a very good research direction to improve our understanding of the loss surface . Although our paper does not answer these questions , we appreciate the reviewer ’s advice for this valuable future research direction .

Summary :
The paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks . In particular :
- in the case of deep linear networks , they characterize whether a critical point is a global optimum or a saddle point by a simple criterion . This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point ( i.e. , none is a local minimum ) , by relaxing some hypotheses and adding a simple criterion to know in which case we are .
- in the case of nonlinear network , they provide a sufficient condition for a solution to be a global optimum , using a function space approach .

Quality :
The quality is very good . The paper is technically correct and nontrivial . All proofs are provided and easy to follow .

Clarity :
The paper is very clear . Related work is clearly cited , and the novelty of the paper well explained . The technical proofs of the paper are in appendices , making the main text very smooth .

Originality :
The originality is weak . It extends a series of recent papers correctly cited . There is some originality in the proof which differs from recent related papers .

Significance :
The result is not completely surprising , but it is significant given the lack of theory and understanding of deep learning . Although the model is not really relevant for deep networks used in practice , the main result closes a question about characterization of critical points in simplified models if neural network , which is certainly interesting for many people .

Thank you very much for the review . We especially appreciate that the reviewer recognized the quality and clarity of our paper . Since we think that the reviewer has a good understanding of our paper and the reviewer did not have any specific questions for us , we would like to comment a little more about the significance of this paper .

We would like to emphasize that our results extend the previous “ existence ” theorems in Kawaguchi ’ 16 to “ computational ” theorems that can actually help optimization of linear neural networks . In other words , previous works on linear neural networks only proved that there exist only global minima and saddle points , whereas we provide * computable * tests for distinguishing global minima from others . This means that we can use the conditions while running optimization algorithms to determine which kind of critical point we are at , and choose the next action accordingly .

Aside from this computational perspective , considering that optimizing deep linear networks is a nonconvex problem , our checkable global optimality conditions are interesting in their own right , because in the worst cases even checking local optimality of nonconvex problems could be intractable .


- I think title is misleading , as the more concise results in this paper is about linear networks I recommend adding linear in the title i.e. changing the title to … deep LINEAR networks

- Theorems 2.1 , 2.2 and the observation ( 2 ) are nice !

- Theorem 2.2 there is no discussion about the nature of the saddle point is it strict ? Does this theorem imply that the global optima can be reached from a random initialization ? Regardless of if this theorem can deal with these issues , a discussion of the computational implications of this theorem is necessary .

- I ’m a bit puzzled by Theorems 4.1 and 4.2 and why they are useful . Since these results do not seem to have any computational implications about training the neural nets what insights do we gain about the problem by knowing this result ? Further discussion would be helpful .


We thank the reviewer for their effort in reviewing our paper and for the encouragement .

We agree that the main content of this paper is about linear networks , but since we also have some preliminary results on nonlinear case ( albeit in the abstract functional space setting ) , we kept a more general title to serve as a small indicator of this .

Our theorems do not imply anything about strict saddle property of linear neural networks . In fact , it was shown by Kawaguchi ’ 16 that in linear neural networks there are many non-strict saddle points i.e. saddle points without negative eigenvalues . So , our theorems do not imply that global optima can always be reached by random initialization and just running SGD - like methods . However , there is actually some computational implication of these theorems ; with these global optimality conditions , whenever we reach a critical point we can always efficiently check if it 's a global minimum or a saddle point . If we are indeed at a global minimum , we can just return the current point and terminate . If we are at a saddle , we can then intentionally add random perturbations to the point and try to escape the saddle .

Our nonlinear results are in a function space setting , so their implications are limited in computational aspects . However , we believe that these results can be good initial steps from the theoretical point of view ; for example , we can see that one of the sufficient conditions for global optimality is the Jacobian matrix being full rank . Given that a nonlinear function can locally be linearly approximated using Jacobians , this connection is already interesting . An extension of the function space viewpoint to cover different architectures or design new architectures ( that have “ better ” properties when viewed via the function space view ) should also be possible and worth studying .

The paper solves the problem of how to do autonomous resets , which is an important problem in real world RL . The method is novel , the explanation is clear , and has good experimental results .

Pros :
1 . The approach is simple , solves a task of practical importance , and performs well in the experiments .
2 . The experimental section performs good ablation studies wrt fewer reset thresholds , reset attempts , use of ensembles .

Cons :
1 . The method is evaluated only for 3 tasks , which are all in simulation , and on no real world tasks . Additional tasks could be useful , especially for qualitative analysis of the learned reset policies .
2 . It seems that while the method does reduce hard resets , it would be more convincing if it can solve tasks which a model without a reset policy couldnt . Right now , the methods without the reset policy perform about equally well on final reward .
3 . The method wont be applicable to RL environments where we will need to take multiple non-invertible actions to achieve the goal ( an analogy would be multiple levels in a game ) . In such situations , one might want to use the reset policy to go back to intermediate “ start ” states from where we can continue again , rather than the original start state always .

Conclusion / Significance : The approach is a step in the right direction , and further refinements can make it a significant contribution to robotics work .

Revision : Thanks to the authors for addressing the issues I raised , I revise my review to 7

We thank AnonReviewer3 for recognizing the importance of the problem we aim to solve , and for noting that our simple method is supported with “ good ablation studies . ” We have addressed the issues raised by the reviewer , as discussed below :

1 . We have run experiments on two additional environments ( ball in cup and peg insertion ) , so the revised version of the paper shows experiments on 5 simulated environments ( Section 6 , paragraph 1 ) . Videos of the learned policies visualize the learned forward + reset policies are available on the project website : https://sites.google.com/site/mlleavenotrace/
Experimental evaluation of five distinct domains compares favorably to most RL papers that have appeared in ICLR in the past . While we agree that real - world evaluation of our method would be excellent , this is going substantially beyond the typical evaluation for ICLR RL work .

2 . We ran additional environments that show that , in certain difficult situations , our method can solve tasks which a model without a reset policy cannot . Newly - added Sections 6.1 and 6.6 demonstrate this result in two settings . We summarize these results below in a separate post entitled “ Additional Experiments . ”

3 . We expanded Section 4 paragraph 2 to clarify our assumption that there exists a reversible goal state . This is indeed a limitation of our approach , which we note in the paper ( Section 4 paragraph 2 ) . We show experimentally that our method can be applied to a number of realistic tasks , such as locomotion and manipulation . Extending our work to tasks with irreversible goal states by resetting to intermediate goals is a great idea , and would make for interesting future work .

If one is committed to doing value- function or policy - based RL for an episodic task on a real physical system , then one has to come up with a way of resetting the domain for new trials . This paper proposes a good way of doing this : learn a policy for resetting at the same time as learning a policy for solving the problem . As a side effect , the Q values associated with the reset policy can be used to predict when the system is about to enter an unrecoverable state and " forbid " the action .

It is , of course , necessary that the domain be , in fact , reversible ( or , at least , that it be possible to reach a starting state from at least one goal state - - and it 's better if that goal state is not significantly harder to reach than other goal states .

There were a couple of places in the paper that seemed to be to be not strictly technically correct .

It says that the reset policy is designed to achieve a distribution of final states that is equivalent to a starting distribution on the problem . This is technically fairly difficult , as a problem , and I do n't think it can be achieved through standard RL methods . Later , it is clearer that there is a set of possible start states and they are all treated as goal states from the perspective of the reset policy . That is a start set , not a distribution . And , there 's no particular reason to think that the reset policy will not , for example , always end up returning to a particular state .

Another point is that training a set of Q functions from different starting states generates some kind of an ensemble , but I do n't think you can guarantee much about what sort of a distribution on values it will really represent . Q learning + function approximation can go wrong in a variety of ways , and so some of these values might be really gross over or under estimates of what can be achieved even by the policies associated with those values .

A final , higher - level , methodological concern is that , it seems to me , as the domains become more complex , rather than trying to learn two ( or more ) policies , it might be more effective to take a model - based approach , learn one model , and do reasoning to decide how to return home ( and even to select from a distribution of start states ) and / or to decide if a step is likely to remove the robot from the " resettable " space .

All this aside , this seems like a fairly small but well considered and executed piece of work . I 'm rating it as marginally above threshold , but I indeed find it very close to the threshold .

We thank AnonReviewer1 for noting the main goal of our paper and recognizing how we incorporate safety using the learned reset policy , and further thank the reviewer for finding our paper a “ well considered and executed piece of work . ” We ’ve addressed the concerns raised by the reviewer with clarifications in the main text , which we detail below .

Assumption that environment is reversible - This assumption is indeed a limitation of our approach , which we note in the paper ( Section 4 paragraph 2 ) . We have expanded this section to clarify this detail . We show experimentally that our method can be applied to a number of realistic tasks , such as locomotion and manipulation . Extending our work to tasks with irreversible goal states is a great idea , and would make for interesting future work .

Initial state distribution - You correctly note that the reset policy might always reset to the same state , thus failing to sample from the full initial state distribution . We have corrected this technical error in the revised version of the paper ( Section 4 , paragraph 2 ) by adding the additional assumption that the initial state distribution be unimodal and have narrow support . We also expanded the discussion of “ safe sets ” in Section 4.2 paragraph 1 to clarify the difference between the initial state distribution , the reset policy ’s reward , and the safe set . We also describe a method to detect if there is mismatch between the the initial state distribution and the reset policy final state distribution .

Q functions - We learn an ensemble of Q functions , each of which is a sampled from the posterior distribution over Q functions given the observed data . We expanded Section 4.4 paragraph 1 to note how this technique has been established in previous work ( “ Deep Exploration via Bootstrapped DQN ” [ Osband 2016 ] and “ UCB Exploration via Q-Ensembles ” [ Chen 2017 ] ) . In general , we are not guaranteed that samples from a distribution are close to its mean . However , our experiments on ensemble aggregation ( taking the min , mean or max over the Q functions ) had little effect on policy reward . If “ gross under / over-estimation ” had occurred , taking the min / max over the ensemble would have resulted in markedly lower reward . We expanded Appendix A paragraph 2 to explain this finding .

Model - based alternative - We appreciate the comment regarding a potential model - based alternative to our method . However , we are not aware of any past model - based methods for solving this task . We would be happy to attempt a comparison or add a discussion if the reviewer has a particular prior method in mind . The early aborts in our method provide one method of identifying irreversible states . A model - based alternative could also serve this function . We believe that our early aborts , which only require learning a single reset policy , are simpler than learning a model of the environment dynamics and hypothesize that our approach will scale better to complex environments .

( This delayed review is based on the deadline version of the paper . )

This paper proposes to learn by RL a reset policy at the same time that we learn the forward policy , and use the learned reset Q-function to predict and avoid actions that would prevent reset — an indication that they are " unsafe " in some sense .

This idea ( both parts ) is interesting and potentially very useful , particularly in physical domains where reset is expensive and exploration is risky . While I 'm sure the community can benefit from ideas of this kind , it really needs clearer presentations of such ideas . I can appreciate the very intuitive and colloquial style of the paper , however the discussion of the core idea would benefit from some rigor and formal definitions .

Examples of intuitive language that could be hiding the necessary complexities of a more formal treatment :

1 . In the penultimate paragraph of Section 1 , actions are described as " reversible " , while a stochastic environment may be lacking such a notion altogether ( i.e. there 's no clear inverse if state transitions are not deterministic functions ) .

2 . It 's not clear whether the authors suggest that the ability to reset is a good notion of safety , or just a proxy to such a notion . This should be made more explicit , making it clearer what this proxy misses : states where the learned reset policy fails ( whether due to limited controllability or errors in the policy ) , that are nonetheless safe .

3 . In the last paragraph of Section 3 , a reset policy is defined as reaching p_0 from * any * state . This is a very strong requirement , which is n't even satisfiable in most domains , and indeed the reset policies learned in the rest of the paper do n't satisfy it .

4 . What are p_0 and r_r in the experiments ? What is the relation between S_{reset} and p_0 ? Is there a discount factor ?

5 . In the first paragraph of Section 4.1 , states are described as " irreversible " or " irrecoverable " . Again , in a stochastic environment a more nuanced notion is needed , as there may be policies that take a long time to reset from some states , but do so eventually .

6 . A definition of a " hard " reset would make the paper clearer .

7 . After ( 1 ) , states are described as " allowed " . Again , preventing actions that are likely to hinder reset cannot completely prevent any given state in a stochastic environment . It also seems that ( 2 ) describes states where some allowed action can be taken , rather than states reachable by some allowed action . For both reasons , Algorithm 1 does not prevent reaching states outside S* , so what is the point of that definition ?

8 . The paper is not explicit about the learning dynamics of the reset policy . It should include a figure showing the learning curve of this policy ( or some other visualization ) , and explain how the reset policy can ever gain experience and learn to reset from states that it initially avoids as unsafe .

9 . Algorithm 1 is unclear on how a failed reset is identified , and what happens in such case — do we run another forward episode ? Another reset episode ?

Thank you for the comments ! It seems that all the concerns have to do with the writing in the paper and are straightforward to fix . We have addressed all the concerns raised about the paper in this review . Given that all issues have been addressed , we would appreciate if the reviewer could take another look at the paper .

1 . We have clarified our definition of reversible action in Section 1 paragraph 4 . For deterministic MDPs , we say an action is reversible if it leads to a state from which there exists a reset policy that can return to a state with high density under the initial state distribution . For stochastic MDPs , we say an action is reversible if the probability that an oracle reset policy that can reset from the next state is greater than some safety threshold . Note that definition for deterministic MDPs is a special case of the definition for stochastic MDPs .

2 . The ability of an oracle reset policy to reset is a good notion of safety . In our algorithm , we approximate this notion of safety , assuming that whether our learned reset policy can reset in N episodes is a good proxy for whether an oracle reset policy can reset . We have clarified Section 1 paragraph 4 to make this distinction clear . We also added Appendix B to discuss handling errors in Q value estimation . In this section , we describe how Leave No Trace copes with overestimates and underestimates of Q values .

3 . We have corrected this technical error in Section 3 paragraph 2 by redefining the reset policy as being able to reach p_0 from any state reached by the forward policy . That our learned reset policy only learns to reset from states reached by the forward policy is indeed a limitation of our method . However , note that early aborts help the forward policy avoid visiting states from which the reset policy is unable to reach p_0 .

4 . For the continuous control environments , the initial state distribution p_0 is uniform distribution centered at a “ start pose . ” We use a discount factor \gamma = 0.99 . Both details have been noted in Appendix F.3 paragraph 2 . The reset reward r_r is a hand - crafted approximation to p_0 . For example , in the Ball in Cup environment , r_r is proportional to the negative L2 distance from the ball to the origin ( below the cup ) . For cliff cheetah , r_r includes one term that is proportional to the distance of the cheetah to the origin , and another term indicating whether the cheetah is standing . S_{reset} is the set of states where r_r ( s ) is greater than 0.7 ( Appendix C.3 paragraph 2 )

5 . We have clarified Section 4.1 paragraph 1 to explain how our proposed algorithm handles both cases : states from which it is impossible to reset and states from which resetting would take prohibitively many steps . In both cases , the cumulative discounted reward ( and hence the value function ) will be low . By performing an early abort when the value function is low , we avoid both cases .

6 . We added a definition of “ hard reset ” to Section 4.2 paragraph 1 : A hard reset is an action that resamples that state from the initial state distribution . Hard resets are available to an external agent ( e.g. a human ) but not the learned agent .

7 . We acknowledge that the proposed algorithm does not guarantee that we never visit unsafe states . In Appendix A , we have added a proof that Leave No Trace would only visit states that are safe in expectation if it had access to the true Q values . Appendix A.3 discusses the approximations we make in practice that can cause Leave No Trace to visit unsafe states . Finally , Appendix B discusses how Leave No Trace handles errors incurred by over / under-estimates of Q values .

8 . Newly added Appendix D visualizes the training dynamics by plotting the number of time steps in each episode before an early abort . Initially , early aborts occur near the initial state distribution , so the forward episode lengths are quite short . As the reset policy improves , early aborts occur further from the initial state , as indicated by longer forward episode lengths . Newly added Appendix B discusses how Leave No Trace handles errors incurred by over / under-estimates of Q values . It describes how Leave No Trace learns that an “ unsafe ” state is actually safe .

9 . We detect failed resets in line 12 of Algorithm 1 . We have added a comment to help clarify this . When a failed reset is detected , a hard reset occurs ( line 13 ) .

This paper proposes the idea of having an agent learning a policy that resets the agent 's state to one of the states drawn from the distribution of starting states . The agent learns such policy while also learning how to solve the actual task . This approach generates more autonomous agents that require fewer human interventions in the learning process . This is a very elegant and general idea , where the value function learned in the reset task also encodes some measure of safety in the environment .

All that being said , I gave this paper a score of 6 because two aspects that seem fundamental to me are not clear in the paper . If clarified , I 'd happily increase my score .

1 ) * Defining state visitation / equality in the function approximation setting :* The main idea behind the proposed algorithm is to ensure that " when the reset policy is executed from any state , the distribution over final states matches the initial state distribution p_0 " . This is formally described , for example , in line 13 of Algorithm 1 .
The authors " define a set of safe states S_{reset} \subseteq S , and say that we are in an irreversible state if the set of states visited by the reset policy over the past N episodes is disjoint from S_{reset} . " However , it is not clear to me how one can uniquely identify a state in the function approximation case . Obviously , it is straightforward to apply such definition in the tabular case , where counting state visitation is easy . However , how do we count state visitation in continuous domains ? Did the authors manually define the range of each joint / torque / angle that characterizes the start state ? In a control task from pixels , for example , would the exact configuration of pixels seen at the beginning be the start state ? Defining state visitation in the function approximation setting is not trivial and it seems to me the authors just glossed over it , despite being essential to your work .

2 ) * Experimental design for Figure 5 * : This setup is not clear to me at all and in fact , my first reaction is to say it is wrong . An episodic task is generally defined as : the agent starts in a state drawn from the distribution of starting states and at the moment it reaches the goal state , the task is reset and the agent starts again . It does n't seem to be what the authors did , is that right ? The sentence : " our method learns to solve this task by automatically resetting the environment after each episode , so the forward policy can practice catching the ball when initialized below the cup " is confusion . When is the task reset to the " status quo " approach ? Also , let 's say an agent takes 50 time steps to reach the goal and then it decides to do a soft-reset . Are the time steps it is spending on its soft -reset being taken into account when generating the reported results ?


Some other minor points are :

- The authors should standardize their use of citations in the paper . Sometimes there are way too many parentheses in a reference . For example : " manual resets are necessary when the robot or environment breaks ( e.g. Gandhi et al. ( 2017 ) ) " , or " Our methods can also be used directly with any other Q-learning methods ( ( Watkins & Dayan , 1992 ; Mnih et al. , 2013 ; Gu et al. , 2017 ; Amos et al. , 2016 ; Metz et al. , 2017 ) ) "

- There is a whole line of work in safe RL that is not acknowledged in the related work section . Representative papers are :
[ 1 ] Philip S. Thomas , Georgios Theocharous , Mohammad Ghavamzadeh : High - Confidence Off-Policy Evaluation . AAAI 2015 : 3000-3006
[ 2 ] Philip S. Thomas , Georgios Theocharous , Mohammad Ghavamzadeh : High Confidence Policy Improvement . ICML 2015 : 2380-2388

- In the Preliminaries Section the next state is said to be drawn from s_{t + 1 } ~ P ( s '| s , a ) . However , this hides the fact the next state is dependent on the environment dynamics and on the policy being followed . I think it would be clearer if written : s_{t + 1 } ~ P ( s '| s , \pi( a|s ) ) .

- It seems to me that , in Algorithm 1 , the name ' Act ' is misleading . Should n't it be ' ChooseAction ' or ' Epsilon Greedy ' ? If I understand correctly , the function ' Act ' just returns the action to be executed , while the function ' Step ' is the one that actually executes the action .

- It is absolutely essential to depict the confidence intervals in the plots in Figure 3 . Ideally we should have confidence intervals in all the plots in the paper .

We thank Reviewer 4 for the comments and for finding our paper a “ very elegant and general idea . ” The main comments had to do with clarity - we have addressed these in the revised version . We would appreciate of the reviewer would reevaluate the paper given the new clarifications .

1 . ( State visitation ) We implement our algorithm in a way that avoids having to test whether two states are equal ( indeed , a challenging problem ) . In Equation 4 , we define the set of safe states S_{reset} implicitly using the reset reward function r_r ( s ) . In particular , we say a state is safe if the the reset reward is greater than some threshold ( 0.7 in our experiments ) . For example , in the pusher task , the reset reward is the distance from a certain ( x , y , z ) point , so S_{reset} is the set of points within some distance of this point . We added a comment to line 13 of the algorithm clarifying how we test if a state is in S_{reset} .

2 . ( Figure 5 ) We clarified our description of the environments in Section 6 paragraph 1 to note that the episode is not terminated when the agent reaches a goal state . For the experiment in Figure 5 , the ‘ forward - only ’ baseline and our method are non-episodic - the environment is never reset and no hard resets are used ( Section 6.1 ) . The ‘status quo ’ baseline is episodic , doing a hard reset every T time steps ( for ball in cup , T = 200 steps ) . We reworded the confusing sentence about our method as follows : “ In contrast , our method learns to solve this task by automatically resetting the environment after each attempt , so the forward policy can practice catching the ball without hard resets . ” All results in the paper include time steps for both the forward task and the reset task . We clarified this in Section 6.1 paragraph 1 . This highlights a strength of our method : even though the agent spends a considerable amount of time learning the reset task , it still learns to do the forward task in roughly the same number of total steps ( steps for forward task + steps for reset task ) .


Minor points :

1 . Citations . We ’ve removed the extra parenthesis for the Q-learning references . Generally , we use “ e.g. ” in citations when the citation is an example of the described behavior .

2 . Thanks for the additional references ! We ’ve included them in Section 2 paragraph 1 .

3 . We chose to separate out the policy from the transition dynamics . Action a_{t} is sampled from \pi( a_{t} | s_{t} ) and depends on the policy ; next state s_{t} is sampled from P ( s_{t + 1 } | s_{t} , a_{t} ) and depends on the transition dynamics .

4 . Good idea . We ’ve changed “ Act ( ) ” to “ ChooseAction ( ) ” in Algorithm 1 .

5 . For Figure 3 , we agree confidence intervals would be helpful . We ca n’t regenerate the plot in the next 24 hours before the rebuttal deadline , but will include confidence intervals in the camera-ready version .

The authors have undertaken a large scale empirical evaluation on sensitivity and generalization for DNNs within the scope of image classification . They are investigating the suitability of the F -norm of the input-output Jacobian in large scale DNNs and they evaluate sensitivity and generalization metrics across the input space , both on and of the data manifold . They convincingly present strong empirical evidence for the F-norm of the Jacobian to be predictive and informative of generalization of the DNN within the image classification domain .

The paper is well written . The problem is clearly presented and motivated . Most potential questions of a reader as well as interesting details are supplied by footnotes and the appendix .
The contributions are to my knowledge both novel and significant .
The paper seem to be technically correct . The methodology and conclusions are reasonable .
I believe that this is important work and applaud the authors for undertaking it . I hope that the interesting leads will be further investigated and that similar studies will be conducted beyond the scope of image classification . '
The research and further investigations would be strengthened if they would include a survey on the networks presented in the literature in a similar manner as the authors did with the generated networks within the presented study . For example compare networks from benchmark competitions in terms of sensitivity and generalization using the metrics presented here .

Please define " generalization gap " and show how you calculate / estimate it . The term us used differently in much of the machine learning literature ( ? ) . Given this and that the usually sought after generalization error is unobtainable due to the unknown joint distribution over data and label , it is necessary to clarify the precise meaning of " generalization gap " and how you calculated it . I intuitively understand but I am not sure that the metric I have in mind is the same as the one you use . Such clarification will also improve the accessibility for a wider audience .

Figure 4 :
I find Figure 4 : Center a bit confusion . Is it there to show where on the x- axis of Figure 4 : Top , the three points are located ? Does this mean that the points are not located at pi /3 , pi , 5 pi / 3 as indicated in the figure and the vertical lines of the figure grid ? If it is not , then is it maybe possible to make the different sub-figure in Figure 4 more distinctive , as to not visually float into each other ?

Figure 5 :
The figure makes me curious about what the regions look like close to the training points , which is currently hidden by the content of the inset squares . Maybe the square content can be made fully transparent so that only the border is kept ? The three inset squares could be shown right below each sub-figure , aligned at the x- axis with the respective position of each of the data points .


( 1 )
>> Please define " generalization gap " and show how you calculate / estimate it . The term us used differently in much of the machine learning literature ( ? ) . Given this and that the usually sought after generalization error is unobtainable due to the unknown joint distribution over data and label , it is necessary to clarify the precise meaning of " generalization gap " and how you calculated it . I intuitively understand but I am not sure that the metric I have in mind is the same as the one you use . Such clarification will also improve the accessibility for a wider audience .

Thank you for the comment , we have included the definition in the Appendix A.4 in the second revision .

We define generalization gap as the difference between train and test accuracy on the whole train and test sets . Precisely ,

Generalization gap = ( # correctly classified training images ) / ( 50K ) - ( # correctly classified test images ) / ( 10K ) .

( all training and test sets are of size 50 K and 10 K respectively )
------------------------------------------------------------------------------------------------------------------------------------------------------
( 2 )
>> Figure 4 :
I find Figure 4 : Center a bit confusion . Is it there to show where on the x- axis of Figure 4 : Top , the three points are located ? Does this mean that the points are not located at pi /3 , pi , 5 pi / 3 as indicated in the figure and the vertical lines of the figure grid ? If it is not , then is it maybe possible to make the different sub-figure in Figure 4 more distinctive , as to not visually float into each other ?

We apologize for the confusing figure . Central figure ( now top in the second revision ) is there for the exact reason you mention ( to show where the points are located , which indeed should be pi /3 , pi , 5 pi / 3 ) . We have separated the subfigures further and aligned the digits with the values of pi /3 , pi and 5 pi / 3 precisely in the second revision .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 3 )
>> Figure 5 :
The figure makes me curious about what the regions look like close to the training points , which is currently hidden by the content of the inset squares . Maybe the square content can be made fully transparent so that only the border is kept ? The three inset squares could be shown right below each sub-figure , aligned at the x- axis with the respective position of each of the data points .

Thank you for the interesting suggestion ! We have produced the requested figures with only the boundaries overlayed :
-- before training : https://www.dropbox.com/s/14xcvoval4eluz4/boundaries_before_transparent.png?dl=1;
-- after training : https://www.dropbox.com/s/lj7y9eimnqw0lsd/boundaries_after_transparent.png?dl=1.

We do not observe any special behavior at such scale . This is in agreement with Figure 3 ( bottom ) , showing that the density of transitions around the points changes slowly .
------------------------------------------------------------------------------------------------------------------------------------------------------

Thank you for the detailed review and helpful comments ! We are pleased that you found our work useful .

This work investigates sensitivity and generalisation properties of neural networks with respect to a number of metrics aimed at quantifying the robustness with respect to data variability , varying parameters and representativity of training / testing data .
The validation is based on the Jacobian of the network , and in the detection of the “ transitions ” associated to the data space . These measures are linked , as the former quantifies the sensitivity of the network respect to infinitesimal data variations , while the latter quantifies the complexity of the modelled data space .
The study explores a number of experimental setting , where the behaviour of the network is analysed on synthetic paths around training data , from pure random data points , to curves interpolating different / same data classes .
The experimental results are performed on CIFAR10 , CIFAR100 , and MNIST . Highly - parameterised networks seem to offer a better generalisation , while lower Jacobian norm are usually associated to better generalisation and fewer transitions , and can be obtained with data augmentation .

The paper proposes an interesting analysis aimed at the empirical exploration of neural network properties , the proposed metrics provide relevant insights to understand the behaviour of a network under varying data points .

Major remarks .

The proposed investigation is to my opinion quite controversial . Interesting data variation does not usually corresponds to linear data change . When considering the linear interpolation of training data , the authors are actually creating data instances not compatible with the original data source : for example , the pixel - wise intensity average of digits is not a digit anymore . For this reason , the conclusions drawn about the model sensitivity are to my opinion based a potentially uninteresting experimental context . Meaningful data variation can be way more complex and high -dimensional , for example by considering spatial warps of digits , or occlusions and superpositions of natural images . This kind of variability is likely to correspond to real data changes , and may lead to more reliable conclusions . For this reason , the proposed results may provide little indications of the true behaviour of the models data in case of meaningful data variations .

Moreover , although performed within a cross-validation setting , training and testing are still applied to the same dataset . Cross -validation does n’t rule out validation bias , while it is also known that the classification performance significantly drops when applied to independent “ unseen ” data , provided for example in different cohorts . I would expect that highly parameterised models would lead to worse performance when applied to genuinely independent cohorts , and I believe that this work should extend the investigation to this experimental setting .

Minor remarks .

The authors should revise the presentation of the proposed work . The 14 figures ( ! ) of main text are not presented in the order of appearance . The main one ( figure 1 ) is provided in the first paragraph of the introduction and never discussed in the rest of the paper .



( 2 )
>> Moreover , although performed within a cross-validation setting , training and testing are still applied to the same dataset . Cross -validation does n’t rule out validation bias , while it is also known that the classification performance significantly drops when applied to independent “ unseen ” data , provided for example in different cohorts . I would expect that highly parameterised models would lead to worse performance when applied to genuinely independent cohorts , and I believe that this work should extend the investigation to this experimental setting .

We would like to address your concern . Could you please expand on what you mean by " genuinely independent cohorts " ?

Are you concerned that MNIST images in train and test may have digits written by same individuals ? If so , we believe this should be less of a problem in Fashion -MNIST , CIFAR10 , and CIFAR100 datasets where we see similar results .

We would like to provide some additional information regarding our training and evaluation procedure , hoping that this might address your concern . Train and test data are balanced random 50 K and 10K i.i.d. samples respectively . We train all our networks for a large number of gradient steps ( 2^18 or 2^19 when applicable ) without any regularization / validation / early stopping . We then evaluate all quantities mentioned in the paper on the whole 50 K or 10 K datasets respectively , when applicable .

Please let us know if the above answers your question . If no , we will be happy to expand on it once we fully understand your request .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 3 )
>> The authors should revise the presentation of the proposed work . The 14 figures ( ! ) of main text are not presented in the order of appearance .

Thank you , we have rearranged the figures in the order of presentation in the second revision .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 4 )
>> The main one ( figure 1 ) is provided in the first paragraph of the introduction and never discussed in the rest of the paper .

While we acknowledge the importance of this figure , we consider it as motivation for our study ( ultimately leading to key results presented in figures 3 [ sensitivity along a trajectory intersecting the data manifold ] and 4 in the first paper revision / 9 in the second revision [ Jacobian norm correlating with generalization ] ) , which is why it is only mentioned in the introduction .
------------------------------------------------------------------------------------------------------------------------------------------------------

We thank you for the careful and insightful review . We believe that we were able to address your concerns both in terms of rebuttal and new experimental evidence , and hope that you will raise your score as a result .


( 1 )
>> The proposed investigation is to my opinion quite controversial . Interesting data variation does not usually corresponds to linear data change . When considering the linear interpolation of training data , the authors are actually creating data instances not compatible with the original data source : for example , the pixel - wise intensity average of digits is not a digit anymore . For this reason , the conclusions drawn about the model sensitivity are to my opinion based a potentially uninteresting experimental context . Meaningful data variation can be way more complex and high -dimensional , for example by considering spatial warps of digits , or occlusions and superpositions of natural images . This kind of variability is likely to correspond to real data changes , and may lead to more reliable conclusions . For this reason , the proposed results may provide little indications of the true behaviour of the models data in case of meaningful data variations .

We believe there to be two potential concerns in this remark and shall address them separately .


1.A ) The concern of linear interpolation of the training data being incompatible with the original data source , and as such our sampling trajectories in section 4.1 not containing meaningful data variations . This is by design ! Such a trajectory will indeed lie mostly outside of the data manifold , yet intersect it in 3 points . Measuring our metrics along these trajectories allow us to draw conclusions about the behavior of a trained neural network near the data manifold and away from it ( Figure 3 ) .

Otherwise , in the rest of the paper , transitions are counted along a trajectory interpolating horizontal translations of an image ( see definition in section 3.2 ) , which do represent a complex curve along a meaningful data variation ( translation ) . We agree that analyzing a richer set of transformations within the data manifold would be interesting . However , characterizing data variation is a complex field of study , and we believe that translations provide a well defined and tractable set of transformations which typically remain within the data distribution .

Finally , the best - performing metric of our work , that is the Frobenius norm of the Jacobian ( see definition in section 3.1 ) is averaged over individual data points and does not hinge on any kind of interpolation !


1.B ) We can alternatively interpret your remark as being skeptical regarding the Jacobian norm reflecting sensitivity with respect to meaningful data variations . Indeed , it does not , and as described in section 3.1 , it reflects sensitivity to isotropic perturbations .

Motivated by this concern , we have performed an additional experiment measuring the norm of the Jacobian of the output with respect to horizontal shift , hence sensitivity to a meaningful data variation ( translation ) along the data manifold ( bottom part of the figure , in contrast to the top ) : https://www.dropbox.com/s/cmh2s3eqb7vihj9/horizontal_translation_jacobian.pdf?dl=1

We observe an effect qualitatively similar to ( yet less noisy than ) when the Frobenius norm of the input-output Jacobian is considered ( top part of the figure , or Figure 8 in the first paper revision / Figure 9 ( bottom ) in the second revision ) .

However , we believe our current results still provide a useful insight . Different datasets have different axes of data variations . Which those are may not always be clear : indeed , understanding all the meaningful axes of data variations would essentially amount to solving the problem of generating natural images . Yet in the absence of preconceived notions of what directions are meaningful , the input-output Jacobian can be a universal metric indicative of generalization , as evidenced by our experiments on 4 datasets ( MNIST , Fashion -MNIST , CIFAR10 , CIFAR100 ) which definitely have different notions of meaningful data variations .
------------------------------------------------------------------------------------------------------------------------------------------------------

This paper proposes an analysis of the robustness of deep neural networks with respect to data perturbations .

* Quality *
The quality of exposition is not satisfactory . Actually , the paper is pretty difficult to evaluate at the present stage and it needs a drastic change in the writing style .

* Clarity *
The paper is not clear and highly unstructured .

* Originality *
The originality is limited for what regards Section 3 : the proposed metrics are quite standard tools from differential geometry . Also , the idea of taking into account the data manifold is not brand new since already proposed in “ Universal Adversarial Perturbation ” at CVPR 2017 .

* Significance *
Due to some flaws in the experimental settings , the relevance of the presented results is very limited . First , the authors essentially exploit a customized architecture , which has been broadly fine - tuned regarding hyper - parameters , gating functions and optimizers . Why not using well established architectures ( such as DenseNets , ResNets , VGG , AlexNet ) ?
Moreover , despite having a complete portrait of the fine-tuning process is appreciable , this compromises the clarity of the figures which are pretty hard to interpret and absolutely not self - explanatory : probably it ’s better to only consider the best configuration as opposed to all the possible ones .
Second , authors assume that circular interpolation is a viable way to traverse the data manifold . The reviewer believes that it is an over-simplistic assumption . In fact , it is not guaranteed a priori that such trajectories are geodesic curves so , a priori , it is not clear why this could be a sound technique to explore the data manifold .

CONS :
The paper is difficult to read and needs to be thoroughly re-organized . The problem is not stated in a clear manner , and paper ’s contribution is not outlined . The proposed architectures should be explained in detail . The results of the sensitivity analysis should be discussed in detail . The authors should explain the approach of traversing the data manifold with ellipses ( although the reviewer believes that such approach needs to be changed with something more principled ) . Figures and results are not clear .
The authors are kindly asked to shape their paper to match the suggested format of 8 pages + 1 of references ( or similar ) . The work is definitely too long considered its quality . Additional plots and discussion can be moved to an appendix .
Despite the additional explanation in Footnote 6 , the graphs are not clear . Probably authors should avoid to present the result for each possible configuration of the hyper- parameters , gatings and optimizers and just choose the best setting .
Apart from the customized architecture , authors should have considered established deep nets , such as DenseNets , ResNets , VGG , AlexNet .
The idea of considering the data manifold within the measurement of complexity is a nice claim , which unfortunately is paired with a not convincing experimental analysis . Why ellipses should be a proper way to explore the data manifold ? In general , circular interpolation is not guaranteed to be geodesic curves which lie on the data manifold .

Minor Comments :
Sentence to rephrase : “ We study common in the machine learning community ways to ... ”
Please , put the footnotes in the corresponding page in which it is referred .
The reference to ReLU is trivially wrong and need to be changed with [ Nair & Hinton ICML 2010 ]

** UPDATED EVALUATION AFTER AUTHORS ' REBUTTAL **
We appreciated the effort in providing specific responses and we also inspected the updated version of the paper . Unfortunately , despite the authors ' effort , the reviewer deems that the conceptual issues that have been highlighted are still present in the paper which , therefore , is not ready for acceptance yet .


( 5 )
>> Second , authors assume that circular interpolation is a viable way to traverse the data manifold . The reviewer believes that it is an over-simplistic assumption . In fact , it is not guaranteed a priori that such trajectories are geodesic curves so , a priori , it is not clear why this could be a sound technique to explore the data manifold .

The interpretation of the reviewer is incorrect . Nowhere in the text did we claim to traverse the data manifold with an ellipse .

Ellipses intersecting the data manifold at 3 specific points are studied only in section 4.1 to track our metrics along a trajectory with respect to how close a point to the data manifold is , which is an appropriate choice for this purpose .

In the rest of the paper , the transitions metric is computed along a trajectory that interpolates horizontal translations of an image ( as stated in section 3.2 ) , which is not an ellipse and generally lies within the data manifold for translation invariant datasets . In fact we augment the training data with translations for one experiment .

We have edited the toy illustration of such a trajectory in Figure 2 ( right ) to make this more clear in the second revision .

In addition , we never claimed any of our curves to be geodesics and fail to see how this property is necessary for the purpose of our work .

Finally , the relationship between the Frobenius norm of the network Jacobian and generalization is averaged over individual data points and has nothing to do with traversing the data manifold .

We thank the reviewer for the useful feedback and will improve the exposition in the next revision .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 6 )
>> The authors should explain the approach of traversing the data manifold with ellipses ( although the reviewer believes that such approach needs to be changed with something more principled ) .

Please see discussion about trajectories above ( 5 ) .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 7 )
>> Probably authors should avoid to present the result for each possible configuration of the hyper- parameters , gatings and optimizers and just choose the best setting .

Please see the relevant discussion above ( 4 ) .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 8 )
>> Apart from the customized architecture , authors should have considered established deep nets , such as DenseNets , ResNets , VGG , AlexNet .

Please see relevant discussion above ( 3 ) .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 9 )
>> The idea of considering the data manifold within the measurement of complexity is a nice claim , which unfortunately is paired with a not convincing experimental analysis . Why ellipses should be a proper way to explore the data manifold ? In general , circular interpolation is not guaranteed to be geodesic curves which lie on the data manifold .

Please see the relevant discussion above ( 5 ) .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 10 )
>> Sentence to rephrase : “ We study common in the machine learning community ways to ... ”

Thank you , we have changed the wording in the second revision .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 11 )
>> Please , put the footnotes in the corresponding page in which it is referred .

Thank you , we have fixed the footnotes in the second revision .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 12 )
>> The reference to ReLU is trivially wrong and need to be changed with [ Nair & Hinton ICML 2010 ]

Thank you for pointing this out , we have fixed the reference in the second revision .
------------------------------------------------------------------------------------------------------------------------------------------------------

We thank the reviewer for the detailed feedback ! We will work to improve the clarity of our work in the next revision .


( 1 )
>> The originality is limited for what regards Section 3 : the proposed metrics are quite standard tools from differential geometry .

We did not claim to propose novel metrics anywhere in the paper ; on the contrary , we cite prior work that used / introduced them in section 2 .

The novelty of this work is in performing extensive evaluation of these metrics on trained neural networks and relating them to generalization ( which is also emphasized multiple times in section 2 ) .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 2 )
>> Also , the idea of taking into account the data manifold is not brand new since already proposed in “ Universal Adversarial Perturbation ” at CVPR 2017 .

Thank you for the very interesting reference ! We now cite it in related work in the second revision .

However , we disagree with the claim that this work diminishes the novelty of ours . Nowhere in the paper did we assert to be the first to " take into account the data manifold " . Our claim was to compare behavior of a trained network on and off the data manifold ( see last paragraph of section 2 ) .

A great deal of previous research has examined the statistics of natural stimuli ( e.g. dating at least as far back as Barlow , 1959 ) . The specific paper you reference is a very interesting exploration of universal adversarial perturbations . However , it does not investigate the behavior of the network outside of the data manifold .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 3 )
>> Due to some flaws in the experimental settings , the relevance of the presented results is very limited . First , the authors essentially exploit a customized architecture , which has been broadly fine - tuned regarding hyper - parameters , gating functions and optimizers . Why not using well established architectures ( such as DenseNets , ResNets , VGG , AlexNet ) ?

Thank you for your suggestion ! Evaluating our metrics on the proposed architectures is indeed a very interesting direction for future research .

However , we disagree that this establishes a flaw in our experiments . On the contrary , the architectures you suggest are ones that are extremely customized and fine -tuned , while the set of fully - connected ( FC ) architectures we consider are quite generic .

The reason for considering FC networks in this work was to perform a large - scale evaluation of the computationally - intensive metrics in a very wide variety of settings , to understand the resulting distribution over network behaviors , rather than measuring the behavior in a small number of hand - tuned scenarios ( while extending the hundreds of thousands of experiments performed in this work onto complex convolutional architectures is beyond the scope of this work ) .

We further emphasize that :
-- Almost all networks considered in this work have achieved 100 % accuracy on the whole training set .
-- Best-performing configurations yield test accuracies competitive with state - of - the- art results for FC networks .
-- We evaluate our results on 4 different datasets of varying complexity ( MNIST , Fashion -MNIST , CIFAR10 , CIFAR100 ) .
For this reason we believe our work presents results that are both comprehensive and appropriate to different generalization regimes .

We will make sure to improve our presentation and emphasize the above in our next revision .
------------------------------------------------------------------------------------------------------------------------------------------------------
( 4 )
>> Moreover , despite having a complete portrait of the fine-tuning process is appreciable , this compromises the clarity of the figures which are pretty hard to interpret and absolutely not self - explanatory : probably it ’s better to only consider the best configuration as opposed to all the possible ones .

This change would run counter to a principal strength of the paper -- that we examine the distribution over network behavior for a wide range of hyper - parameters and datasets ( over thousands of experiments ) . Showing only the best - performing models would completely obfuscate the insights drawn from this analysis and significantly detract from our paper .

We will make sure to revise our presentation to make this point more clear .
------------------------------------------------------------------------------------------------------------------------------------------------------

Thank you for the updated evaluation and for taking the time to review the revised submission .

We believe we have addressed all of the original concerns in our rebuttal . We would greatly appreciate any further feedback on what specific " conceptual issues " remain unresolved , so that we can use your feedback to further improve our paper .

We have also uploaded a new revision that further improves clarity and exposition , especially related to areas that you flagged as unclear .

SUMMARY

The model evaluates symbolic algebraic / trigonometric equalities for validity , with an output unit for validity level at the root of a tree of LSTM nodes feeding up to the root ; the structure of the tree matches the parse tree of the input equation and the type of LSTM cell at each node matches the symbol at that node in the equation : there is a different cell type for each symbol . It is these cell types that are learned . The training data includes labeled true and false algebraic / trigonometric identities ( stated over symbols for variables ) as well as function - evaluation equalities such as " tan ( 0.28 ) = 0.29 " and decimal - expansion equations like " 0.29 = 2*10 ^ ( - 1 ) + 9*10 ^ ( - 2 ) " . I believe continuous values like " 0.29 " in the preceding expressions are encoded as the literal value of a single unit ( feeding into an embedding unit of type W_{num} ) , whereas the symbols proper ( including digit numerals ) are encoded as 1 - hot vectors ( feeding into an embedding unit of type W_{symb} ) .
Performance is at least 97 % when testing on unseen expressions of the same depth ( up to 4 ) as the training data . Performance when trained on 3 levels ( among 1 - 4 ) and testing on generalization to the held - out level is at least 96 % when level 2 is held out , at least 92 % when level 4 is withheld . Performance degrades ( even on symbolic identities ) when the function - evaluation equalities are omitted , and degrades when LSTM cells are replaced by plain RNN cells . The largest degradation is when the tree structure is replaced ( presumably ) by a sequence structure .
Performance was also tested on a fill - in- the-blank test , where a symbol from a correct equation was removed and all possible replacements for that symbol with expressions of depth up to 2 were tested , then ranked by the resulting validity score from the model . From the graph it looks like an accuracy of about 95 % was achieved for the 1 - best substituted expression ( accuracy was about 32 % for a sequential LSTM ) .

WEAKNESSES

* The title is misleading ; " blackbox function evaluation " does not suggest what is intended , which is training on function - evaluation equations . The actual work is more interesting than what the title suggests .
* The biggest performance boost ( roughly 15 % ) arises from use of the tree structure , which is given by an oracle ( implemented in a symbolic expression parser , presumably ) : the network does not design its own example - dependent structure .
* What does the sympy baseline mean in Table 2 ? We are only told that sympy is a " symbolic solver " . Yet the sympy performance scores are in the 70 - 80 % range . If the solver ’s performance is that weak , why is it used during generation of training data to determine the validity of possible equations ?
* Given that this is a conference on " learning representations " it would have been nice to see at least a * little * examination of the learned representations . It would be easy to do some interesting tests . How well does the vector embedding for " 2*10 ^ ( - 1 ) + 9*10 ^ ( - 2 ) " match the vector for the real value 0.29 ? W_{num} embeds a continuum of real values in R^d : what is this 1 - dimensional embedding manifold like ? How do the embeddings of different integers provided by W_{sym} relate to one another ? My rating would have been higher had there been some analysis of the learned representations .
* We are told only that the " hidden dimension … varies " ; it would be nice if the text or results tables gave at least some idea of what magnitude of embedding dimension we ’re talking about .

STRENGTHS

The weaknesses above notwithstanding , this is a very interesting piece of work with impressive results .
* The number of functions learned , 28 , is a quantum jump from previous studies using 8 or fewer functions .
* It is good to see the power of training the same system to learn the semantics of functions from the equations they satisfy AND from the values they produce .
* The inclusion of decimal - expansion equations for relating numeral embeddings to number embeddings is clever .
* The general method used for randomly generating a non-negligible proportion of true equations is useful .
* The evaluation of the model is thorough and clear .
* In fact the exposition in the paper as a whole is very clear .

We would like to thank the reviewer for the constructive feedback . Here is our response :

I believe continuous values like " 0.29 " in the preceding expressions are encoded as the literal value of a single unit ( feeding into an embedding unit of type W_{num} ) , whereas the symbols proper ( including digit numerals ) are encoded as 1 - hot vectors ( feeding into an embedding unit of type W_{symb} ) .
This understanding is correct

Misleading title : We can change the title to “ Combining Symbolic and Function Evaluation Expressions for Training Neural Programs ” if the reviewer feels that this reflects better what we are doing .

Example -dependent structure : If I understand the question correctly , the network ’s structure is indeed example dependent and is indicated by the input equation . We have not used any symbolic expression parser to construct the equation parses . The neural network ’s structure is dynamic and its structure depends on the parse of the input equation that comes naturally with it . We hypothesize that the input equation ’s expression tree in indeed the best compositionality one can obtain as it represents the natural composition of the equations . In fact , if we had access to this clear composition tree in NLP tasks , the models would have been more accurate . many programming languages have access to the tree structure of the program . Without the tree , the problem will be very challenging and we would investigate learning the structure in the future .

Sympy performance :
We used Sympy to check the correctness of the generated equations . If the correctness of an equation is verified by sympy then it is added to the dataset . Therefore , sympy has a 100 % accuracy for predicting correct equalities in our dataset . It is only the incorrect equalities that cause Sympy ’s performance to drop as we explain below .
In order to assess Sympy ’s performance , we give each equation to Sympy . It either returns , True , or False , or returns the equation in its original form ( indicating that sympy is incapable of deciding whether the equality holds or not ) . Let ’s call this the Unsure class . In the reported Sympy accuracies we have treated the Unsure class as a miss-classification . Another approach is to perform majority class prediction on the Unsure class . This will result in the same number as shown in table 2 since our majority class is True ( 50.24 % are correct ) . In order to be fair , we can also treat the Unsure class as a fair coin toss and report half as correctly predicted . If we do this , the Sympy row in table 2 will be updated with these numbers : 90.81 & - & 90.00 & 94.46 & 91.45 & 84.54 . If the reviewer believes that this approach is better we can update these numbers in Table 2 .
We have also added this explanation to the paper . Given some other solver as oracle for adding equations , it would have been interesting to evaluate the accuracy of sympy for predicting the correctness of those equations .

Examination of learned representations : This is a very good suggestion . In order to examine the learned representations we have depicted Figure 4 . In order to see how close the vector embedding of an expression , say cos ( 0.8 ) is to the vector embedding of 0.69 , we have presented Figure 4 ( c ) which is similar to what the reviewer is suggesting about the decimal representation , if we understand correctly . We have also performed a minor modification as explained in the revisions , which makes it easier to interpret the learned representation . Our W_num block encodes the representation of floating point numbers like 0.29 . We have trained a decoder , W_num^{ - 1 } , that decodes the d-dimensional representation of 0.29 back to the actual number . Moreover , Table 4 ( b ) indicates that the vector embeddings of tan ( x ) for x close to 0.28 results in vector embeddings that are close to 0.29 which is the correct value for tan ( 0.28 ) with precision 2 .

Hidden dimension : The hidden dimension is chosen from set { 10 , 20 , 50} . This has been added to the paper as explained in the revisions .

This paper proposes a model that predicts the validity of a mathematical expression ( containing trigonometric or elementary algebraic expressions ) using a recursive neural network ( TreeLSTM ) . The idea is to take the parse tree of the expression , which is converted to the recursive neural network architecture , where weights are tied to the function or symbol used at that node . Evaluation is performed on a dataset generated specifically for this paper .

The overall approach described in this paper is technically sound and there are probably some applications ( for example in online education ) . However the novelty factor of this paper is fairly low — recursive neural nets have been applied to code / equations before in similar models . See , for example , “ Learning program embeddings to propagate feedback on student code ” by Piech et al , which propose a somewhat more complex model applied to abstract syntax trees of student written code .

I ’m also not completely sure what to make of the experimental results . One weird thing is that the performance does not seem to drop off for the models as depth grows . Another strange thing is that the accuracies reported do not seem to divide the reported test set sizes ( see , e.g. , the depth 1 row in Table 2 ) . It would also be good to discuss the Sympy baseline a bit — being symbolic , my original impression was that it would be perfect all the time ( if slow ) , but that does n’t seem to be the case , so some explanation about what exactly was done here would help . For the extrapolation evaluation — evaluating on deeper expressions than were in the training set — I would have liked the authors to be more ambitious and see how deep they could go ( given , say , up to depth 3 training equations ) .



We would like to thank the reviewer for the constructive feedback . Here is our response :

the novelty factor of this paper is fairly low : We would like to emphasize that the main contribution of the paper is combining high - level symbolic and function evaluation expressions , which none of the existing work has done . We are proposing a new framework for modeling mathematical equations . This framework includes defining new problems , equation validation and equation completion , as well as introducing a dataset generation method and a recursive neural network that combines these function evaluation and symbolic expressions . Indeed treeLSTMs are not new , however , using them to combine both the symbolic and function evaluation expressions by incorporating different loss functions and terminal types is novel .

Dataset only for this paper : This dataset makes sure that there is a good coverage of the properties of different mathematical functions which is critical for proper training of the model . It contains correct and incorrect math equations . An alternative approach that extracts equations online , unfortunately , does not result in enough equations for training . Moreover , we cannot obtain any negative equations . Our proposed dataset generation results in a large scale data of any mathematical domain given a small number of axioms from that domain .

Performance drop :
One weird thing is that the performance does not seem to drop off for the models as depth grows
-- It is indeed interesting that the performance drops only marginally as depth grows in Table 2 . The reason is , In this experiment we are assessing generalizability to unseen equations and not unseen depths . Therefore , the training data has access to equations of all depths . The results indicate that the model has learned to predict equations of all depths well . It also indicates that the dataset has a good coverage of equations of all depths that ensures good performance across all depths .
-- Furthermore , the extrapolation experiment ( table 3 ) shows that generalization to equations of smaller depth is easier than generalization to higher depth equations .
Another strange thing is that the accuracies reported do not seem to divide the reported test set sizes ( see , e.g. , the depth 1 row in Table 2 )
There was a typo in table 2 in row “ test set size ” and column “ depth 1 ” , which we have addressed in the revision . The correct number is 5 + 3 instead of 5 + 2 . Thank you for pointing this out .

Sympy performance :
-- We used Sympy to check the correctness of the generated equations . If the correctness of an equation is verified by sympy then it is added to the dataset . Therefore , sympy has a 100 % accuracy for predicting correct equalities in our dataset . It is only the incorrect equalities that cause Sympy ’s performance to drop as we explain below .
-- In order to assess Sympy ’s performance , we give each equation to Sympy . It either returns , True , or False , or returns the equation in its original form ( indicating that sympy is incapable of deciding whether the equality holds or not ) . Let ’s call this the Unsure class . In the reported Sympy accuracies we have treated the Unsure class as a miss-classification . Another approach is to perform majority class prediction on the Unsure class . This will result in the same number as shown in table 2 since our majority class is True ( 50.24 % are correct ) . In order to be fair , we can also treat the Unsure class as a fair coin toss and report half as correctly predicted . If we do this , the Sympy row in table 2 will be updated with these numbers : 90.81 & - & 90.00 & 94.46 & 91.45 & 84.54 . If the reviewer believes that this approach is better we can update these numbers in Table 2 .
-- We have also added this explanation to the paper . Given some other solver as oracle for adding equations , it would have been interesting to evaluate the accuracy of sympy for predicting the correctness of those equations .

Accuracy on equations of larger depth given up to depth 3 equations : This is indeed a very interesting test . We performed this test for training equations of up to depth 3 and the performance on equations of depth 5 is 89 % and for equations of depth 6 is 85 % for the tree LSTM model + data . We will add the full results of this extra experiment to the paper .

This paper considers the task of learning program embeddings with neural networks with the ultimate goal of bug detection program repair in the context of students learning to program . Three NN architectures are explored , which leverage program semantics rather than pure syntax . The approach is validated using programming assignments from an online course , and compared against syntax based approaches as a baseline .

The problem considered by the paper is interesting , though it 's not clear from the paper that the approach is a substantial improvement over previous work . This is in part due to the fact that the paper is relatively short , and would benefit from more detail . I noticed the following issues :

1 ) The learning task is based on error patterns , but it 's not clear to me what exactly that means from a software development standpoint .
2 ) Terms used in the paper are not defined / explained . For example , I assume GRU is gated recurrent unit , but this is n't stated .
3 ) Treatment of related work is lacking . For example , the Cai et al . paper from ICLR 2017 is not considered
4 ) If I understand dependency reinforcement embedding correctly , a RNN is trained for every trace . If so , is this scalable ?

I believe the work is very promising , but this manuscript should be improved prior to publication .

Thank you for the review . We clarify below the four specific points raised .

1 . By “ error patterns ” , we mean different types of errors that students made in their programming submissions . This work focuses on providing quality feedback to students . It may be extended in future work to help software developers , where error patterns can correspond to different classes of errors that developers may make . However , it is not the consideration for the current version of the paper .

2 . We will clarify all abbreviations and terms used in the paper .

Yes , GRU is Gated Recurrent Unit .

3 . The results of our latest experiments clearly indicate that this work substantially improves prior work . We briefly highlight the main reasons below . First , there are fundamental differences between the syntactic program traces explored in prior work ( Reed & De Freitas ( 2015 ) ) and the “ semantic program traces ” considered in our work . Consider the example in Figure 1 . According to Reed & De Freitas ( 2015 ) , the two sorting algorithms will have an identical representation with respect to statements that modify the variable A :

A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp

Our representation , on the other hand , can capture their semantic differences in terms of program states by also only considering the variable A :

Bubble Insertion
[ 5 , 5,1,4 , 3 ] [ 5 , 5 , 1 , 4,3 ]
[ 5,8,1,4 , 3 ] [ 5 , 8, 1, 4,3 ]
[ 5, 1,1,4 , 3 ] [ 5 , 1, 1, 4,3 ]
[ 5,1,8,4 , 3 ] [ 5 , 1, 8, 4,3 ]
[ 1,1,8,4 , 3 ] [ 5 , 1, 4, 4,3 ]
[ 1,5,8,4 , 3 ] [ 5 , 1, 4 , 8,3 ]
[ 1,5,4, 4 , 3 ] [ 5 , 1, 4,3,3 ]
[ 1,5,4, 8, 3 ] [ 5 , 1, 4,3 , 8 ]
[ 1,4,4, 8, 3 ] [ 1 , 1, 4,3 , 8 ]
[ 1,4,5 , 8, 3 ] [ 1 , 5, 4,3, 8 ]
[ 1,4,5,3 , 3 ] [ 1 , 4, 4,3, 8 ]
[ 1,4,5,3 , 8 ] [ 1 ,4 , 5 , 3 , 8 ]
[ 1,4,3,3 , 8 ] [ 1 ,4,3,3 , 8 ]
[ 1,4,3 , 5 , 8 ] [ 1 ,4, 3 , 5 , 8 ]
[ 1,3,3 , 5 , 8 ] [ 1 ,3, 3 , 5 , 8 ]
[ 1,3,4 , 5 , 8 ] [ 1 ,3 , 4 , 5 , 8 ]

This example also illustrates concretely the point made in Section 1 that minor syntactic differences can lead to signiﬁcant semantic differences . Therefore , the approach of Reed & De Freitas is insufficient to capture such semantic differences . As another example , consider the following two programs :

static void Main ( string [ ] args )
{
string str = String . Empty ;
int x = 0 ;
x ++;
}

static void Main ( string [ ] args )
{
string s = " " ;
int y = 0 ;
y = y +1 ;
}

According to the representation proposed in Reed & De Freitas ( 2015 ) , the first program is represented as [ string str = String . Empty , int x = 0 , x + + ] , while the second represented as [ string s = " " , int y = 0 , y = y +1 ] . Although the two programs share the same semantics , they are represented differently due to syntactic variations . In contrast , our work captures the same semantic trace for both programs , i.e. , [ [ “ ” , NA ] , [ “ ” , 0 ] , [ “ ” , 1 ] ] .

To sum up , the embedding proposed in Reed & De Freitas ( 2015 ) is a syntactic representation , and cannot precisely capture a program ’s semantics and abstract away its syntactic redundancies . Consequently , the encoder will not be able to learn the true feature dimensions in the embeddings . We also performed additional experiments to contrast the two trace - based approaches . We used the same configuration of encoder ( cf. Section 5 ) to embed the syntactic traces on the same datasets for the same classification problem . The results are as follows :

Problems Reed & De Freitas ( 2015 ) Token AST Dependency Model
Print Chessboard 26.3 % 16.8 % 16.2 % 99.3 %
Count Parentheses 25.5 % 19.3 % 21.7 % 98.8 %
Generate Binary Digits 23.8 % 21.2 % 20.9 % 99.2 %

Although syntactic traces result in better accuracy than Token and AST , they are still significantly worse than semantic embeddings introduced in our work .

Our revision will include the representation proposed in Reed & De Freitas ( 2015 ) for the example programs in Figure 1 . It will also include the experimental setup ( in Section 5 ) and the new results ( in a new column of Table 3 ) .

We will also add a citation to Cai et al. ( 2017 ) , which uses the exact same program representation as Reed & De Freitas ( 2015 ) . The other contributions in Cai et al. ( 2017 ) are unrelated to our work .

4 . The first paragraph of Section 4.3 addresses the scalability of the dependency architecture that you questioned .
“ ... Processing each variable id with a single RNN among all programs in the dataset will not only cause memory issues , but more importantly the loss of precision … ”

We hope that our response helped address your concerns . Please let us know if you have any additional questions . Thank you .


Dear reviewer :

Can you please inform us whether there are any additional clarifications needed beyond those in our response ? We have also uploaded a revision of our paper that incorporates ( 1 ) the requested clarifications in the reviews and ( 2 ) additional experimental results from comparing our embeddings with syntactic trace based program embeddings ( Reed & De Freitas ( 2015 ) and Cai et. al ( 2017 ) ) .

Summary of paper : The paper proposes an RNN - based neural network architecture for embedding programs , focusing on the semantics of the program rather than the syntax . The application is to predict errors made by students on programming tasks . This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements . The neural network is trained using this program traces with an objective for classifying the student error pattern ( e.g. list indexing , branching conditions , looping bounds ) .

---

Quality : The experiments compare the three proposed neural network architectures with two syntax - based architectures . It would be good to see a comparison with some techniques from Reed & De Freitas ( 2015 ) as this work also focuses on semantics - based embeddings .
Clarity : The paper is clearly written .
Originality : This work does n't seem that original from an algorithmic point of view since Reed & De Freitas ( 2015 ) and Cai et. al ( 2017 ) among others have considered using execution traces . However the application to program repair is novel ( as far as I know ) .
Significance : This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand .

---

Some questions / comments :
- Do we need to add the print statements for any new programs that the students submit ? What if the structure of the submitted program does n't match the structure of the intended solution and hence adding print statements can not be automated ?

---

References

Cai , J. , Shin , R. , & Song , D. ( 2017 ) . Making Neural Programming Architectures Generalize via Recursion . In International Conference on Learning Representations ( ICLR ) .

We appreciate your point on the differences between our work and Reed & De Freitas ( 2015 ) . We have given a detailed discussion regarding this point in our response to AnonReviewer2 , which we include below for your convenience .

There are fundamental differences between the syntactic program traces explored in prior work ( Reed & De Freitas ( 2015 ) ) and the “ semantic program traces ” considered in our work . Consider the example in Figure 1 . According to Reed & De Freitas ( 2015 ) , the two sorting algorithms will have an identical representation with respect to statements that modify the variable A :

A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp
A [ j ] = A [ j + 1 ]
A [ j + 1 ] = tmp

Our representation , on the other hand , can capture their semantic differences in terms of program states by also only considering the variable A :

Bubble Insertion
[ 5 , 5,1,4 , 3 ] [ 5 , 5 , 1 , 4,3 ]
[ 5,8,1,4 , 3 ] [ 5 , 8, 1, 4,3 ]
[ 5, 1,1,4 , 3 ] [ 5 , 1, 1, 4,3 ]
[ 5,1,8,4 , 3 ] [ 5 , 1, 8, 4,3 ]
[ 1,1,8,4 , 3 ] [ 5 , 1, 4, 4,3 ]
[ 1,5,8,4 , 3 ] [ 5 , 1, 4 , 8,3 ]
[ 1,5,4, 4 , 3 ] [ 5 , 1, 4,3,3 ]
[ 1,5,4, 8, 3 ] [ 5 , 1, 4,3 , 8 ]
[ 1,4,4, 8, 3 ] [ 1 , 1, 4,3 , 8 ]
[ 1,4,5 , 8, 3 ] [ 1 , 5, 4,3, 8 ]
[ 1,4,5,3 , 3 ] [ 1 , 4, 4,3, 8 ]
[ 1,4,5,3 , 8 ] [ 1 ,4 , 5 , 3 , 8 ]
[ 1,4,3,3 , 8 ] [ 1 ,4,3,3 , 8 ]
[ 1,4,3 , 5 , 8 ] [ 1 ,4, 3 , 5 , 8 ]
[ 1,3,3 , 5 , 8 ] [ 1 ,3, 3 , 5 , 8 ]
[ 1,3,4 , 5 , 8 ] [ 1 ,3 , 4 , 5 , 8 ]

This example also illustrates concretely the point made in Section 1 that minor syntactic differences can lead to signiﬁcant semantic differences . Therefore , the approach of Reed & De Freitas is insufficient to capture such semantic differences . As another example , consider the following two programs :

static void Main ( string [ ] args )
{
string str = String . Empty ;
int x = 0 ;
x ++;
}

static void Main ( string [ ] args )
{
string s = " " ;
int y = 0 ;
y = y +1 ;
}

According to the representation proposed in Reed & De Freitas ( 2015 ) , the first program is represented as [ string str = String . Empty , int x = 0 , x + + ] , while the second represented as [ string s = " " , int y = 0 , y = y +1 ] . Although the two programs share the same semantics , they are represented differently due to syntactic variations . In contrast , our work captures the same semantic trace for both programs , i.e. , [ [ “ ” , NA ] , [ “ ” , 0 ] , [ “ ” , 1 ] ] .

To sum up , the embedding proposed in Reed & De Freitas ( 2015 ) is a syntactic representation , and cannot precisely capture a program ’s semantics and abstract away its syntactic redundancies . Consequently , the encoder will not be able to learn the true feature dimensions in the embeddings . We also performed additional experiments to contrast the two trace - based approaches . We used the same configuration of encoder ( cf. Section 5 ) to embed the syntactic traces on the same datasets for the same classification problem . The results are as follows :

Problems Reed & De Freitas ( 2015 ) Token AST Dependency Model
Print Chessboard 26.3 % 16.8 % 16.2 % 99.3 %
Count Parentheses 25.5 % 19.3 % 21.7 % 98.8 %
Generate Binary Digits 23.8 % 21.2 % 20.9 % 99.2 %

Although syntactic traces result in better accuracy than Token and AST , they are still significantly worse than semantic embeddings introduced in our work .

Our revision will include the representation proposed in Reed & De Freitas ( 2015 ) for the example programs in Figure 1 . It will also include the experimental setup ( in Section 5 ) and the new results ( in a new column of Table 3 ) .

We will also add a citation to Cai et al. ( 2017 ) , which uses the exact same program representation as Reed & De Freitas ( 2015 ) . The other contributions in Cai et al. ( 2017 ) are unrelated to our work .

We hope that our response helped address your concerns . Please let us know if you have any additional questions . Thank you .


Dear reviewer :

Our earlier reply mistakenly omitted our answer to the question in your review . Our apologies , and we include the answer below . The instrumentation for adding print statements to a program is fully automated , and requires no manual effort or any assumption on the program ’s code structure . It traverses the program ’s abstract syntax tree and inserts the appropriate print statement after each side - effecting program statement , i.e. , a statement that changes the values of some program variables .

Can you please inform us whether there are any additional clarifications needed beyond those in our response ? We have also uploaded a revision of our paper that incorporates ( 1 ) the requested clarifications in the reviews and ( 2 ) additional experimental results from comparing our embeddings with syntactic trace based program embeddings ( Reed & De Freitas ( 2015 ) and Cai et. al ( 2017 ) ) .

The authors present 3 architectures for learning representations of programs from execution traces . In the variable trace embedding , the input to the model is given by a sequence of variable values . The state trace embedding combines embeddings for variable traces using a second recurrent encoder . The dependency enforcement embedding performs element - wise multiplication of embeddings for parent variables to compute the input of the GRU to compute the new hidden state of a variable . The authors evaluate their architectures on the task of predicting error patterns for programming assignments from Microsoft DEV204.1X ( an introduction to C# offered on edx ) and problems on the Microsoft CodeHunt platform . They additionally use their embeddings to decrease the search time for the Sarfgen program repair system .

This is a fairly strong paper . The proposed models make sense and the writing is for the most part clear , though there are a few places where ambiguity arises :

- The variable " Evidence " in equation ( 4 ) is never defined .

- The authors refer to " predicting the error patterns " , but again do n't define what an error pattern is . The appendix seems to suggest that the authors are simply performing multilabel classification based on a predefined set of classes of errors , is this correct ?

- It is not immediately clear from Figures 3 and 4 that the architectures employed are in fact recurrent .

- Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable , is this correct ?

Assuming that the authors can address these clarity issues , I would in principle be happy for the paper to appear .

Thank you for the helpful suggestions . Below , we answer the questions that you raised in the review .

Our revision will clarify the definition of the “ Evidence ” variable , which , in short , denotes the result of multiplying weight on the program embedding vector and then adding the bias .

Yes , “ predicting the error patterns ” means classifying the kinds of errors that students made in their programs .

The encoders in Figures 3 and 4 are recurrent as they encode variable traces ( each variable trace is a sequence of variable values ) and states ( a state is a set of variable values at a particular program location ) . The figures in our revision will make these clearer .

Dependencies happen primarily in assignment statements . API calls with side effects also introduce dependencies . For example , in the code snippet below , “ sb ” depends on “ s ” :

StringBuilder sb = new StringBuilder ( ) ;
String s = “ str ” ;
sb. Append ( s ) ;


Dear reviewer :

We have uploaded a revision of our paper that incorporates ( 1 ) the requested clarifications in the reviews and ( 2 ) additional experimental results from comparing our embeddings with syntactic trace based program embeddings ( Reed & De Freitas ( 2015 ) and Cai et. al ( 2017 ) ) . Please let us know if any further clarifications are needed .


Quality : Although the research problem is an interesting direction the quality of the work is not of a high standard . My main conservation is that the idea of perturbation in semantic latent space has not been described in an explicit way . How different it will be compared to a perturbation in an input space ?

Clarity : The use of the term " adversarial " is not quite clear in the context as in many of those example classification problems the perturbation completely changes the class label ( e.g. from " church " to " tower " or vice - versa )

Originality : The generation of adversarial examples in black - box classifiers has been looked in GAN literature as well and gradient based perturbations are studied too . What is the main benefit of the proposed mechanism compared to the existing ones ?

Significance : The research problem is indeed a significant one as it is very important to understand the robustness of the modern machine learning methods by exposing them to adversarial scenarios where they might fail .

pros :
( a ) An interesting problem to evaluate the robustness of black - box classifier systems
( b ) generating adversarial examples for image classification as well as text analysis .
( c ) exploiting the recent developments in GAN literature to build the framework forge generating adversarial examples .

cons :
( a ) The proposed search algorithm in the semantic latent space could be computationally intensive . any remedy for this problem ?
( b ) Searching in the latent space z could be strongly dependent on the matching inverter $ I_\gamma ( . ) $ . any comment on this ?
( c ) The application of the search algorithm in case of imbalanced classes could be something that require further investigation .

Thanks for the comments .

Input perturbations vs . latent perturbations : We demonstrate an illustrative example in Figure 1 ( d , e ) showing the differences compared to perturbations in input space in Figure 1 ( b , c ) . There are more FGSM examples provided in Table 1 showing the advantages of our approach . Moreover , approaches that add noise directly to the input are not applicable to complex data such as text because of the discrete nature of the domain . Adding imperceivable changes to the sentences is impossible , and perturbations often result in sentences that are not grammatical . Our framework can generate grammatical sentences that are meaningfully similar to the input by searching in the latent semantic space . There are also examples in Section 3.2 and appendix showing this advantage of our approach .

Related work : To the best of our knowledge , there is no existing work on generating natural adversaries against black - box classifiers utilizing GANs . Other attack methods , none of which utilize GANs , either have access to the gradients of white - box classifiers , or train substitution models mimicking the target classifiers to attack . Further , these methods still add perturbations in input space , while our approach attacks target black - box classifiers directly and searches in the latent semantic space , generating natural adversaries that are legible / grammatical , meaningfully similar to the input , and helpful to interpret and evaluate the black - box classifiers , as demonstrated in our results . Please point us to the GAN literature that generates adversaries against black - box classifiers as mentioned in the review , and we will be happy to compare against them .

Using the term " adversarial " : Yes , there is an implicit assumption that the generated samples are within the same class if the added perturbations are small enough , and the generated samples look as if they belong to different classes when the perturbations are large . However , note that it is also the case for FGSM and other such approaches : when their \epsilon is small , the noise is imperceivable ; but with a large \epsilon , one often finds noisy instances that might be in a different class ( see Table 1 , digit 8 for an example ) . While we do observe this behavior in some cases , the corresponding classifiers require much more substantial changes to the input and that is why we utilize our approach to evaluate black - box classifier . We will clarify this in the revision of the paper .

Matching inverter : The generator / inverter in our approach work in similar way as the decoder / encoder in autoencoders . It is true that the quality of generated samples depends on these two components together . In Section 6 , we mention that the fine-tuning of the latent vector produced by the inverter can further refine the generated adversarial examples , indicating that more powerful inverters are promising future directions of current work .

Search algorithm : Gradient - based search methods such as FGSM are not applicable to our setup because of black - box classifiers and discrete domain application . We have an improved search algorithm by using a coarse - to- fine strategy that we will include in the revision ( see our reply to Reviewer 1 for more details ) .


Summary :
A method for creation of semantical adversary examples in suggested . The ‘ semantic ’ property is measured by building a latent space with mapping from this space to the observable ( generator ) and back ( inverter ) . The generator is trained with a WGAN optimization . Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space . The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences . It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected .
Page 3 : It seems that the search algorithm has a additional parameter : r_0 , the size of the area in which search is initiated . This should be explicitly said and the parameter value should be stated .
Page 4 :
- the implementation details of the generator , critic and invertor networks are not given in enough details , and instead the reader is referred to other papers . This makes this paper non-clear as a stand alone document , and is a problem for a paper which is mostly based on experiments and their results : the main networks used are not described .
- the visual examples are interesting , but it seems that they are able to find good natural adversary examples only for a weak classifier . In the MNist case , the examples for thr random forest are nautral and surprising , but those for the LE - Net are often not : they often look as if they indeed belong to the other class ( the one pointed by the classifier ) . In the churce - vs. tower case , a relatively weak MLP classifier was used . It would be more instructive to see the results for a better , convolutional classifier .
Page 5 :
- the description of the various networks used for text generation is insufficient for understanding :
o The AREA is described in two sentences . It is not clear how this module is built , was loss was it used to optimize in the first place , and what elements of it are re0used for the current task
o ‘ inverter ’ here is used in a sense which is different than in previous sections of the paper : earlier it denoted the mapping from output ( images ) to the underlying latent space . Here it denote a mapping between two latent spaces .
o It is not clear what the ‘ four-layers strided CNN ’ is : its structure , its role in the system . How is it optimized ?
o In general : a block diagram showing the relation between all the system ’s components may be useful , plus the details about the structure and optimization of the various modules . It seems that the system here contains 5 modules instead of the three used before ( critic , generator and inverter ) , but this is not clear enough . Also which modules are pre-trained , which are optimized together , a nd which are optimized separately is not clear .
o SNLI data should be described : content , size , the task it is used for


Pro :
- A novel idea of producing natural adversary examples with a GAN
- The generated examples are in some cases useful for interpretation and network understanding
- The method enables creation of adversarial examples for block box classifiers
Cons
- The idea implementation is basic . Specifically search algorithm presented is quite simplistic , and no variations other than plain local search were developed and tested
- The generated adversarial examples created for successful complex classifiers are often not impressive and useful ( they are either not semantical , or semantical but correctly classified by the classifier ) . Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers .



Thanks for the review .

Details : We held out a lot of implementation details due to the space constraints , but will gladly incorporate them in subsequent versions . We will include some of the more important ones you mentioned in the next revision , with the rest in the appendix . In the first step of the search algorithm , it samples from the range of ( 0 , \ Delta r ] with r_0 = 0 . The Stanford Natural Language Inference ( SNLI ) corpus is a collection of 570 k human-written English sentence pairs manually labeled with whether each hypothesis is entailed by , contradicts , or is neutral to the premise , supporting the task of recognizing textual entailment . We will also include diagrams showing the relations between the components in our text generation framework and provide their implementation details in the appendix .

Quality of adversaries : Yes , generating impressive natural adversaries against more accurate classifiers is difficult , since they require much more substantial changes to the original inputs and a more accurate representation of the data manifold than the current GANs are able to encode . But in essence , we utilize this exact phenomena to evaluate the accuracy and robustness of black - box classifiers qualitatively and quantitatively as shown in experiments , and hope to continue improving our approach to generate even better examples for such classifiers .

Search algorithm : We have an improved search algorithm based on a coarse - to- fine idea that iteratively shrinks the upper bound of \ Delta z . We will include this modification that results in much more efficient generation of samples in the revision ( more details in the response to Reviewer 1 ) .

The authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation ( instead of in the original input data space ) . The details of their proposed method are covered in Algorithm 1 on Page 12 , where an additional GAN ( generative adversarial network ) I_{\gamma} , which can be regarded as the inverse function of the original GAN G_{\theta} , is trained to learn a map from the original input data space to the latent z-space . The authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural ( legible , grammatical , and semantically similar to the input ) .

Generally , I think that the paper is written well ( except some issues listed at the end ) . The intuition of the proposed approach is clearly explained and it seems very reasonable to me .
My main concern , however , is in the current sampling - based search algorithm in the latent z-space , which the authors have already admitted in the paper . The efficiency of such a search method decreases very fast when the dimensions of the z-space increases . Furthermore , such an approximation solution based on the sampling may be not close to the original optimal solution z* in Equation ( 3 ) . This makes me feel that there is large room to further advance the paper . Another concern is that the authors have not provided sufficient number of examples to show the advantages of their proposed method over the other method ( such as FGSM ) in generating the adversaries . The example in Table 1 is very good ; but more examples ( especially involving the quantitative comparison ) are needed to demonstrate the claimed advantages . For example , could the authors add such a comparison in Human Evaluation in Section 4 to support the claim that the adversaries generated by their method are more natural ?

Other issues are listed as follows :
( 1 ) . Could you explicitly specify the dimension of the latent z-space in each example in image and text domain in Section 3 ?
( 2 ) . In Tables 7 and 8 , the human beings agree with the LeNet in >= 58 % of cases . Could you still say that your generated “ adversaries ” leading to the wrong decision from LeNet ? Are these really “ adversaries ” ?
( 3 ) . How do you choose the parameter \lambda in Equation ( 2 ) ?


Thank you for the comments .

Search algorithm : Gradient - based search methods such as FGSM are not applicable to our setup because of black - box classifiers and applications with discrete domains . We have an improved version of the search algorithm that uses a coarse - to- fine strategy to iteratively minimize the upper-bound of \ Delta z based on fewer samples , and then performs finer search in the restricted range recursively . We observe around 4 times speedup in practice and will include more details in the revision .

Comparison : It is difficult to compare against FGSM quantitatively regarding how " natural " the adversaries are , but we will include more examples in the revision . On one hand , FGSM can add such a small magnitude noise that our eyes do not perceive . On the other hand , the noise added by FGSM , when amplified , looks random without any interpretable meaning to us . It is also worth mentioning that users found ~ 80 % of our generated sentences natural ( legible / grammatical ) , a domain for which FGSM cannot be applied at all .

Details : The dimension of latent z vector for MNIST , LSUN , and SNLI are 64 , 128 , and 300 correspondingly . And we choose \lambda = 10 to emphasize the reconstruction error in latent space , after trying out different values and inspecting generated samples . We will include these details in the revision .

This paper studies an approach of data augmentation where a convex combination of multiple samples is used as a new sample . While the use of such convex combination ( mixing features ) is not new , this paper proposes to use a convex combination of corresponding labels as the label of the new sample ( mixing labels ) . The authors motivate the proposed approach in the context of vicinal risk minimization , but the proposed approach is not well supported by theory . Experimental results suggest that the proposed approach significantly outperforms the baseline of using only the standard data augmentation studied in Goyal et al . ( 2017 ) .

While the idea of mixing not only features but also labels is new and interesting , its advantage over the existing approach of mixing only features is not shown . The authors mention " interpolating only between inputs with equal label did not lead to the performance gains of mixup , " but this is not shown in the experiments . The authors cite recent work by DeVries & Taylor ( 2017 ) and Pereyra et al. ( 2017 ) , but the technique of combining multiple samples for data augmentation have been a popular approach . See for example a well cited paper by Chawla et al . ( 2002 ) . The baseline should thus be mixing only features , and this should be compared against the proposed approach of mixing both features and labels .

N. V. Chawla et al. , SMOTE : Synthetic Minority Over-sampling Technique , JAIR 16 : 321-357 ( 2002 ) .

Minor comments :

Figure 1 ( b ) : How should I read this figure ? For example , what does the color represent ?

Table 1 : What is an epoch for mixup ? How does the per epoch complexity of mixup copare against that of ERM ?

Table 2 : The test error seems to be quite sensitive to the number of epochs . Why not use validation to determine when to stop training ?

Table 2 : What is the performance of mixup + dropout ?

===

I appreciate the thorough revision . The empirical advantages over baselines including SMOTE and others are now well demonstrated in the experimental results . It is also good to see that mixup is complementary to dropout , and the combined method works even better than either .

I understand and appreciate the authors ' argument as to why mixup should work , but it is not sufficiently convincing to me why a convex combination in Euclidean space should produce good data distribution . Convex combination certainly changes the manifold . However , the lack of sufficient theoretical justification is now well complemented by extensive experiments , and it will motivate more theoretical work .


We thank the Anonymous Reviewer 2 for comments and feedback .

A major concern of Reviewer 2 is that the paper should include " mixing features " as a baseline .

First of all , we thank Reviewer 2 for raising this point and referring to the SMOTE paper . In the latest revised version ( now available for download ) , we have included a new ablation study section which thoroughly compares mixup against related data augmentation ideas and gives further support to mixup 's advantage .

Moreover , we would like to clarify that mixup and previous work have other important differences . Specifically , the SMOTE algorithm only makes convex combinations of the raw inputs between * nearest neighbors of the same class * . The recent work by DeVries & Taylor ( 2017 ) also follows the same - class nearest neighbor idea of SMOTE , albeit in the feature space ( e.g. the embedding space of an autoencoder ) . This is in sharp contrast to our proposed mixing strategy , which makes convex combination of randomly drawn raw inputs pairs from the training set . In the revised submission , we highlight these differences and demonstrate that each of them is essential for achieving better performance .

Conceptually , a desideratum of data augmentation is that the augmented data should be as diverse as possible while covering the space of data distribution . In the case where the data distribution is a low dimensional manifold ( such as the Swiss Roll Dataset ) , typically the number of data n and the dimensionality of the input space d satisfy d < < log ( n ) , it suffices to augment the training set by interpolating the nearest neighbors . However , if on the other hand d >> log ( n ) , as is the case in typical vision and speech classification tasks , nearest neighbors provide insufficient information to recover the geometry of the data distribution , and training samples other than nearest neighbors can provide a lot of additional geometric information of the data distribution . Therefore , compared with interpolating nearest neighbors ( of either the same class , or the entire training set ) , interpolating random pairs of training data provides a better coverage of the data distribution . Empirically , in our new ablation studies , we find that mixing nearest neighbors provides little ( if any ) improvement over ERM .

As we have justified the interpolation of random training data pairs from potentially different classes , it remains to decide which loss function to use if the synthetic data is a convex combination of samples from two classes . One choice is to assign a single label to the synthetic sample , presumably using the label of the closer sample from the two inputs used to generate the synthetic sample . However , this choice creates an abrupt change of target around a 50 - 50 mix , while not being able to distinguish a 55 - 45 mix from a 95 - 5 mix . The natural solution to these two problems is to also mix the labels using the same weights as the input mix , which makes sure a 55 - 45 mix of inputs is more similar to a 45 - 55 mix , rather than a 95 - 5 one . Empirically , in our new ablation studies , we find that " only mixing the inputs " provides some regularization effects ( so that the performance of using smaller weight decay improves ) , but very limited performance gain over ERM .

Please also refer to the reply to Reviewer 3 for more theoretical justifications .

Regarding the " minor comments " :

Figure 1 ( b ) : - Green : Class 0 , Orange : Class 1 , Blue shading indicates p ( y =1 ) . It is now clarified in the revised version .

Table 1 : One epoch of mixup training is the same as one epoch of normal training , with the same number of minibatches and the same minibatch size . The only change in the training loop is the input mixing step and the label mixing step . Therefore , the computational complexity and actually training time remain ( almost ) the same .

Table 2 : For these experiments , the dataset contains a large portion of corrupt labels . In this case , * without proper regularization * , the test error is indeed quite sensitive to the number of epochs , and one can use cross-validation to determine when to stop . However , even if we only consider the best test errors achieved during the training process , mixup still has a significant advantage over dropout , and dropout has a significant advantage over ERM .

Table 2 : What is the performance of mixup + dropout ?
This is a very good question . We conduct additional experiments combining mixup and dropout , both with medium regularization strength . We observe that the best parameter setting of the combined method is comparable with mixup in terms of the best test error during the training process , but outperforms mixup in terms of the test error at the last epoch . This suggests that mixup combined with dropout is even more resistant to corrupt labels . The updated results are available now . We thank Reviewer 2 for raising this interesting question , and will include proper acknowledgement in the final version .

I enjoyed reading this well - written and easy - to- follow paper . The paper builds on the rather old idea of minimizing the empirical vicinal risk ( Chapelle et al. , 2000 ) instead of the empirical risk . The authors ' contribution is to provide a particular instance of vicinity distribution , which amounts to linear interpolation between samples . This idea of linear interpolation on the training sample to generate additional ( adversarial , in the words of the authors ) data is definitely appealing to prevent overfitting and improve generalization performance at a mild computational cost ( note that this comment does not just apply to deep learning ) . This notion is definitely of interest to machine learning , and to the ICLR community in particular . I have several comments and remarks on the concept of mixup , listed below in no particular order . My overall opinion on the paper is positive and I stand for acceptance , provided the authors answer the points below . I would especially be interested in discussing those with the authors .

1 - While data augmentation literature is well acknowledged in the paper , I would also like to see a comment on domain adaptation , which is a very closely related topic and of particular interest to the ICLR community .

2 - Paragraph after Eq. ( 1 ) , starting with " Learning " and ending with " ( Szegedy et al. , 2014 ) " : I am not so familiar with the term memorization , is this just a fancy way of talking about overfitting ? If so , you might want to rephrase this paragraph with terms more used in the machine learning community . When you write " one trivial way to minimize [ the empirical risk ] is to memorize the training data " , do you mean output a predictor which only delivers predictions on $ X_i$ , equal to $ Y_i$ ? If so , this is again not specific to deep learning and I feel this should be a bit more discussed .

3 - I have not found in the paper a clear heuristics about how pairs of training samples should be picked to create interpolations . Picking at random is the simplest however I feel that a proximity measure on the space $ \mathcal { X}$ on which samples live would come in handy . For example , sampling with a probability decreasing as the Euclidean distance seems a natural idea . In any case , I strongly feel this discussion is missing in the paper .

4 - On a related note , I would like to see a discussion on how many " adversarial " examples should be used . Since the computational overhead cost of computing one new sample is reasonable ( sampling from a Beta distribution + one addition ) , I wonder why $ m$ is not taken very large , yielding more accurate estimates of the empirical risk . A related question : under what conditions does the vicinal risk converge ( in expectation for example ) to the empirical risk ? I think some comments would be nice .

5 - I am intrigued by the last paragraph of Section 5 . What do the authors exactly have in mind when they suggest that mixup could be generalized to regression problems ? As far as I understood the paper , since $ \tilde{y}$ is defined as a linear interpolation between $ y_i$ and $ y_j$ , this formulation only works for continuous $ y $s , like in regression . This formulation is not straightforwardly transposable to classification for example . I therefore am quite confused about the fact that the authors present experiments on classification tasks , with a method that writes for regression .

6 - Writing linear interpolations to generate new data points implicitly makes the assumption that the input and output spaces ( $ \mathcal{X} $ and $ \mathcal { Y}$ ) are convex . I have no clear intuition wether this is a limitation of the authors ' proposed method but I strongly feel this should be carefully addressed by a comment in Section 2 .

We thank the Anonymous Reviewer 1 for interesting comments and feedback .

1 . Domain adaptation is indeed a related problem as one can consider a model trained with Vicinal Risk Minimization will be more robust to small drift in the input distribution . The experience on adversarial examples tends to validate this hypothesis . Indeed , the distance between the original distribution and that of the adversarial examples is typically small . A full study of the suitability of mixup to the broader domain adaptation problems is however beyond the scope of this paper .

We now include a brief discussion about domain adaptation . Inspired by this question , we brainstormed about the possibility of using mixup for domain adaptation in two ways .

( 1 ) Assume a large source - domain dataset D_s = { ( xs_1 , ys_1 ) , ... , ( xs_N , ys_N ) } , and a small target - domain dataset D_t = { ( xt_1 , yt_1 ) , ... , ( xt_n , yt_n ) }. We are interested in learning D_t with auxiliary knowledge from D_s . Using mixup we could do so by training our classifier on the synthetic pairs :

x = a * xt_i + ( 1 - a ) * xs_j ,
y = a * yt_i + ( 1 - a ) * xs_j ,

For random pairs of indices i \in { 1 , … , n } , j \in { 1 , … , N } , and a particular distribution for the mixing coefficient a . If a is concentrated around one , this process recovers ERM training on the target domain . Otherwise , mixup produces synthetic examples just outside the target domain , by interpolating into the source domain .

( 2 ) Alternatively , mixup can simply be used as a within-domain data augmentation method in existing domain adaptation algorithms . For example , some recent work ( e.g. https://arxiv.org/abs/1702.05464) train a network to have similar embedding distributions for both the source and the target domains , using shared weights , moment matching or discriminator loss . With mixup , we can use the same mixing weights lambda ( or hyperparameter alpha ) for both the source and the target domain samples , and require the learned embeddings to be similar . This forces the model to match the embedding distributions of two domains at a broader region in the embedding space , potentially improving the transfer performance .

2 . Correct , by memorization we mean perfect overfitting , as discussed in ( Zhang et al. , 2017 ) . We will clarify this issue , as well as mentioning that this is a pathology more general than deep learning .

3 . At the core of mixup lies its simplicity : pairs of training examples are chosen at random . We have considered other possibilities based on nearest neighbours , but random pairing was much simpler ( it does not require to specify a norm in X ) , and produced better results . We now dedicate a paragraph in the manuscript to describe our choice , as well as proposing the ones by the reviewer for future work .

4 . Mixup does not involve computing adversarial examples . Instead , mixup constructs synthetic examples by interpolating random pairs of points from the training set . New synthetic examples are constructed on - the-fly at a negligible cost for each training iteration . Note that * adversarial * examples are only constructed in our experiment to verify the robustness of mixup networks to adversarial attacks . Adversarial examples are never constructed during training . They are only constructed in that particular experiment at test time , to verify the robustness of each network .

For some examples of VRM converging to ERM , we suggest the original paper of Chapelle et al . An example discussed in that paper is the equivalence of adding Gaussian perturbation to inputs and ERM with L2 regularization .

5 . Mathematically , mixup works for both classification and regression problems : for classification , all our experiments mix labels when parameterized as one - hot continuous vectors . For example : 0.3 * ( 0 , 1 , 0 ) + 0.7 * ( 1 , 0 , 0 ) = ( 0.7 , 0.3 , 0 ) . However , we have n't done any experiments on regression problems , and therefore we are interested to see i ) if regression performance improves with mixup , and ii ) how the regression curves are regularized by mixup .

6 . We do not make a convexity assumption , in the sense that linear combination of samples fall outside the set of natural images , and therefore we are forcing the classifier to give reasonable ( mixup ) labels " outside the convex set " . Overall , linear interpolation is not a limitation , but a simple and powerful way to inform the classifier about how the label changes in the neighbourhood of an image ( “ this is one direction along which a ‘ cat ’ becomes closer to a ‘ dog ’ ” , etc ) . In the case where the input space is more structural ( e.g. the space of graphs ) , a convex combination of the raw inputs may not be valid . However , we can always make the convex combination in the embedding space , which is supposed to be a vector space .

Theoretical contributions : None. Moreover , there is no clear theoretical explanation for why this approach ought to work . The authors cite ( Chapelle et al. , 2000 ) and actually most of the equations are taken from there , but the authors do not justify why the proposed distribution is a good approximation for the true p( x , y ) .

Practical contributions : The paper introduces a new technique for training DNNs by forming a convex combination between two training data instances , as well as changing the associated label to the corresponding convex combination of the original 2 labels .

Experimental results . The authors show mixup provides improvement over baselines in the following settings :
* Image Classification on Imagenet . CIFAR - 10 and CIFAR - 100 , across architectures .
* Speech data
* Memorization of corrupted labels
* Adversarial robustness ( white box and black box attacks )
* GANs ( though quite a limited example , it is hard to generalize from this setting to the standard problems that GANs are used for ) .
* Tabular data .

Reproducibility : The provided website to access the source code is currently not loading . However , experiment hyperparameters are meticulously recorded in the paper .

Key selling points :
* Good results across the board .
* Easy to implement .
* Not computationally expensive .

What is missing :
* Convincing theoretical arguments for why combining data and labels this way is a good approach . Convex combinations of natural images does not result in natural images .
* Baseline in which the labels are not mixed , in order to ensure that the gains are not coming from the data augmentation only . Combining the proposed data augmentation with label smoothing should be another baseline .
* A thorough discussion on mixing in feature space , as well as a baseline which mizes in feature space .
* A concrete strategy for obtaining good results using the proposed method . For example , for speech data the authors say that “ For mixup , we use a warm - up period of five epochs where we train the network on original training examples , since we find it speeds up initial convergence . “ Would be good to see how this affects results and convergence speed . Apart from having to tune the lambda hyperparameter , one might also have to tune when to start mixup .
* Figure 2 seems like a test made to work for this method and does not add much to the paper . Yes , if one trains on convex combination between data , one expects the model to do better in that regime .
* Label smoothing baseline to put numbers into perspective , for example in Figure 4 .





We thank the Anonymous Reviewer 3 for comments and feedback .

The following exposition concerns theoretical arguments for the proposed approach , as well as empirical comparisons with mixing only the inputs , mixing in feature space , label smoothing ( with and without mixing the inputs ) . We include all these baselines in an ablation study section in the latest revision ( now available for download ) , giving further support to mixup 's advantage . We now explain the theoretical motivations :

One desideratum of data augmentation is that the augmented data should match the statistics of the training set . This is because augmented data that mismatch the statistics of the training set are not likely to match the statistics of the test set , leading to less effective augmentation . From the perspective of training , using augmented data that deviate too much from the training set statistics will also cause * high training error on the original training set* , and in turn hurt generalization . In the following , we argue that input space interpolation better matches the statistics of the training set .

Classifiers are nothing but functions of the input space . Therefore , similarity between the statistics of the augmented data and those of the data distribution in input space is more important than perceptual quality or semantic interpretability . Perceptual similarity does not correlate well with metric distance in the input space . For example , a blurred image usually looks similar to its original sharp version , while in input space they may have a large ( L2 ) distance . On the other hand , by directly interpolating in the input space , our method is bound to not lose or significantly alter the statistical information in the data distribution . Therefore , despite not looking real , the mixup images can be better synthetic data for training classifiers than images from latent space interpolations . Empirically , in our ablation studies , we see a gradual degradation of accuracy when interpolating in higher layers of representation .

Another desideratum is that the augmented data should be as diverse as possible to cover the space spanned by the training set . In the case where the data distribution is a low dimensional manifold ( such as the Swiss Roll Dataset ) , typically the number of data n and the dimensionality of the input space d satisfy d < < log ( n ) , it suffices to augment the training set by interpolating the nearest neighbors . However , if on the other hand d >> log ( n ) , as is the case in typical vision and speech classification tasks , nearest neighbors provide insufficient information for recovering the geometry of the data distribution , and training samples other than nearest neighbors can provide a lot of additional geometric information of the data distribution . Therefore , compared with interpolating nearest neighbors , interpolating random pairs of training data provides a better coverage of the data distribution . Empirically , in our ablation studies , we find that mixing nearest neighbors provides little ( if any ) improvement over ERM .

As we have justified the interpolation of training data from potentially different classes , it remains to decide which synthetic label to use if the synthetic input is a convex combination of samples from two classes . One choice is to assign a single label to the synthetic sample , presumably using the label of the closer sample from the two inputs used to generate the synthetic sample . However , this choice creates an abrupt change of target around a 50 - 50 mix , while not being able to distinguish a 55 - 45 mix from a 95 - 5 mix . The natural solution to these two problems is to also mix the labels using the same weights , which makes sure a 55 - 45 mix of inputs is more similar to a 45 - 55 mix , rather than a 95 - 5 one . Empirically , in our ablation studies , we find that only mixing the inputs provides some regularization effects , but very limited performance gain over ERM .

One might also consider replacing the label interpolation in mixup with label smoothing or similar target space regularizers ( e.g. Pereyra et al. , 2017 ) . However , label smoothing and similar work are not designed for between - class input interpolation in that it assigns a small but fixed amount of probability to every class that is not the correct class . Therefore , this loss also has the two problems mentioned above . Empirically , in our ablation studies , we find that adding label smoothing to ERM or replacing label interpolation with label smoothing in mixup provide only limited performance gain over ERM .

Other comments :

" warm - up " : we believe using SGD with momentum and the standard learning rate schedule ( i.e. reducing the learning rate when training error plateaus ) is sufficient for good performance .

" Figure 2 " : it shows that the ERM model has improper behaviors not only in adversarial directions but also between training points , which to the best of our knowledge is not commonly known . It also provides a direct motivation for mixup .

This paper introduces LAX / RELAX , a method to reduce the variance of the REINFORCE gradient estimator . The method builds on and is directly inspired by REBAR . Similarly to REBAR , RELAX is an unbiased estimator , and the idea is to introduce a control variate that leverages the reparameterization gradient . In contrast to REBAR , RELAX learns a free-from control variate , which allows for low - variance gradient estimates for both discrete and continuous random variables . The method is evaluated on a toy experiment , as well as the discrete VAE and reinforcement learning . It effectively reduces the variance of state - of - the- art methods ( namely , REBAR and actor-critic ) .

Overall , I enjoyed reading the paper . I think it is a neat idea that can be of interest for researchers in the field . The paper is clearly explained , and I found the experiments convincing . I have minor comments only .

+ Is there a good way to initialize c_phi prior to optimization ? Given that c_phi must be a proxy for f ( ) , maybe you can take advantage of this observation to find a good initialization for phi ?

+ I was confused with the Bernoulli example in Appendix B. Consider the case theta= 0.5 . Then , b= H ( z ) takes value 1 if z>0 , and 0 otherwise . Thus , p( z|b , theta ) should assign mass zero to values z>0 when b=0 , which does not seem to be the case with the proposed sampling scheme in page 11 , since v*theta= 0.5 *v , which gives values in [ 0,0.5 ] . And similarly for the case b=1 .

+ Why is the method called LAX ? What does it stand for ?

+ In Section 3.3 , it is unclear to me why rho !=phi. Given that c_phi( z ) =f ( sigma_lambda ( z ) ) + r_rho ( z ) , with lambda being a temperature parameter , why is n't rho renamed as phi ? ( the first term does n't seem to have any parameters ) . In general , this section was a little bit unclear if you are not familiar with the REBAR method ; consider adding more details .

+ Consider adding a brief review of the REBAR estimator in the Background section for those readers who are less familiar with this approach .

+ In the abstract , consider adding two of the main ideas that the estimator relies on : control variates and reparameterization gradients . This would probably be more clear than " based on gradients of a learned function . "

+ In the first paragraph of Section 3 , the sentence " f is not differentiable or not computable " may be misleading , because it is unclear what " not computable " means ( one may think that it can not be evaluated ) . Consider replacing with " not analytically computable . "

+ In Section 3.3 , it reads " differentiable function of discrete random variables , " which does not make sense .

+ Before Eq. 11 , it reads " where epsilon_t does not depend on theta " . I think it should be the distribution over epsilon_t what does n't depend on theta .

+ In Section 6.1 , it was unclear to me why t=.499 is a more challenging setting .

+ The header of Section 6.3.1 should be removed , as Section 6.3 is short .

+ In Section 6.3.1 , there is a broken reference to a figure .

+ Please avoid contractions ( does n't , we 'll , it 's , etc . )

+ There were some other typos ; please read carefully the paper and double - check the writing . In particular , I found some missing commas , some proper nouns that are not capitalized in Section 5 , and others ( e.g. , " an learned , " " gradient decent " ) .

Dear reviewer 2 ,

Thank you for your kind words and detailed feedback . Some small changes have been made to address your comments . I will address your comments in the order they were written . Your original comments will appear in [ brackets ] and my responses will follow .

[ + Is there a good way to initialize c_phi prior to optimization ? Given that c_phi must be a proxy for f ( ) , maybe you can take advantage of this observation to find a good initialization for phi ? ]

Good question . This was touched upon in section 3.3 where we use the concrete relaxation to initialize our control variate and then learn an offset parameterized by a neural network . However this approach is only applicable when f is known . We also did not experiment with different structures for the control variate beyond different neural network architectures . This is an interesting idea which we leave to further work to explore .


[ + I was confused with the Bernoulli example in Appendix B. Consider the case theta= 0.5 . Then , b= H ( z ) takes value 1 if z>0 , and 0 otherwise . Thus , p( z|b , theta ) should assign mass zero to values z>0 when b=0 , which does not seem to be the case with the proposed sampling scheme in page 11 , since v*theta= 0.5 *v , which gives values in [ 0,0.5 ] . And similarly for the case b=1 . ]

This issue was due to a typo in Appendix B where theta was swapped for ( 1 - theta ) . This has been fixed in the new version .


[ + Why is the method called LAX ? What does it stand for ? ]

We first coined “ RELAX ” as an alternative to REBAR that learned the continuous “ relax ” - ation . We then developed LAX , and since it was a simpler version of relax , we chose a simpler name . We realize that these are n’t particularly descriptive names , and welcome any suggestions for naming these estimators .

[ + In Section 3.3 , it is unclear to me why rho !=phi. Given that c_phi( z ) =f ( sigma_lambda ( z ) ) + r_rho ( z ) , with lambda being a temperature parameter , why is n't rho renamed as phi ? ( the first term does n't seem to have any parameters ) . In general , this section was a little bit unclear if you are not familiar with the REBAR method ; consider adding more details . ]

The paper has been updated to specify that phi = {rho , lambda}


[ + Consider adding a brief review of the REBAR estimator in the Background section for those readers who are less familiar with this approach . ]

The paper mentions that REBAR can be viewed as a special case of the RELAX method where the concrete relaxation is used as the control variate . For brevity ’s sake , a full explanation of REBAR was left out .


[ + In the abstract , consider adding two of the main ideas that the estimator relies on : control variates and reparameterization gradients . This would probably be more clear than " based on gradients of a learned function . " ]

The abstract has been slightly changed to mention that the method involves control variates .


[ + In the first paragraph of Section 3 , the sentence " f is not differentiable or not computable " may be misleading , because it is unclear what " not computable " means ( one may think that it can not be evaluated ) . Consider replacing with " not analytically computable . " ]

Good point . ” not computable ” has been removed from that sentence .


[ + In Section 3.3 , it reads " differentiable function of discrete random variables , " which does not make sense . ]

By this , it was meant that the function f is differentiable , but we may be evaluating it only on a discrete input . An example of such a function would be f( x ) = x^2 , where x = {0 , 1 } . Here f is differentiable when its domain is the real numbers but we are evaluating it restricted to {0 , 1 } . This wording was removed to avoid confusion .


[ + Before Eq. 11 , it reads " where epsilon_t does not depend on theta " . I think it should be the distribution over epsilon_t what does n't depend on theta . ]

This section has been reworded to make this more clear

[ + In Section 6.1 , it was unclear to me why t=.499 is a more challenging setting . ]

The closer that t gets to .5 means that the values of f ( 0 ) and f ( 1 ) get closer together . This means a Monte Carlo estimator of the gradient will require more samples to converge to the correct value .


[ + The header of Section 6.3.1 should be removed , as Section 6.3 is short . ]

Section 6.3.1 has been removed and consolidated into 6.3


[ + In Section 6.3.1 , there is a broken reference to a figure ]

Good catch . The broken reference has been fixed in section 6.3

[ + Please avoid contractions ( does n't , we 'll , it 's , etc . ) ]

Contractions have been removed .

[ + There were some other typos ; please read carefully the paper and double - check the writing . In particular , I found some missing commas , some proper nouns that are not capitalized in Section 5 , and others ( e.g. , " an learned , " " gradient decent " ) . ]

These typos have been fixed . Thank you for your attention to detail , it has improved the text considerably .


This paper suggests a new approach to performing gradient descent for blackbox optimization or training discrete latent variable models . The paper gives a very clear account of existing gradient estimators and finds a way to combine them so as to construct and optimize a differentiable surrogate function . The resulting new gradient estimator is then studied both theoretically and empirically . The empirical study shows the benefits of the new estimator for training discrete variational autoencoders and for performing deep reinforcement learning .

To me , the main strengths of the paper is the very clear account of existing gradient estimators ( among other things it helped me understand obscurities of the Q-prop paper ) and a nice conceptual idea . The empirical study itself is more limited and the paper suffers from a few mistakes and missing information , but to me the good points are enough to warrant publication of the paper in a good conference like ICLR .

Below are my comments for the authors .

---------------------------------
General , conceptual comments :

When reading ( 6 ) , it is clear that the framework performs regression of $ c_\phi$ towards the unknown $ f$ simultaneously with optimization over $ c_\phi$ .
Taking this perspective , I would be glad to see how the regression part performs with respect to standard least square regression ,
i.e. just using $ ||f ( b ) - c_\phi ( b ) ||^2 $ as loss function . You may compare the speed of convergence of $ c_\phi$ towards $ f $ using ( 6 ) and the least squared error .
You may also investigate the role of this regression part into the global g_ LAX optimization by studying the evolution of the components of ( 6 ) .

Related to the above comment , in Algo. 1 , you mention " f ( . ) " as given to the algo . Actually , the algo does not know f itself , otherwise it would not be blackbox optimization . So you may mean different things . In a batch setting , you may give a batch of [ x , f ( x ) ( , cost ( x ) ? ) ] points to the algo . You more probably mean here that you have an " oracle " that , given some x , tells you f( x ) on demand . But the way you are sampling x is not specified clearly .

This becomes more striking when you move to reinforcement learning problems , which is my main interest . The RL algorithm itself is not much specified . Does it use a replay buffer ( probably not ) ? Is it on - policy or off - policy ( probably on -policy ) ? What about the exploration policy ? I want to know more ... Probably you just replace ( 10 ) with ( 11 ) in A2C , but this is not clearly specified .

In Section 4 , can you explain why , in the RL case , you must introduce stochasticity to the inputs ? Is this related to the exploration issue ( see above ) ?

Last sentence of conclusion : you are too allusive about the relationship between your learned control variate and the Q-function . I do n't get it , and I want to know more ...

-----------------------------------
Local comments :

Backpropagation through the void : I do n't understand why this title . I 'm not a native english speaker , I 'm probably missing a reference to something , I would be glad to get it .

Figure 1 right . Caption states variance , but it is log variance . Why does it oscillate so much with RELAX ?

Beginning of 3.1 : you may state more clearly that optimizing $ c_\phi$ the way you do it will also " minimize " the variance , and explain better why ( " we require the gradient of the variance of our gradient estimator " ... ) . It took me a while to get it .

In 3.1.1 a weighting based on $ \d / \d\theta log p( b ) $ => should n't you write $ ... log p( b|\theta ) $ as before ?

Figure 2 is mentioned in p.3 , it should appear much sooner than p6 .

In Figure 2 , there is nothing about the REINFORCE PART . Why ?

In 3.4 you alternate sums over an infinite horizon and sums over T time steps . You should stick to the T horizon case , as you mention the case T=1 later .

p6 Related work

The link to the work of Salimans 2017 is far from obvious , I would be glad to know more ...

Q-prop ( Haarnoja et al .,2017 ) : this is not the adequate reference to Q-prop , it should be ( Gu et al. 2016 ) , you have it correct later ;)

Figure 3 : why do you stop after so few epochs ? I wondered how expensive is the computation of your estimator , but since in the RL case you go up to 50 millions ( or 4 millions ? ) , it 's probably not the issue . I would be glad to see another horizontal lowest validation error for your RELAX estimator ( so you need to run more epochs ) .
" ELBO " should be explained here ( it is only explained in the appendices ) .

6.2 , Table 1 : Best obtained training objective : what does this mean ? Should it be small or large ? You need to explain better . How much is the modest improvement ( rather give relative improvement in the text ? ) ? To me , you should not defer Table 3 to an appendix ( nor Table 4 ) .

Figure 4 : Any idea why A2C oscillates so much on inverted pendulum ? Any idea why variance starts to decrease after 500 episodes using RELAX ? Is n't related to the combination of regression and optimization , as suggested above ?

About Double Inverted Pendulum , Appendix E3 mentions 50 million frames , but the figure shows 4 millions steps . Where is the truth ?

Why do you give steps for the reward , and episodes for log-variance ? The caption mentions " variance ( log-scale ) " , but saying " log-variance " would be more adequate .

p9 : the optimal control variate : what is this exactly ? How do you compare a control variate over another ? This may be explained in Section 2 .

GAE ( Kimura , 2000 ) . I 'm glad you refer to former work ( there is a very annoying tendency those days to refer only to very recent papers from a small set of people who do not correctly refer themselves to previous work ) , but you may nevertheless refer to John Schulman 's paper about GAEs anyways ... ;)

Appendix E.1 could be reorganized , with a common hat and then E.1.1 for one layer model ( s ? ) and E.1.2 for the two layer model ( s ? )

A sensitivity analysis wrt to your hyper - parameters would be welcome , this is true for all empirical studies .

In E2 , is the output layer linear ? You just say it is not ReLU ...

The networks used in E2 are very small ( a standard would be 300 and 400 neurons in hidden layers ) . Do you have a constraint on this ?

" As our control variate does not have the same interpretation as the value function of A2C , it was not directly clear how to add reward bootstrapping and other variance reduction techniques common in RL into our model . We leave the task of incorporating these and other variance reduction techniques to future work . "
First , this is important , so if this is true I would move this to the main text ( not in appendix ) .
But also , it seems to me that the first sentence of E3 contradicts this , so where is the truth ?

{0.01,0.003,0.001 } I do n't believe you just tried these values . Most probably , you played with other values before deciding to perform grid search on these , right ?
The same for 25 in E3.

Globally , you experimental part is rather weak , we would expect a stronger methodology , more experiments also with more difficult benchmarks ( half - cheetah and the whole gym zoo ; ) ) , more detailed analyses of the results , but to me the value of your paper is more didactical and conceptual than experimental , which I really appreciate , so I will support your paper despite these weaknesses .

Good luck ! :)

---------------------------------------
Typos :

p5
monte-carlo => Monte ( -) Carlo ( no - later ... )
taylor => Taylor
you should always capitalize Section , equation , table , figure , appendix , ...

gradient decent => descent ( twice )

p11 : probabalistic

p15 ELU ( Djork - ... => missing space



Dear reviewer 1 ,

Thank you for your detailed comments and positive reception of the work . I will address your comments in the order in which you wrote them . For clarity , I have placed your comments in [ brackets ] and my responses will follow in plain text .

Global Comments :

[ When reading ( 6 ) , it is clear that the framework performs regression of $ c_\phi$ towards the unknown $ f$ simultaneously with optimization over $ c_\phi$ .
Taking this perspective , I would be glad to see how the regression part performs with respect to standard least square regression ,
i.e. just using $ ||f ( b ) - c_\phi ( b ) ||^2 $ as loss function . You may compare the speed of convergence of $ c_\phi$ towards $ f $ using ( 6 ) and the least squared error .
You may also investigate the role of this regression part into the global g_ LAX optimization by studying the evolution of the components of ( 6 ) . ]

We did not experiment with the L2 loss directly . I do believe this should be explored in further work as the L2 loss has less computational overhead than the Monte Carlo variance estimate that was used in this work . This was tested in the concurrently submitted “ Sample -efficient Policy Optimization with Stein Control Variate “ and in that work , the Monte Carlo variance estimate was found to produce better results in some settings . We ourselves are curious about the relative performance of minimizing the L2 loss vs the variance , but will probably leave that for further work .

[ Related to the above comment , in Algo. 1 , you mention " f ( . ) " as given to the algo . Actually , the algo does not know f itself , otherwise it would not be blackbox optimization . So you may mean different things . In a batch setting , you may give a batch of [ x , f ( x ) ( , cost ( x ) ? ) ] points to the algo . You more probably mean here that you have an " oracle " that , given some x , tells you f( x ) on demand . But the way you are sampling x is not specified clearly . ]

You are correct in your understanding the algorithm . For clarity of notation , we have decided to keep the algorithm box as it is .

[ This becomes more striking when you move to reinforcement learning problems , which is my main interest . The RL algorithm itself is not much specified . Does it use a replay buffer ( probably not ) ? Is it on - policy or off - policy ( probably on -policy ) ? What about the exploration policy ? I want to know more ... Probably you just replace ( 10 ) with ( 11 ) in A2C , but this is not clearly specified . ]

We do not use a reply - buffer , our method is on-policy . We simply replace ( 10 ) with ( 11 ) as you mentioned . Since we mention the similarities between our algorithm and A2C , we have decided to not elaborate further . Exploration is encouraged due to entropy regularization which is commonly used in policy - gradient methods . A note has been added to the experimental details section in the Appendix to explain this .

[ In Section 4 , can you explain why , in the RL case , you must introduce stochasticity to the inputs ? Is this related to the exploration issue ( see above ) ? ]

We were trying to explain that none of these methods can be used without modification to optimize parameters of a deterministic discrete function , since there would be no exploration . In the RL case , stochasticity is introduced by using a stochastic policy and exploration is encouraged due to entropy regularization . For brevity and clarity , this paragraph has been removed .

[ Last sentence of conclusion : you are too allusive about the relationship between your learned control variate and the Q-function . I do n't get it , and I want to know more … ]

Here we are simply mentioning the potential of training the control variate using off policy data as is done in the Q-prop paper . We believe further theoretical work must be done to better understand the relationship between the optimal control variate and the optimal Q-function , so we do not make any claims about this relationship .

( continued in next comment )


Local Comments :

[ Backpropagation through the void : I do n't understand why this title . I 'm not a native english speaker , I 'm probably missing a reference to something , I would be glad to get it .]

The title alludes to the scope of the method . Normally we refer to backprop “ through ” something . If we use our method to estimate gradients through an unknown or non-existent computation graph , we could say we are backpropagating through “ the void ” . However , I would guess that you are not the only one confused by the title .

[ Figure 1 right . Caption states variance , but it is log variance . Why does it oscillate so much with RELAX ? ]

The caption has been changed to log-variance . We guess that the oscillation is due to interactions that arise as the control variate is trained . Since the parameters of the distribution p are constantly changing , the control variate is consistently a step behind p , which may account for these oscillations . We are aware of this phenomenon but have left it for further work to analyze in more detail .

[ Beginning of 3.1 : you may state more clearly that optimizing $ c_\phi$ the way you do it will also " minimize " the variance , and explain better why ( " we require the gradient of the variance of our gradient estimator " ... ) . It took me a while to get it .]

We believe it is clear that by minimizing a Monte Carlo estimate of the variance , we will be minimizing the variance of the estimator . The first sentence of the second paragraph in section 3.1 has been slightly changed to avoid confusion . The title of the section has also been changed to be more clear .



[ In 3.1.1 a weighting based on $ \d / \d\theta log p( b ) $ => should n't you write $ ... log p( b|\theta ) $ as before ? ]

The paper has been updated to include the dependence on theta in the distribution .


[ Figure 2 is mentioned in p.3 , it should appear much sooner than p6 . ]

Due to length restrictions , it was difficult to place this figure elsewhere in the paper .



[ In Figure 2 , there is nothing about the REINFORCE PART . Why ? ]

We wanted to focus on comparing the learned control variates of REBAR and RELAX .


[ In 3.4 you alternate sums over an infinite horizon and sums over T time steps . You should stick to the T horizon case , as you mention the case T=1 later . ]

Good point , we changed our notation to use a finite horizon of T.


[ p6 Related work ]

Can you clarify what you mean here ?


[ The link to the work of Salimans 2017 is far from obvious , I would be glad to know more … ]

When applied to RL , our algorithm is an extension of policy - gradient optimization . We felt it reasonable to refer to alternative approaches such as that of Salimans et al . to inform the reader of concurrent , but orthogonal work .



[ Q- prop ( Haarnoja et al .,2017 ) : this is not the adequate reference to Q-prop , it should be ( Gu et al. 2016 ) , you have it correct later ;) ]

Thanks , we corrected this .

[ Figure 3 : why do you stop after so few epochs ? I wondered how expensive is the computation of your estimator , but since in the RL case you go up to 50 millions ( or 4 millions ? ) , it 's probably not the issue . I would be glad to see another horizontal lowest validation error for your RELAX estimator ( so you need to run more epochs ) .
" ELBO " should be explained here ( it is only explained in the appendices ) . ]

In our VAE experiments , we were interested in comparing with the baseline of REBAR . For that reason , we followed their experimental setup which , runs for 2 million iterations . We added a note that ELBO is shorthand for evidence lower-bound .


[ 6.2 , Table 1 : Best obtained training objective : what does this mean ? Should it be small or large ? You need to explain better . How much is the modest improvement ( rather give relative improvement in the text ? ) ? To me , you should not defer Table 3 to an appendix ( nor Table 4 ) . ]

Good points . We change “ best objective ” to “ highest obtained ELBO ” to indicate that higher ELBOs are better . We could n’t move the tables up because of space constraints .

[ Figure 4 : Any idea why A2C oscillates so much on inverted pendulum ? Any idea why variance starts to decrease after 500 episodes using RELAX ? Is n't related to the combination of regression and optimization , as suggested above ? ]

Regarding the inverted pendulum , we believe this is due to the larger variance of the baseline gradient estimator . Regarding the variance on the double pendulum , we do not fully understand this phenomenon , thus we do not make any claims about it .

( continued in next comment )


[ About Double Inverted Pendulum , Appendix E3 mentions 50 million frames , but the figure shows 4 millions steps . Where is the truth ? ]

This was due to a typo in the appendix . The figure is correct - it was run for 5 million steps , and we corrected the appendix to reflect this .



[ Why do you give steps for the reward , and episodes for log-variance ? The caption mentions " variance ( log-scale ) " , but saying " log-variance " would be more adequate . ]

The variance was estimated at every episode since we needed to run a full episode to compute the policy gradient . After every training episode , 100 episodes were run to generate samples from our gradient estimator which were used to estimate the variance of the estimator . We run the algorithms for a fixed number of steps to be consistent with previous work .



[ p9 : the optimal control variate : what is this exactly ? How do you compare a control variate over another ? This may be explained in Section 2 . ]

The optimal control variate is the control variate which produces a gradient estimator with the lowest possible variance .



[ GAE ( Kimura , 2000 ) . I 'm glad you refer to former work ( there is a very annoying tendency those days to refer only to very recent papers from a small set of people who do not correctly refer themselves to previous work ) , but you may nevertheless refer to John Schulman 's paper about GAEs anyways ... ;) ]

Thanks for the pointer , this reference was added .


[ Appendix E.1 could be reorganized , with a common hat and then E.1.1 for one layer model ( s ? ) and E.1.2 for the two layer model ( s ? ) ]

The appendix was reorganized as per your suggestions .



[ A sensitivity analysis wrt to your hyper - parameters would be welcome , this is true for all empirical studies . ]

In general , we did not find the algorithm to be very sensitive to hyperparameters other than the learning rate . We agree that adding a sensitivity analysis would be an improvement . We have added this to the experimental details in the Appendix along with a sentence which presents the best performing hyperparameters .


[ In E2 , is the output layer linear ? You just say it is not ReLU … ]

The Appendix was changed to note that the output layers were linear .


[ The networks used in E2 are very small ( a standard would be 300 and 400 neurons in hidden layers ) . Do you have a constraint on this ? ]

There was no hard constraint on network size . These networks were chosen because they worked well with the baseline A2C algorithm , and is a standard choice in the literature , and the OpenAI baselines .


[ " As our control variate does not have the same interpretation as the value function of A2C , it was not directly clear how to add reward bootstrapping and other variance reduction techniques common in RL into our model . We leave the task of incorporating these and other variance reduction techniques to future work . "
First , this is important , so if this is true I would move this to the main text ( not in appendix ) .
But also , it seems to me that the first sentence of E3 contradicts this , so where is the truth ? ]


We moved this section to the main text , and clarified why we do n’t use reward bootstrapping for discrete , but use it for continuous experiments . We do so with the continuous control RL tasks by structuring the control variate as C = V ( s ) + c( a , s ) , where V was trained as the value function in A2C .


[ {0.01,0.003,0.001 } I do n't believe you just tried these values . Most probably , you played with other values before deciding to perform grid search on these , right ?
The same for 25 in E3 . ]

For all the experiments , the hyperparameter values used for grid search mentioned in the appendices ( E1 , E2 , E3 ) were the only ones tried . No other values were tried because of computational constraints . These values were not chosen because they gave us better result , but because we thought they would make a comprehensive grid .


[ Globally , you experimental part is rather weak , we would expect a stronger methodology , more experiments also with more difficult benchmarks ( half - cheetah and the whole gym zoo ; ) ) , more detailed analyses of the results , but to me the value of your paper is more didactical and conceptual than experimental , which I really appreciate , so I will support your paper despite these weaknesses . ]

Yes , we believe that further experimentation is warranted , but feel that our current results demonstrate the effectiveness of our method . Thank you for your support .

Typos :

We have fixed all of the typos that you noticed . Thanks .

The paper considers the problem of choosing the parameters of distribution to maximize an expectation over that distribution . This setting has attracted a huge interest in ML communities ( that is related to learning policy in RL as well as variational inference with hidden variables ) . The paper provides a framework for such optimization , by interestingly combining three standard ways .

Given Tucker et al , its contribution is somehow incremental , but I think it is an interesting idea to use neural networks for control variate to handle the case where f is unknown .

The main issue of this paper seems to be in limited experimental results ; they only showed three quite simple experiments ( I guess they need to focus one setting ; RL or VAE ) . Moreover , it would be good to actually show if the variation of g_hat is much smaller than other standard methods .

There is missing reference at page 8.



Dear Reviewer 3 ,

Thank you for your overall positive review of the work . To respond to your specific criticisms :

1 ) “ Given Tucker et al , its contribution is somehow incremental ” . We did build closely on Tucker et. al. , but we generalized their method , and expanded its scope substantially . REBAR was only applicable to known computation graphs with discrete random variables , making it inapplicable to reinforcement learning or continuous random variables .

2 ) “ they only showed three quite simple experiments ( I guess they need to focus one setting ; RL or VAE ) ” . Our first experiment was deliberately simple , so we could visualize the learned control variate . Regarding our VAE experiments , achieving state of the art on discrete variational autoencoders may constitute a “ simple ” experiment , but it ’s not clear that this is a problem . We included the RL experiments to demonstrate the breadth of problems to which our method can be applied , since this is one of the main advantages of RELAX over REBAR .

3 ) “ it would be good to actually show if the variation of g_hat is much smaller than other standard methods ” . Figures 1 and 4 both have plots labeled ‘ log variance ’ , showing exactly this .


The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate , while keeping the number of training epochs constant . This has the advantage that the training process can be better parallelized , allowing for faster training if hundreds of GPUs are available for a short time . The theory part of the paper briefly reviews the relationship between learning rate , batch size , momentum coefficient , and the noise scale in stochastic gradient descent . In the experimental part , it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time , and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size . It is shown that simultaneously increasing the momentum parameter and the batch size also allows for fewer parameters , albeit at the price of some loss in performance .

COMMENTS :

The paper presents a simple observation that seems very relevant especially as computing resources are becoming increasingly available for rent on short time scales . The observation is explained well and substantiated by clear experimental evidence . The main issue I have is with the part about momentum . The paragraph below Eq. 7 provides a possible explanation for the performance drop when $ m$ is increased . It is stated that at the beginning of the training , or after increasing the batch size , the magnitude of parameter updates is suppressed because $ A$ has to accumulate gradient signals over a time scale $ B / ( N ( 1 - m ) ) $ . The conclusion in the paper is that training at high momentum requires additional training epochs before $ A$ reaches its equilibrium value . This effect is well known , but it can easily be remedied . For example , the update equations in Adam were specifically designed to correct for this effect . The mechanism is called " bias -corrected moment estimate " in the Adam paper , arXiv:1412.6980 . The correction requires only two extra multiplications per model parameter and update step . Could n't the same or a very similar trick be used to correctly rescale $ A$ every time one increases the batch size ? It would be great to see the equivalent of Figure 7 with correctly rescaled $ A$ .

Minor issues :
* The last paragraph of Section 5 refers to a figure 8 , which appears to be missing .
* In Eqs. 4 & 5 , the momentum parameter $ m$ is not yet defined ( it will be defined in Eqs . 6 & 7 below ) .
* It appears that a minus sign is missing in Eq . 7 . The update steps describe gradient ascent .
* Figure 3 suggests that most of the time between the first and second change of the noise scale ( approx . epochs 60 to 120 ) are spent on overfitting . This suggests that the number of updates in this segment was chosen unnecessarily large to begin with . It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy .
* It would be interesting to see a version of figure 5 where the horizontal axis is the number of epochs . While reducing the number of updates allows for faster training if a large number of parallel hardware instances are available , the total cost of training is still governed by the number of training epochs .
* It appears like the beginning of the second paragraph in Section 5.2 describes figure 1 . Is this correct ?

We thank the reviewer for their positive review ,

We will edit our discussion of momentum in section 4 to explain the problem more clearly . We are currently running experiments to double check , but we do not believe that the “ bias -corrected moment estimate ” trick will remove the performance gap when training at very large momentum coefficient . This is for two reasons :

1 ) When one uses momentum , one introduces a new timescale into the dynamics , the time required for the direction of the parameter updates to change / forget old gradients . When one trains with large batch sizes and large momentum coefficients , this timescale becomes several epochs long . This invalidates the scaling rules , which assume this timescale is negligible . This issue arises throughout training , not just at initialization / after changing the noise scale .

2 ) The “ bias -corrected moment estimate ” ensures that the expected magnitude of the parameter update at the start of training is correct , but it does not ensure that the variance in this parameter update is correct . As a result , bias correction introduces a very large noise scale at the start of training , which decays as the bias correction term falls . The same issue will arise if we used bias correction to reset the accumulation during training at a noise scale step ; in fact it would temporarily increase the noise scale every time we try to reduce it .

Responding to the minor issues raised :
i ) Our apologies , this should be figure 7 b , we will fix it .
ii ) The momentum coefficient is defined in the first line of the paragraph following eqns 4/5 .
iii ) Yes , we will fix this .
iv ) We will check our conclusions hold when we reduce the number of epochs here , however we keep to pre-existing schedules in the paper to emphasize that our techniques can be applied without hyper - parameter tuning .
v ) All curves in figure 5 saw the same number of training epochs .
vi ) The first two schedules described in this paragraph match figure 1 , however the following two schedules are new .

The paper represents an empirical validation of the well - known idea ( it was published several times before )
to increase the batch size over time . Inspired by recent works on large - batch studies , the paper suggests to adapt the learning rate as a function of the batch size .

I am interested in the following experiment to see how useful it is to increase the batch size compared to fixed batch size settings .

1 ) The total budget / number of training samples is fixed .
2 ) Batch size is scheduled to change between B_min and B_max
3 ) Different setting of B_min and B_max >= B_min are considered , e.g. , among [ 64 , 128 , 256 , 512 , ...] or [ 64 , 256 , 1024 , ...] if it is too expensive .
4 ) Drops of the learning rates are scheduled to happen at certain times represented in terms of the number of training samples passed so far ( not parameter updates ) .
5 ) Learning rates and their drops should be rescaled taking into account the schedule of the batch size and the rules to adapt learning rates in large - scale settings as by Goyal .

We thank the reviewer for their positive review .

We 'd like to emphasize that our paper verifies a stronger claim than previous works . While previous papers have proposed increasing the batch size over time instead of decaying the learning rate , our work demonstrates that we can directly convert decaying learning rate schedules into increasing batch size schedules and vice-versa ; obtaining identical learning curves on both training and test sets for the same number of training epochs seen . To do so , we replace decaying the learning rate by a factor q by increasing the batch size by the same factor q . This strategy allows us to convert between small and large batch training schedules without hyper - parameter tuning , which enabled us to achieve efficient large batch training , with batches of 65,000 examples on ImageNet .

We may have misunderstood , but we believe that we provided the experiment suggested in the review in section 5.1 ( figures 1 , 2 and 3 ) . We consider three schedules , each of which decay the noise scale by a factor of 5 after ~ 60 , ~ 120 and ~ 160 epochs . Each schedule sees the same number of training examples . The “ decaying learning rate schedule ” achieves this by using a constant batch size of 128 and decaying the learning rate by a factor of 5 at each step . The “ increasing batch schedule ” holds the learning rate fixed and increases the batch size by a factor of 5 at the same steps . Finally the “ hybrid ” schedule is mix of the two strategies . All three curves achieve identical training curves in terms of number of examples seen ( figure 2a ) , and achieve identical final test accuracy ( figure 3a ) . In this sense , decaying the learning rate and increasing the batch size are identical ; they require the same amount of computation to reach the same training / test accuracies . However if one increases the batch size one can benefit from greater parallelism to reduce wall clock time .

## Review Summary

Overall , the paper 's paper core claim , that increasing batch sizes at a linear
rate during training is as effective as decaying learning rates , is
interesting but does n't seem to be too surprising given other recent work in
this space . The most useful part of the paper is the empirical evidence to
backup this claim , which I ca n't easily find in previous literature . I wish
the paper had explored a wider variety of dataset tasks and models to better
show how well this claim generalizes , better situated the practical benefits
of the approach ( how much wallclock time is actually saved ? how well can it be
integrated into a distributed workflow ? ) , and included some comparisons with
other recent recommended ways to increase batch size over time .


## Pros / Strengths

+ effort to assess momentum / Adam / other modern methods

+ effort to compare to previous experimental setups


## Cons / Limitations

- lack of wallclock measurements in experiments

- only ~ 2 models / datasets examined , so difficult to assess generalization

- lack of discussion about distributed / asynchronous SGD


## Significance

Many recent previous efforts have looked at the importance of batch sizes
during training , so topic is relevant to the community . Smith and Le ( 2017 )
present a differential equation model for the scale of gradients in SGD ,
finding a linear scaling rule proportional to eps N /B , where eps = learning
rate , N = training set size , and B = batch size . Goyal et al ( 2017 ) show how
to train deep models on Image Net effectively with large ( but fixed ) batch
sizes by using a linear scaling rule .

A few recent works have directly tested increasing batch sizes during
training . De et al ( AISTATS 2017 ) have a method for gradually increasing batch
sizes , as do Friedlander and Schmidt ( 2012 ) . Thus , it is already reasonable to
practitioners that the proposed linear scaling of batch sizes during training
would be effective .

While increasing batch size at the proposed linear scale is simple and seems
to be effective , a careful reader will be curious how much more could be
gained from the backtracking line search method proposed in De et al .


## Quality

Overall , only single training runs from a random initialization are used . It
would be better to take the best of many runs or to somehow show error bars ,
to avoid the reader wondering whether gains are due to changes in algorithm or
to poor exploration due to bad initialization . This happens a lot in Sec. 5.2.

Some of the experimental setting seem a bit haphazard and not very systematic .
In Sec. 5.2 , only two learning rate scales are tested ( 0.1 and 0.5 ) . Why not
examine a more thorough range of values ?

Why not report actual wallclock times ? Of course having reduced number of
parameter updates is useful , but it 's difficult to tell how big of a win this
could be .

What about distributed SGD or asyncronous SGD ( hogwild ) ? Small batch sizes
sometimes make it easier for many machines to be working simultaneously . If we
scale up to batch sizes of ~ N /10 , we can only get 10 x speedups in
parallelization ( in terms of number of parameter updates ) . I think there is
some subtle but important discussion needed on how this framework fits into
modern distributed systems for SGD .


## Clarity

Overall the paper reads reasonably well .

Offering a related work " feature matrix " that helps readers keep track of how
previous efforts scale learning rates or minibatch sizes for specific
experiments could be valueable . Right now , lots of this information is just
provided in text , so it 's not easy to make head - to-head comparisons .

Several figure captions should be updated to clarify which model and dataset
are studied . For example , when skimming Fig. 3 's caption there is no such
information .

## Paper Summary

The paper examines the influence of batch size on the behavior of stochastic
gradient descent to minimize cost functions . The central thesis is that
instead of the " conventional wisdom " to fix the batch size during training and
decay the learning rate , it is equally effective ( in terms of training / test
error reached ) to gradually increase batch size during training while fixing
the learning rate . These two strategies are thus " equivalent " . Furthermore ,
using larger batches means fewer parameter updates per epoch , so training is
potentially much faster .

Section 2 motivates the suggested linear scaling using previous SGD analysis
from Smith and Le ( 2017 ) . Section 3 makes connections to previous work on
finding optimal batch sizes to close the generaization gap . Section 4 extends
analysis to include SGD methods with momentum .

In Section 5.1 , experiments training a 16 - 4 ResNet on CIFAR - 10 compare three
possible SGD schedules : * increasing batch size * decaying learning rate *
hybrid ( increasing batch size and decaying learning rate ) Fig. 2 , 3 and 4 show
that across a range of SGD variants ( + / - momentum , etc ) these three schedules
have similar error vs . epoch curves . This is the core claimed contribution :
empirical evidence that these strategies are " equivalent " .

In Section 5.3 , experiments look at Inception - ResNet - V2 on ImageNet , showing
the proposed approach can reach comparable accuracies to previous work at even
fewer parameter updates ( 2500 here , vs. ∼ 14000 for Goyal et al 2007 )


We thank the reviewer for their positive assessment of our work .
To respond to the comments raised :

The wall clock time is primarily determined by the hardware researchers have at their disposal ; not the quality of the research / engineering they have done . In the paper we choose to focus on the number of parameter updates , because we believe this is the simplest and most meaningful scientific measure of the speed of training . Assuming one can achieve perfect parallelism , the number of parameter updates and the wall clock time are identical . However we can confirm here that , using the increasing batch size trick , we were able to train ResNet - 50 to 76.1 % validation accuracy on ImageNet in 29 minutes . With a constant batch size , we achieve comparable accuracy in 44 minutes ( replicating the set - up of Goyal et al . ) . This significantly under-estimates the gains available , as we only increased the batch size to 16 k in these experiments , not 64 k as in the paper .

One of the goals of large batch training is to remove the need for asynchronous SGD , which tends to slightly reduce test set accuracies . Since we are now able to scale the batch size to several thousand training examples and train accurate ImageNet models in under an hour with synchronous SGD , the incentive to use asynchronous training is much reduced . Intuitively , asynchronous SGD behaves somewhat like an increased momentum coefficient , averaging the gradient over recent parameter values .

We chose to focus on clarity , rather than including many equivalent experiments under different architectures , however we have checked that our claims are also valid for a DNN on MNIST and ResNet - 50 on ImageNet . This is also why we do not present an exhaustive range of learning rate scales in section 5.2 ; we wanted to keep the figures clean and easy to interpret . It 's worth noting that our observations also match theoretical predictions . We will update the figure captions to clarify which model / dataset they refer to .

This paper suggests a simple yet effective approach for learning with weak supervision . This learning scenario involves two datasets , one with clean data ( i.e. , labeled by the true function ) and one with noisy data , collected using a weak source of supervision . The suggested approach assumes a teacher and student networks , and builds the final representation incrementally , by taking into account the " fidelity " of the weak label when training the student at the final step . The fidelity score is given by the teacher , after being trained over the clean data , and it 's used to build a cost - sensitive loss function for the students . The suggested method seems to work well on several document classification tasks .

Overall , I liked the paper . I would like the authors to consider the following questions -

- Over the last 10 years or so , many different frameworks for learning with weak supervision were suggested ( e.g. , indirect supervision , distant supervision , response - based , constraint - based , to name a few ) . First , I 'd suggest acknowledging these works and discussing the differences to your work . Second - Is your approach applicable to these frameworks ? It would be an interesting to compare to one of those methods ( e.g. , distant supervision for relation extraction using a knowledge base ) , and see if by incorporating fidelity score , results improve .

- Can this approach be applied to semi-supervised learning ? Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self - training framework ?

- The paper emphasizes that the teacher uses the student 's initial representation , when trained over the clean data . Is it clear that this step in needed ? Can you add an additional variant of your framework when the fidelity score are computed by the teacher when trained from scratch ? using different architecture than the student ?

- I went over the authors comments and I appreciate their efforts to help clarify the issues raised .


Q5 - The paper emphasizes that the teacher uses the student 's initial representation when trained over the clean data . Is it clear that this step in needed ? Can you add an additional variant of your framework when the fidelity sores are computed by the teacher when trained from scratch ? using a different architecture than the student ?

A5 - In the current model , first the representation of the data is learned by the student using weakly annotated data , then , using the learned representation , we fit the teacher on the data with strong ( true ) labels . Providing the teacher with the learned representation by the student has three main reasons :
First of all , it has been shown that we can learn effective representation of the data if we have a large quantity of data available , this can be either by learning the distribution of the data using unlabeled example , or learning representation of the data downstream of the main task using a large set of weakly labeled data . However , for many tasks using just a small amount of data with true labels , we will not be able to model the underlying distribution of the data . Since in FWL , the teacher is trained only on data with strong labels ( which is a small set ) , sharing the learned representation of the previous step alleviates this problem and the teacher can enjoy the learned knowledge from the large quantity of the weakly annotated data .
In our setup , we make use of a Gaussian Process as the teacher . It is an interesting direction to search for meaningful kernels on structured data , i.e strings in our case . There exist some works to define such non-vectorial kernels that are designed by experts and are domain specific [ 2 , 3 ] . However , our goal here is to learn the representation along with solving the main classification task . Even though sme papers connect Gaussian process and deep neural networks , we are not aware of a reliable method for end to end training to learn the input features of GP better than the features learned by a neural network . So we do not learn the representation of the data as part of the step # 2 , but borrow it from step # 1 . Likewise , we do not learn the kernels of the GPs and only learn the vectorial representation of their inputs .
As another possible advantage of the current setup , we let the teacher see the data through the student 's lens . This may in particular help the teacher , in step # 3 , to provide better annotation ( and confidence ) for the training of the student when the teacher is aware of the idea of the student about metric properties of the input space learned in step # 1 . Note that the input representation of the student is trained in the step # 3 and is not fully identical with that of the teacher which is kept fixed . We tested the case where the teacher used the input representation of the student in step # 3 but the accuracy dropped considerably . We ascribe this observation to the covariate shift in the input of a trained GP .

We made these points more explicit in the revised version of the submission ( Section 2 , where we are explaining step # 2 ) . A variant of FWL would be to use one ( or more ) neural network ( s ) as the teacher [ 1 ] to be able to learn representation in step # 2 , but we believe it is necessary for the teacher and student to both agree on the metric space they see in the input .

[ 1 ] Anonymous , Deep Neural Networks as Gaussian Processes , under submission at ICLR2018 , https://openreview.net/forum?id=B1EA-M-0Z
[ 2 ] Eskin E , Weston J , Noble WS , Leslie CS . Mismatch string kernels for SVM protein classification . InAdvances in neural information processing systems 2003 ( pp. 1441-1448 ) .
[ 3 ] Gärtner T. A survey of kernels for structured data . ACM SIGKDD Explorations Newsletter . 2003 Jul 1;5 ( 1 ) :49-58

First of all , we would like to thank the reviewer for the valuable suggestions and comments . Here we respond to the questions one by one :

Q1 - Over the last 10 years or so , many different frameworks for learning with weak supervision were suggested .
First , I 'd suggest acknowledging these works and discussing the differences to your work .

A1 - In the revised manuscript , we ’ve included some of the main works in the area of “ learning with weak supervision ” ( some of which had been left out in the original submission due to the page limit ) in the related work section and discussed how FWL is related to them .

Q2 - Second - Is your approach applicable to these frameworks ? It would be an interesting to compare to one of those methods ( e.g. , distant supervision for relation extraction using a knowledge base ) , and see if by incorporating fidelity score , results improve .

A2 - In general , in order to employ FWL , we need a large set of data with weak labels . These weak labels can be devised using methods like distant supervision , indirect supervision , constraint - based supervision , etc .
In our paper , for instance , for the ranking task , we use a weak annotator based on a heuristic function that can be considered as a form of distant supervision , and for the classification task , as the weak annotation , we use labels in the word level to infer labels in the sentence level which can be kind of considered as an indirect supervision approach with a slightly different setup .
The interesting question would be how different approaches for providing the weak annotation may affect the performance of FWL , and in a more general perspective , how sensitive is FWL to the quality of the weak annotations . In the original submission , in section “ 4.1 Handling The Bias - Variance Trade-off ” , we included a simple analysis on how employing different weak annotators with different qualities ( in terms of accuracy on test data ) affects the performance of FWL in the toy problem . Since this point is also raised by one of the other reviewers , we add extra analysis to the revised version for the ranking task , “ 4.3 . The sensitivity of the FWL to the Quality of the Weak Annotator ” . The analysis shows that the achieved improvement by FWL over the weak annotator decreases in the presence of a more accurate weak annotator . The reason could be the hypothesis that a good annotator makes better use of data and leaves less room for improvement by the teacher .

Q3 - Can this approach be applied to semi-supervised learning ?

A3 - Yes . In fact , the proposed approach is applicable in the semi-supervised setup , where we can define one or more so -called “ weak annotators ” , to provide additional ( albeit noisy ) sources of weak supervision for unlabeled data . This can be done based on heuristics rules , or using a “ weaker ” or biased classifiers trained on e.g. non - expert crowd - sourced data or data from different domains that are related , or distant supervision where an external knowledge source is employed to devise labels . Providing such weak annotations is possible for a large class of tasks that are considered to be solved in the semi-supervised learning setup .

Q4 - Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self - training framework ?

A4 - If we correctly understood , the question here is “ In what circumstances does taking the confidence ( fidelity ) score by FWL into account yield no improvement or even hurt the performance of the student model , while it learns from weakly annotated data ? ” .
This could be an interesting direction that merits more detailed investigation . The failures of applying the confidence score are either estimating a high confidence score for a bad training label ( case # 1 ) , or estimating a low confidence for a good training label ( case # 2 ) . From the set of controlled experiments , we have done in particular on the toy problem , we found that probably due to the Bayesian nature of the teacher in FWL , case # 1 is less likely to happen compared to case # 2 . The explanation is that generation of bad labels is mostly due to the lack of enough strong data to fit a good GP . When the number of data points on which the GP is fitted is extremely low , the uncertainty is high almost all over the space leading to low confidences . So , in most cases , bad labels come with fairly low confidence .
In our design , case # 2 would not happen for samples with strong labels ( since GP is fitted on them and the uncertainty is almost zero at those points ) , however , the teacher might reject good weak examples by assigning a low confidence score to them . This is not a crucial situation as 1 . generating extra weak examples is not expensive in our setup , 2 . the rejected weak example has already contributed to the parameter updates of the student in the pretraining with its original weak label ( step # 1 ) . Nonetheless , having lots of case # 2 leads to slower convergence of the model during training .


The problem of interest is to train deep neural network models with few labelled training samples . The specific assumption is there is a large pool of unlabelled data , and a heuristic function that can provide label annotations , possibly with varying levels of noises , to those unlabelled data . The adopted learning model is of a student / teacher framework as in privileged learning / knowledge distillation / model compression , and also machine teaching . The student ( deep neural network ) model will learn from both labelled and unlabelled training data with the labels provided by the teacher ( Gaussian process ) model . The teacher also supplies an uncertainty estimate to each predicted label . How about the heuristic function ? This is used for learning initial feature representation of the student model . Crucially , the teacher model will also rely on these learned features . Labelled data and unlabelled data are therefore lie in the same dimensional space .

Specific questions to be addressed :
1 ) Clustering of strongly - labelled data points . Thinking about the statement “ each an expert on this specific region of data space ” , if this is the case , I am expecting a clustering for both strongly - labelled data points and weakly - labelled data points . Each teacher model is trained on a portion of strongly - labelled data , and will only predict similar weakly - labelled data . On a related remark , the nice side - effect is not right as it was emphasized that data points with a high - quality label will be limited . As well , GP models , are quite scalable nowadays ( experiments with millions to billions of data points are available in recent NIPS / ICML papers , though , they are all rely on low dimensionality of the feature space for optimizing the inducing point locations ) . It will be informative to provide results with a single GP model .
2 ) From modifying learning rates to weighting samples . Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate , it is more “ intuitive ” to use it to modify the sampling procedure of mini-batches ( akin to baseline # 4 ) ; sample with higher probability data points with higher certainty . Here , experimental comparison with , for example , an SVM model that takes into account instance weighting will be informative , and a student model trained with logits ( as in knowledge distillation / model compression ) .



Regarding the point : “ I am expecting a clustering for both strongly - labelled data points and weakly - labelled data points ” : In FWL , during training , in step # 2 , we train multiple GPs as the teacher using only the data with “ strong labels ” which is a rather small set . In step #3 , we go through both data with strong and weak labels and for each data point , we assign each point to a teacher based on the centroid of the teacher ’s corresponding cluster . Therefore , each teacher predicts the new label in its territory . The predicted labels are almost the same as the original labels for the strongly labeled points and hopefully better labels for the weakly labeled data points . The confidence for the newly labeled point is also reported by its corresponding GP ( teacher ) .
Clustering the weakly - labeled data points means having multiple student models as well . However , during the recall , we do not want to have multiple students which is computationally and space - wise prohibitive . Having a separate student corresponding to each teacher prevent the makes each student almost blind with respect to other clusters which is not desirable . The single student defined in our framework enables it to have a holistic view of the entire input space . We want our main task to be solved by a single student which is assumed expressive enough . The entire framework is designed to help this single student to settle on a better local optimum enjoying multiple teachers in the distillation framework . One important point here is that in FWL the teacher can be implemented by any predictor that can provide uncertainty for its prediction [ 1 ] . Even though , we resort to GP and a small strongly labeled dataset to capture this uncertainty , we should argue that the concept is applicable also when the uncertainty signal is provided from outside the dataset .
-----
[ 1 ] Anonymous , Deep Neural Networks as Gaussian Processes , under submission at ICLR2018 , https://openreview.net/forum?id=B1EA-M-0Z

Q2 - From modifying learning rates to weighting samples . Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate , it is more “ intuitive ” to use it to modify the sampling procedure of mini-batches ( akin to baseline # 4 ) ; sample with higher probability data points with higher certainty . Here , experimental comparison with , for example , an SVM model that takes into account instance weighting will be informative , and a student model trained with logits ( as in knowledge distillation / model compression ) .

A2 - We think the suggested comparison is the case mainly when samples are seen by the model with the frequency proportional to the certainty of their label .
We designed a new experiment in which , we kept the architectures of the student and the teacher and the procedure of the first two steps of the FWL fixed . We changed the step # 3 as follows : For each sample in D_{sw} ( dataset consisting of strongly and weakly labeled data points which are relabeled by the teacher and each label is associated with a confidence ) , we normalize the confidence scores for all training samples and set the normalized score of each sample as its probability to be sampled . Afterwards , we train student model by sampling mini-batches from D_{sw} with respect to the probabilities associated with each sample , without considering their confidence as a multiplicative factor for the learning rate .
This means that more confident the teacher is about the generated label for each sample , the more chance that sample has to be seen by the student model .
We have added a new subsection , “ 4.4 . From modifying the learning rates to weighted sampling ” , to the revised manuscript to report our observations . Based on the results , compared to the original FWL , the performance of FWL with sampling increases rapidly in the beginning but it slows down afterward . We have looked into the sampling procedure and noticed that the confidence scores provided by the teacher form a rather skewed distribution and there is a strong bias toward sampling from data points that are either in or closed to the points in the dataset with strong labels , as GP has less uncertainty around these points and the confidence scores are high . We observed that the performance of the FWL with sampling gets closer to the performance of FWL after many epochs , while FWL had already a long convergence . The skewness of the confidence distribution makes FWL with sampling to have a tendency for more exploitation than exploration , however , FWL has more chance to explore the input space , while it controls the effect of updates on the parameters for samples based on their merit .
We believe FWL with sampling can be improved by having a better strategy for sampling from a skewed distribution or using approaches for active learning and selective sampling which is out of the scope of this paper .

First of all , we would like to thank the reviewer for the valuable suggestions and comments . Here we respond to the questions one by one :

Q1 - Clustering of strongly - labelled data points . Thinking about the statement “ each an expert on this specific region of data space ” , if this is the case , I am expecting a clustering for both strongly - labelled data points and weakly - labelled data points . Each teacher model is trained on a portion of strongly - labelled data , and will only predict similar weakly - labelled data . On a related remark , the nice side - effect is not right as it was emphasized that data points with a high - quality label will be limited . As well , GP models , are quite scalable nowadays ( experiments with millions to billions of data points are available in recent NIPS / ICML papers , though , they are all rely on low dimensionality of the feature space for optimizing the inducing point locations ) . It will be informative to provide results with a single GP model .

A1 - Regarding clustered GP : We used Sparse Gaussian Process implemented in GPflow to build our entire implementation in tensorflow . As the respected reviewer has mentioned , the algorithm is scalable in the sense that it is not O ( N^3 ) as original GP is . It introduces inducing points in the data space and defines a variational lower bound for the marginal likelihood . The variational bound can now be optimized by stochastic methods which make the algorithm applicable to large datasets . However , the tightness of the bound depends on the locations which are found through the optimization process . We empirically observed that a single GP does not give a satisfactory accuracy on left - out test dataset . We hypothesized that this can be due to the inability of the algorithm to find good inducing points when only a few of them are available . Then we increased the number of inducing points which trades off the scalability of the algorithm because it scales with O ( NM^2 ) where M is the number of inducing points . We guess this can be due to the observation that our datasets are distributed in a highly sparse way within the high dimensional embedding space . We also tried to cure the problem by means of PCA to reduce input dimension but it did not result in a considerable improvement . Due to this empirical evidence , we clustered the truly labeled dataset and used a separate GP for each cluster . The overall performance of the algorithm improved and we may be able to argue that clustered GP makes use of the data structure roughly close to the idea of KISS - GP [ 1 ] . In inducing - point methods ( with m inducing points and n training samples ) , normally it is assumed that m < <n for computational and storage saving . However , we have this intuition that few number of inducing points make the model unable to explore the inherent structure of data . By employing several GPs , we were able to use a large number of total inducing points even m>n which seemingly better exploits the structure of datasets . Because our work was not aimed to be a close investigation of GP , we considered clustered GP as the engineering side of the work which is a tool to give us a measure of confidence . Other tools such as a single GP with inducing points that form a Kronecker or Toeplitz covariance matrix are also conceivable . Therefore , we do not of course claim that we have proposed a new method of inference for GP because of the lack of theoretical reasoning for the use of multiple GPs . In the end , as was asked by the reviewer , the result for single GP is included as a part of clustered GP section in the Appendix A ( Detailed description of clustered GP ) in the revised manuscript showing the effectiveness of the the initial clustering and local GPs . Moreover , an abstract of this response is also added to the in Appendix A and also main text to enlighten the reason for using clustered GP .
-----
[ 1 ] Wilson , A. and Nickisch , H. , 2015 , June . Kernel interpolation for scalable structured Gaussian processes ( KISS - GP ) . In International Conference on Machine Learning ( pp. 1775-1784 ) .

The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data . This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable . The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student - teacher framework . The authors evaluate their proposed methods on one toy problem and two real - world problems . The paper is well written , easy to follow , and have good experimental study . My main problem with the paper is the lack of enough motivation and justification for the proposed method ; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work . Here are some questions that comes to my mind : ( 1 ) Why first building a student model only using the weak data and why not all the data together to train the student model ? To me , it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data ? ( 2 ) What are the sensitivity of the procedure to how weakly the weak data are annotated ( this could be studied using both toy example and real - world examples ) ? ( 3 ) The authors explicitly suggest using an unsupervised method ( check Baseline no .1 ) to annotate data weakly ? Why not learning the representation using an unsupervised learning method ( unsupervised pre training ) ? This should be at least one of the baselines .
( 4 ) the idea of using surrogate labels to learn representation is also not new . One example work is " Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks " . The authors did n't compare their method with this one .


Q4 - the idea of using surrogate labels to learn representation is also not new . One example work is " Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks " . The authors did n't compare their method with this one .

A4 - Thanks for pointing out this paper . The referred work is based on self - supervised feature learning in which the idea is to exploit different labelings that are freely available besides or within the data by defining a surrogate task which uses the intrinsic signals to learn better ( e.g. generic , robust , descriptive and invariant ) features . The learned features are then transferred to be used for a supervised task ( e.g. object classification or description matching ) . We argue that in FWL we do not learn representation through a proxy task . We learn the representation ( and pretrain the student model ) downstream of the main task , but with pseudo- labels ( noisy labels ) . Nonetheless , we can say that representation learning in step # 1 is solving a surrogate task of approximating the expert knowledge , for which a noisy supervision signal is provided by the weak annotator .
In the response to the previous question , we have added a baseline for the sentiment classification task in which a surrogate task is used to learn the representation ( see A3 ) . Furthermore , we discussed the advantage of the current setup of the step # 1 , i.e. learning the representation downstream of a task same as the final target task that we want to solve but with lower accuracy in the labels . Here we again summarize them and add one more point :
1 . Using the main task with weak labels in step # 1 leads to a representation that complies better with the target task .
2 . In the current setup of the step # 1 , we also pretrain the student model , so representation learning is actually part of student pre-training in a weakly supervised manner ( in A3 , we explained why this is needed ) .
3 . In addition to the above points , the definition of the surrogate task in self - supervision depends on the problem to be solved . For instance , if the surrogate task is defined such that it yields features invariant to color , it cannot be used to differentiate objects with different colors . However , in our setup the step # 1 is seen as a surrogate task is inherently in accordance with the main task ( in fact they are the same , but with different accuracy in the label space ) and we do not need to think about the suitable surrogate task for the feature learning phase .
Looking to the FWL from the perspective of self - supervised feature learning is pretty interesting and valuable to mention . We have added this point ( Section 2 , where we are explaining step # 1 ) and the related papers ( in related work section ) to the revised version of the submission to include this point of view as well .


Q3 - The authors explicitly suggest using an unsupervised method ( check Baseline no .1 ) to annotate data weakly ? Why not learning the representation using an unsupervised learning method ( unsupervised pre-training ) ? This should be at least one of the baselines .

A3 - The suggestion in this comment is to learn the representation of the data , in the first step , in an unsupervised manner .
We have already tried this idea for both tasks , i.e. removing the first step and replacing it with learning the representation in an unsupervised ( or self - supervised feature learning ) way : In the document ranking task , as the representation of documents and queries , we use weighted averaging over pre-trained embeddings of their words based on their inverse document frequency [ 1 ] . In the sentiment analysis task , we use skip-thoughts [ 2 ] which tries to estimate representation for sentences by defining a surrogate task in which given a sentence , the goal is to predict the sentence before and after , using autoencoders . We have used these representations as the input for the GP in step # 2 and # 3 . In both tasks , the performance drops dramatically . There might be two main reasons for that :
1 . Learning the representation of the input data downstream of the main task that we are going to solve leads to representations that are better suited to the FWL ( in terms of the communications between teacher and student ) compared to a task - independent unsupervised way .
2 . The other reason for losing performance by replacing the first step with learning representation in an unsupervised ( self - supervised ) way is that although the main goal of step # 1 is to learn a representation of the data for the given task , we pretrain all the parameters of the student network ( not just the representation layer ) in that step . So as mentioned in the paper , in step # 3 “ [ ...] for data points where the teacher is not confident , we down - weight the training steps of the student . This means that at these points , we keep the student function as it was trained on the weak data in Step # 1 . ” In summary , the first step initializes the layers of both embedding network and classification network .
We have added the aforementioned experiments as extra baselines ( baseline # 7 , FWL_unsuprep ) to the paper and in the results and discussions ( section 3.2 and 3.3 ) , we elaborate more on the importance of the FWL setup for step # 1 .
------------
[ 1 ] Mostafa Dehghani , Hamed Zamani , Aliaksei Severyn , Jaap Kamps , and W. Bruce Croft . Neural ranking models with weak supervision . In SIGIR ’17 , 2017 .
[ 2 ] Ryan Kiros , Yukun Zhu , Ruslan R Salakhutdinov , Richard Zemel , Raquel Urtasun , Antonio Torralba , and Sanja Fidler . Skip-thought vectors . NIP2015 , 2015 .

First of all , we would like to thank the reviewer for the valuable suggestions and comments . Here we respond to the questions one by one :

Q0 - My main problem with the paper is the lack of enough motivation and justification for the A0 - proposed method ; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work .

A0 - As we have discussed in the introduction section of the manuscript , the motivation is making the efficient use of training data to achieve better performance on test data . The situation where training data consists of a small set with good labels and a large set with weak labels is fairly common . For instance , in large scale classification tasks ( e.g. ImageNet ) , a large number of people annotate images via Amazon Mechanical Turk . We can assume the labels generated by AMT are weak because we are not sure whether the distant labelers were concentrated enough or in a good mood or not . In addition , we may have a smaller set of labelers who are experts and concentrated on the annotation task . In this sense , we can consider the labels generated by the second group as strong labels . Our proposed framework can be used in cases where this split of the dataset into {small and strong labels , large and weak labels} is possible . We tested our framework on NLP and IR tasks because the weak labels can be generated by a well - known heuristic function . However , the weak labels can also be generated by a separate set of mass labelers whose performance is weaker than a small set of strong labelers as was pointed out in the answer to the previous question . As another example in the field of machine vision , we can think of a pre-trained weak classifier which acts based on hand crafted features like SIFT or HoG . This trained classifier can then be used to label a large set of images and assign each image a weak label . In this framework , SIFT - based classifier substitutes the heuristic weak annotator of our paper . The other components and the overall framework remain unchanged .

Here are some questions that comes to my mind :
Q1 - Why first building a student model only using the weak data and why not all the data together to train the student model ? To me , it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combining with the strong data ?

A1 - We had experiments when both weak and strong data is used to build the representation ( let ’s call it mixed setup ) . For both tasks , we observed no statistically significant difference ( based on paired two - tailed t-test ) between the performance of mixed setup and the setup proposed in the manuscript where only weak data is used to build the representation . Here are the results of these experiments :

Ranking task :
[ Robust04 dataset : Map=0.3105 / nDCG@20=0.46211 ]
[ ClueWeb dataset : Map=0.1456 / nDCG@20=0.2439 ]

Sentiment Classification task :
[ SemEval - 14 : F1 = 0.7474 ]
[ SemEval - 15 : F1 = 0.6811 ]

We think the overall scores do not change since the strong data will eventually contribute to the parameter updates of the representation learning layer of the student model in step # 3 ( in Figure 1.c , the representation layer in the student model benefits from the gradient updates of samples from D_{sw} , which includes data with strong labels as well ) . Considering the fact that the final scores do not change significantly , we choose the current exposition as it is more generic in the sense that student does not need to see the strong labels in the step # 1 . This is important especially when weak and strong data are not available together due to , for instance , privacy issues .

Q2 - What are the sensitivity of the procedure to how weakly the weak data are annotated ( this could be studied using both toy example and real - world examples ) ?

A2 - In the original version of the submission , we have a small experiment in section “ 4.1 Handling The Bias -Variance Trade -off ” , in which instead of f( x ) = 2sinc ( x ) , we use f( x ) = x + 1 as a weaker annotator and we observed worse performance in particular for high values of the parameter \beta . In this experiment , we actually aimed at studying the effect of parameter \beta . We agree that having analysis on the sensitivity of the FWL to the quality of the weak annotation is beneficial , so we added a subsection , “ 4.3 . The sensitivity of the FWL to the Quality of the Weak Annotator ” , to the revised version of the submission in which we discussed the performance of FWL on the task of ranking , given four weak annotators with different accuracies . As it is expected , the performance of FWL depends on the quality of the employed weak annotator . We also observed that the better the performance of the weak annotator was , the less the improvement of FWL over its corresponding weak annotator on test data would be .

In this paper , the authors produced quite cool videos showing the acquisition of highly complex skills , and they are happy about it . If you read the conclusion , this is the only message they put forward , and to me this is not a scientific message .

A more classical summary is that the authors use PPO , a state - of - the - art deep RL method , in a context where two agents are trained to perform competitive games against each other . They reuse a very recent " dense reward " technique to bootstrap the agent skills , and then anneal it to zero so that the competitive rewards obtained from defeating the opponent takes the lead . They study the effect of this annealing process ( considered as a curriculum ) and of various strategies for sampling the opponents . The main outcome is the acquisition of a large variety of useful skills , just observed from videos of the competitions .

The main issue with this paper is the lack of scientific analysis of the results , together with many local issues in the presentation of these results .
Below , I talk directly to the authors .

---------------------------------

The related work subsection is just a list of works , it should explain how the proposed work position itself with respect to these works .


In Section 5.2 , you are just describing " cool " behaviors observed from your videos .
Science is about producing quantitative results , analyzing them and discussing them .
I would be glad to read more science about these cool behaviors . Can you define a repertoire of such behaviors ?
Determine how often they are discovered ? Study how the are represented in the networks ?
Anything beyond " look , that 's great ! " would make the paper better ...

By the end of Section 5.2 , you allude to transfer learning phenomena .
It would be nice to study these transfer effects in your results with a quantitative methodology .

Section 5.3 is more scientific , but it has serious issues .

In all subfigures in Figure 3 , the performance of opponents should be symmetric around 50 % . This is not the case for subfigures ( b ) and ( c - 1 ) . Why ?
Do they correspond to non- zero sum game ? The x-label is " version " . Do n't you mean " number of epochs " , or something like this ? Why do the last 2 images
share the same caption ?

I had a hard time understanding the message from Table 1 . It really needs a line before the last row and a more explicative caption .

Still in 5.3 , " These results echo " ... : can you characterize this echo ? What is the relationship to this other work ?

Again , " These results shed further light " : further with respect to what ? Can you be more explicit about what we learn ?

Also , I find that annealing a kind of reward with respect to another is a weak form of curriculum learning . This should be further discussed .

In Section 5.4 , the idea of using many opponents from many stages of learning in not new .
If I 'm correct , the same was done in evolutionary method to escape the " arms race " dead - end in prey-predator races quite a while ago ( see e.g. " Coevolving predator and prey robots : Do “ arms races ” arise in artificial evolution ? " Nolfi and Floreano , 1998 )

Section 5.5.1 would deserve a more quantitative presentation of the effect of randomization .
Actually , in Fig5 : the axes are not labelled . I do n't believe it shows a win-rate . So probably the caption ( or the image ) is wrong .

In Section 5.5.2 , you " suspect this is because ... " .
The role of a scientific paper is to clearly establish results and explanation from solid quantitative analysis .

-------------------------------------------
More local comments :

Abstract :

" Normally , the complexity of the trained agent is closely related to the complexity of the environment . " Here you could cite Herbert Simon ( 1962 ) .

" In this paper , we point out that a competitive multi-agent environment trained with self - play can produce behaviors that are far more complex than the environment itself . "
Well , for an agent , the other agent ( s ) are part of its environment , are n't they ? So I do n't like this perspective that the environment itself is " simple " .

Intro :

" RL is exciting because good RL exists . " I do n't believe this is a strong argument . There are many good things that exist which are not exciting .

" In general , training an agent to perform a highly complex task requires a highly complex environment , and these can be difficult to create . " Well , the standard perspective is the other way round : in general , you face a complex problem , then you need to design a complex agent to solve it , and this is difficult .

" This happens because no matter how weak or strong an agent is , an environment populated with other agents of comparable strength provides the right challenge to the agent , facilitating maximally rapid learning and avoiding getting stuck . " This is not always true . The literature is full of examples where two - players competition end - up with oscillations between to solutions rather than ever-increasing skill performance . See the prey-predator literature pointed above .

" in the domain of continuous control , where balance , dexterity , and manipulation are the key skills . " In robotics , dexterity , and manipulation usually refer to using the robot 's hand ( s ) , a capability which is not shown here .

In preliminaries , notation , what you describe corresponds to the framework of Dec- POMDPs , you should position yourself with respect to this framework ( see e.g. Memory - Bounded Dynamic Programming for DEC-POMDPs . S Seuken , S Zilberstein )

In PPO description : Let l_t ( \theta ) ... denote the likelihood ratio : of what ?

p5 :
would train on the dense reward for about 10 - 15 % of the trainig epochs . So how much is \alpha_t ? How did you tune it ? Was it hard ?

p6 :

you give to the agent the mass : does the mass change over time ???

In observations : Are both agents given different observations ? Could you specify which is given what ?

In Algorithms parameters : why do you have to anneal longer for kick - and -defend ? What is the underlying phenomenon ?

In Section 5 , the text mentions Fig5 before Fig4 .

-------------------------------------------------
Typos :

p4 :
research ( Andrychowicz => missing space
straight forward => straightforward

p5 :
agent like humanoid ( s )
from exi ( s ) ting work

p6 :
eq . 1 => Eq. ( 1 ) ( you should use \eqref{} )
In section 4.1 => In Section 4.1 ( same p7 for Section 4.2 )

" One question that arises is the extent to which the outcome of learning is affected by this exploration reward and to explore the benefit of this exploration reward . As already argued , we found the exploration reward to be crucial for learning as otherwise the agents are unable to explore the sparse competition reward . " => One question that arises is the extent to which the outcome of learning is affected by this exploration reward and to explore its benefit . As already argued , we found it to be crucial for learning as otherwise the agents are unable to explore the sparse competition reward .

p8:
in a local minima => minimum

p9 :
in references , you have Jakob Foerster and Jakob N Foerster => try to be more consistent .

p10 , In Laetitia Matignon et al . ... markov => Markov

p11 , I would rename C_{alive} as C_{standing }

There is a strong disagreement among reviewers ( top paper according to one , clear rejection according to me ) , so I believe some discussion is necessary .

I completely agree that the videos are awesome , but in itself a nice video does not make a strong scientific paper . Engineering ( and most of time a good deal of science ) is about producing new interesting phenomena , but science is also about rigorously analyzing these results ( rigorously generally means in a quantitative way ) and extracting scientific messages from these analyses .

Quoting this excellent blog post
http://www.inference.vc/my-thoughts-on-alchemy/
" It 's Okay to use non-rigorous methods , but ... it 's not okay to use non-rigorous evaluation " .

Let me just take 3 examples to show you that this paper lacks rigor everywhere ( the first two are admittedly minor points , but they feed my general feeling about this paper . Most other points are present in my review below , and the authors did not make the effort to address all of them ) :

Taken from their reply to my review :

1 ) " “ In all subfigures in Figure 3 , the performance of opponents should be symmetric around 50 % . This is not the case for subfigures ( b ) and ( c - 1 ) . Why ? Do they correspond to non- zero sum game ? ”
Reply :
The games are not zero-sum , there is some chance of a draw as described in Section 3 . "

Well , in case of a draw , both players should get 0 , so this cannot be a correct explanation of why the game is not zero sum . If I can not trust the answer , can I trust the results themselves ?

2 ) 4 . " “ Why do the last 2 images share the same caption ? ”
Reply :
Because the kick - and - defend game is asymmetric , so there are two plots -- one where keeper is trained with curriculum and another where kicker is trained with curriculum . "

Fine . I checked in the new version . The only sentence about Fig3 is " We plot the average win-rates over 800 games at various intervals during training in Fig. 3 , for the sumo and kick - and - defend environments . " Not a word about the above fact . The reader has to guess that . By the way , why do n't we get any result for the " run to goal " case ? What are the " various intervals " ? Etc.

3 ) This one is much more serious , about transfer learning . Any rigorous transfer learning paper will precisely define a source domain and a target domain and will measure ( quantitatively ) the performance in the target domain with and without training first in the source domain . Here we just get " The agent trained in a non competitive setting falls over immediately ( with episodes lasting less than 10 steps on average ) , whereas the competitive agent is able to withstand strong forces for hundreds of steps . This difference ( in terms of length of episode till agent falls over ) can be quantified easily over many random episodes , we only included the qualitative results as the difference is huge and easily visible in the videos . "
OK , this is awesome . But to me , the evaluation methodology can not just be " visual " . This would be perfectly okay for the " scientific american " magazine , but this is not OK for a strong scientific conference . The evaluation itself has to be performed by the computer , because this makes it necessary to rigorously define all the conditions of this evaluation ( define fall and stand , define the initial posture , define the number of steps you evaluate , define the wind variations , define everything ) . The computer will also be able to perform a statistical analysis , which is completely lacking here : did this phenomenon appear every time ? If not , what are the chances ? For instance , given the lack of quantitative analysis , one cannot determine if another method to come will perform better or worse .

To me , this paper is representative of what Ali Rahimi 's talk was about ( the talk was given after I wrote my review ) . So sorry guys , but though I like the videos and I must admit that my review is a little too harsh because the paper irritated me , I 'm still considering that this paper should be rejected .






Thank you for taking the time to review our work . We have taken care of the typos and appropriate minor changes in the updated draft . We answer specific questions below .

1 . “ In Section 5.2 , you are just describing " cool " behaviors observed from your videos ..... Study how the are represented in the networks ? Anything beyond " look , that 's great ! " would make the paper better … ”
Reply :
There are four main contributions put forth in this paper : ( 1 ) Identifying that a competitive multi-agent interaction can serve as a proxy for complexity of the environment and allows for a natural curriculum for training RL agents ( 2 ) Developing 4 new simulated environments which can serve as a test - bed for future research on training competitive agents ( Section 3 ) ( 3 ) Developing opponent - sampling strategies and a simple ( yet effective ) strategy for dealing with sparse rewards ( Section 4 ) -- both of which lead to effective training in the competitive environments ( as evaluated through quantitative ablation studies in section 5.3 and 5.4 ) ( 4 ) Demonstrating compelling results on the four competitive 3D environments with two different agent morphologies -- showing remarkable dexterity in the agents without explicit supervision .
We believe qualitative evaluation through observing agents ’ behavior is an important part of evaluating the success of the proposed contributions , and in section 5.2 we analyze qualitatively many random episodes . Section 4 discusses the parallel implementation of PPO ( a recent policy optimization algorithm , see section 2 ) , opponent sampling and exploration curriculum which are crucial technical ideas making the results possible . Section 5.3 , 5.4 , 5.5 contain rigorous quantitative analysis of the main ideas which make these results possible .
The results might be what one may expect , but executing on the idea is very much non-trivial -- as also noted by other reviewers . This paper is an exposé of the potential of competitive multi-agent training and we agree there is a lot of potential for more future work in this area .

2 . “ By the end of Section 5.2 , you allude to transfer learning phenomena . It would be nice to study these transfer effects in your results with a quantitative methodology . ”
Reply :
We studied a particular case of transfer in the sumo environment , where an agent trained in a competitive setting demonstrated robust standing behavior in a single - agent setting without any modifications or fine- tuning of the policies . The agent trained in a non competitive setting falls over immediately ( with episodes lasting less than 10 steps on average ) , whereas the competitive agent is able to withstand strong forces for hundreds of steps . This difference ( in terms of length of episode till agent falls over ) can be quantified easily over many random episodes , we only included the qualitative results as the difference is huge and easily visible in the videos .

3 . “ In all subfigures in Figure 3 , the performance of opponents should be symmetric around 50 % . This is not the case for subfigures ( b ) and ( c - 1 ) . Why ? Do they correspond to non- zero sum game ? ”
Reply :
The games are not zero-sum , there is some chance of a draw as described in Section 3.

4 . “ Why do the last 2 images share the same caption ? ”
Reply :
Because the kick - and - defend game is asymmetric , so there are two plots -- one where keeper is trained with curriculum and another where kicker is trained with curriculum .

5 . “ I had a hard time understanding the message from Table 1 . It really needs a line before the last row and a more explicative caption . ”
Reply :
Added . It is also described in detail in Section 5.4

6 . “ Also , I find that annealing a kind of reward with respect to another is a weak form of curriculum learning . This should be further discussed . ”
This is discussed in section 4.1

7 . “ Actually , in Fig5 : the axes are not labelled . I do n't believe it shows a win-rate . So probably the caption ( or the image ) is wrong . ”
Reply :
The y-label is the fractional win-rate ( and not % ) , we have clarified this .

8 . “ would train on the dense reward for about 10 - 15 % of the training epochs . So how much is \alpha_t ? How did you tune it ? Was it hard ? ”
Reply :
Note that \alpha_t is an annealing factor , so it ’s value starts from 1 and is annealed to 0 over some number of epochs . 10 - 15 % of training epochs is the typical window for annealing \alpha_t and the exact values are given in the experiments section 5.1 . There is no direct tuning of \alpha_t required , instead the horizon for annealing was tuned in { 250 , 500 , 750 , 1000 } epochs and the value giving highest win-rate was selected . More quantitative analysis of annealing is in section 5.3


Understanding how -and - why complex motion skills emerge is an complex and interesting problem .
The method and results of this paper demonstrate some good progress on this problem , and focus on
the key point that competition introduces a natural learning curriculum .
Multi-agent competitive learning has seen some previous work in setting involving physics - based skills
or actual robots . However , the results in this paper are compelling in taking this another good step forward .
Overall the paper is clearly written and I believe that it will have impact .

list of pros & cons
+ informative and unique experiments that demonstrate emergent complexity coming from the natural curriculum
provided by competitive play , for physics - based settings
+ likely to be of broad interest
- likely large compute resources needed to replicate or build on the results
- paper is not anonymous to this reviewer , given the advance publicity for this work when it was released
== > overall this paper will have impact and advances the state of the art , particular wrt to curriculums
In many ways , it is what one might expect . But executing on the idea is very much non-trivial .

other comments

Can you comment on the difficulty of designing the " games " themselves ?
It is often difficult to decide apriori when a game is balanced ; game designers of any kind
spend significant time on this . Perhaps it is easier for some of the types of games investigated in
this paper , but if you did have any issues with games becoming unbalanced , that would be worthwhile commenting on .
Game design is also the next level of learning in many ways . :-)

The opponent sampling strategy is one of the key results of the paper .
It could be brought to the fore earlier , i.e. , in the abstract .

How much do the exploration rewards matter ?
If two classes of agents are bootstrapped with different flavours of exploration rewards , how much would it matter ?

It would be generally interesting to describe when during the learning various " strategies " emerged ,
and in what order .

Adding sensory delays might enable richer decoy strategies .

The paper could comment on the further additional complexity that might result from situations
that allow for collaboration as well as competition . ( ok , I now see that this is mentioned in the conclusions )

The Robocup tournaments for robot soccer ( real and simulated ) have for a long time provided
a path to growing skills and complexity , although under different constraints , and perhaps less interesting
in terms of one - on - one movement skills .

Section 2 , " Notation "
why are the actions described as being discrete here , when the paper uses continuous actions ?
Also , " $ \pi_{\theta}$ can be Gaussian " : better to say that it * is * Gaussian in this paper .

" lead to different algorithm *s * "

Are there torque limits , and if so , what are they ?

sec 4 : " We do multiple rollouts for each agent * pair * " ( ? )

" Such rewards have been previously researched for simple tasks like walking forward and standing up "
Given the rather low visual quality and overly - powerful humanoids of the many of the published " solutions " ,
perhaps " simple " is the wrong qualifer .

Figure 2 : curve legend ?

" exiting work " ( sic )

4.2 Opponent sampling :
" simultaneously training " should add " in opponent pairs " ( ?)

5.1 " We use both MLP and LSTM "
should be " We compare MLP and LSTM ... " ( ?)

For " kick and defend " and " you shall not pass " , are there separate attack and defend policies ?
It seems that these are unique in that the goals are not symmetric , whereas for the other tasks they are .
Would be worthwhile to comment on this aspect .

episodic length T , eqn ( 1 )
It 's not clear at this point in the paper if T is constant or not .

Observations : " we also give the centre-of - mass based inertia * tensor * " ( ? )

" distance from the edge of the ring "
How is this defined ?

" none of the agents observe the complete global state "
Does this really make much of a difference ? Most of the state seems visible .

" But these movement strategies " -> " These movement strategies ... "

sec 5.4 suggest to use $ \mathrm { Uniform} ( ... ) $

" looses by default " ( sic )


We thank the reviewer for carefully reading the paper and their encouraging feedback . We have taken care of the appropriate minor changes in the updated draft . We answer specific questions below .

` 1 . " Can you comment on the difficulty of designing the " games " themselves ? "
Reply :
We picked a few simple competitive games for these results . It is important to appropriately set termination conditions for the games . We did observe some difficulty in games becoming unbalanced , for example in kick - and - defend it was important to use a longer horizon for annealing exploration reward as the defender takes a longer time to learn and adding termination penalties for moving beyond the goal area as well as additional rewards for defending and not falling over on termination lead to more balanced win-rates . In future we will explore more complex game designs where the agents can make use of environmental resources to compete or have to overcome environmental obstacles in addition to competing .

2 . " How much do the exploration rewards matter ? "
Reply :
We analyzed the extreme cases where agents have or do not have an exploration reward , as well as the case when the exploration reward is never annealed . The summary is to use exploration reward but anneal it . We also experimented with additional reward terms for interaction with opponent in Sumo environment initially but did n’t observe any significant benefits and chose the simplest form of exploration rewards .

3 . " Are there torque limits , and if so , what are they ? "
Reply :
We used the default limits in the gym environment for ant and humanoid body , which is bounded between [ - 0.4 , 0.4 ]

4 . " For " kick and defend " and " you shall not pass " , are there separate attack and defend policies ? It seems that these are unique in that the goals are not symmetric , whereas for the other tasks they are . "
Reply :
Yes , that is correct , we have noted this is section 5.1

5 . " distance from the edge of the ring . How is this defined ? "
Reply :
It is the radial distance of the agent from the edge . So if R is ring radius and r is the agent 's distance from center then we give ( R -r ) as input .


This paper demonstrates that a competitive multi-agent environment trained with self - play can produce behaviors that are far more complex than the environment itself and such environments come with a natural curriculum by introducing several multi-agent tasks with competing goals in a 3D world with simulated physics . It utilizes a decentralized training approach and use distributed implementation of PPO for very large scale multiagent training . This paper addresses the challenges in applying distributed PPO to train multiple competitive agents , including the problem of exploration with sparse reward by using full roll - outs and use the dense exploration reward which is gradually annealed to zero in favor of the sparse competition reward . It makes training more stable by selecting random old parameters for the opponent .

Although the technical contributions seem to be not quite significant , this paper is well written and introduces a few new domains which are useful for studying problems in multiagent reinforcement learning . The paper also makes it clear regarding the connections and distinctions to many existing work .

Minor issues :

E[ Loss ] in table 1 is undefined .

In the notation section , the observation model is missing , and the policy is restricted to be reactive .

Uniform ( v , \deta v) -> Uniform ( \deta v , v)


We thank the reviewer for carefully reading the paper and their positive feedback . We have taken care of the appropriate minor changes in the updated draft .

This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process .

The draft is well - written , and the method is clearly explained . However , I have the following concerns for this draft :

1 . The technical contribution is not enough . First , the use of reinforcement learning is quite straightforward . Second , the proposed method seems not significantly different from the architecture search method in [ 1 ] [ 2 ] – their major difference seems to be the use of “ remove ” instead of “ add ” when manipulating the parameters . It is unclear whether this difference is substantial , and whether the proposed method is better than the architecture search method .

2 . I also have concern with the time efficiency of the proposed method . Reinforcement learning involves multiple rounds of knowledge distillation , and each knowledge distillation is an independent training process that requires many rounds of forward and backward propagations . Therefore , the whole reinforcement learning process seems very time - consuming and difficult to be generalized to big models and large datasets ( such as ImageNet ) . It would be necessary for the authors to make direct discussions on this issue , in order to convince others that their proposed method has practical value .

[ 1 ] Zoph , Barret , and Quoc V. Le. " Neural architecture search with reinforcement learning . " ICLR ( 2017 ) .
[ 2 ] Baker , Bowen , et al . " Designing Neural Network Architectures using Reinforcement Learning . " ICLR ( 2017 ) .



>>> 2 . I also have concern with the time efficiency of the proposed method . Reinforcement learning involves multiple rounds of knowledge distillation , and each knowledge distillation is an independent training process that requires many rounds of forward and backward propagations . Therefore , the whole reinforcement learning process seems very time - consuming and difficult to be generalized to big models and large datasets ( such as ImageNet ) . It would be necessary for the authors to make direct discussions on this issue , in order to convince others that their proposed method has practical value .

The second point is a reasonable criticism , which we have ourselves mentioned and discussed in the appendix ( Section 12 ) . Efficiency is a criticism of current architecture search methods in general . Evaluating architectures to obtain a discriminative signal for learning is fundamentally expensive process and is currently an active research topic . Our paper does indeed address this by proposing several improvements over existing architecture search methods .

We define a bounded state space ( teacher architecture ) and a two -stage policy system in order to reduce the length of rollouts and make credit assignment more efficient ( Section 3.2 ) . We also demonstrate generalization experiments which could improve training on larger models ( Section 4.6 ) . We think that there are many interesting research directions that can be explored .

To address issues regarding efficiency of our approach more directly , we have run experiments on ImageNet32x32 [ 3 ] , which we will include in the next revision . We hope that this larger scale experiment directly addresses the concern of running too many iterations of training to find a good compressed architecture . We will also include average runtimes for various networks and datasets in order to give the reader a sense of how long the experiments take . We hope that these updated results would be sufficient to convince the reviewer that the approach is not prohibitive in practice .

[ 1 ] Zoph , Barret , and Quoc V. Le. " Neural architecture search with reinforcement learning . " ICLR ( 2017 ) .
[ 2 ] Baker , Bowen , et al . " Designing Neural Network Architectures using Reinforcement Learning . " ICLR ( 2017 ) .
[ 3 ] Chrabaszcz , Patryk et al . “ A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets . ” ArXiV ( 2017 ) .

>>>1 . The technical contribution is not enough . First , the use of reinforcement learning is quite straightforward . Second , the proposed method seems not significantly different from the architecture search method in [ 1 ] [ 2 ] – their major difference seems to be the use of “ remove ” instead of “ add ” when manipulating the parameters . It is unclear whether this difference is substantial , and whether the proposed method is better than the architecture search method .

We thank you for your comments and feedback . The second point is helpful to improving our work and we have taken steps to address the comment and improve the paper . The first point however , mischaracterizes our work by stating that “ the major difference is ‘ remove ’ instead of ‘ add ’ ” and is not supported by any discussion of the critical details of our approach . It also indiscriminately trivializes the significance of the growing field of model compression . While at a high level , model compression does require removing parameters as opposed to adding parameters , the important research question is * how * parameters should be removed . Our answer to this question is composed of three technical contributions and critical details of the paper : ( 1 ) Two-stage policy structure and design of search space ( 2 ) Generalization capabilities of learned compression agent ( 3 ) Multiobjective reward function and constraints . The criticism of our technical contribution was not supported by any discussion of these contributions -- we hope you can provide an evaluation based on these key details of our work , which we highlight below :

( 1 ) Two -stage policy structure and design of search space : We dedicated Section 3.2 to describing a novel two -stage learning procedure which is critical for learning a better model architecture for the task of model compression . The basic idea is that the architecture search performs a coarse - to- fine search strategy to evaluate large structural changes ( i.e. number of layers ) before fine tuning each component ( i.e. , filter size ) . This not only reduces the computation required to train models in the second stage ( since models have been compressed on a macro level in the first stage ) , but also reduces the dimensionality of the action sequence in both stages , making credit assignment easier . Again , our experiments provide empirical evidence showing that this approach works well . To the best of our knowledge , this is the first work to describe such a strategy and demonstrate its effectiveness . We would like to hear comments that take these technical contributions into account .

( 2 ) Generalization capabilities of our compression networks : Section ( 4.6 ) outlines our method for learning generalized policies for a family of networks ( e.g. , ResNet , VGG ) . The basic idea is that since many deep learning practitioners typically use a common subset of successful network architectures , it is essential that we can learn policies that can generalize across specific families of teacher networks . We have provided a method for learning such policies and have shown empirically that a single policy can be used for entire family of teacher networks . To the best of our knowledge , this is the first work to show the possibility of such generalization to families of architectures . Furthermore , this is a unique contribution of our method , and one that is not directly applicable to [ 1 , 2 ] since they build architectures from scratch while we use the teacher model as the initialization . R2 has offered no comments on this result and experiments .

( 3 ) Multiobjective reward function and constraints : Our task is not simply to obtain high statistical performance as in [ 1 , 2 ] , but to achieve compression while maintaining good statistical performance , a competing objective . [ 1 , 2 ] therefore cannot perform the task of automatic architecture search for compression . Our approach on the other hand , does perform automatic architecture search for compression . We achieve this by introducing model compression specific reward - balancing and constraint satisfaction approaches as detailed in Section 3.3 . One could argue that using the approaches of [ 1 , 2 ] with a modified reward of compression and accuracy could be compared to our approach . However , this approach could result in lengthy models which are small in number of parameters ( e.g. too many consecutive ReLUs ) . If an additional constraint on model length is added to the reward , the optimization procedure becomes harder and it is unclear whether such an approach would have any advantages over our proposed method . Section ( 11 ) on reward design covers some of the challenges with designing a reward function for model compression . This important contribution over [ 1 , 2 ] was left unmentioned by R2 ; we hope they can offer their opinion on this point .

Summary :
The manuscript introduces a principled way of network to network compression , which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model . The first policy , specialized on architecture selection , iteratively removes layers , starting with architecture of the teacher model . After the first policy is finished , the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding . This organization of the action space , together with a smart reward design achieves impressive compression results , given that this approach automates tedious architecture selection . The reward design favors low compression / high accuracy over high compression / low performance while the reward still monotonically increases with both compression and accuracy . As a bonus , the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models .

Review :
The manuscript describes the proposed algorithm in great detail and the description is easy to follow . The experimental analysis of the approach is very convincing and confirms the author ’s claims .
Using the teacher network as starting point for the architecture search is a good choice , as initialization strategies are a critical component in knowledge distillation . I am looking forward to seeing work on the research goals outlined in the Future Directions section .

A few questions / comments :
1 ) I understand that L_{1 , 2 } in Algorithm 1 correspond to the number of layers in the network , but what do N_{1 , 2 } correspond to ? Are these multiple rollouts of the policies ? If so , should n’t the parameter update theta_{{shrink , remove} , i} be outside the loop over N and apply the average over rollouts according to Equation ( 2 ) ? I think I might have missed something here .
2 ) Minor : some of the citations are a bit awkward , e.g. on page 7 : “ algorithm from Williams Williams ( 1992 ) . I would use the \citet command from natbib for such citations and \citep for parenthesized citations , e.g. “ ... incorporate dark knowledge ( Hinton et al. , 2015 ) ” or “ The MNIST ( LeCun et al. , 1998 ) dataset ... ”
3 ) In Section 4.6 ( the transfer learning experiment ) , it would be interesting to compare the performance measures for different numbers of policy update iterations .
4 ) Appendix : Section 8 states “ Below are the results ” , but the figure landed on the next page . I would either try to force the figures to be output at that position ( not in or after Section 9 ) or write " Figures X - Y show the results " . Also in Section 11 , Figure 13 should be referenced with the \ref command
5 ) Just to get a rough idea of training time : Could you share how long some of the experiments took with the setup you described ( using 4 TitanX GPUs ) ?
6 ) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments ?
7 ) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution ?

Overall , this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript , I would consider giving it the full score .

We thank R3 for their thorough and detailed review of the paper . We have included our responses to the questions below and made the relevant changes to our paper where required .

>>> 1 ) I understand that L_{1 , 2 } in Algorithm 1 correspond to the number of layers in the network , but what do N_{1 , 2 } correspond to ? Are these multiple rollouts of the policies ? If so , should n’t the parameter update theta_{{shrink , remove} , i} be outside the loop over N and apply the average over rollouts according to Equation ( 2 ) ? I think I might have missed something here .
# 1 . The rollouts were omitted in order to simplify the presentation of the algorithm . N refers to the number of total iterations ( or policy updates ) for which the policy is trained .
>>> 2 ) Minor : some of the citations are a bit awkward , e.g. on page 7 : “ algorithm from Williams Williams ( 1992 ) . I would use the \citet command from natbib for such citations and \citep for parenthesized citations , e.g. “ ... incorporate dark knowledge ( Hinton et al. , 2015 ) ” or “ The MNIST ( LeCun et al. , 1998 ) dataset ... ”
# 2 . Thank you for this suggestion , we have fixed the citations in the new revision .
>>> 3 ) In Section 4.6 ( the transfer learning experiment ) , it would be interesting to compare the performance measures for different numbers of policy update iterations .
# 3 . Figure 10 in the appendix shows the plots over multiple policy update iterations when the pre-trained policies are used .
>>> 4 ) Appendix : Section 8 states “ Below are the results ” , but the figure landed on the next page . I would either try to force the figures to be output at that position ( not in or after Section 9 ) or write " Figures X - Y show the results " . Also in Section 11 , Figure 13 should be referenced with the \ref command
# 4 . We have fixed this in the new revision .
>>> 5 ) Just to get a rough idea of training time : Could you share how long some of the experiments took with the setup you described ( using 4 TitanX GPUs ) ?
# 5 . We have added a new section to the appendix in the revision providing details on the runtime of the experiments . In general , the shortest experiment ( VGG -13/MNIST ) took about 4 hours , while the longest experiment ( ResNet34 / ImageNet32x32 ) took about 272 hours in total . The experiments actually only used a single TitanX GPU . We have updated the paper to reflect this .
>>> 6 ) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments ?
# 6 . Yes we used standard data augmentation techniques . This is discussed in Section 10.2 of the paper .
>>> 7 ) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution ?
# 7 . We say the network is degenerate if its size exceeds that of the teacher or if the size of the FC layer is greater than 50,000 .

On the positive side the paper is well written and the problem is interesting .

On the negative side there is very limited innovation in the techniques proposed , that are indeed small variations of existing methods .


We thank you for the review . We would appreciate if the review contained a more concrete technical discussion of the work instead of unsupported negative statements . We hope that the reviewer appreciates that we have put in substantial work into this paper and is willing to continue this discussion in a more meaningful manner .

This review appears to repeat the criticism brought up by R2 , which we strongly disagree with . We have explained our stance and provided supporting details in the response below .

Our answer to this question is composed of three technical contributions and critical details of the paper : ( 1 ) Two-stage policy structure and design of search space ( 2 ) Generalization capabilities of learned compression agent ( 3 ) Multiobjective reward function and constraints . The criticism of our technical contribution was not supported by any discussion of these contributions -- we hope you can provide an evaluation based on these key details of our work , which we highlight below :

( 1 ) Two -stage policy structure and design of search space : We dedicated Section 3.2 to describing a novel two -stage learning procedure which is critical for learning a better model architecture for the task of model compression . The basic idea is that the architecture search performs a coarse - to- fine search strategy to evaluate large structural changes ( i.e. number of layers ) before fine tuning each component ( i.e. , filter size ) . This not only reduces the computation required to train models in the second stage ( since models have been compressed on a macro level in the first stage ) , but also reduces the dimensionality of the action sequence in both stages , making credit assignment easier . Again , our experiments provide empirical evidence showing that this approach works well . To the best of our knowledge , this is the first work to describe such a strategy and demonstrate its effectiveness . We would like to hear comments that take these technical contributions into account .

( 2 ) Generalization capabilities of our compression networks : Section ( 4.6 ) outlines our method for learning generalized policies for a family of networks ( e.g. , ResNet , VGG ) . The basic idea is that since many deep learning practitioners typically use a common subset of successful network architectures , it is essential that we can learn policies that can generalize across specific families of teacher networks . We have provided a method for learning such policies and have shown empirically that a single policy can be used for entire family of teacher networks . To the best of our knowledge , this is the first work to show the possibility of such generalization to families of architectures . R1 has offered no comments on this result and experiments .

( 3 ) Multiobjective reward function and constraints : Our task is not simply to obtain high performance as in [ 1 , 2 ] , but to achieve compression while maintaining good performance , a competing objective . [ 1 , 2 ] therefore cannot perform the task of automatic architecture search for compression . Our approach on the other hand , does perform automatic architecture search for compression . We achieve this by introducing model compression specific reward - balancing and constraint satisfaction approaches as detailed in Section 3.3 . One could argue that using the approaches of [ 1 , 2 ] with a modified reward of compression and accuracy could be compared to our approach . However , this approach could result in lengthy models which are small in number of parameters ( e.g. too many consecutive ReLUs ) . If an additional constraint on model length is added to the reward , the optimization procedure becomes harder and it is unclear whether such an approach would have any advantages over our proposed method . Section ( 11 ) on reward design covers some of the challenges with designing a reward function for model compression . This important contribution over [ 1 , 2 ] was left unmentioned by R1 ; we hope they can offer their opinion on this point .

[ 1 ] Zoph , Barret , and Quoc V. Le. " Neural architecture search with reinforcement learning . " ICLR ( 2017 ) .
[ 2 ] Baker , Bowen , et al . " Designing Neural Network Architectures using Reinforcement Learning . " ICLR ( 2017 ) .

The paper extends the idea of eigenoptions , recently proposed by Machado et al . to domains with stochastic transitions and where state features are learned . An eigenoption is defined as an optimal policy for a reward function defined by an eigenvector of the matrix of successor representation ( SR ) , which is an occupancy measure induced here by a uniform policy . In high -dimensional state space , the authors propose to approximate that matrix with a convolutional neural network ( CNN ) . The approach is evaluated in a tabular domain ( i.e. , rooms ) and Atari games .

Overall the paper is well - written and quite clear . The proposed ideas for the extension seem natural ( i.e. , use of SR and CNN ) . The theorem stated in the paper seems to provide an interesting link between SR and the Laplacian . However , a few points are not clear to me :
- Is the result new or not ? If I understand correctly , Stachenfeld et al . discussed this result , but did n't prove it . Is that correct ? So the provided proof is new ?
- Besides , how are D and W exactly defined ?
- Finally , as the matrix is not symmetric , do real eigenvalues always exist ?

The execution of the proposed ideas in the experiments was a bit disappointing to me . The approximated eigenoption was simply computed as a one -step greedy policy . Besides , the eigenoptions seem to help for exploration ( as a uniform policy was used ) as indicated by plot 3 ( d ) , but could they help for other tasks ( e.g. , learn to play Atari games faster or better ) ? I think that would be a more useful measure for the learned eigenoptions .

During learning SR and the features , what would be the impact if the gradient for SR estimation were also propagated ?

In Figure 4 , the trajectories generated by the different eigenoptions are barely visible .

Some typos :
- Section 2.1:
in the definition of G_t , the expectation is taken over p as well
I_w and T_w should be a subset of S

- in ( 2 ) , the hat is missing over \ Psi
in the definition of v_\pi( s ) , r only depends on s' ? This seems inconsistent with the previous definition of \psi

- p. 6 :
in the definition of L_{SR} ( s , s ' ) , why \psi takes \ phi( s ) as argument ?

- in conclusion :
that that

Thank you for your feedback and thorough review . We believe our paper is better now that we took your input into consideration .

Regarding the execution of the proposed ideas in the experiments , in the new version of our submission we provide a different measure of the usefulness of eigenoptions . We now also show , in the tabular case , how they can be used to improve the agent ’s control performance ( Figure 4 in the main text and Figures 16 - 19 in the Appendix ) . In this new set of experiments the agent takes a random walk over eigenoptions while learning to maximize reward with primitive actions . Such an approach speeds up learning dramatically as a consequence of the agent being able to better explore the environment .

The responses to the other questions asked are itemized below :

-- Is the result new or not ? If I understand correctly , Stachenfeld et al . discussed this result , but did n't prove it . Is that correct ? So the provided proof is new ?

Yes , that is correct . Stachenfeld et al. ( 2014 ) discussed the result but did not provide a formal proof of it , nor the relationship between the eigenvalues of both approaches . Also , because the authors provided an informal discussion , they were not very precise in their claims , ignoring for example the fact that the equivalence we discuss is true only if the generated graph is regular ( i.e. , the size of the action set is the same across every state ) . To be precise , below is what Stanchenfeld et al. ( 2014 ) wrote : “ Under a random walk policy , the transition matrix is given by $ T = D^{ - 1 }W $ . If $ \phi$ is an eigenvector of the random walk ’s graph Laplacian $ I - T$ , then $ D^{1 / 2 } \phi$ is an eigenvector of the normalized graph Laplacian . The corresponding eigenvector for the discounted Laplacian , $ I - \gamma T$ , is $ \gamma \phi$ . Since the matrix inverse preserves the eigenvectors , the normalized graph Laplacian has the same eigenvectors as the SR , $ M = ( I - \gamma T ) ^{ - 1 }$ , scaled by $ \gamma D^{ - 1 /2 }$ . ”


-- Besides , how are D and W exactly defined ?

D and W are defined for PVFs . We use the same definition given by Machado et al . ( 2017 ) : W is the graph ’s adjacency matrix and D is the diagonal matrix whose entries are the row sums of W. W( i , j ) is defined to be 1 if there is an action that allows the agent to go from state i to state j.

-- As the matrix is not symmetric , do real eigenvalues always exist ?

We thank the reviewer for the question about the matrix symmetry and its eigenvalues because we made this discussion clearer in the paper now . The eigenvalues / eigenvector are not necessarily real for the eigendecomposition of an asymmetric matrix . However , the right eigenvectors obtained from the singular value decomposition of the matrix are always real ( this is what we do in the ALE experiments ) .

-- During learning SR and the features , what would be the impact if the gradient for SR estimation were also propagated ?

We did not investigated this possibility . Because this is the first demonstration of eigenoptions discovery from raw pixels , we wanted to keep the learning process as simple as possible . Thus , we avoided the interaction between the loss function of the SR and the reconstruction error . This is something we plan to investigate in future work .

-- In Figure 4 , the trajectories generated by the different eigenoptions are barely visible .

This was in fact intentional . We did not want to use those results to focus on the trajectory that led the agent to the highlighted state , but on the final state itself . We did so by plotting the mass of visitation of each state . If the trajectories were visible , it would mean that the agent was navigating through the environment without a clear purpose . We wanted to show the exact opposite . That the agent was clearly spending the vast majority of the time in a specific location . We made this clearer in the updated version of our submission .

-- in the definition of L_{SR} ( s , s ' ) , why \psi takes \ phi( s ) as argument ?

In the definition of $ L_{SR} ( s , s’ ) $ , $ \psi$ takes $ \phi( s ) $ as argument because we are implicitly referring to Figure 2 , in which we labeled the output of some layers as functions . We define $ \psi$ to be the SR module while $ \phi$ is the output of the representation learning module . We really appreciate this question , it made us realize the need to further clarify this in the paper , which we also did in the updated version of our submission .

Naturally , we also fixed all the typos you listed ( thank you for that ) .


- This paper shows an equivalence between proto value functions and successor representations . It then derives the idea of eigen options from the successor representation as a mechanism for option discovery . The paper shows that even under a random policy , the eigen options can lead to purposeful options

- I think this is an important conceptual paper . Automatic option discovery from raw sensors is perhaps one of the biggest open problems in RL research . This paper offers a new conceptual setup to look at the problem and consolidates different views ( successor repr , proto values , eigen decomposition ) in a principled manner .

- I would be keen to see eigen options being used inside of the agent . Have authors performed any experiments ?

- How robust are the eigen options for the Atari experiments ? Basically how hand picked were the options ?

- Is it possible to compute eigenoptions online ? This seems crucial for scaling up this approach

Thank you for your kind review . We have just updated our submission to include a new set of results where the eigenoptions are used inside of the agent . We show how eigenoptions can also be used to improve the agent ’s control performance ( Figure 4 in the main text and Figures 16 - 19 in the Appendix ) . We show that one can take a random walk over eigenoptions while learning to maximize reward with primitive actions . Such an approach speeds up learning dramatically , since the agent can better explore the environment .

Regarding the robustness of the eigenoptions in the Atari experiments , we were able to select the options in a fairly straightforward way : by looking at those that generated a high density of visitation in a particular location on the screen . We did have multiple similar options we ended - up not reporting for clarity . We also had options that would not move the agent to anywhere ( probably because of gamma being set to 0 ) and others in which the agent was happy regardless of the action taken ( likely because the agent was trying to maximize features that were not under its control ) . We consider the results presented in this paper promising because we were able to replicate , using raw pixels , the results Machado et al. ( 2017 ) obtained when using the RAM state of the game ( that encodes explicit information the agent cares about ) . However , we do think some extra work still needs to be done on option pruning . We do have some ideas , such as pruning options based on whether the agent can in fact maximize the returned generated by the eigenpurpose and pruning options that lead to the same distribution that other options lead . This is something we want to further investigate in a future work to allow us to easily obtain a set of useful options .

Finally , we are very excited about the direction of research you asked about ( computing eigenoptions online ) . This is definitely something we are planning to investigate in the future work . It should be possible to compute the eigenoptions online . There are incremental methods capable of estimating the singular value decomposition of a matrix , aside from other methods capable of discovering the top k eigenvectors of a matrix ( notice that our method is much more stable than eigenoptions obtained from PVFs , which needs to estimate the * bottom * k eigenvectors ) . Once we have the eigenvectors , we could actually learn all eigenoptions simultaneously through off - policy learning . Also , it is not far fetched to imagine an algorithm that learns the intra-option policy and the policy over options simultaneously , bootstrapping from the option - critic architecture , for instance .



Eigenoption Discovery Through the Deep Successor Representation

The paper is a follow up on previous work by Machado et al . ( 2017 ) showing how proto-value functions ( PVFs ) can be used to define options called “ eigenoptions ” . In essence , Machado et al. ( 2017 ) showed that , in the tabular case , if you interpret the difference between PVFs as pseudo- rewards you end up with useful options . They also showed how to extend this idea to the linear case : one replaces the Laplacian normally used to build PVFs with a matrix formed by sampling differences phi ( s ' ) - phi ( s ) , where phi are features . The authors of the current submission extend the approach above in two ways : they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear one . Interestingly , the way they do so is through the successor representation ( SR ) . Stachenfeld et al. ( 2014 ) have showed that PVFs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all SRs of an MDP . Thus , if we have the SR matrix we can replace the Laplacian mentioned above . This provides benefits already in the tabular case , since SRs naturally extend to domains with stochastic dynamics . On top of that , one can apply a trick similar to the one used in the linear case -- that is , construct the matrix representing the diffusion model by simply stacking samples of the SRs . Thus , if we can learn the SRs , we can extend the proposed approach to the nonlinear case . The authors propose to do so by having a deep neural network similar to Kulkarni et al . ( 2016 ) 's Deep Successor Representation . The main difference is that , instead of using an auto-encoder , they learn features phi( s ) such that the next state s' can be recovered from it ( they argue that this way psi( s ) will retain information about aspects of the environment the agent has control over ) .

This is a well - written paper with interesting ( and potentially useful ) insights . I only have a few comments regarding some aspects of the paper that could perhaps be improved , such as the way eigenoptions are evaluated .

One question left open by the paper is the strategy used to collect data in order to compute the diffusion model ( and thus the options ) . In order to populate the matrix that will eventually give rise to the PVFs the agent must collect transitions . The way the authors propose to do it is to have the agent follow a random policy . So , in order to have options that lead to more direct , " purposeful " behaviour , the agent must first wander around in a random , purposeless , way , and hope that this will lead to a reasonable exploration of the state space .

This problem is not specific to the proposed approach , though : in fact , any method to build options will have to resolve the same issue . One related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built : the diffusion time , or the expected number of steps between any two states of an MDP when following a random walk . First , although this metric makes intuitive sense , it is unclear to me how much it reflects control performance , which is what we ultimately care about . Perhaps more important , measuring performance using the same policy used to build the options ( the random policy ) seems somewhat unsatisfactory to me . To see why , suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state space . In this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the paper . Conversely , if the diffusion time were measured under another policy , it also seems likely that options built with a random policy would not perform so well ( assuming that the state space is reasonably large to make an exhaustive exploration infeasible ) . More generally , we want options built under a given policy to reduce the diffusion time of other policies ( preferably ones that lead to good control performance ) .

Another point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the Atari experiments described in Section 4.2 . In the last paragraph of page 7 the authors mention that eigenoptions are more effective in reducing the diffusion time than “ random options ” built based on randomly selected sub-goals . However , looking at Figure 4 , the terminal states of the eigenoptions look a bit like randomly - selected sub-goals . This is especially true when we note that only a subset of the options are shown : given enough random options , it should be possible to select a subset of them that are reasonably spread across the state space as well .

Interestingly , one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints . As explained in the second paragraph of page 8 , instead of learning policies to maximize the pseudo- rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead ( which is the same as having a policy learned with a discount factor of zero ) . The fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo- rewards that are very informative . As a comparison , when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic policies . Therefore , one could argue that the proposed approach not only generate useful options , it also gives rise to dense pseudo- rewards that make it easier to build the policies associated with them .

Thank you for such a careful analysis of our paper . The main point you raised was about how we evaluated the eigenoptions . Initially we did not evaluate the options beyond the diffusion time because this metric seems to be related to the agent ’s control performance ( Machado et al. , 2017 ) . However , after reading the reviews , we do realize this is not something we should gloss over . Thus , we have just updated our submission to include a new set of results , in the tabular case , showing how eigenoptions can also be used to improve the agent ’s control performance ( Figure 4 in the main text and Figures 16 - 19 in the Appendix ) . We show that one can take a random walk over eigenoptions while learning to maximize reward with primitive actions . Such an approach speeds up learning dramatically , since the agent can better explore the environment .

We hope this new experiment also addresses , at least partially , the concern about looking at the diffusion time only over a uniform random walk . We focus on the diffusion time under a random walk because we are interested in the setting in which the agent cannot easily stumble upon a non-zero reward in the environment . In this case , most model - free agents just act randomly . We do agree that ideally we should be able to do better than the random wandering our agents do . However , this is a very hard thing to do given the fact that the agent has no information about the world , as the reviewer points out with the fact that all papers in the literature rely on that . Hopefully this paper is a step towards this direction . Our evaluation does show that augmenting the agent ’s action set with eigenoptions makes the exploration process much more efficient in this case , after the short period action without purpose .

We also really appreciate the reviewer ’s interpretation of our results in the ALE . We added the discussion / contrast between the sparseness of rewards generated by random subgoals and the subgoals generated by the eigenoptions . We fully agree with that point . Finally , notice it is not straightforward to define a valid random subgoal state when using function approximation because states can not be uniquely identified . If we define , for example , a specific pixel configuration to be the random sub-goal , it is not clear we can actually observe such random configuration . Our algorithm naturally deals with this issue as well .


---- updates : ----

I had a ton of comments and concerns , and I think the authors did an admirable job in addressing them . I think the paper represents a solid empirical contribution to this area and is worth publishing in ICLR .

---- original review follows : ----

This paper is about learning sentence embeddings by combining a bunch of training signals : predicting the next & previous sentences ( skip-thought ) , predicting the sentence 's translation , classifying entailment relationships between two sentences , and predicting the constituent parse of a sentence . This is a simple idea that combines a bunch of things from prior work into one framework and yields strong results , outperforming most prior work on most tasks .

I think this paper is impressive in how it scales up training to use so many tasks and such large training sets for each task . That and its strong experimental results make it worthy of publication . It 's not very surprising that adding more tasks and data improves performance on average across downstream tasks , but it is nice to see the experimental results in detail . While many people would think of this idea , few would have the resources and expertise necessary to do it justice . I also like how the authors move beyond the standard sentence tasks to evaluate also on the Quora question duplicate task with different amounts of training data and also consider the sentence characteristic / syntactic property tasks . It would be great if the authors could release their pretrained sentence representation model so that other researchers could use it .

I do have some nitpicks here and there with the presentation and exposition , and I am concerned that at times the paper appears to be minimizing its weaknesses , but I think these are things that can be addressed in the next revision . I understand that sometimes it 's tempting to minimize one 's weaknesses in order to get a paper accepted because the reviewers may not understand the area very well and may get hung up on the wrong things . I understand the area well and so all the feedback I offer below comes from a place of desiring this paper 's publication while also desiring it to be as accurate and helpful for the community as possible .

Below I 'll discuss my concerns with the experiments and description of the results .

Regarding the results in Table 2 :

The results in Table 2 seem a little bit unstable , as it is unclear which setting to use for the classification tasks ; maybe it depends on the kind of classification being performed . One model seems best for the sentiment tasks ( " + 2L + STP " ) while other models seem best for SUBJ and MPQA . Adding parsing as a training task hurts performance on the sentence classification tasks while helping performance on the semantic tasks , as the authors note . It is unclear which is the best general model . In particular , when others write papers comparing to the results in this paper , which setting should they compare to ? It would be nice if the authors could discuss this .

The results reported for the CNN - LSTM of Gan et al . do not exactly match those of any single row from Gan et al , either v1 or v2 on arxiv or the published EMNLP version . How were those specific numbers selected ?

The caption of Table 2 states " All results except ours are taken from Conneau et al . ( 2017 ) . " However , Conneau et al ( neither the latest arxiv version nor the published EMNLP version ) does not include many of the results in the table , such as CNN - LSTM and DiscSent mentioned in the following sentence in the caption . Did the authors replicate the results of those methods themselves , or report them from other papers ?

What does bold and underlining indicate in Table 2 ? I could n't find this explained anywhere .

At the bottom of Table 2 , in the section with approaches trained from scratch on these tasks , I 'd suggest including the 89.7 SST result of Munkhdalai and Yu ( 2017 ) and the 96.1 TREC result of Zhou et al. ( 2016 ) ( as well as potentially other results from Zhou et al , since they report results on others of these datasets ) . The reason this is important is because readers may observe that the paper 's new method achieves higher accuracies on SST and TREC than all other reported results and mistakenly think that the new method is SOTA on those tasks . I 'd also suggest adding the results from Radford et al . ( 2017 ) who report 86.9 on MR and 91.4 on CR . For other results on these datasets , including stronger results in non-fixed - dimensional - sentence - embedding transfer settings , see results and references in McCann et al . ( 2017 ) . While the methods presented in this paper are better than prior work in learning general purpose , fixed - dimensional sentence embeddings , they still do not produce state - of - the- art results on that many of these tasks , if any . I think this is important to note .

For all tasks for which there is additional training , there 's a confound due to the dimensionality of the sentence embeddings across papers . Using higher - dimensional sentence embeddings leads to more parameters in the linear model being trained on the task data . So it is unclear if the increase in hidden units in rows with " + L " is improving the results because of providing more weights for the linear model or whether it is learning a better sentence representation .

The main sentence embedding results are in Table 2 , and use the SentEval framework . However , not all tasks are included . The STS Benchmark results are included , which use an additional layer trained on the STS Benchmark training data just like the SICK tasks . But the other STS results , which use cosine similarity on the embedding space directly without any retraining , are only included in the appendix ( in Table 7 ) . The new approach does not do very well on those unsupervised tasks . On two years of data it is better than InferSent and on two years it is worse . Both are always worse than the charagram - phrase results of Wieting et al ( 2016a ) , which has 66.1 on 2012 , 57.2 on 2013 , 74.7 on 2014 , and 76.1 on 2015 . Charagram - phrase trains on automatically - generated paraphrase phrase pairs , but these are generated automatically from parallel text , the same type of resource used in the " + Fr " and " + De " models proposed in this submission , so I think it should be considered as a comparable model .

The results in the bottom section of Table 7 , reported from Arora et al ( 2016 ) , were in turn copied from Wieting et al ( 2016 b ) , so I think it would make sense to also cite Wieting et al ( 2016 b ) if those results are to be included . Also , it does n't seem appropriate to designate those as " Supervised Approaches " as they only require parallel text , which is a subset of the resources required by the new model .

There are some other details in the appendix that I find concerning :

Section 8 describes how there is some task - specific tuning of which function to compute on the encoder to produce the sentence representation for the task . This means that part of the improvement over prior work ( especially skip-thought and InferSent ) is likely due to this additional tuning . So I suppose to use these sentence representations in other tasks , this same kind of tuning would have to be done on a validation set for each task ? Does n't that slightly weaken the point about having " general purpose " sentence representations ?

Section 9 provides details about how the representations are created for different training settings . I am confused by the language here . For example , the first setting ( " + STN + Fr + De " ) is described as " A concatenation of the representations trained on these tasks with a unidirectional and bidirectional GRU with 1500 hidden units each . " I 'm not able to parse this . I think the authors mean " The sentence representation h_x is the concatenation of the final hidden vectors from a forward GRU ( with 1500 - dimensional hidden vectors ) and a bidirectional GRU ( also with 1500 - dimensional hidden vectors ) " . Is this correct ?

Also in Sec 9 : I found it surprising how each setting that adds a training task uses the concatenation of a representation with that task and one without that task . What is the motivation for doing this ? This seems to me to be an important point that should be discussed in Section 3 or 4 . And when doing this , are the concatenated representations always trained jointly from scratch with the special task only updating a subset of the parameters , or do you use the fixed pretrained sentence representation from the previous row and just concatenate it with the new one ? To be more concrete , if I want to get the encoder for the second setting ( " + STN + Fr + De + NLI " ) , do I have to train two times or can I just train once ? That is , the train - once setting would correspond to only updating the NLI - specific representation parameters when training on NLI data ; on other data , all parameters would be updated . The train-twice setting would first train a representation on " + STN + Fr + De " , then set it aside , then train a separate representation on " + STN + Fr + De + NLI " , then finally concatenate the two representations as my sentence representation . Do you use train - once or train-twice ?

Regarding the results in Table 3 :

What do bold and underline indicate ?

What are the embeddings corresponding to the row labeled " Multilingual " ?

In the caption , I ca n't find footnote 4 .

The caption includes the sentence " our embeddings have 1040 pairs out of 2034 for which atleast one of the words is OOV , so a comparison with other embeddings is n't fair on RW . " How were those pairs handled ? If they were excluded , then I think the authors should not report results on RW . I suspect that most of the embeddings included in the table also have many OOVs in the RW dataset but still compute results on it using either an unknown word embedding or some baseline similarity of zero for pairs with an OOV . I think the authors should find some way ( like one of those mentioned , or some other way ) of computing similarity of those pairs with OOVs . It does n't make much sense to me to omit pairs with OOVs .

There are much better embeddings on SimLex than the embeddings whose results are reported in the table . Wieting et al. ( 2016a ) report SimLex correlation of 0.706 and Mrkšić et al. ( 2017 ) report 0.751 . I 'd suggest adding the results of some stronger embeddings to better contextualize the embeddings obtained by the new method . Some readers may mistakenly think that the embeddings are SOTA on SimLex since no stronger results are provided in the table .


The points below are more minor / specific :

Sec. 2 :

In Sec. 2 , the paper discusses its focus on fixed - length sentence representations to distinguish itself from other work that produces sentence representations that are not fixed - length . I feel the motivation for this is lacking . Why should we prefer a fixed - length representation of a sentence ? For certain downstream applications , it might actually be easier for practitioners to use a representation that provides a representation for each position in a sentence ( Melamud et al. , 2016 ; Peters et al. , 2017 ; McCann et al. , 2017 ) rather than an opaque sentence representation . Some might argue that since sentences have different lengths , it would be appropriate for a sentence representation to have a length proportional to the length of the sentence . I would suggest adding some motivation for the focus on fixed - length representations .

Sec. 4.1:

" We take a simpler approach and pick a new task to train on after every parameter update sampled uniformly . An NLI minibatch is interspersed after every ten parameter updates on sequence - to- sequence tasks "
These two sentences seem contradictory . Maybe in the first sentence " pick a new task " should be changed to " pick a new sequence - to - sequence task " ?

Sec. 5.1:

typo : " updating the parameters our sentence " --> " updating the parameters of our sentence "

Sec. 5.2:

typo in Table 4 caption : " and The " -- > " . The "

typo : " parsing improvements performance " --> " parsing improves performance "


In general , there are many missing citations for the tasks , datasets , and prior work on them . I understand that the authors are pasting in numbers from many places and just providing pointers to papers that provide more citation info , but I think this can lead to mis-attribution of methods . I would suggest including citations for all datasets / tasks and methods whose results are being reported .


References :

McCann , Bryan , James Bradbury , Caiming Xiong , and Richard Socher . " Learned in translation : Contextualized word vectors . " CoRR 2017 .

Melamud , Oren , Jacob Goldberger , and Ido Dagan . " context2vec : Learning Generic Context Embedding with Bidirectional LSTM . " CoNLL 2016 .

Mrkšić , Nikola , Ivan Vulić , Diarmuid Ó. Séaghdha , Ira Leviant , Roi Reichart , Milica Gašić , Anna Korhonen , and Steve Young . " Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints . " TACL 2017 .

Munkhdalai , Tsendsuren , and Hong Yu . " Neural semantic encoders . " EACL 2017 .

Pagliardini , Matteo , Prakhar Gupta , and Martin Jaggi . " Unsupervised Learning of Sentence Embeddings using Compositional n- Gram Features . " arXiv preprint arXiv:1703.02507 ( 2017 ) .

Peters , Matthew E. , Waleed Ammar , Chandra Bhagavatula , and Russell Power . " Semi-supervised sequence tagging with bidirectional language models . " ACL 2017 .

Radford , Alec , Rafal Jozefowicz , and Ilya Sutskever . " Learning to generate reviews and discovering sentiment . " arXiv preprint arXiv:1704.01444 2017 .

Wieting , John , Mohit Bansal , Kevin Gimpel , and Karen Livescu . " Charagram : Embedding words and sentences via character n-grams . " EMNLP 2016a.

Wieting , John , Mohit Bansal , Kevin Gimpel , and Karen Livescu . " Towards universal paraphrastic sentence embeddings . " ICLR 2016b .

Zhou , Peng , Zhenyu Qi , Suncong Zheng , Jiaming Xu , Hongyun Bao , and Bo Xu. " Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling . " COLING 2016 .


Hi ,

Thank you for your thorough and in - depth review . We appreciate the feedback and constructive criticism . We ’ve addressed some your concerns below .

( 1 ) “ The results in Table 2 seem a little bit unstable , as it is unclear which setting to use for the classification tasks ; maybe it depends on the kind of classification being performed . One model seems best for the sentiment tasks ( " + 2L + STP " ) while other models seem best for SUBJ and MPQA . Adding parsing as a training task hurts performance on the sentence classification tasks while helping performance on the semantic tasks , as the authors note . It is unclear which is the best general model . In particular , when others write papers comparing to the results in this paper , which setting should they compare to ? It would be nice if the authors could discuss this . ”

( Response 1 ) Indeed , there is no free lunch when it comes to which of these models to pick . As we argue in the motivation of the paper , there may not always be a single ( or multiple ) training objective that results in improvements across all possible transfer learning benchmarks . To understand how the addition of tasks impacts performance quantitatively , below we report the average improvement across all 10 tasks from Table 2 over Infersent ( AllNLI ) for each row ( for our models ) . The results are : 0.01|0.99|1.33|1.39|1.47|1.74 ( * ) , where we have reversed the order of last two experiments to make this point clearer here and in the revised manuscript . The 1.74 score results from a model with more capacity . When capacity remains constant ( as in the other experiments ) we see from these results that adding more tasks helps on average , even for parsing ( ex . 1.39 vs 1.47 for a model of same architecture ) . However , adding more capacity in the + 2L experiment also helps . While measuring the average improvement over Infersent on all SentEval tasks makes sense in this work , it may not necessarily be a good metric for other approaches , such as the sentiment neuron work of Radford et al . that learns to encode some aspects of a sentence better than others .

* For MRPC and STSB we consider only the F1 score and Spearman scores respectively and we also multiply the SICK - R scores by 100 to have all differences in the same scale .

( 2 ) “ The results reported for the CNN - LSTM of Gan et al . do not exactly match those of any single row from Gan et al , either v1 or v2 on arxiv or the published EMNLP version . How were those specific numbers selected ? ”

( Response 2 ) Thank you for pointing this out . The results reported for the CNN - LSTM model of Gan et al . correspond to the best of their combine + emb and combine models in their v1 arXiv submission . We made a small mistake when reporting their results for MPQA , which should be 89.1 instead of 89.0 . We have updated this row with results from their latest arXiv revision .

( 3 ) “ What does bold and underlining indicate in Table 2 ? I could n't find this explained anywhere . ”

( Response 3 ) We used the same underline and bold semantics as in Conneau et al ( 2017 ) . Bold numbers indicate the best performing transfer model on a given task . Underlines are used for each task to indicate both our best performing model as well as the best performing transfer model that is n't ours . We ’ve clarified this in our revised manuscript .

( 4 ) “ At the bottom of Table 2 , in the section with approaches trained from scratch on these tasks , I 'd suggest including the 89.7 SST result of Munkhdalai and Yu ( 2017 ) and the 96.1 TREC result of Zhou et al. ( 2016 ) ( as well as potentially other results from Zhou et al , since they report results on others of these datasets ) . The reason this is important is because readers may observe that the paper 's new method achieves higher accuracies on SST and TREC than all other reported results and mistakenly think that the new method is SOTA on those tasks . I 'd also suggest adding the results from Radford et al . ( 2017 ) who report 86.9 on MR and 91.4 on CR . For other results on these datasets , including stronger results in non-fixed - dimensional - sentence - embedding transfer settings , see results and references in McCann et al . ( 2017 ) . While the methods presented in this paper are better than prior work in learning general purpose , fixed - dimensional sentence embeddings , they still do not produce state - of - the- art results on that many of these tasks , if any . I think this is important to note . ”

( Response 4 ) These are great suggestions , we will certainly add more prior work in the “ Approaches trained from scratch on these tasks ” section of Table 2 . Aside from clearing up any confusion about our model being state - of - the-art on these tasks , this will also give readers a sense of the gap between transfer learning approaches and purely supervised , task -specifc models . We ’ve edited our manuscript to add results from Munkhdalai and Yu ( 2017 ) , Zhou et al. ( 2016 ) and Radford et al . ( 2017 ) . Regarding comparisons with McCann et al. ( 2017 ) , it is unclear where it fits . It is a transfer approach but uses a heavily parameterized neural network as it ’s task specific classifier in contrast to this and previous work that has used linear classifiers .

( 5 ) “ For all tasks for which there is additional training , there 's a confound due to the dimensionality of the sentence embeddings across papers . Using higher - dimensional sentence embeddings leads to more parameters in the linear model being trained on the task data . So it is unclear if the increase in hidden units in rows with " + L " is improving the results because of providing more weights for the linear model or whether it is learning a better sentence representation . “

( Response 5 ) We observed improvements when increasing the number of hidden units even on tasks that are not evaluated using parametric models , such as in the STS * tasks ( Appendix Table 7 ) . We considered two models trained on the same subset of tasks but with different GRU hidden state sizes ( + STN + Fr + De + NLI vs + STN + Fr + De + NLI + L ) . The following were our results on the STS12/13/14/15/16 benchmarks ( Appendix Table 7 ) in the format |small vs large| . | 60.8 vs 61.2|53.5 vs 53.4|64 vs 65.3|73.4 vs 74.6|64.9 vs 66.2 | . These results have been added to the paper .

( 6 ) “ The main sentence embedding results are in Table 2 , and use the SentEval framework . However , not all tasks are included . The STS Benchmark results are included , which use an additional layer trained on the STS Benchmark training data just like the SICK tasks . But the other STS results , which use cosine similarity on the embedding space directly without any retraining , are only included in the appendix ( in Table 7 ) . The new approach does not do very well on those unsupervised tasks . On two years of data it is better than InferSent and on two years it is worse . Both are always worse than the charagram - phrase results of Wieting et al ( 2016a ) , which has 66.1 on 2012 , 57.2 on 2013 , 74.7 on 2014 , and 76.1 on 2015 . Charagram - phrase trains on automatically - generated paraphrase phrase pairs , but these are generated automatically from parallel text , the same type of resource used in the " + Fr " and " + De " models proposed in this submission , so I think it should be considered as a comparable model . “

( Response 6 ) We believe that our model does n’t do as well on the STS benchmarks since our sentence representations capture certain features about a sentence , like sentence length , word order , and others , which are n’t very informative for tasks like semantic similarity . Even with a simple parametric model such as logistic regression in Table 2 , the model can learn to down - weight such features and up - weight the ones that are actually useful to determine semantic similarity ( STSB ) . With the STS evaluations that use cosine similarities , sentence vectors are expected to be similar in “ every aspect ” , which may not be a realistic expectation . We ’ve also added charagram to Table 7 in our revised manuscript .

( 7 ) “ The results in the bottom section of Table 7 , reported from Arora et al ( 2016 ) , were in turn copied from Wieting et al ( 2016 b ) , so I think it would make sense to also cite Wieting et al ( 2016 b ) if those results are to be included . Also , it does n't seem appropriate to designate those as " Supervised Approaches " as they only require parallel text , which is a subset of the resources required by the new model . “

( Response 7 ) Thank you for pointing this out . We will certainly cite Weiting et al ( 2016 b ) . We designated those models as “ Supervised Approaches ” since they were marked as such in Arora et al ( 2016 ) .

( 8 ) “ Section 8 describes how there is some task - specific tuning of which function to compute on the encoder to produce the sentence representation for the task . This means that part of the improvement over prior work ( especially skip-thought and InferSent ) is likely due to this additional tuning . So I suppose to use these sentence representations in other tasks , this same kind of tuning would have to be done on a validation set for each task ? Does n't that slightly weaken the point about having " general purpose " sentence representations ? ”

( Response 8 ) We only tune the way in which we compute the fixed length sentence representations given all of the GRU ’s hidden states ( i.e. , by max - pooling or picking the last hidden state ) . The GRU itself is still “ general purpose ” . Moreover , there appears to be an clear trend to infer which tasks benefit from max- pooling ( sentiment related ) and which benefit from using the last hidden state ( others ) .

( 9 ) “ Section 9 provides details about how the representations are created for different training settings . I am confused by the language here . For example , the first setting ( " + STN + Fr + De " ) is described as " A concatenation of the representations trained on these tasks with a unidirectional and bidirectional GRU with 1500 hidden units each . " I 'm not able to parse this . I think the authors mean " The sentence representation h_x is the concatenation of the final hidden vectors from a forward GRU ( with 1500 - dimensional hidden vectors ) and a bidirectional GRU ( also with 1500 - dimensional hidden vectors ) " . Is this correct ? ”

( Response 9 ) Yes , this is correct . We ’ve clarified this in our revised manuscript according to your suggestion .

( 10 ) “ Also in Sec 9 : I found it surprising how each setting that adds a training task uses the concatenation of a representation with that task and one without that task . What is the motivation for doing this ? This seems to me to be an important point that should be discussed in Section 3 or 4 . And when doing this , are the concatenated representations always trained jointly from scratch with the special task only updating a subset of the parameters , or do you use the fixed pretrained sentence representation from the previous row and just concatenate it with the new one ? To be more concrete , if I want to get the encoder for the second setting ( " + STN + Fr + De + NLI " ) , do I have to train two times or can I just train once ? That is , the train - once setting would correspond to only updating the NLI - specific representation parameters when training on NLI data ; on other data , all parameters would be updated . The train-twice setting would first train a representation on " + STN + Fr + De " , then set it aside , then train a separate representation on " + STN + Fr + De + NLI " , then finally concatenate the two representations as my sentence representation . Do you use train - once or train-twice ? “

( Response 10 ) Our approach corresponds to your description of “ train-twice ” . We adopted the concatenation strategy described in this paper since it requires training only 1 model per set of training objectives ( except for the initial set of objectives : + STN + Fr + De ) .

( 11 ) “ Regarding the results in Table 3 : What do bold and underline indicate ? What are the embeddings corresponding to the row labeled " Multilingual " ? ”

( Response 11 ) Bold and underline have the same semantics as in Table 2 . The Multilingual embeddings correspond to Faruqui & Dyer ’s ( 2014 b ) “ Improving Vector Space Word Representations Using Multilingual Correlation ” ( we missed citing this paper , thanks for pointing this out ) .

( 12 ) “ The caption includes the sentence " our embeddings have 1040 pairs out of 2034 for which atleast one of the words is OOV , so a comparison with other embeddings is n't fair on RW . " How were those pairs handled ? If they were excluded , then I think the authors should not report results on RW . I suspect that most of the embeddings included in the table also have many OOVs in the RW dataset but still compute results on it using either an unknown word embedding or some baseline similarity of zero for pairs with an OOV . I think the authors should find some way ( like one of those mentioned , or some other way ) of computing similarity of those pairs with OOVs . It does n't make much sense to me to omit pairs with OOVs . ”

( Response 12 ) The evaluation suite provided by Faruqui & Dyer ( wordvectors.org ) , which we use , simply excludes words that are OOV . We have removed the RW column in the revised manuscript .
( 13 ) “ There are much better embeddings on SimLex than the embeddings whose results are reported in the table . Wieting et al. ( 2016a ) report SimLex correlation of 0.706 and Mrkšić et al. ( 2017 ) report 0.751 . I 'd suggest adding the results of some stronger embeddings to better contextualize the embeddings obtained by the new method . Some readers may mistakenly think that the embeddings are SOTA on SimLex since no stronger results are provided in the table . ”

( Response 13 ) Thank you for the pointers ! We ’ve added results from Weiting et al. ( 2016a ) and Mrkšić et al. ( 2017 ) to Table 3 in the revised manuscript .

( 14 ) “ In Sec. 2 , the paper discusses its focus on fixed - length sentence representations to distinguish itself from other work that produces sentence representations that are not fixed - length . I feel the motivation for this is lacking . Why should we prefer a fixed - length representation of a sentence ? For certain downstream applications , it might actually be easier for practitioners to use a representation that provides a representation for each position in a sentence ( Melamud et al. , 2016 ; Peters et al. , 2017 ; McCann et al. , 2017 ) rather than an opaque sentence representation . Some might argue that since sentences have different lengths , it would be appropriate for a sentence representation to have a length proportional to the length of the sentence . I would suggest adding some motivation for the focus on fixed - length representations . ”

( Response 14 ) A thorough analysis of the pros and cons of fixed - length versus variable length sentence representations is something we hope to explore in the future . Some of the advantages of fixed - length representations include ( a ) being able to compute a straightforward non- parametric similarity score between two sentences ( such as in the STS * tasks ) ( b ) easy to use simple task - specific classifiers on top of the features extracted by our RNN ( in contrast to work by McCann et al ( 2017 ) that uses a heavily parameterized task - specific classifier ) ( c ) easy to manipulate aspects of a sentence with gradient based optimization for controllable generation such as in Mueller et al ( 2017 ) .

Also , similar to McCann et al ( 2017 ) , it is possible to represent a sentence using the all of the RNN ’s hidden states instead of just the last as in this work . In the classic , attention free encoder - decoder paradigm the architecture encourages all the necessary information for subsequent tasks from a variable length input to be captured in a fixed length vector . Concatenation of the intermediate hidden unit representations is possible but would likely contain redundant information with respect to the final code layer . However , in future work , we would like to compare our approach more directly with McCann et al ( 2017 ) using their proposed Bi-attentive classification network with all of the hidden states of our general purpose multi-task GRU instead of just the last .

( 15 ) " We take a simpler approach and pick a new task to train on after every parameter update sampled uniformly . An NLI minibatch is interspersed after every ten parameter updates on sequence - to- sequence tasks "
These two sentences seem contradictory . Maybe in the first sentence " pick a new task " should be changed to " pick a new sequence - to - sequence task " ?

( Response 15 ) Thank you for the above suggestion and noticing typos , we ’ve made the edits you suggested to clarify things .

References

Mueller , Jonas , David Gifford , and Tommi Jaakkola . " Sequence to better sequence : continuous revision of combinatorial structures . " International Conference on Machine Learning . 2017 .

Follow - Up Comments
----

I continue to argue that this paper makes a contribution to a major open question , and clearly warrants acceptance .

I agree with R1 that the results do not tell a completely clear story , and that the benefits of pretraining are occasionally minimal or absent . However , R1 uses this as the basis to argue for rejection , which does not seem reasonable to me at all . This limitation is an empirical fact that the paper has done a reasonable job of revealing , and it does not take away the paper 's reason for existence , since many of the results are still quite strong , and the trends do support the merit of the proposed approach .

The authors mostly addressed my main concern , which was the relatively weak ablation . More combinations would be nice , but assuming reasonable resource constraints , I think the authors have done their due diligence , and the paper makes a clear contribution . I disagree with the response , though , that the authors can lean on other papers to help fill in the ablation — every paper in this area uses subtly different configurations .

I have one small lingering concern , which is not big enough to warrant acceptance : R2 's point 10 is valid — the use of multiple RNNs trained on different objectives in the ablation experiments unexpected and unusual , and deserves mention in the body of the paper , rather than only in an appendix .

----
Original Review
---

This paper explores a variety of tasks for the pretraining of a bidirectional GRU sentence encoder for use in data- poor downstream tasks . The authors find that the combination of supervised training with NLI , MT , and parsing , plus unsupervised training on the SkipThought objective yields a model that robustly outperforms the best prior method on every task included in the standard SentEval suite , and several others .

This paper is n't especially novel . The main results of the paper stem from a combination of a few ideas that were ripe for combination ( SkipThought from Kiros , BiLSTM - max and S/MNLI from Conneau , MT from McCann , parsing following Luong , etc . ) . However , the problem that the paper addresses is a major open issue within NLP , and the paper is very well done , so it would be in the best interest of all involved to make sure that the results are published promptly . I strongly support acceptance .

My one major request would be a more complete ablation analysis . It would be valuable for researchers working on other languages ( among others ) to know which labeled or unlabeled datasets contributed the most . Your ablation does not offer enough evidence to one to infer this --- among other things , NLI and MT are never presented in isolation , and parsing is never presented without those two . Minimally , this should involve presenting results for models trained separately on each of the pretraining tasks .

I 'll also echo another question from Samuel 's comment : Could you say more about how you conducted the evaluation on the SentEval tasks ? Did your task -specific model ( or the training / tuning procedure for that model ) differ much from prior work ?

Details :

The paragraph starting " we take a simpler approach " is a bit confusing . If task batches are sampled * uniformly * , how is NLI be sampled less often than the other tasks ?

Given how many model runs are presented , and that the results do n't uniformly favor your largest / last model , it 'd be helpful to include some kind of average of performance across tasks that can be used as a single - number metric for comparison . This also applies to the word representation evaluation table .

When comparing word embeddings , it would be helpful to include the 840B - word release of GloVe embeddings . Impressionistically , that is much more widely used than the older 6B - word release for which Faruqui reports numbers . This is n't essential to the paper , but it would make your argument in that section more compelling .

" glove " => GloVe ; " fasttext " => fast Text

Hi ,

Thank you for your reviews and positive feedback . We ’ve drafted responses to your concerns below .

( 1 ) " My one major request would be a more complete ablation analysis . It would be valuable for researchers working on other languages ( among others ) to know which labeled or unlabeled datasets contributed the most . Your ablation does not offer enough evidence to one to infer this --- among other things , NLI and MT are never presented in isolation , and parsing is never presented without those two . Minimally , this should involve presenting results for models trained separately on each of the pretraining tasks . "

( Response 1 ) Regarding your point and in response to Sam Bowman ’s comment we ran a model with just skipthoughts whose results have been added to Table 2 of the revised manuscript . Experiments for other tasks in isolation have already been presented in prior work ( ex : NLI - Conneau et al ( 2017 ) , MT - Hill et al ( 2015 ) , Skipthoughts - Kiros et al ( 2015 ) ) , the results from those experimental configurations are shown in Table 2 . Analysing the results of prior work in Table 2 in the context of our experiments we can examine the impact of different tasks on the quality of the learned representations . For example the results of Conneau et al ( 2017 ) that use NLI outperform Skipthoughts ( Kiros et al 2015 ) which in turn outperforms NMT En - Fr ( Hill et al 2015 ) . But as we note in our experiment ( + STN + Fr + De ) , it is possible to combine the benefits of skipthoughts and NMT to yield comparable performance to NLI ( delta of 0.01 from Infersent ) . Adding more tasks and increasing model capacity ( to match Conneau et al ) from this point appears to produce reasonable improvements . An even more thorough analysis of the impact of each task on the learned representations is definitely something we intend to focus on in future work .

( 2 ) " I 'll also echo another question from Samuel 's comment : Could you say more about how you conducted the evaluation on the SentEval tasks ? Did your task -specific model ( or the training / tuning procedure for that model ) differ much from prior work ? "

( Response 2 ) We used the SentEval evaluation suite , as is , with a single change from the default 5 - fold cross validation to 10 - fold cross validation . This was to match the same setting used by Conneau et al ( 2017 ) , following instructions on their GitHub page “ kfold ( int ) : k in the kfold -validation . Set to 10 to be comparable to published results ( default : 5 ) ” .

We also tuned the way in which the fixed - length sentence representation is computed given the hidden states corresponding to each word , by either max - pooling all the hidden states or selecting the last hidden state .

( 3 ) " The paragraph starting " we take a simpler approach " is a bit confusing . If task batches are sampled * uniformly * , how is NLI be sampled less often than the other tasks ? "

( Response 3 ) All tasks except NLI are sampled uniformly . We intersperse one NLI minibatch after 10 updates on other tasks as described . We have changed the description in the revised manuscript to indicate that we sample a new sequence - to- sequence task uniformly , making it clear that NLI is sampled less frequently .

( 4 ) " Given how many model runs are presented , and that the results do n't uniformly favor your largest / last model , it 'd be helpful to include some kind of average of performance across tasks that can be used as a single - number metric for comparison . This also applies to the word representation evaluation table . "

( Response 4 ) Thank you for suggesting this . To quantify transfer performance with a single number , we use the mean difference of our models from Infersent ( AllNLI ) in Table 2 across all 10 tasks ( * ) . The results are : 0.01|0.99|1.33|1.39|1.47|1.74 . As evident from the results , adding more tasks certainly seems to help on average , even in the parsing scenario ( 1.39 vs 1.47 for a model of same architecture and capacity ) ; however , adding more capacity ( + 2L ) seems to have a greater impact .

However , since the tasks presented in Table 2 are reasonably diverse , one might build representations that are suited to small subsets of these tasks but not others ( see for example Radford et al ( 2017 ) , who achieve very impressive results on sentiment classification tasks such as MR/CR / SST , etc . ) . Having a single evaluation score across all tasks might not be meaningful in such cases .

* For MRPC and STSB we consider only the F1 score and Spearman scores respectively and we also multiply the SICK - R scores by 100 to map all differences to the same scale .

( 5 ) " When comparing word embeddings , it would be helpful to include the 840B - word release of GloVe embeddings . Impressionistically , that is much more widely used than the older 6B - word release for which Faruqui reports numbers . This is n't essential to the paper , but it would make your argument in that section more compelling . "

( Response 5 ) Thank you for the suggestion . The results for the glove 840B vectors are : | 0.34|0.41|0.71|0.46|0.57|0.71|0.76|0.8 |. Since this release contains case - sensitive vectors , we only pick vectors for the lower - cased words . These results have been included in the revised manuscript .

This paper shows that learning sentence representations from a diverse set of tasks ( skip-thought objective , MT , constituency parsing , and natural language inference ) produces .
The main contribution of the paper is to show learning from multiple tasks improves the quality of the learned representations .
Experiments on various text classification and sentiment analysis datasets show that the proposed method is competitive with existing approaches .
There is an impressive number of experiments presented in the paper , but the results are a bit mixed , and it is not always clear that adding more tasks help .

I think this paper addresses an important problem of learning general purpose sentence representations .
However , I am unable to draw a definitive conclusion from the paper .
From Table 2 , the best performing model is not always the one with more tasks .
For example , adding a parsing objective can either improve or lower the performance quite significantly .
Could it be that datasets such as MRPC , SICK , and STSB require more understanding of syntax ?
Even if this is the case , why adding this objective hurt performance for other datasets ?
Importantly , it is also not clear whether the performance improvement comes from having more unlabeled data ( even if it is trained with the same training objective ) or having multiple training objectives .
Another question I have is that if there is any specific reason that language modeling is not included as one of the training objectives to learn sentence representations , given that it seems to be the easiest one to collect training data for .

The results for transfer learning and low resource settings are more positive .
However , it is not surprising that pretraining parts of the model on a large amount of unlabeled data helps when there is not a lot of labeled examples .

Overall , while the main contribution of the paper is that having multiple training objectives help learning better sentence , I am not yet convinced by the experiments that this is indeed the case .

Hi ,

Thank you for your reviews . We appreciate the feedback and constructive criticism . We ’ve drafted some responses to your comments below .

1 ) " I think this paper addresses an important problem of learning general purpose sentence representations .
However , I am unable to draw a definitive conclusion from the paper .
From Table 2 , the best performing model is not always the one with more tasks .
For example , adding a parsing objective can either improve or lower the performance quite significantly .
Could it be that datasets such as MRPC , SICK , and STSB require more understanding of syntax ?
Even if this is the case , why adding this objective hurt performance for other datasets ? "

( Response 1 ) In response to your concern about performance not strictly increasing with the addition of more tasks , we believe that there is no free lunch when it comes to which of these models to use . As we argue in the motivation of the paper , there may not always be a single ( or multiple ) training objective that results in improvements across all possible transfer learning benchmarks . Different training objectives will result in different learned inductive biases which in turn will affect results on different transfer learning tasks . In future work , we hope to uncover some of these inductive biases in greater detail which we can use to understand the relationships between the tasks used to train our model and its performance on a particular transfer task .

However , a closer look at the improvements obtained with the addition of more tasks in Table 2 reveals that adding more tasks does indeed help on average ( across all 10 tasks ) . To quantify transfer performance with a single number , we use the mean difference of our models from Infersent ( AllNLI ) in Table 2 across all 10 tasks ( * ) . The results are : 0.01|0.99|1.33|1.39|1.47|1.74 . As evident from the results , adding more tasks certainly seems to help on average , even in the parsing scenario ( 1.39 vs 1.47 for a model of same architecture and capacity ) however adding more capacity ( + 2L ) seems to have a bigger impact .

Regarding your concerns about not seeing improvements across all tasks when adding a new task , this could be because although we add more tasks , we do not increase model capacity . Increased capacity may be required to learn all tasks effectively , or it may be that the inductive biases learned with the addition of new tasks are not useful for some subset of tasks ( as you point out regarding the “ understanding of syntax ” in MRPC , SICK and STSB ) .

* For MRPC and STSB we consider only the F1 score and Spearman scores respectively and we also multiply the SICK - R scores by 100 to map all differences to the same scale .

( 2 ) " Importantly , it is also not clear whether the performance improvement comes from having more unlabeled data ( even if it is trained with the same training objective ) or having multiple training objectives .
Another question I have is that if there is any specific reason that language modeling is not included as one of the training objectives to learn sentence representations , given that it seems to be the easiest one to collect training data for . "

( Response 2 ) Since all models were run for a total of 7 days , those with the same capacity see the same number of training examples and also undergo the same number of parameter updates . Adding more tasks increases the diversity of data the model observes . The skip-thoughts experiment compared to the skip-thoughts + translation result illustrates this point particularly well , showing increased performance from adding the translation task which serves as a substitute for the alternative single task setup with additional skip-thought training data . Statistically speaking over even more experiments , as discussed above we observe a general trend that with a fixed training example budget the addition of new tasks generally improves performance .

We did not train our models with an ( unconditional ) language modeling objective because we believe that this does not lend itself easily to learning fixed length sentence representations . For example , the hidden state corresponding to the last word in a sentence is very unlikely to capture the entire history of the sentence . Also , a teacher - forced language modeling objective also emphasizes one- step - ahead prediction which we felt was not very well suited to learning representations that capture aspects of the sentence as a whole .

In this paper the authors propose to use RNNs and LSTMs for channel coding . But I have the impression the authors completely miss the state of the art in channel coding and the results are completely useless for any current communication system . I believe that machine learning , in general , and deep learning , in particular , might be of useful for physical layer communications . I just do not see why it would be useful for channel coding over the AWGN channel . Let me explain .

If the decoder knows that the encoder is using a convolutional code , why does it need to learn the decoder instead of using the Viterbi or BCJR algorithms that are known to be optimal for sequences and symbols , respectively . I cannot imagine an scenario in which the decoder does not know the convolutional code that it is being used and the encoder sends 120,000 bits of training sequence ( useless bits from information standpoint ) for the decoder to learn it . More important question , do the authors envision that this learning is done every time there is a new connection or it is learnt once and for all . If it is learnt every time that would be ideal if we were discovering new channel codes everyday , clearly not the case . If we learnt it one and for all and then we incorporated in the standard that would only make sense if the GRU structure was computationally better than the BCJR or Viterbi . I would be surprise if it is . If instead of using 2 or 3 memories , we used 6 - 8 does 120,000 bits be good enough or we need to exponentially increase the training sequence ? So the first result in the paper shows that a tailored structure for convolutional encoding can learn to decode it . Basically , the authors are solving a problem that does not need solving .

For the Turbocodes the same principle as before applies . In this case the comments of the authors really show that they do not know anything about coding . In Page 6 , we can read : “ Unlike the convolutional codes , the state of the art ( message - passing ) decoders for turbo codes are not the corresponding MAP decoders , so there is no contradiction in that our neural decoder would beat the message - passing ones ” . This is so true , so I expected the DNN structure to be significantly better than turbodecoding . But actually , they do not . These results are in Figure 15 page 6 and the solution for the turbo decoders and the DNN architecture are equivalent . I am sure that the differences in the plots can be explained by the variability in the received sequence and not because the DNN is superior to the turbodecoder . Also in this case the training sequence is measured in the megabits for extremely simple components . If the convolutional encoders were larger 6 - 8 bits , we would be talking about significantly longer training sequences and more complicated NNs .

In the third set the NNs seems to be superior to the standard methods when burst -y noise is used , but the authors seems to indicate that that NN is trained with more information about these bursts that the other methods do not have . My impression is that the authors would be better of focusing on this example and explain it in a way that it is reproducible . This experiment is clearly not well explained and it is hard to know if there is any merit for the proposed NN structure .

Finally , the last result would be the more interesting one , because it would show that we can learn a better channel coding and decoding mechanism that the ones humans have been able to come up with . In this sense , if NNs can solve this problem that would be impressive and would turn around how channel coding is done nowadays . If this result were good enough , the authors should only focus in it and forget about the other 3 cases . The issue with this result is that it actually does not make sense . The main problem with the procedure is that the feedback proposal is unrealistic , this is easy to see in Figure 16 in which the neural encoder is proposed . It basically assumes that the received real - valued y_k can be sent ( almost ) noiselessly to the encoder with minimal delay and almost instantaneously . So the encoder knows the received error and is able to cancel it out . Even if this procedure could be implemented , which it cannot be . The code only uses 50 bits and it needed 10^7 iterations ( 500 Mbs ) to converge . The authors do not show how far they are from the Shannon limit , but I can imagine that with 50 bit code , it should be pretty far .

We know that with long enough LDPC codes we can ( almost ) reach the Shannon limit , so new structure are not needed . If we are focusing on shorter codes ( e.g. latency ? ) then it will be good to understand why do we need to learn the channel codes . A comparison to the state of the art would be needed . Because clearly the used codes are not close to state of the art . For me the authors either do not know about coding or are assuming that we do not , which explains part of the tone of this review .



Q5 . NN is trained with more information about burst noise model than others ? My impression is that the authors would be better of focusing on this example and explain it in a way that it is reproducible .
A5 . The two state - of- the- art heuristics - erasure thresholding and saturation thresholding ( Safavi-Naeini et al. 2015 ) that we are comparing the NN decoder against to - fully utilize the knowledge on the bursty channel model . Specifically , the threshold value in those methods is choosen based on the burst noise model . We believe the experiment is fully explained and entirely reproducible ( we are also uploading our code base on Github after the ICLR review process is completed ) . Perhaps the reviewer can be specific on exactly which parts of the experiment could use a better explanation .

Q6 . The feedback proposal is unrealistic . It basically assumes that the received real - valued y_k can be sent ( almost ) noiselessly to the encoder with minimal delay and almost instantaneously . So the encoder knows the received noise and is able to cancel it out .
A6 . ( 1 ) The AWGN channel with output feedback is the most classical of models in communication theory ( studied by Shannon himself in 1956 . There has been a huge effort in the literature over the ensuing decades ( the important of which we have amply cited in our manuscript ) and is of very basic importance to multiple professional societies ( including IEEE communication society and IEEE Information theory society ) . Although idealistic , it provides a valuable training ground to understand how to use feedback to more efficiently communicate .
( 2 ) The " W " in the phrase AWGN ( which is our channel model refers to " white " which means the noise is memoryless across different time symbols . So even a single time step delay ( not to mention " minimal delay " ) does not allow the " the encoder knows the received noise and is able to cancel it out . " Perhaps the reviewer would like to reconsider his / her ratiocination ?

Below are detailed comments .

Q1 . Why should one use data to learn the Viterbi / BCJR / Turbo when we know them already ?
A1 . This is because , we only know the optimal algorithms in simple settings and how to generalize them to more complicated or unknown channel models is sometimes unclear . We demonstrate that neural networks that learn from data can yield more robust and adaptable algorithms in those settings . This is the point of Section 3 and is elaborated below .

Two advantages to RNN decoders , that go beyond mimicing Viterbi or BCJR :
( 1 ) Robustness : Viterbi and BCJR decoders are known to be vulnerable to changes in the channel , as those are highly tailored for the AWGN . We show in Section 3 , via numerical experiments with T-dsitributed noise , that the neural network decoder trained on AWGN is much more robust against the changes in the channel . This makes , among other things , our neural network decoder much more attractive alternative to Viterbi or BCJR decoders in practice , where the channel model is not available .
( 2 ) Adaptivity : It is not easy to extend the idea of Viterbi decoder and iterative Turbo decoding beyond the simple convolutional codes and the standard Gaussian channel ( or any other Discrete Memoryless Channel ) . On the other hand , our neural network decoder provides a new paradigm for decoding that can be applied to any encoder and any channel , as it learns from training examples . To showcase the power of this “ adaptivity ” , we show improved performance on bursty channels .

A more stark example of the utility presents itself in the feedback channel . There exists no known practical encoding - decoding scheme for a feedback channel . Only because we have a neural network decoder that can adapt to any encoder , we are able to find a novel encoder ( also NN based ) that uses the feedback information correctly and achieves the performance significantly better than any other competing schemes . This would have not been possible without a NN decoder and the techniques we learned in training one to mimic the simple Viterbi .

Q2 . Learning is done every time there is a new connection or it is learnt once and for all ?
A2 . We are not sure if we understand the question . The channel models in communication are *statistical * ( and in particular the AWGN one ) and codes are built to have a probabilistically good performance . The question of " learning done every time a connection is made " does not arise .

Q3 . If instead of using 2 or 3 memories , we used 6 - 8 does 120,000 bits be good enough or we need to exponentially increase the training sequence ?
A3 . First note that even the Viterbi / BCJR decoder 's computational complexity increases exponentially in the ( memory ) state dimension . While we have not tried a careful analysis on the complexity of neural decoders for codes with state dimension higher than 3 , we observe the following : from dimension 2 to 3 , we increased the size of neural decoder - from 2 layered bi- GRUs ( 200 hidden nodes ) to 2 layered bi-LSTMs ( 400 hidden nodes ) . The reason we havent explored a careful study of the memory size in neural network decoding is the following : modern coding theory recommends improving the ' quality ' of the convolutional code not by increasing the memory state dimension , but via the ' turbo effect ' . The advantage of turbo codes over convolutional codes is that ​it uses convolutional codes with a short memory as the constituent codes , but the interleaver allows for very long range memory , that is naturally decoded via iterative methods . The end result is that turbo codes can be decoded with a far lesser decoding complexity than convolutional codes with a very long memory , for the same BER performance . Indeed , turbo codes have largely replaced convolutional codes in modern practice .

Q4 . In Figure 15 page 6 , the solution for the turbo decoders and the DNN architecture are equivalent ? I am sure that the differences in the plots can be explained by the variability in the received sequence and not because the DNN is superior to the turbodecoder .
A4 . Turbo codes have been used in every day cellular communication systems since 2000 and their decoders have been highly optimized .
( 1 ) A nontrivial improvement in BER performance over state of the art would be considered a major development and discussion topic in standard body documents ( we refer the reviewer to Annex B of 3GPP TS 36.101 ( cited at the bottom of page 1 of our manuscript ) from this summer for a feel for how large an impact on standard body decisions minor improvements to BER performance can have ) . Indeed , a " significantly better than turbo decoding " performance would qualify as a very major result in communication theory with correspondingly large impact on practice .
( 2 ) Our BER / BLER performance is averaged over 100000 blocks and the standard deviation is infinitesimally small . This is contradiction to the statement of being " sure that it can be can be explained by the variability in the received sequence . "

Error-correcting codes constitute a well - researched area of study within communication engineering . In communication , messages that are to be transmitted are encoded into binary vector called codewords that contained some redundancy . The codewords are then transmitted over a channel that has some random noise . At the receiving end the noisy codewords are then decoded to recover the messages . Many well known families of codes exist , notably convolutional codes and Turbo codes , two code families that are central to this paper , that achieve the near optimal possible performance with efficient algorithms . For Turbo and convolutional codes the efficient MAP decodings are known as Viterbi decoder and the BCJR decoder . For drawing baselines , it is assumed that the random noise in channel is additive Gaussian ( AWGN ) .

This paper makes two contributions . First , recurrent neural networks ( RNN ) are proposed to replace the Viterbi and BCJR algorithms for decoding of convolutional and Turbo decoders . These decoders are robust to changes in noise model and blocklength - and shows near optimal performance .

It is unclear to me what is the advantage of using RNNs instead of Viterbi or BCJR , both of which are optimal , iterative and runs in linear time . Moreover the authors point out that RNNs are shown to emulate BCJR and Viterbi decodings in prior works - in light of that , why their good performance surprising ?

The second contribution of the paper constitutes the design and decoding of codes based on RNNs for a Gaussian channel with noisy feedback . For this channel the optimal codes are unknown . The authors propose an architecture to design codes for this channel . This is a nice step . However , in the performance plot ( figure 8 ) , the RNN based code - decoder does not seem to be outperforming the existing codes except for two points . For both in high and low SNR the performance is suboptimal to Turbo codes and a code by Schalkwijk & Kailath . The section is also super- concise to follow . I think it was necessary to introduce an LSTM encoder - it was hard to understand the overall encoder . This is an issue with the paper - the authors previously mentioned ( 8,16 ) polar code without mentioning what the numbers mean .

However , I overall liked the idea of using neural nets to design codes for some non-standard channels . While at the decoding end it does not bring in anything new ( modern coding theory already relies on iterative decoders , that are super fast ) , at the designing - end the Gaussian feedback channel part can be a new direction . This paper lacks theoretical aspect , as to no indication is given why RNN based design / decoders can be good . I am mostly satisfied with the experiments , barring Fig 8 , which does not show the results that the authors claim .


Thank you for your comments .

1 . Representability , Learnability and Generalization :
There are three aspects to showing that a learning problem can be solved through a parametric architecture .

( 1 ) Representability : The ability to represent the needed function through a neural network . For Viterbi / BCJR algorithms , this representability was shown in prior work by handcrafting parameters that represent the Viterbi / BCJR algorithms . We note that neural networks with sufficient number of parameters can indeed represent any function through the universal approximation theorem for feedforward networks and RNNs ( Cybenko , G.1989 , Siegelmann , H.T.&Sontag ,E.D.1995 ) and therefore this result is not that surprising .

( 2 ) Learnability : Can the required function be learnt directly through gradient descent on the observed data ? For Viterbi and BCJR , learnability was neither known through prior work nor is it obvious . One of the main contributions of our work is that those algorithms can be learnt from observed data .

( 3 ) Generalization : Does the learnt function / algorithm generalize to unobserved data ? We show this not only at the level of new unobserved codewords , but also show that the learnt algorithm trained on shorter blocks of length 100 can generalize well to longer blocks of length up to 10,000 . Such generalization is rare in many realistic problems .

To summarize , out of the three aspects , only representability was known from prior work ( and , we agree with the reviewer , that it is the least surprising given universal representability ) . Learnability and generalization of the learnt Viterbi and BCJR algorithms to much larger block lengths are both unknown from prior art and they are surprising , and interesting in their own right . We note that Viterbi and BCJR algorithms are useful in machine learning beyond communications problem , representing dynamic programming and forward - backward algorithms , respectively .

Peter Elias introduced convolutional codes in 1955 but efficient decoding through dynamic programming ( Viterbi decoding ) was only available in 1967 requiring mathematical innovation . We note that ability to learn the Viterbi algorithm from short block length data ( which can be generated by full - search ) and generalizing them to much longer blocks implies an alternative methodology to solve the convolutional code problem . Such an approach could have significant benefits in problems where corresponding mathematically optimal algorithms are not known at the moment .

We demonstrate the power of this approach by studying the problem of channel - with - feedback where no good coding schemes are known despite 70 years of research .

2 . Advantages of using RNNs instead of Viterbi or BCJR :
There are two advantages to RNN decoders , that go beyond mimicing Viterbi / BCJR .

( 1 ) Robustness : Viterbi and BCJR decoders are known to be vulnerable to changes in the channel , as those are highly tailored for the AWGN . We show in Section 3 , via numerical experiments with T-dsitributed noise , that the neural network decoder trained on AWGN is much more robust against the changes in the channel . This makes , among other things , our neural network decoder much more attractive alternative to Viterbi / BCJR decoders in practice , where the channel model is not available .

( 2 ) Adaptivity : It is not easy to extend the idea of Viterbi decoder and iterative Turbo decoding beyond the simple convolutional codes and the standard Gaussian channel ( or any other Discrete Memoryless Channel ) . On the other hand , our neural network decoder provides a new paradigm for decoding that can be applied to any encoder and any channel , as it learns from training examples . To showcase the power of this “ adaptivity ” , we show improved performance on the bursty channel .

A more stark example of the utility presents itself in the feedback channel . There exists no known practical encoding - decoding scheme for a feedback channel . Only because we have a neural network decoder that can adapt to any encoder , we are able to find a novel encoder ( also neural network based ) that uses the feedback information correctly and achieves the performance significantly better than any other competing schemes . This would have not been possible without a neural network decoder and the techniques we learned in training one to mimic the simple Viterbi .

3 . Updated curve for new codes on AWGN channel with feedback :
We have improved our encoder significantly by borrowing the idea of zero-padding from coding theory . In short , most of the errors occurs in the last bit , whose feedback information was not utilized by our encoder . We resolved this issue by padding a zero in the end of information bits ( Hence , the codeword length is 3 ( K + 1 ) for K information bits ) . This significantly improves the performance as shown in the new Figure 8 . A full description of the encoder - decoder architecture is provided in Appendix D.

4 . We replaced " ( 8,16 ) polar code " by “ rate 1 / 2 polar code over 8 information bits ” .

This paper shows how RNNs can be used to decode convolutional error correcting codes . While previous recent work has shown neural decoders for block codes results had limited success and for small block lengths .
This paper shows that RNNs are very suitable for convolutional codes and achieves state of the art performance for the first time .
The second contribution is on adaptivity outside the AWGN noise model . The authors show that their decoder performs well for different noise statistics outside what it was trained on . This is very interesting and encouraging . It was not very clear to me if the baseline decoders ( Turbo / BCJR ) are fairly compared here since better decoders may be used for the different statistics , or some adaptivity could be used in standard decoders in various natural ways .

The last part goes further in designing new error correcting schemes using RNN encoders and decoders for noisy feedback communication .
For this case capacity is known to be impossible to improve , but the bit error error can be improved for finite lenghts .
It seems quite remarkable that they beat Schalkwijk and Kailath and shows great promise for other communication problems .

The paper is very well written with good historical context and great empirical results . I think it opens a new area for information theory and communications with new tools .

My only concern is that perhaps the neural decoders can be attacked with adversarial noise ( which would not be possible for good - old Viterbi ) . It would be interesting to discuss this briefly .
A second ( related ) concern is the lack of theoretical understanding of these new decoders . It would be nice if we could prove something about them , but of course this will probably be challenging .



Thank you for your comments .

1 . Neural decoders can be attacked with adversarial noise :
This is a great point , which is related to the current ongoing advances in other areas of neural networks ( e.g. classification ) . At a high level , there are two types of adversarial noise that can hurt our approach . The first one is poisoning training data . If we are training on data collected from real channels , an adversary who knows that we are training can intervene and add adversarial noise to make our trained decoder useless . This proposes an interesting game between the designer ( us ) and the attacker , in the form of how much noise power does the adversary need in order to make our decoder fail . This we believe is a fascinating research question , and we will add discussions in the final version of our manuscript .
The second type of attack is adversarial examples , where at the test time an adversary changes the channel to make our decoder fail . In this scenario , both Viterbi and our decoder are vulnerable . Our numerical experiments on robustness is inspired by such scenarios , where we show that neural network decoders are more robust against such attacks ( or natural dynamic changes ) in Section 3.

2 . Fair comparison to baseline decoders :
There are two ways to run fair experiments on other channels , in our opinion . One is to mimic dynamic environment of real world by using encoder - decoders that are tailored for AWGN ( both Turbo / BCJR and Neural Network decoder trained on AWGN ) and see how robust it is against changes in the channel . This is the experiments we run with T-distribution . Another is , as suggested by the reviewer , to design decoders based on the new statistics of the channel that work well outside of AWGN . This is the experiments we run with bursty channels . We agree that these two experiments are addressing two different questions , but we believe we are fair in the comparisons to competing decoders within each setting .

3 . Theoretical understanding of these neural decoders / coding schemes is a challenging but very interesting future research direction .


******
Update : revising reviewer score to 6 after acknowledging revisions and improved manuscript
******

The authors propose a new regularization term modifying the VAE ( Kingma et al 2013 ) objective to encourage learning disentangling representations .
Specifically , the authors suggest to add penalization to ELBO in the form of - KL ( q ( z ) || p ( z ) ) , which encourages a more global criterion than the local ELBOs .
In practice , the authors decide that the objective they want to optimize is unwieldy and resort to moment matching of covariances of q( z ) and p( z ) via gradient descent .
The final objective uses a persistent estimate of the covariance matrix of q and upgrades it at each mini-batch to perform learning .

The authors use this objective function to perform experiments measuring disentanglement and find minor benefits compared to other objectives in quantitative terms .

Comments :
1 . The originally proposed modification in Equation ( 4 ) appears to be rigorous and as far as I can tell still poses a lower bound to log ( p ( x ) ) . The proof could use the result posed earlier : KL ( q ( z ) || p ( z ) ) is smaller than E_x KL ( q ( z|x ) ||p ( z|x ) ) .
2 . The proposed moment matching scheme performing decorrelation resembles approaches for variational PCA and especially independent component analysis . The relationship to these techniques is not discussed adequately . In addition , this paper could really benefit from an empirical figure of the marginal statistics of z under the different regularizers in order to establish what type of structure is being imposed here and what it results in .
3 . The resulting regularizer with the decorrelation terms could be studied as a modeling choice . In the probabilistic sense , regularizers can be seen as structural and prior assumptions on variables . As it stands , it is unnecessarily vague which assumptions this extra regularizer is making on variables .
4 . Why is using the objective in Equation ( 4 ) not tried and tested and compared to ? It could be thought that subsampling would be enough to evaluate this extra KL term without any need for additional variational parameters \psi . The reason for switching to the moment matching scheme seems not well motivated here without showing explicitly that Eq ( 4 ) has problems .
5 . The model seems to be making on minor progress in its stated goal , disentanglement . It would be more convincing to clarify the structural properties of this regularizer in a statistical sense more clearly given that experimentally it seems to only have a minor effect .
6 . Is there a relationship to NICE ( Laurent Dinh et al ) ?
7 . The infogan is also an obvious point of reference and comparison here .
8 . The authors claim that there are no models which can combine GANs with inference in a satisfactory way , which is obviously not accurate nowadays given the progress on literature combining GANs and variational inference .

All in all I find this paper interesting but would hope that a more careful technical justification and derivation of the model would be presented given that it seems to not be an empirically overwhelming change .

Thank you for the careful reading of the paper and thoughtful comments !

[ [ Lower bound to log p( x ) : ] ]
The proposed objective is indeed a lower bound to the evidence log p( x ) . However the proof is really trivial and does n’t need to use Inequality ( 3 ) . It can simply be shown by using nonnegativity of the distance between q( z ) and p( z ) ( KL or any other divergence ) -- since standard ELBO is a lower bound , subtracting any nonnegative quantity is also a lower bound .

[ [ Optimizing objective ( 4 ) : ] ]
Optimizing ( 4 ) with KL ( q ( z ) || p ( z ) ) ( = E_{q ( z ) } log ( q ( z ) / p ( z ) ) ) is not tractable with sampling because of the \log q( z ) term ( q ( z ) = E_{p ( x ) } q( z|x ) ) . One possibility ( as we discuss in the paper ) is to use variational form of KL as first proposed in ( Nguyen et al. , 2010 ) and later in ( Nowozin et al. , 2016 ) for GANs , however it will involve training a separate “ discriminator ” that is expected to approximate the KL divergence at every iteration , which can be minimized using backprop . We have tried this recently and found that the covariance matrix of z~q ( z ) has significant off- diagonal entries ( ie , it does not decorrelate the latents well ) .

[ [ Structural properties / assumptions in the regularizer : ] ]
The proposed regularizer KL ( q ( z ) || p ( z ) ) is trying to match the inferred prior q( z ) to the hypothesized prior p( z ) , which will automatically happen if p_\theta ( x ) is close to p( x ) ( the model is good ) and q( z|x ) is close to p( z|x ) . We have also discussed this in the paragraph just after Eq ( 3 ) . The regularizer is trying to enforce this explicitly which is totally natural ( unless the hypothesized prior itself is too unreasonable ) .

[ [ Relationship to NICE ( Dinh et al , 2016 ) : ] ]
NICE is another framework to do density estimation with latent variables where encoder and decoder are exact inverses of each other by design ( encoder maps from data distribution to a factored distribution of same dimensionality following a specific architecture that allows for easy inverse computation , and hence easy sampling as well as tractable maximum likelihood based learning ) . It can be compared / contrasted with the VAE which our proposed method is based upon -- eg , the reconstruction error is zero by design . We restrict ourselves to the VAE in this work .

[ [ Comparison with InfoGAN : ] ]
Unlike VAE , InfoGAN does not have a trained inference mechanism for real observations . Though it has a network Q ( shared with discriminator ) that implicitly minimizes E_{x ~ G ( z ) } KL ( p ( z|x ) || q( z||x ) ) , this is targeted towards inference for fake examples from the generator . [ Higgins et al , 2017 ] compare \beta - VAE with InfoGAN finding that \beta- VAE outperforms and one of the reasons could be the lack of a true inference model .

[ [ Inference in GANs : ] ]
We are aware of ALI [ Dumoulin et al , ‎ 2017 ] and BiGAN [ Donahue et al , 2017 ] which still suffer from bad reconstruction ( D ( E ( x ) ) is far from x ) as observed in [ Kumar et al , 2017 ] . There is also a recent work [ Arora et al , 2017 ] showing that the encoder in ALI / BiGAN may potentially learn non-informative codes .

[ Arora et al , 2017 ] Theoretical limitations of Encoder - Decoder GAN architectures , arXiv:1711.02651 2017
[ Dumoulin et al , ‎ 2017 ] Adversarially learned inference , ICLR 2017
[ Donahue et al , 2017 ] Adversarial feature learning , ICLR 2017
[ Kumar et al , 2017 ] Semi-supervised Learning with GANs : Manifold Invariance with Improved Inference , NIPS 2017


This paper describes DIP - VAE , an improvement on the beta- VAE framework for learning a disentangled representation of the data generative factors in the visual domain . The authors propose to augment the standard VAE ELBO objective with an extra term that minimises the covariance between the latents . Unlike the original beta- VAE objective which implicitly minimises such covariance individually for each observation x , the DIP - VAE objective does so while marginalising over x . This difference removes the tradeoff between reconstruction quality and disentangling reported for beta - VAE , since DIP - VAE maintains sensitivity of q( z|x ) to each observation x , and hence achieves disentangling while preserving the sharpness of reconstructions .

Pros :
- the paper is well written
- it makes a contribution to an important line or research ( unsupervised disentangled representation learning )
- the covariance minimisation proposed in the paper looks like an easy to implement yet impactful change to the VAE objective to encourage disentanglement while preserving reconstruction quality
- it directly compares the performance of DIP - VAE to that of beta- VAE showing significant improvements in terms of disentangling metric and reconstruction error

Cons :
- I am yet to be fully convinced how well the approach works . Table 1 and Figure 1 look good , but other figures are either tangental to the main point of the paper , or impossible to read due to the small scale . For example , the qualitative evaluation of the latent traversals is almost impossible due to the tiny scale of Table 5 ( should n't this be a Figure rather than a Table ? )
- The authors concentrate a lot on the CelebA dataset , however I believe the comparison with beta- VAE would be a lot clearer on the dSprites dataset ( https://github.com/deepmind/dsprites-dataset) ( the authors call it 2D shapes ) . I would like to see latent traversals of the best DIP - VAE vs beta - VAE to demonstrate good disentangling and the improvements in reconstruction quality . This ( and larger Table 5 ) would be better use of space compared to Table 2 and Figure 2 for example , which I feel are somewhat tangental to the main message of the paper and are better suited for the appendix .
- I wonder how the authors calculated the disentanglement metric on CelebA , given that the ground truth attributes in the dataset are often rather qualitative ( e.g. attractiveness ) , noisy ( many can be considered an inaccurate description of the image ) , and often do not align with the data generative factors discoverable through unsupervised modeling of the data distribution
- Table 3 - the legends for the axes are too small and are impossible to read . Also it would be helpful to normalise the scales of the heat plots in the second row .
- Table 3 - looking at the correlations with the ground truth factors , it seems like beta - VAE did not actually disentangle the latents . Would be nice to see the corresponding latent traversal plots to ensure that the baseline is actually trained well .

I am willing to increase my score for the paper if the authors can address my points . In particular I would like to see a clear comparison in terms of latent traversals on dSprites between beta-VAE and DIP - VAE models presented in Table 3 . I would also like to see where these particular models lie in Figure 1 .

---------------------------
---- UPDATE ----------
---------------------------
I have increased my score after reading the revised version of the manuscript .

Thank you for the careful reading of the paper and thoughtful comments ! Your comments encouraged us to look carefully into the alignment of Disentanglement metric score of [ Higgins et al , 2017 ] ( which we refer to as “ Z - diff score ” in the revised version ) with the quality of latent traversal plots . We found that it is not a reliable indicator of disentanglement , more so in the case of 2D Shapes ( dsprites ) data , and we were so far using the Z-diff score to pick the best model . We have taken this time to revise the paper as follows :

[ [ Latent traversal plots , along with a new metric for measuring disentanglement -- SAP Score : ] ]
We added a small section ( Sec 3 ) describing a novel metric for measuring disentanglement , referred as Separated Attribute Predictability ( SAP ) score . It works by fitting a slope / intercept ( for regression ) or a threshold ( real number , for classification ) to predict each of the known generative factors ( or attributes ) using each latent dimension individually . This gives us a matrix S of size ( # latent - dims x # attributes ) indicating goodness of linear fit of the generative factors to the individual latent dimensions . For each attribute ( column of S ) we take the difference of top two scores and average these for all attributes to get the final SAP score . We observe that this score is a much better indicator of disentanglement seen in the decoder ’s output for the single latent traversals . It is also easier to compute and does not involve training any classifier . We present the plots of both Z-diff score and SAP score vs the reconstruction error in the revised version ( Fig. 1 and 2 ) , along with the latent traversal plots for 2D Shapes data ( Fig. 3 and 4 ) .
It is clear from Fig 3 ( and from Fig 1 SAP scores vs reconstruction ) that VAE is the worst in terms of disentanglement . \ beta-VAE ( beta=4 ) improves over it for disentanglement but DIP - VAE -II ( lambda=5 ) outperforms it with better disentanglement and reconstruction quality . \ beta-VAE ( beta=60 ) provides even better disentanglement ( Fig 3 ) but scale remains entangled with X - pos and Y-pos . DIP - VAE -II ( lambda=500 ) yields less reconstruction error and better disentanglement in latent traversals with less entangling b/w scale and Y-pos / X-pos .

[ [ Two variants of DIP - VAE : ] ]
We have added another variant ( DIP - VAE - II , Eq. 7 ) in the revised version that yields better SAP - score / reconstruction - error tradeoff ( and latent traversals ) for 2D Shapes data ( please see Fig 1 ) . Note that in terms of Z-diff score / reconstruction error trade - off , DIP - VAE - I is still better than DIP - VAE - II for 2D Shapes data , which led us to conclude that Z-diff score is not a good metric for disentanglement .

[ [ Disentanglement metric ( Z - diff score ) for CelebA : ] ]
We compute this score for CelebA in the same manner as for 2D Shapes . Although some of the CelebA attributes can not be taken as “ generative factors ” , we still observe that disentanglement metric scores are correlated well with the disentanglement seen in the latent traversal plots .

[ [ Correlation plots in the submitted version : ] ]
The label - correlation plots shown in the submitted version for \beta- VAE ( beta=4 and 20 ) were for the fully - trained models after convergence ( as can be seen in Fig 3 , the generated images from the decoder are good ) . For these \beta values , the latent dimensions are indeed entangled as also reflected in the SAP score ( Fig 1 in the revised version ) and in the latent traversal plots ( Fig 3 in the revised version ) . However , we later found out that in the submitted version , \beta - VAE for \beta=60 , was not trained to convergence . We have fixed it in the revised version and \beta=60 gives the best SAP and Z-diff scores for \beta - VAE along with good disentanglement in the latent traversals ( Fig 3 ) , although with bad reconstruction quality . We have omitted correlation plots in the revised version as SAP score already captures it as the explained variance of linear regression fit .


########## UPDATED AFTER AUTHOR RESPONSE # # # # # # # # # #

Thanks for the good revision and response that addressed most of my concerns . I am bumping up my score .

###############################################


This paper presents a Disentangled Inferred Prior ( DIP - VAE ) method for learning disentangled features from unlabeled observations following the VAE framework . The basic idea of DIP - VAE is to enforce the aggregated posterior q( z ) = E_x [ q ( z | x ) ] to be close to an identity matrix as implied by the commonly chosen standard normal prior p( z ) . The authors propose to moment - match q( z ) given it is hard to minimize the KL- divergence between q( z ) and p ( z ) . This leads to one additional term to the regular VAE objective ( in two parts , on - and off-diagonal ) . It has the similar property as beta -VAE ( Higgins et al. 2017 ) but without sacrificing the reconstruction quality . Empirically the authors demonstrate that DIP - VAE can effectively learn disentangled features , perform comparably better than beta- VAE and at the same time retain the reconstruction quality close to regular VAE ( beta - VAE with beta = 1 ) .

The paper is overall well - written with minor issues ( listed below ) . I think the idea of enforcing an aggregated ( marginalized ) posterior q( z ) to be close to the standard normal prior p( z ) makes sense , as opposed to enforcing each individual posterior q( z|x ) to be close to p( z ) as ( beta- ) VAE objective suggests . I would like to make some connection to some work on understanding VAE objective ( Hoffman & Johnson 2016 , ELBO surgery : yet another way to carve up the variational evidence lower bound ) where they derived something along the same line of an aggregated posterior q( z ) . In Hoffman & Johnson , it is shown that KL ( q ( z ) | p ( z ) ) is in fact buried in ELBO , and the inequality gap in Eq ( 3 ) is basically a mutual information term between z and n ( the index of the data point ) . Similar observations have led to the development of VAMP - prior ( Tomczak & Welling 2017 , VAE with a VampPrior ) . Following the derivation in Hoffman & Johnson , DIP - VAE is basically adding a regularization parameter to the KL ( q ( z ) | p ( z ) ) term in standard ELBO . I think this interpretation is complementary to ( and in my opinion , more clear than ) the one that ’s described in the paper .

My concerns are mostly regarding the empirical studies :

1 . One of my main concern is on the empirical results in Table 1 . The disentanglement metric score for beta- VAE is suspiciously lower than what ’s reported in Higgins et al. , where they reported a 99.23 % disentanglement metric score on 2D shape dataset . I understand the linear classier is different , but still the difference is too large to ignore . Hence my current more neutral review rating .

2 . Regarding the correlational plots ( the bottom row of Table 3 and 4 ) , I do n’t think I can see any clear patterns ( especially on CelebA ) . I wonder what ’s the point of including them here and if there is a point , please explain them clearly in the paper .

3 . Figure 2 is also a little confusing to me . If I understand the procedure correctly , a good disentangled feature would imply smaller correlations to other features ( i.e. , the numbers in Figure 2 should be smaller for better disentangled features ) . However , looking at Figure 2 and many other plots in the appendix , I do n’t think DIP - VAE has a clear win here . Is my understanding correct ? If so , what exactly are you trying to convey in Figure 2 ?

Minor comments :

1 . In Eq ( 6 ) I think there are typos in terms of the definition of Cov_q ( z ) ( z ) ? It appears as only the second term in Eq ( 5 ) .

2 . Hyperparameter subsection in section 3 : Should n’t \lambda_od be larger if the entanglement is mainly reflected in the off-diagonal entries ? Why the opposite ?

3 . Can you elaborate on how a running estimate of Cov_p ( x ) ( \mu ( x ) ) is maintained ( following Eq ( 6 ) ) . It ’s not very clear at the current state of the paper .

4 . Can we have error bars in Table 2 ? Some of the numbers are possibly hitting the error floor .

5 . Table 5 and 6 are not very necessary , unless there is a clear point .

Thank you for the careful reading of the paper and thoughtful comments ! We were not aware of the work [ Hoffman & Johnson 2016 , ELBO Surgery ] and it looks like the method can also be motivated from that perspective . However we are not sure about your note “ the inequality gap in Eq ( 3 ) is basically a mutual information term between z and n ( the index of the data point ) ” -- it looks like the gap between KL ( q ( z|x ) ||p ( z ) ) and KL ( q ( z ) || p ( z ) ) is the mutual information term between z and n , whereas the Inequality ( 3 ) compares KL ( q ( z|x ) ||p ( z|x ) ) and KL ( q ( z ) || p ( z ) ) .

[ [ Disentanglement metric score on \beta - VAE : ] ]
We found out that \beta- VAE for \beta=60 was not trained to convergence which now gives the best metric score for \beta - VAE on 2DShapes data ( 95.7 % ) . We have updated the Table 1 and Figure 1 with new results . [ Higgins et al , 2017 ] report 99.23 % score on 2D shape which is close to what we get . Apart from the linear classifier , the difference could also be due to the evaluation protocol where in [ Higgins et al , 2017 ] that trained 30 \beta - VAE models with different random seeds and “ discarded the bottom 50 % of the thirty resulting scores and reported the remaining results ” ( quoting verbatim from [ Higgins et al , 2017 ] ) . We also discovered in this duration that the metric proposed in [ Higgins et al , 2017 ] is not a good indicator of disentanglement seen in the latent traversal plots ( ie , decoder ’s output by varying one latent while fixing others ) . We added a short section ( Sec 3 ) on the new metric we propose ( referred as Separated Attribute Predictability or SAP score ) which is much better aligned with the subjective disentanglement we see in the latent traversals . We have also added plots for SAP score vs reconstruction error ( Fig 1 and 2 ) .

[ [ Correlation plots : ] ]
We agree that these plots were not conveying any insights or quantitative measure for disentanglement . We have omitted them in the revised version .

[ [ Fig 2 in the submitted version : ] ]
As CelebA dataset has many ground truth attributes which are correlated with each other , it is not possible to infer different dimensions of latents capturing these ( at least with the current approaches ) . Through this plot we were trying to show that the top attributes corresponding to a given dimension are semantically more similar for our method compared to the baselines . As you rightly noticed this is a subjective question so we have omitted these plots in the revised version .

[ [ Cov_{q ( z ) } [ z ] in Eq 6 : ] ]
The first term in Cov_{q ( z ) } [ z ] in Eq 5 is a diagonal matrix ( expectation of variance of variational posterior , which is a Gaussian with diagonal covariance ) and contributes only to the variances of z~q ( z ) , so in the regularizer we had considered only the second term Cov_{p ( x ) } [ \mu ( x ) ] which is a dense square matrix . However we have now included another variant ( DIP - VAE - II ) where the regularizer uses complete Cov_{q ( z ) } [ z ] . This actually provides better results on 2D Shapes data .

[ [ Hyperparameters \lambda_od and \lambda_d : ] ]
We have included a discussion on this in the paragraph after Eq 5 . Essentially , penalizing the off-diagonal entries of Cov_{p ( x ) } [ \mu ( x ) ] also ends up reducing the diagonals of this matrix as off-diagonal are really derived from the diagonals ( product of square root of diagonals for each example followed by averaging over examples ) . Hence holding the diagonals to a fixed value was important . We found that \lambda_d > \lamda_od was better for decreasing the covariance without impacting the variance .




[ [ Running estimate of Cov_p ( x ) ( \mu ( x ) ) : ] ]
If the estimate using the current minibatch is B and the previous cumulative estimate is C , we take a combination B + a * C with ‘ a ’ being the inertia parameter ( 0.95 or so ) and then normalize by ( 1 / 1 - a ) . C is treated as constant while backpropagating the gradients .


The authors adapts stochastic natural gradient methods for variational inference with structured inference networks . The variational approximation proposed is similar to SVAE by Jonhson et al. ( 2016 ) , but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter . The authors then extends and adapts the natural gradient method by Khan & Lin ( 2017 ) to optimize all the variational parameters . In the experiments the authors generally show improved convergence over SVAE .

The idea seems promising but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation . The main motivation seems to be that it is easier to optimize .

- In the last two sentences of the updates for \theta_PGM you mention that you need to do SVI / VMP to compute the function \eta_x\theta . Might this also suffer from non- convergence issues like you argue SVAE does ? Or do you simply mean that computation of this is exact using regular message passing / Kalman filter / forward - backward ?
- It was not clear to me why we should use a Gaussian approximation for the \theta_NN parameters ? The prior might be Gaussian but the posterior is not ? Is this more of a simplifying assumption ?
- There has recently been interest in using inference networks as part of more flexible variational approximations for structured models . Some examples of related work missing in this area is " Variational Sequential Monte Carlo " by Naesseth et al. ( 2017 ) / " Filtering Variational Objectives " by Maddison et al. ( 2017 ) / " Auto-encoding sequential Monte Carlo " Le et al. ( 2017 ) .
- Section 2.1 , paragraph nr 5 , " algorihtm " -> " algorithm "


Thanks for your review . Following reviewers suggestions , we will make the following major changes in our next draft :
- We will clearly explain that our main motivation is to improve over the method of Johnson et.al. , 2016 .
- We will clarify the description , especially the conjugacy requirements , and add detailed discussion on the applicability and limitations of our approach .
- We will clean - up the description of our algorithm in Section 4 .
- We will add an experiment on non-conjugate mixture model to demonstrate the generality of our approach . We will add a larger experiment for clustering MNIST digits .
--------------
DETAILED COMMENTS
Reviewer : “ Might this also suffer from non - convergence issues like you argue SVAE does ? Or do you simply mean that computation of this is exact using regular message passing / Kalman filter / forward - backward ? ”

Response : Our method does not have convergence issues , and is guaranteed to converge under mild conditions discussed in Khan and Lin 2017 . And yes , the updates are exact and obtained using regular message passing .
--------------
Reviewer : “ It was not clear to me why we should use a Gaussian approximation for the \theta_NN parameters ? “

Response : You are right . We can use any appropriate exponential family approximation . The updates are similar to Khan and Lin ’s method . We will change this in the final draft .
--------------
Thanks for the citations . We will add them in the paper


We have made the following changes in the revised version :
- Introduction is modified to show that our method is an improvement over Johnson et. al. ’s method , and it builds upon Khan and Lin ’s method .
- Section 2 modified to clearly show the issues with Johnson et.al . ’s method .
- Section 3 modified to clarify the conjugacy requirements of inference network . We have added many illustrative examples .
- Section 4 modified to simplify the algorithm description . We have added a pseudo-code .
- In Section 5 we added a new result on Student ’s -t mixture model .

This paper presents a variational inference algorithm for models that contain
deep neural network components and probabilistic graphical model ( PGM )
components .
The algorithm implements natural - gradient message - passing where the messages
automatically reduce to stochastic gradients for the non-conjugate neural
network components . The authors demonstrate the algorithm on a Gaussian mixture
model and linear dynamical system where they show that the proposed algorithm
outperforms previous algorithms . Overall , I think that the paper proposes some
interesting ideas , however , in its current form I do not think that the novelty
of the contributions are clearly presented and that they are not thoroughly
evaluated in the experiments .

The authors propose a new variational inference algorithm that handles models
with deep neural networks and PGM components . However , it appears that the
authors rely heavily on the work of ( Khan & Lin , 2017 ) that actually provides
the algorithm . As far as I can tell this paper fits inference networks into
the algorithm proposed in ( Khan & Lin , 2017 ) which boils down to i ) using an
inference network to generate potentials for a conditionally - conjugate
distribution and ii ) introducing new PGM parameters to decouple the inference
network from the model parameters . These ideas are a clever solution to work
inference networks into the message- passing algorithm of ( Khan & Lin , 2017 ) ,
but I think the authors may be overselling these ideas as a brand new algorithm .
I think if the authors sold the paper as an alternative to ( Johnson , et al. , 2016 )
that does n't suffer from the implicit gradient problem the paper would fit into
the existing literature better .

Another concern that I have is that there are a lot of conditiona- conjugacy
assumptions baked into the algorithm that the authors only mention at the end
of the presentation of their algorithm . Additionally , the authors briefly state
that they can handle non-conjugate distributions in the model by just using
conjugate distributions in the variational approximation . Though one could do
this , the authors do not adequately show that one should , or that one can do this
without suffering a lot of error in the posterior approximation . I think that
without an experiment the small section on non- conjugacy should be removed .

Finally , I found the experimental evaluation to not thoroughly demonstrate the
advantages and disadvantages of the proposed algorithm . The algorithm was applied
to the two models originally considered in ( Johnson , et al. , 2016 ) and the
proposed algorithm was shown to attain lower mean-square errors for the two
models . The experiments do not however demonstrate why the algorithm is
performing better . For instance , is the ( Johnson , et al. , 2016 ) algorithm
suffering from the implicit gradient ? It also would have been great to have
considered a model that the ( Johnson , et. al. , 2016 ) algorithm would not work
well on or could not be applied to show the added applicability of the proposed
algorithm .

I also have some minor comments on the paper :
- There are a lot of typos .
- The first two sentences of the abstract do not really contribute anything
to the paper . What is a powerful model ? What is a powerful algorithm ?
- DNN was used in Section 2 without being defined .
- Using p ( ) as an approximate distribution in Section 3 is confusing notation
because p ( ) was used for the distributions in the model .
- How is the covariance matrix parameterized that the inference network produces ?
- The phrases " first term of the inference network " are not clear . Just use The
DNN term and the PGM term of the inference networks , and better still throw
in a reference to Eq . ( 4 ) .
- The term " deterministic parameters " was used and never introduced .
- At the bottom of page 5 the extension to the non-conjugate case should be
presented somewhere ( probably the appendix ) since the fact that you can do
this is a part of your algorithm that 's important .


Thanks for the review . Following reviewers suggestions , we will make the following major changes in our next draft :
- We will clearly explain that our main motivation is to improve over the method of Johnson et.al. , 2016 .
- We will clarify the description , especially the conjugacy requirements , and add detailed discussion on the applicability and limitations of our approach .
- We will clean - up the description of our algorithm in Section 4 .
- We will add an experiment on non-conjugate mixture model to demonstrate the generality of our approach . We will add a larger experiment for clustering MNIST digits .
-----------
DETAILED COMMENTS
Reviewer : “ However , it appears that the authors rely heavily on the work of ( Khan & Lin , 2017 ) that actually provides the algorithm . [ ...] I think the authors may be overselling these ideas as a brand new algorithm . ” .

Response : Thanks for letting us know . This was not our intention . We will modify the write - up to clarify our contributions over Khan and Lin , 2017 and not oversell our method .
-----------
Reviewer : “ I think if the authors sold the paper as an alternative to ( Johnson , et al. , 2016 ) that does n't suffer from the implicit gradient problem the paper would fit into the existing literature better . ”

Response : That ’s a good point and we will write about this a bit more clearly in the paper . Our method is not just an alternative over Jonson et. al. 2016 , but it is a generalization of their method . We propose a Variational Message Passing framework for complex models which are not covered by the method of Johnson et. al. 2016 . We will modify the introduction and discussion to clarify these points . We will also add an experiment on a non-conjugate model as an evidence of the generality of our approach .
-----------
Reviewer : “ Another concern that I have is that there are a lot of conditional - conjugacy assumptions baked into the algorithm that the authors only mention at the end of the presentation of their algorithm . ”

Response : We agree and we will clarify the text to reflect the following point : Our method works for general non- conjugate models , but our inference network is restricted to a conjugate model where the normalizing constant is easy to compute .
-----------
Reviewer : “ The authors briefly state that they can handle non-conjugate distributions in the model [ ... ] . Though one could do this , the authors do not adequately show that one should , or that one can do this without suffering a lot of error in the posterior approximation . ”

Response : We will modify the text to clarify this . We will also add an example of a non-conjugate mixture model and show how to design a conjugate inference network for this problem . We will also add a paragraph explaining how to generalize this procedure to general graphical models .
-----------
Reviewer : “ the experimental evaluation do not thoroughly demonstrate the advantages and disadvantages of the proposed algorithm … . The experiments do not however demonstrate why the algorithm is performing better . ”

Response : Thanks for pointing this out . The goal of our experiments was to show that , when PGM is a conjugate model , our method performs similar to the method of Johnson et. al . The advantage of our approach is the simplicity of our method , as well as its generality . This is mentioned in the first paragraph in Section 5 .
-----------
Reviewer : “ is the ( Johnson , et al. , 2016 ) algorithm suffering from the implicit gradient ? ”

Response : On small datasets , we did not observe the implicit gradient issue with the method of Johnson et. al . But in principle we expect this to be a problem for complex models .
-----------
Reviewer : “ It also would have been great to have considered a model that the ( Johnson , et. al. , 2016 ) algorithm would not work well on or could not be applied to show the added applicability of the proposed algorithm . ”

Response : Thanks for the suggestion . We will add an experiment for non-conjugate mixture model , where the method of Johnson et. al. does not apply .
-----------
Thanks for further suggestions . We will modify the abstract to remove the line about “ powerful models and algorithms ” . We will take other comments into account as well . Thanks !


We have made the following changes in the revised version :
- Introduction is modified to show that our method is an improvement over Johnson et. al. ’s method , and it builds upon Khan and Lin ’s method .
- Section 2 modified to clearly show the issues with Johnson et.al . ’s method .
- Section 3 modified to clarify the conjugacy requirements of inference network . We have added many illustrative examples .
- Section 4 modified to simplify the algorithm description . We have added a pseudo-code .
- In Section 5 we added a new result on Student ’s -t mixture model .

The paper seems to be significant since it integrates PGM inference with deep models . Specifically , the idea is to use the structure of the PGM to perform efficient inference . A variational message passing approach is developed which performs natural - gradient updates for the PGM part and stochastic gradient updates for the deep model part . Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference .
The paper does a good job of explaining the challenges of inference , and provides a systematic approach to integrating PGMs with deep model updates . As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters , the proposed architecture does not require this , due to the re-parameterization which is an important contribution .

The motivation of the paper , and the description of its contribution as compared to existing methods can be improved . One of the main aspects it seems is generality , but the encodings are specific to 2 types PGMs . Can this be generalized to arbitrary PGM structures ? How about cases when computing Z is intractable ? Could the proposed approach be adapted to such cases . I was not very sure as to why the proposed method is more general than existing approaches .

Regarding the experiments , as mentioned in the paper the evaluation is performed on two fairly small scale datasets . the approach shows that the proposed methods converge faster than existing methods . However , I think there is value in the approach , and the connection between variational methods with DNNs is interesting .

Thanks for your review . Following reviewers suggestions , we will make the following major changes in our next draft :
- We will clearly explain that our main motivation is to improve over the method of Johnson et.al. , 2016 .
- We will clarify the description , especially the conjugacy requirements , and add detailed discussion on the applicability and limitations of our approach .
- We will clean - up the description of our algorithm in Section 4 .
- We will add an experiment on non-conjugate mixture model to demonstrate the generality of our approach . We will add a larger experiment for clustering MNIST digits .
----------
DETAILED COMMENTS
Reviewer : “ One of the main aspects it seems is generality , but the encodings are specific to 2 types PGMs . Can this be generalized to arbitrary PGM structures ? How about cases when computing Z is intractable ? ”

Response : We agree that the write - up is not clear . We will improve this . Our method can handle arbitrary PGM structure in the model similar to the method of Khan and Lin 2017 . The inference network however is restricted to cases where Z is tractable . Our method therefore simplifies inference by choosing an inference network which has a simpler form than the original model .
---------
Reviewer : “ Regarding the experiments , as mentioned in the paper the evaluation is performed on two fairly small scale datasets . ”

Response : We agree with your point . Our comparisons are restricted because the existing implementation of SVAE baseline did not scale to large problems . We will add two more experiments as promised above .


We have made the following changes in the revised version :
- Introduction is modified to show that our method is an improvement over Johnson et. al. ’s method , and it builds upon Khan and Lin ’s method .
- Section 2 modified to clearly show the issues with Johnson et.al . ’s method .
- Section 3 modified to clarify the conjugacy requirements of inference network . We have added many illustrative examples .
- Section 4 modified to simplify the algorithm description . We have added a pseudo-code .
- In Section 5 we added a new result on Student ’s -t mixture model .

I like the idea of the paper . Momentum and accelerations are proved to be very useful both in deterministic and stochastic optimization . It is natural that it is understood better in the deterministic case . However , this comes quite naturally , as deterministic case is a bit easier ;) Indeed , just recently people start looking an accelerating in stochastic formulations . There is already accelerated SVRG , Jain et al 2017 , or even Richtarik et al ( arXiv : 1706.01108 , arXiv :1710.10737 ) .

I would somehow split the contributions into two parts :
1 ) Theoretical contribution : Proposition 3 ( + proofs in appendix )
2 ) Experimental comparison .

I like the experimental part ( it is written clearly , and all experiments are described in a lot of detail ) .

I really like the Proposition 3 as this is the most important contribution of the paper . ( Indeed , Algorithms 1 and 2 are for reference and Algorithm 3 was basically described in Jain , right ? ) .

Significance : I think that this paper is important because it shows that the classical HB method cannot achieve acceleration in a stochastic regime .

Clarity : I was easy to read the paper and understand it .

Few minor comments :
1 . Page 1 , Paragraph 1 : It is not known only for smooth problems , it is also true for simple non-smooth ( see e.g. https://link.springer.com/article/10.1007/s10107-012-0629-5)
2 . In abstract : Line 6 - not completely true , there is accelerated SVRG method , i.e. the gradient is not exact there , also see Recht ( https://arxiv.org/pdf/1701.03863.pdf) or Richtarik et al ( arXiv : 1706.01108 , arXiv :1710.10737 ) for some examples where acceleration can be proved when you do not have an exact gradient .
3 . Page 2 , block " 4 " missing " . " in " SGD We validate " ....
4 . Section 2 . I think you are missing 1 / 2 in the definition of the function . Otherwise , you would have a constant " 2 " in the Hessian , i.e. H= 2 E[xx ^T ] . So please define the function as f_i ( w ) = 1 / 2 ( y - < w , x_i > ) ^ 2 . The same applies to Section 3.
5 . Page 6 , last line , .... was downloaded from " pre " . I know it is a link , but when printed , it looks weird .



Thanks a lot for insightful comments . We have updated the paper taking into account several of your comments . We will make more updates according to your suggestions .


Paper organization : we will try to better organize the paper to highlight the contributions .
Proposition 3 's importance : yes , your assessment is spot on .

Minor comment 1 , 2 : Thanks for pointing the minor mistake , we have updated the corresponding lines . Papers such as Accelerated SVRG , Recht et al. are offline stochastic accelerated methods . The paper of Richtarik ( arXiv :1706.01108 ) deals with solving consistent linear systems in the offline setting ; ( arXiv :1710.10737 ) is certainly relevant and we will add more detailed comparison with this line of work .
Minor comment 3 , 5 : thanks for pointing out the typos . They are fixed .
Minor comment 4 : Actually , the problem is a discrete problem where one observes one hot vectors in 2 - dimensions , each of the vectors can occur with probability 1 / 2 . So this is the reason why the Hessian does not carry an added factor of 2 .




I wonder how the ASGD compares to other optimization schemes applicable to DL , like Entropy - SGD , which is yet another algorithm that provably improves over SGD . This question is also valid when it comes to other optimization schemes that are designed for deep learning problems . For instance , Entropy -SGD and Path - SGD should be mentioned and compared with . As a consequence , the literature analysis is insufficient .

Authors provided necessary clarifications . I am raising my score .






Thanks for your comments .

We have cited Entropy SGD and Path SGD papers and discuss the differences in Section 6 ( related works ) . However , both the methods are complementary to our method .

Entropy SGD adds a local strong convexity term to the objective function to improve generalization . However , currently we do not understand convergence rates or generalization performance of the technique rigorously , even for convex problems . The paper proposes to use SGD to optimize the altered objective function and mentions that one can use SGD + momentum as well ( below algorithm box on page 6 ) . Naturally , one can use the ASGD method as well to optimize the proposed objective function in the paper .

Path SGD uses a modified SGD like update to ensure invariance to the scale of the data . Here again , the main goal is orthogonal to our work and one can easily use ASGD method in the same framework .


I only got access to the paper after the review deadline ; and did not have a chance to read it until now . Hence the lateness and brevity .

The paper is reasonably well written , and tackles an important problem . I did not check the mathematics .

Besides the missing literature mentioned by other reviewers ( all directly relevant to the current paper ) , the authors should also comment on the availability of accelerated methods inn the finite sum / ERM setting . There , the questions this paper is asking are resolved , and properly modified stochastic methods exist which offer acceleration over SGD ( and not through minibatching ) . This paper does not comment on these developments . Look at accelerated SDCA ( APPROX , ASDCA ) , accelerated SVRG ( Katyusha ) and so on .

Provided these changes are made , I am happy to suggest acceptance .





Thanks for the references , we have included them in the paper and added a paragraph in Section 6 providing detailed comparison and key differences that we summarize below :

ASDCA , Katyusha , accelerated SVRG : these methods are " offline " stochastic algorithms that is they require multiple passes over the data and require multiple rounds of full gradient computation ( over the entire training data ) . In contrast , ASGD is a single pass algorithm and requires gradient computation only a single data point at a time step . In the context of deep learning , this is a critical difference , as computing gradient over entire training data can be extremely slow . See Frostig , Ge , Kakade , Sidford `` Competing with the ERM in a single pass " ( https://arxiv.org/pdf/1412.6606.pdf) for a more detailed discussion on online vs offline stochastic methods .

Moreover , the rate of convergence of the ASDCA depend on \sqrt{\kappa n} while the method studied in this paper has \sqrt{\kappa \tilde{kappa}} dependence where \tilde{kappa} can be much smaller than n.











The paper describes a way of combining a causal graph describing the dependency structure of labels with two conditional GAN architectures ( causalGAN and causalBEGAN ) that generate images conditioning on the binary labels . Ideally , this type of approach should allow not only to generate images from an observational distribution of labels ( e.g. P( Moustache= 1 ) ) , but also from unseen interventional distributions ( e.g. P( Male=0 | do ( Moustache = 1 ) ) .

Maybe I misunderstood something , but one big problem I have with the paper is that for a “ causalGAN ” approach it does n’t seem to do much causality . The ( known ) causal graph is only used to model the dependencies of the labels , which the authors call the “ Causal Controller ” . On this graph , one can perform interventions and get a different distribution of labels from the original causal graph ( e.g. a distribution of labels in which women have the same probability as men of having moustaches ) . Given the labels , the rest of the architecture are extensions of conditional GANs , a causalGAN with a Labeller and an Anti-Labeller ( of which I ’m not completely sure I understand the necessity ) and an extension of a BEGAN . The results are not particularly impressive , but that is not an issue for me .

Moreover sometimes the descriptions are a bit imprecise and unstructured . For example , Theorem 1 is more like a list of desiderata and it already contains a forward reference to page 7 . The definition of intervention in the Background applies only to do -interventions ( Pearl 2009 ) and not to general interventions ( e.g. consider soft , uncertain or fat-hand interventions ) .

Overall , I think the paper proposes some interesting ideas , but it does n’t explore them yet in detail . I would be interested to know what happens if the causal graph is not known , and even worse cannot be completely identified from data ( so there is an equivalence class of possible graphs ) , or potentially is influenced by latent factors . Moreover , I would be very curious about ways to better integrate causality and generative models , that do n’t focus only on the label space .


Minor details :
Personally I ’m not a big fan of abusing colons ( “ : ” ) instead of points ( “ . ” ) . See for example the first paragraph of the Related Work .

EDIT : I read the author 's rebuttal , but it has not completely addressed my concerns , so my rating has not changed .

Thank you for your comments and feedback .

- On the use of causality :

You are correct , the causal controller is the causal part of our paper . The point is that we assume the causal graph structure but not the functions that determine the structural equations . The novelty is that the structural equations can be modeled with neural networks and learned through adversarial training .

The second novelty is that by creating the image conditional GAN ( with a labeler and anti-labeler ) , we can provably guarantee that we sample from conditional and interventional distributions of labels and images . The complexity of having a labeler and an anti-labeler is needed for our proof .

Another interesting byproduct of our method is that the image generation ( which is essentially a conditional GAN ) can be creative , i.e. , produce images that never appear in the training set which does not happen for other conditional GANs .

- On structural suggestions : Thank you for your comments on structuring and presentation . Among other changes , we will remove the “ informal theorem ” and replace with the a statement in words .

- When the causal graph is unknown :

It is indeed very interesting to extend our framework for learning the causal graph structure or when there are latent variables .
We investigate the effect of using the wrong causal graph in the appendix of the paper . We see that , as long as CIs in the data are respected , a wrong causal graph can also be learned with a GAN . As it is evident from this observation , it is not trivial to infer causality from how well the data can be fit . This is an interesting direction for future work .

In their paper " CausalGAN : Learning Causal implicit Generative Models with adv. training " the authors address the following issue : Given a causal structure between " labels " of an image ( e.g. gender , mustache , smiling , etc. ) , one tries to learn a causal model between these variables and the image itself from observational data . Here , the image is considered to be an effect of all the labels . Such a causal model allows us to not only sample from conditional observational distributions , but also from intervention distributions . These tasks are clearly different , as nicely shown by the authors ' example of " do ( mustache = 1 ) " versus " given mustache = 1 " ( a sample from the latter distribution contains only men ) . The paper does not aim at learning causal structure from data ( as clearly stated by the authors ) . The example images look convincing to me .

I like the idea of this paper . IMO , it is a very nice , clean , and useful approach of combining causality and the expressive power of neural networks . The paper has the potential of conveying the message of causality into the ICLR community and thereby trigger other ideas in that area . For me , it is not easy to judge the novelty of the approach , but the authors list related works , none of which seems to solve the same task . The presentation of the paper , however , should be improved significantly before publication . ( In fact , because of the presentation of the paper , I was hesitating whether I should suggest acceptance . ) Below , I give some examples ( and suggest improvements ) , but there are many others . There is a risk that in its current state the paper will not generate much impact , and that would be a pity . I would therefore like to ask the authors to put a lot of effort into improving the presentation of the paper .


- I believe that I understand the authors ' intention of the caption of Fig. 1 , but " samples outside the dataset " is a misleading formulation . Any reasonable model does more than just reproducing the data points . I find the argumentation the authors give in Figure 6 much sharper . Even better : add the expression " P ( male = 1 | mustache = 1 ) = 1 " . Then , the difference is crystal clear .
- The difference between Figures 1 , 4 , and 6 could be clarified .
- The list of " prior work on learning causal graphs " seems a bit random . I would add Spirtes et al 2000 , Heckermann et al 1999 , Peters et al 2016 , and Chickering et al 2002 .
- Male -> Bald does not make much sense causally ( it should be Gender -> Baldness ) ... Aha , now I understand : The authors seem to switch between " Gender " and " Male " being random variables . Make this consistent , please .
- There are many typos and comma mistakes .
- I would introduce the do -notation much earlier . The paragraph on p. 2 is now written without do -notation ( " intervening Mustache = 1 would not change the distribution " ) . But this way , the statements are at least very confusing ( which one is " the distribution " ? ) .
- I would get rid of the concept of CiGM . To me , it seems that this is a causal model with a neural network ( NN ) modeling the functions that appear in the SCM . This means , it 's " just " using NNs as a model class . Instead , one could just say that one wants to learn a causal model and the proposed procedure is called CausalGAN ? ( This would also clarify the paper 's contribution . )
- many realizations = one sample ( not samples ) , I think .
- Fig 1 : which model is used to generate the conditional sample ?
- The notation changes between E and N and Z for the noises . I believe that N is supposed to be the noise in the SCM , but then maybe it should not be called E at the beginning .
- I believe Prop 1 ( as it is stated ) is wrong . For a reference , see Peters , Janzing , Scholkopf : Elements of Causal Inference : Foundations and Learning Algorithms ( available as pdf ) , Definition 6.32 . One requires the strict positivity of the densities ( to properly define conditionals ) . Also , I believe the Z should be a vector , not a set .
- Below eq . ( 1 ) , I am not sure what the V in P_V refers to .
- The concept of data probability density function seems weird to me . Either it is referring to the fitted model , then it 's a bad name , or it 's an empirical distribution , then there is no pdf , but a pmf .
- Many subscripts are used without explanation . r -> real ? g -> generating ? G -> generating ? Sometimes , no subscripts are used ( e.g. , Fig 4 or figures in Sec. 8.13 )
- I would get rid of Theorem 1 and explain it in words for the following reasons . ( 1 ) What is an " informal " theorem ? ( 2 ) It refers to equations appearing much later . ( 3 ) It is stated again later as Theorem 2 .
- Also : the name P_g does not appear anywhere else in the theorem , I think .
- Furthermore , I would reformulate the theorem . The main point is that the intervention distributions are correct ( this fact seems to be there , but is " hidden " in the CIGN notation in the corollary ) .
- Re. the formulation in Thm 2 : is it clear that there is a unique global optimum ( my intuition would say there could be several ) , thus : better write " _a_ global minimum " ?
- Fig. 3 was not very clear to me . I suggest to put more information into its caption .
- In particular , why is the dataset not used for the causal controller ? I thought , that it should model the joint ( empirical ) distribution over the labels , and this is part of the dataset . Am I missing sth ?
- IMO , the structure of the paper can be improved . Currently , Section 3 is called " Background " which does not say much . Section 4 contains CIGMs , Section 5 Causal GANs , 5.1 . Causal Controller , 5.2. CausalGAN , 5.2.1 . Architecture ( which the causal controller is part of ) etc . An alternative could be :
Sec 1 : Introduction
Sec 1.1 : Related Work
Sec 2 : Causal Models
Sec 2.1 : Causal Models using Generative Models ( old : CIGM )
Sec 3 : Causal GANs
Sec 3.1 : Architecture ( including controller )
Sec 3.2 : loss functions
...
Sec 4 : Empricial Results ( old : Sec. 6 : Results )
- " Causal Graph 1 " is not a proper reference ( it 's Fig 23 I guess ) . Also , it is quite important for the paper , I think it should be in the main part .
- There are different references to the " Appendix " , " Suppl. Material " , or " Sec. 8 " -- please be consistent ( and try to avoid ambiguity by being more specific -- the appendix contains ~ 20 pages ) . Have I missed the reference to the proof of Thm 2 ?
- 8.1. contains copy -paste from the main text .
- " proposition from Goodfellow " -> please be more precise
- What is Fig 8 used for ? Is it not sufficient to have and discuss Fig 23 ?
- IMO , Section 5.3. should be rewritten ( also , maybe include another reference for BEGAN ) .
- There is a reference to Lemma 15 . However , I have not found that lemma .
- I think it 's quite interesting that the framework seems to also allow answering counterfactual questions for realizations that have been sampled from the model , see Fig 16 . This is the case since for the generated realizations , the noise values are known . The authors may think about including a comment on that issue .
- Since this paper 's main proposal is a methodological one , I would make the publication conditional on the fact that code is released .




Thank you for your detailed and insightful comments .

- On structural changes , suggestions : Thank you for taking time to point out these points . We will add the listed references and implement all the suggested changes to make the presentation more clear , to make the wording more consistent , to fix typos , and to remove the CiGM concept , explaining it in words .

- On Fig. 1 : Our CausalBEGAN implementation is used to generate this figure .

- On Prop . 1 : Thank you for pointing this out . As you correctly observe , strict positivity of the densities is required for this to be true . We will move our assumption that label distribution is strictly positive to here as an assumption for the theorem .

- On data probability density : The data probability density function corresponds to a hypothetical distribution from which the finite sized dataset was sampled .

- On Theorem 1 ( Informal ) : We will remove the “ informal theorem ” and replace with the a statement in words .

- On formulation in Theorem 2 : Since the optimization is assumed to have been done on the pdf level , and since KL divergence is zero if and only if the distributions are the same , the global minimum is unique , although there may be multiple parameterizations of the network that achieves this global minimum .

- On dataset not being used for causal controller : We have n’t shown the connection to the dataset in the CausalGAN architecture figure since we assume it is already pretrained with the same dataset . We will clarify this in the figure caption .

- On counterfactual samples from distribution : Thank you for your insightful comment . We do not assume that the noise distributions are known ( this is not required for interventional samples to be correct ) . We will add a paragraph explaining that if the noise terms are known , we can use our framework to take counterfactual samples .

- On code availability : The code will be made public and linked in the paper in the camera ready version .

This should be the first work which introduces in the causal structure into the GAN , to solve the label dependency problem . The idea is interesting and insightful . The proposed method is theoretically analyzed and experimentally tested . Two minor concerns are 1 ) what is the relationship between the anti-labeler and and discriminator ? 2 ) how the tune related weight of the different objective functions .

Thank you for your positive comments and feedback .

- The Anti-labeler estimates labels of a given generated image . The Discriminator estimates whether a given image is real or generated , which is standard in the GAN literature . Please see Section 5.2.1 and 5.2.2 for their role and importance . 2 ) We did not scale the different objective functions . The main reason is that the theory we have suggests no scaling is needed , which we observe in practice .

The authors present a model for unsupervised NMT which requires no parallel corpora between the two languages of interest . While the results are interesting I find very few original ideas in this paper . Please find my comments / questions / suggestions below :

1 ) The authors mention that there are 3 important aspects in which their model differs from a standard NMT architecture . All the 3 differences have been adapted from existing works . The authors clearly acknowledge and cite the sources . Even sharing the encoder using cross lingual embeddings has been explored in the context of multilingual NER ( please see https://arxiv.org/abs/1607.00198). Because of this I find the paper to be a bit lacking on the novelty quotient . Even backtranslation has been used successfully in the past ( as acknowledged by the authors ) . Unsupervised MT in itself is not a new idea ( again clearly acknowledged by the authors ) .

2 ) I am not very convinced about the idea of denoising . Specifically , I am not sure if it will work for arbitrary language pairs . In fact , I think there is a contradiction even in the way the authors write this . On one hand , they want to " learn the internal structure of the languages involved " and on the other hand they deliberately corrupt this structure by adding noise . This seems very counter- intuitive and in fact the results in Table 1 suggest that it leads to a drop in performance . I am not very sure that the analogy with autoencoders holds in this case .

3 ) Following up on the above question , the authors mention that " We emphasize , however , that it is not possible to use backtranslation alone without denoising " . Again , if denoising itself leads to a drop in the performance as compared to the nearest neighbor baseline then why use backtranslation in conjunction with denoising and not in conjunction with the baseline itself .

4 ) This point is more of a clarification and perhaps due to my lack of understanding . Backtranslation to generate a pseudo corpus makes sense only after the model has achieved a certain ( good ) performance . Can you please provide details of how long did you train the model ( with denoising ? ) before producing the backtranslations ?

5 ) The authors mention that 100 K parallel sentences may be insufficient for training a NMT system . However , this size may be decent enough for a PBSMT system . It would be interesting to see the performance of a PBSMT system trained on 100 K parallel sentences .

6 ) How did you arrive at the beam size of 12 ? Was this a hyperparameter ? Just curious .

7 ) The comparable NMT set up is not very clear . Can you please explain it in detail ? In the same paragraph , what exactly do you mean by " the supervised system in this paper is relatively small ? "

Thanks for the insightful comments . Please find the answers to the specific points below , which were also addressed in the revised version of the paper :

1 ) We are aware that the basic building blocks of our work come from previous work and , as you note , we try to properly acknowledge that in the paper . However , we do not see this as a weakness , but rather an inherent characteristic of science as a collaborative effort . Our contribution lies in combining these basic building blocks in a novel way to build the first fully unsupervised NMT system . We believe that this is an important contribution on its own : NMT is a highly relevant field where the predominant approach has been supervised and , for the first time , we show that an unsupervised approach is also viable . As such , we think that our work explores a highly original idea and opens a new and exciting research direction .

2 ) This is a very interesting observation , but we think that , paradoxically , corrupting the structure of the language is necessary for the system to learn such structure . Note that , without denoising , this training step would be reduced to a trivial copying task that admits degenerated solutions . The intuition is that one can easily copy a sentence in any language even if they know nothing about that language . In contrast , adding noise to the input makes the task of reconstructing the input non-trivial , and forces the system to learn about the structure of that language to be able to solve it . The intuition in this case is that , if we are given a scrambled sentence in some language , it is not possible for us to recover the original sentence unless we have some knowledge of the language in question . In other words , the idea of denoising is to corrupt the structure of the input , so the system needs to learn the correct structure in order to recover the original uncorrupted input ( this is possible because the system does see the correct structure in the output during training ) . Note that this has also been found to help extract good representations from natural language sentences by other authors ( Hill et al. , 2016 ) . Regarding arbitrary language pairs , we think that this idea is particularly relevant for distant pairs : corrupting the word order of the input makes the shared encoder rely less in this word order , which is necessary for distant language pairs with more divergences in this regard .

3 / 4 ) There seems to be some confusion here on how our training procedure works in relation to backtranslation . Note that each training iteration performs one mini-batch of denoising and one mini-batch of backtranslation in each direction , and the bactranslation step at iteration i uses the model from iteration ( i- 1 ) . This way , denoising and backtranslation keep constantly improving the model , and backtranslation itself uses the most recent model at each time . This is in contrast with traditional backtranslation , where a fixed model is used to backtranslate the entire corpus at one time . In relation to point 3 , while denoising alone is certainly weaker than the nearest neighbor baseline , the combination of denoising and backtransation eventually leads to a stronger model , which backtranslation itself takes advantage of in the following iterations as just described .

5 ) The purpose of this experiment was to show that the proposed system can also benefit from small parallel corpora , making it suitable not only for the unsupervised scenario , but also for the semi-supervised scenario . As such , our point was not to improve the state of the art under these conditions , but rather to show that our work has potential interest beyond the strictly unsupervised scenario .

6 ) A beam size of 12 is very common in previous work ( Sutskever et al. , 2014 ; Sennrich et al. , 2016 a ; b ; He et al. , 2016 ) , so we also adopted it for our experiments without any further exploration .

7 ) The comparable NMT system uses the exact same architecture and hyperparameters as the unsupervised system ( number of layers , hidden units , attention model etc . ) . Furthermore , it incorporates the non-standard variations in Section 3.1 ( dual structure using a shared encoder with fixed embeddings ) . The only difference is that , instead of training it on monolingual corpora using denoising and backtranslation , it is trained on parallel corpora just as standard NMT .

When we say that " the supervised system in this paper is relatively small " , we mean that the size of the model ( number of layers , training time etc. ) is small compared to the state of the art , which explains in part why its results are also weaker . However , note that this also applies to the unsupervised system , which uses the exact same settings . We therefore believe that there is a considerable margin to improve our results by using a larger model .

unsupervised neural machine translation

This is an interesting paper on unsupervised MT . It trains a standard architecture using :

1 ) word embeddings in a shared embedding space , learned using a recent approach that works with only tens of bilingual word papers .

2 ) A encoder - decoder trained using only monolingual data ( should cite http://www.statmt.org/wmt17/pdf/WMT15.pdf). Training uses a “ denoising ” method which is not new : it uses the same idea as contrastive estimation ( http://www.aclweb.org/anthology/P05-1044, a well - known method which should be cited ) .

3 ) Backtranslation .

All though none of these ideas are new , they have n’t been combined in this way before , and that what ’s novel here . The paper is essentially a neat application of ( 1 ) , and is an empirical / systems paper . It ’s essentially a proof -of - concept that it is that it ’s possible to get anything at all using no parallel data . That ’s surprising and interesting , but I learned very little else from it . The paper reads as preliminary and rushed , and I had difficulty answering some basic questions :

* In Table ( 1 ) , I ’m slightly puzzled by why 5 is better than 6 , and this may be because I ’m confused about what 6 represents . It would be natural to compare 5 with a system trained on 100 K parallel text , since the systems would then ( effectively ) differ only in that 5 also exploits additional monolingual data . But the text suggests that 6 is trained on much more than 100 K parallel sentences ; that is , it differs in at least two conditions ( amount of parallel text and use of monolingual text ) . Since this paper ’s primary contribution is empirical , this comparison should be done in a carefully controlled way , differing each of these elements in turn .

* I ’m very confused by the comment on p. 8 that “ the modifications introduced by our proposal are also limiting ” to the “ comparable supervised NMT system ” . According to the paper , the architecture of the system is unchanged , so why would this be the case ? This comment makes it seem like something else has been changed in the baseline , which in turn makes it somewhat hard to accept the results here .

Comment :
* The qualitative analysis is not really an analysis : it ’s just a few cherry - picked examples and some vague observations . While it is useful to see that the system does indeed generate nontrivial content in these cases , this does n’t give us further insight into what the system does well or poorly outside these examples . The BLEU scores suggest that it also produces many low-quality translations . What is different about these particular examples ? ( Aside : since the cross-lingual embedding method is trained on numerals , should we be concerned that the system fails at translating numerals ? )

Questions :
* Contrastive estimation considers other neighborhood functions ( “ random noise ” in the parlance of this paper ) , and it ’s natural to wonder what would happen if this paper also used these or other neighborhood functions . More importantly , I suspect the the neighborhood functions are important : when translating between Indo-European languages as in these experiments , local swaps are reasonable ; but in translating between two different language families ( as would often be the case in the motivating low- resource scenario that the paper does not actually test ) , it seems likely that other neighborhood functions would be important , since structural differences would be much larger .

Presentational comments ( these do n’t affect my evaluation , they ’re mostly observations but they contribute to a general feeling that the paper is rushed and preliminary ) :

* BPE does not “ learn ” , it ’s entirely deterministic .

* This paper is at best tangentially related to decipherment . Decipherment operates under two quite different assumptions : there is no training data for the source language ciphertext , only the ciphertext itself ( which is often very small ) ; and the replacement function is deterministic rather than probabilistic ( and often monotonic ) . The Dou and Knight papers are interesting , but they ’re an adaptation of ideas rather than decipherment per se . Since none of those ideas are used here this feels like hand - waving .

* Future work is vague : “ we would like to detect and mitigate the specific causes … ” “ we also think that a better handling of rare words … ” That ’s great , but how will you do these things ? Do you have specific reasons to think this , or ideas on how to approach them ? Otherwise this is just hand - waving .

Thanks for the insightful review . We have tried to make the paper more clear in the revised version taking these comments into account . Please find the answers to each specific point below :

General :

- To clarify what the semi-supervised and supervised systems represents in Table 1 : ( 5 ) is the same as ( 4 ) , but in addition to training on monolingual corpora using denoising and backtransaltion , it is also trained in a subset of 100 K parallel sentences using standard supervised cross -entropy loss ( it alternates one mini-batch of denoising , one mini-batch of backtranslation and one mini-batch of this supervised training ) . ( 6 ) is the same as ( 5 ) except for two differences : 1 ) it uses the full parallel corpus instead of the subset of 100 K parallel sentences , and 2 ) it does not use any monolingual corpora nor denoising or backtranslation . We think that the main reason why ( 5 ) is better than ( 6 ) is related to the domain : the parallel corpus of ( 6 ) is general , whereas the subset of 100 K parallel sentences and the monolingual corpus used for ( 5 ) are in the news domain just as the test set . While these facts were already mentioned in the paper , the new version includes a more detailed discussion . At the same time , we agree that the fact that the comparable system differs from the semi-supervised system in two aspects makes the comparison more difficult , and we are currently working to extend our experiments accordingly .

- Regarding the comment that “ the modifications introduced by our proposal are also limiting ” to the “ comparable supervised NMT system ” , note that , as discussed in the previous point , the comparable NMT system uses the exact same architecture and hyperparameters as the unsupervised system ( number of layers , hidden units , attention model etc. ) and , as such , it also incorporates the non-standard variations in Section 3.1 ( dual structure using a shared encoder with fixed embeddings ) . These are what we were referring to as “ the modification introduced by our proposal ” , but the only difference between the unsupervised and the supervised systems is that , instead of training in monolingual corpora using denoising and backtranslation , we train in parallel corpora just as in standard NMT . We have tried to make this more clear in the revised version of the paper .

Comment :

- We agree that the qualitative analysis in the current version is limited . It was done mainly to check and illustrate that the proposed unsupervised NMT system generates sensible translations despite the lack of parallel corpora . We believe that a more detailed investigation and analysis into the properties and characteristics of translation generated by unsupervised NMT must be conducted in the future .

- Note that we only use shared numerals to initialize the iterative embedding mapping method , so it is understandable that the system fails to translate numerals after the training of both the embedding mapping and the unsupervised NMT system itself . While it would certainly be possible to perform an ad-hoc processing to translate numerals under the assumption that they are shared by different languages , our point was to show that the system has some logical adequacy issues for very similar concepts ( e.g. different numerals or month names ) .

Questions :

- Thanks for pointing out the connection with contrastive estimation , which is now properly discussed in the revised version of the paper . As for the role of neighborhood functions , we agree that there are many possible choices beyond local swaps , and the optimal choice could greatly depend on the typological divergences between the languages involved . In this regard , we think that this is a very interesting direction to explore in the future , and we have tried to better discuss this matter in the revised version of the paper .

Having said that , please note that we have considered two language pairs ( English - French and English - German ) in our experiments . Despite being indo-european , there are important properties that distinguish these language pairs , such as the verb-final construction and the prevalence of compounding in German in contrast to French . In fact , English - German has often been studied in machine translation as a particularly challenging language pair . For that reason , we believe that the experiments on these two distinct language pairs support the effectiveness of the proposed approach , despite the potential for future investigation on the effect of contrastive functions on the choice of language pairs .

Presentational comments :

- We are aware that BPE is completely deterministic . However , it does require to extract some merge operations that are later applied . The original paper of Sennrich et al. ( 2016 ) refers to this process as " learning " , so we decided to follow their wording .

- The Dou and Knight papers also attempt to build machine translation systems using monolingual corpora , so we briefly discuss and acknowledge their work accordingly even if our approach is completely different . Regarding the choice of the term " decipherment " to refer to that work , we understand that this might not exactly adjust to the common acceptation of the term , but it seems to be the one that the authors themselves use ( e.g. " Unifying bayesian inference and vector space models for improved decipherment " ) . We have rewritten it as " statistical decipherment for machine translation " , which we hope that is more descriptive .

- We have rewritten the future work in the revised version of the paper trying to be more specific .

This paper describes a first working approach for fully unsupervised neural machine translation . The core ideas being this method are : ( 1 ) train in both directions ( French to English and English to French ) in tandem ; ( 2 ) lock the embedding table to bilingual embeddings induced from monolingual data ; ( 3 ) share the encoder between two languages ; and ( 3 ) alternate between denoising auto-encoder steps and back -translation steps . The key to making this work seems to be using a denoising auto-encoder where noise is introduced by permuting the source sentence , which prevents the encoder from learning a simple copy operation . The paper shows real progress over a simple word - to- word baseline for WMT 2014 English - French and English - German . Preliminary results in a semi-supervised setting are also provided .

This is solid work , presenting a reasonable first working system for unsupervised NMT , which had never been done before now . That alone is notable , and overall , I like the paper . The work shares some similarities with He et al . ’s NIPS 2016 paper on “ Dual learning for MT , ” but has more than enough new content to address the issues that arise with the fully unsupervised scenario . The work is not perfect , though . I feel that the paper ’s abstract over - claims to some extent . Also , the experimental section shows clearly that in getting the model to work at all , they have created a model with a very real ceiling on performance . However , to go from not working to working a little is a big , important first step . Also , I found the paper ’s notation and prose to be admirably clear ; the paper was very easy to follow .

Regarding over-claiming , this is mostly an issue of stylistic preference , but this paper ’s use of the term “ breakthrough ” in both the abstract and the conclusion grates a little . This is a solid first attempt at a new task , and it lays a strong foundation for others to build upon , but there is lots of room for improvement . I do n’t think it warrants being called a breakthrough - lots of papers introduce new tasks and produce baseline solutions . I would generally advise to let the readers draw their own conclusions .

Regarding the ceiling , the authors are very up - front about this in Table 1 , but it bears repeating here : a fully supervised model constrained in the same way as this unsupervised model does not perform very well at all . In fact , it consistently fails to surpass the semi-supervised baseline ( which I think deserved some further discussion in the paper ) . The poor performance of the fully supervised model demonstrates that there is a very real ceiling to this approach , and the paper would be stronger if the authors were able to show to what degree relaxing these constraints harms the unsupervised system and helps the supervised one .

The semi-supervised experiment in Sections 2.3 and 4 is a little dangerous . With BLEU scores failing to top 22 for English - French , there is a good chance that a simple phrase - based baseline on the same 100 k sentence pairs with a large target language model will outperform this technique . Any low - resource scenario should include a Moses baseline for calibration , as NMT is notoriously weak with small amounts of parallel data .

Finally , I think the phrasing in Section 5.1 needs to be softened , where it states , “ ... it is not possible to use backtranslation alone without denoising , as the initial translations would be meaningless sentences produced by a random NMT model , ... ” This statement implies that the system producing the sentences for back - translation must be a neural MT system , which is not the case . For example , a related paper co-submitted to ICLR , called “ Unsupervised machine translation using monolingual corpora only , ” shows that one can prime back - translation with a simple word - to- word system similar to the word - to- word baseline in this paper ’s Table 1 .

Thanks for the insightful feedback . Please find our answers below :

- Regarding over-claiming , it was not our intention to exaggerate our contribution , and we in fact share your view on this : we think that our work is a strong foundation for a new and exciting research direction in NMT , but we agree that it is only a first step and there is still a long way to go . We understand that “ breakthrough ” might not be the most appropriate term for this , and we have removed it from the revised version of the paper .

- We find the discussion on the ceiling very interesting and relevant . We agree on the following key observations : 1 ) the comparable supervised system can be seen as a ceiling for the unsupervised system , and 2 ) the comparable supervised system gets relatively poor results . As such , one might conclude that our approach has a hard limit in this ceiling , as any eventual improvement in the proposed training method could at best close the gap with it . However , this also assumes that the ceiling itself is fixed and cannot be improved , which we do not find to be the case . In fact , we think that a very interesting research direction is to identify and address the factors that limited the performance of the comparable supervised system , which should also translate in an improvement for the unsupervised system . We have the following ideas in this regard , which we have better described in the revised version of the paper :

1 ) We did not perform any rigorous hyperparameter exploration , and we favored efficiency over performance in our experimental design . As such , we think that there is a considerable margin to improve our results with some engineering effort , such as using larger models , longer training times , ensembling techniques and better decoding strategies ( length / coverage penalty ) .

2 ) While the constraints that we introduce to make our unsupervised system trainable might also limit its performance , one could design a multi-phase training procedure where these constraints are progressively relaxed . For instance , a key aspect of our design is to use fixed cross-lingual embeddings in the encoder . This is necessary in the early stages of training , as it forces the encoder to use a common word representation for both languages , but it might also limit what it can ultimately learn in the process . For that reason , one could start to progressively update the weights of the encoder embeddings as training progresses . Similarly , one could also decouple the shared encoder into two independent encoders at some point during training , or progressively reduce the noise level .

- Regarding the semi-supervised experiments , note that our point here was not to improve the state of the art under these conditions , but rather to prove that the proposed system can also exploit a ( relatively ) small parallel corpus , showing its potential interest beyond the strictly unsupervised scenario .

- Our statement that “ it is not possible to use backtranslation alone without denoising ” was referring to our training procedure where backtranslation uses the model from the previous iteration . It is true that it does not apply to the general case , as backtranslation could also be used in conjunction with other translation methods ( e.g. embedding nearest neighbor ) , and we have consequently softened the statement in the revised version as suggested .

The authors propose to learn the rigid motion group ( translation and rotation ) from a latent representation of image sequences without the need for explicit labels .
Within their data driven approach they pose minimal assumptions on the model , requiring the group properties ( associativity , invertibility , identity ) to be fulfilled .
Their model comprises CNN elements to generate a latent representation in motion space and LSTM elements to compose these representations through time .
They experimentally demonstrate their method on sequences of MINST digits and the KITTI dataset .

Pros :
- interesting concept of combining algebraic structure with a data driven method
- clear idea development and well written
- transparent model with enough information for re-implementation
- honest pointers to scenarios where the method might not work well

Cons :
- the method is only intrinsically evaluated ( Tables 2 and 3 ) , but not compared with results from other motion estimation methods

Thank you for your review and comments . We have added a comparison to a self - supervised optical flow method to better contextualize our method . See the response to AnonReviewer3 for more details ( under the heading Compare to unsupervised optical flow ) .

Paper proposes an approach for learning video motion features in an unsupervised manner . A number of constraints are used to optimize the neural network that consists of CNN + RNN ( LSTM ) . Constraints stem from group structure of sequences and include associativity and inevitability . For example , forward - backward motions should cancel each other out and motions should be additive . Optimized network is illustrated to produce features that can be used to regress odometry .

Overall the approach is interesting from the conceptual point of view , however , experimental validation is very preliminary . This makes it difficult to asses the significance and viability of the approach . In particular , the lack of direct comparison , makes it difficult to asses whether the proposed group constraints are competitive with brightness constancy ( or similar ) constraints used to learn motion in an unsupervised manner in other papers .

It is true that proposed model may be able to learn less local motion information , but it is not clear if this is what happens in practice . In order to put the findings in perspective authors should compare to unsupervised optical flow approach ( e.g. , unsupervised optical flow produced by one of the proposed CNN networks and used to predict odometer on KITTI for fair comparison ) . Without a comparison of this form the paper is incomplete and the findings are difficult to put in the context of state - of - the-art .

Also , saying that learned features can predict odometry “ better than chance ” ( Section 4.2 and Table 2 ) seems like a pretty low bar for a generic feature representation .

Thank you for your helpful comments and suggestions . We have added a comparison to a recent self - supervised optical flow approach for the KITTI visual odometry experiments as suggested and updated the text accordingly . See below for responses to specific comments .

- Compare to unsupervised optical flow -
To put our method in context , we have included comparisons to a recent method for self - supervised optical flow estimation ( Yu et al 2016 ) . The output of this method is a dense optical flow field . In order to regress from this flow field to the camera motion parameters , we downsample the flow fields and run PCA over the full training set of fields . We then linearly regress from the flow field PCs to the camera motion parameters using least squares . Flow fields are computed at a resolution of 320x96 , and PCA is computed on downsampled flow fields of resolution 160x48 . The results from this method on KITTI are now included in the paper in table 2 and figure 6 ( in the supplement ) .

The full optical flow method outperforms our method . We note that egomotion estimation benefits greatly from maintaining information about spatial position ( which a flow field does , but our method does not ) . KITTI is characterized by stereotyped depth and is reasonably modelled as rigid , and under these circumstances camera translation and rotation can be estimated from a full flow field nearly linearly . The good performance of self - supervised flow + PCA here highlights the clear advantage of domain- restricted models and learning rules in a setting where those domain restrictions are appropriate . Our learning rule and model do not make these more restrictive assumptions but still performs reasonably in this setting .

To further contextualize our method , we also show the flow results as a function of the number of flow PCs included . As shown in figure 6 ( in the supplement ) , our method outperforms the flow method up to four flow PCs , and outperforms estimates of x-dimension rotation up to around ten flow PCs . These results bolster our claim of learning a reasonable representation of motion with minimal domain assumptions .

- Not clear it learns less local motion representation -
Thank you for drawing attention to this point . Unlike standard models of motion , our model is designed so that it can capture nonlocal , nonrigid motion in principle . By contrast , standard models are designed to capture only local motion ( optical flow ) or global motion with rigid structure ( egomotion ) . In practice , it appears that our learning rule does not succeed at capturing all of the nonlocal structure that the model can support . However , the KITTI results show that our trained model can be used to linearly regress reasonable estimates of camera translation and rotation , suggesting that the representation is nonlocal to some extent . The model and learning rule proposed here should serve as a baseline for future , more powerful learning rules that are still able to model nonrigid , nonlocal motion .

- " Better than chance " as a low bar . -
Our results show the feasibility and limits of what can be learned about motion using a model that makes as few assumptions as possible about image motion and by using a minimal learning rule to impose these constraints . As we mention , our method is n't competitive with state of the art results . To improve the interpretability of this method , we have included a systematic comparison of our method with a self - supervised optical flow method as a function of the amount of flow information included in the regression ( table 2 and figure 6 ( supplemental ) ) .

The paper presents a method that given a sequence of frames , estimates a corresponding motion embedding to be the hidden state of an RNN ( over convolutional features ) at the last frame of the sequence . The parameters of the motion embedding are trained to preserve properties of associativity and invertibility of motion , where the frame sequences have been recomposed ( from video frames ) in various way to create pairs of frame sequences with those - automatically obtained - properties . This means , the motion embedding is essentially trained without any human annotations .
Experimentally , the paper shows that in synthetic moving MNIST frame sequences motion embedding discovers different patterns of motion , while it ignores image appearance ( i.e. , the digit label ) . The paper also shows that linear regressor trained in KITTI on top of the unsupervised motion embedding to estimate camera motion performs better than chance .
Q to the authors : what labelled data were used to train the linear regressor in the KITTI experiment ?
Empirically , it appears that supervision by preserving group transformations may not be immensely valuable for learning motion representations .



Pros
1 ) The neural architecture for motion embedding computation appears reasonable
2 ) The paper tackles an interesting problem

Cons
1 ) For a big part of the introduction the paper refers to the problem of `` ```` " learning motion ” or `''understanding motion ” without being specific what it means by that .
2 ) The empirical results are not convincing of the strength of imposing group transformations for self - supervised learning of motion embeddings .
3 ) The KITTI experiment is not well explained as it is not clear how this regressor was trained to predict egomotion out of the motion embedding .


Thank you for your helpful comments . We have modified the paper to better explain the points you mention , and we hope this clarifies the text . See below for responses to specific comments .

- What labelled data for linear regressor , how was KITTI trained ? experiment not well explained -
For the regression experiments , we use the sequences of the KITTI visual odometry benchmark , which include annotations of camera translation and rotation . To test our model on this dataset , we linearly regress from the learned representation to the camera motion between an image pair . We do not fine - tune our model with labelled camera motion , but perform linear regression using least squares on the learned representations . The representations themselves are trained only on KITTI tracking , a different subset of KITTI than we use for least squares . Results are shown on the full set of KITTI visual odometry sequences . We also use least squares to regress from optical flow PCA components in the updated experiments ( we discuss this in more depth in the response to reviewer 3 , under the heading Compare to unsupervised optical flow ) .

We have also expanded figure 4 to explain more clearly the design of the KITTI interpolation experiments . The purpose of this experiment is to test whether our method is sensitive to deviations from the motion subspace - i.e. whether is sensitive to the difference between realistic and subtly unrealistic image sequences .

- " learning motion " , " understanding motion " -
Thank you for pointing out the ambiguity in these terms . By understanding motion , we mean learning a model that characterizes image transformations due to structure - preserving changes in time . Not all changes in image sequences reflect motion ( e.g. other changes are due to cuts in a video , etc. ) , and here we are concerned with characterizing the subspace of image transformations due specifically to motion . We have added a clarification of these terms in the updated manuscript .

Motion has several properties that we use to operationalize to what extent a learned representation characterizes the motion subspace . ( 1 ) A representation that characterizes motion can be read out to estimate the motion in the scene . In particular , it can be used to regress metric properties of motion , such as camera translation and rotation . We test this prediction by regressing to camera translation and rotation on KITTI and by tSNE clustering on MNIST digits , which reveals clustering by translation . ( 2 ) The model should also represent the same motion identically regardless of the image content . For example , a motion of a digit moving at one pixel per frame to the right should be represented the same whether the digit is a " 5 " or a " 3 " . We demonstrate this with the tSNE clustering results in the paper . ( 3 ) Image sequences produced by natural motion should be represented differently than image sequences not produced by natural motion . That is , a representation that characterizes motion can distinguish realistic motion from unrealistic motion . We demonstrate this property in our interpolation experiments .

- Strength of group representations -
We agree that stronger , domain -specific learning rules can be obtained by incorporating more constraints for a specific context . The smoothness constraint used in optical flow is one such rule in the context of locally rigid motion . Here we show that the more general rules based on groups can lead to representations useful for motion .

The group -based learning rules are complementary to more domain-specific learning rules , but they are also applicable in settings where domain-specific rules like smoothness or brightness constancy are not appropriate . We hope that our results will be useful to future work designing learning rules incorporating both more generic and more domain-specific inductive biases for learning motion and other kinds of image transformations .


Learning better policies from logged bandit feedback is a very important problem , with wide applications in internet , e-commerce and anywhere it is possible to incorporate controlled exploration . The authors study the problem of learning the best policy from logged bandit data . While this is not a brand new problem , the important and relevant contribution that the authors make is to do this using policies that can be learnt via neural networks . The authors are motivated by two main applications : ( i ) multi-class classification problems with bandit feedback ( ii ) ad placements problem in the contextual bandit setting .

The main contributions of the authors is to design an output layer that allows training on logged bandit feedback data . Traditionally in the full feedback setting ( setting where one gets to see the actual label and not just if our prediction is correct or incorrect ) one uses cross -entropy loss function to optimize the parameters of a deep neural network . This does not work in a bandit setting , and previous work has developed various methods such as inverse - propensity scoring , self - normalized inverse propensity scoring , doubly robust estimators to handle the bandit setting . The authors in this paper work with self - normalized inverse propensity scoring as the technique to deal with bandit feedback data . the self normalized inverse propensity estimator ( SNIPS ) that the authors use is not a new estimator and has been previously studied in the work of Adith Swaminathan and co-authors . However , this estimator being a ratio is not an easy optimization problem to work with . The authors use a fairly standard reduction of converting ratio problems to a series of constrained optimization problems . This conversion of ratio problems to a series of constrained optimization problems is a standard textbook problem , and therefore not new . But , i like the authors handling of the constrained optimization problems via the use of Lagrangian constraints . It would have been great if the authors connected this to the REINFORCE algorithm of Williams . Unfortunately , the authors do not do a great job in establishing this connection , and I hope they do this in the full version of the paper . The experimental results are fairly convincing and i really do not have any major comments . Here are my " minor " comments .

1 . It would be great if the authors can establish connections to the REINFORCE algorithm in a more elaborate manner . It would be really instructive to the reader .

2 . On page 6 , the authors talk about lowering the learning rate and the learning rate schedule . I am guessing this is because of the intrinsic high variance of the problem . It would be great if the authors can explain in more detail why they did so .

Thank you for the comments . We will follow your suggestion and elaborate on the connection to REINFORCE in the on-policy setting as outlined in the other response . The reason why we reduced the learning rate is two -fold . First , we did observe convergence issues at the setting used for cross -entropy training , and agree hat this is probably due to increased variance . In addition , note that cross-entropy and our ERM objective are simply quite different and produce gradients at different scale . More generally , there is probably more room for improving convergence speed , like the use of alternative minibatch sizes , but this is besides the main point of this paper .

In this paper , the authors propose a new output layer for deep networks allowing training on logged contextual bandit feedback . They propose a counterfactual risk minimization objective which makes the training procedure different from the one that uses conventional cross -entropy in supervised learning . The authors claim that this is the first attempt where Batch Learning from Bandit Feedback ( BLBF ) is performed using deep learning .

The authors demonstrate the derivation steps of their theoretical model and present 2 empirical results . The first result is on visual object classification using CIFAR - 10 . To simulate logged bandit feedback for CIFAR - 10 , the authors perform the standard supervised to bandit conversion using a hand - coded logging policy that achieves 49 % error rate on training data . Using the logged bandit feedback data for the proposed bandit model , the authors are able to achieve substantial improvement ( 13 % error rate ) and given more bandit feedback , the model is able to compete with the same architecture trained on the full - information using cross entropy ( achieving 8.2 % error rate ) .

The second result is a real - world verification of the proposed approach ( as the logged feedback are real and not synthesized using a conversion approach ) which is an advertisement placing task from Criteo ’s display advertising system ( Lefortier et al. , 2016 ) . The task consists of choosing the product to display in the ad in order to maximize the number of clicks . The proposed deep learning approach improve substantially on state - of - the-art . The authors show empirically that the proposed approach is able to have substantial gain compared to other methods . The analysis is done by performing ablation studies on context features which are not effective on linear models .

The paper is well written . The authors make sure to give the general idea of their approach and its motivation , detail the related work and position their proposed approach with respect to it . The authors also propose a new interpretation of the baseline in “ REINFORCE - like ” methods where it makes the counterfactual objective equivariant ( besides the variance reduction role ) . The authors explain their choice of using importance sampling to estimate the counterfactual risk . They also detail the arguments for using the SNIPS estimator , the mathematical derivation for the training algorithm and finally present the empirical results .

Besides the fact that empirical results are impressive , the presented approach allows to train use deep nets when manually labelling full - information feedback is not viable .

In section 4.2 , the neural network that has been used is a 2 layer network with tanh activation . It is clear that the intention of the authors is to show that even with a simple neural architecture , the gain is substantial compared to the baseline method , which is indeed the right approach to go with . Still , it would have been of a great benefit if they had used a deeper architecture using ReLU - based activations .



Thank you for the comments and the suggestion . We are planning to work with Criteo on further improving the results , and we agree that other architectures may perform substantially better . However , the key point of this paper is exploring the properties of our approach , not necessarily squeezing the last bit of performance out of any particular dataset . Note that the CIFAR10 results using the ResNet architecture already demonstrate that training deep and complex models using our approach is possible .

This paper proposes a new output layer in neural networks , which allows them to use logged contextual bandit feedback for training . The paper is well written and well structured .


General feedback :

I would say the problem addressed concerns stochastic learning in general , not just SGD for training neural nets . And it 's not a " new output layer " , but just a softmax output layer ( Eq. 1 ) with an IPS + baseline training objective ( Eq. 16 ) .


Others :

- The baseline in REINFORCE ( Williams ' 92 ) , which is equivalent to introduced Lagrange multiplier , is well known and well defined as control variate in Monte Carlo simulation , certainly not an " ad-hoc heuristic " as claimed in the paper [ see Greensmith et al. ( 2004 ) . Variance Reduction for Gradient Estimates in Reinforcement Learning , JMLR 5 . ]

- Bandit to supervised conversion : please add a supervised baseline system trained just on instances with top feedbacks -- this should be a much more interesting and relevant strong baseline . There are multiple indications that this bandit-to- supervised baseline is hard to outperform in a number of important applications .

- The final objective IPS^lambda is identical to IPS with a translated loss and thus re-introduces problems of IPS in exactly the same form that the article claims to address , namely :
* the estimate is not bounded by the range of delta
* the importance sampling ratios can be large ; samples with high such ratios lead to larger gradients thus dominating the updates . The control variate of the SNIPS objective can be seen as defining a probability distribution over the log , thus ensuring that for each sample that sample ’s delta is multiplied by a value in [ 0 , 1 ] and not by a large importance sampling ratio .
* IPS^lambda introduces a grid search which takes more time and the best value for lambda might not even be tested . How do you deal with it ?

- As author note , IPS^lambda is very similar to an RL - baseline , so results of using IPS with it should be reported as well :
In more detail , Note :
1 . IPS for losses < 0 and risk minimization : raise the probability of every sample in the log irrespective of the loss itself
2 . IPS for losses >0 and risk minimization : lower the same probability
3 . IPS^lambda : by the translation of the loss , it divides the log into 2 groups : a group whose probabilities will be lowered and a group whose probabilities will be raised ( and a third group for delta=lambda but the objective will be agnostic to these )
4 . IPS with a baseline would do something similar but changes over time , which means the above groups are not fixed and might work better . Furthermore , there is no hyperparameter / grid search required for the simple RL - baseline
-> results of using IPS with the RL - baseline should be reported for the BanditNet rows in Table 1 and in CIFAR - 10 experiments .

- What is the feedback in the CIFAR - 10 experiments ? Assuming it 's from [ 0..1 ] , and given the tested range of lambdas , you should run into the same problems with IPS and its degenerate solutions for lambdas >= 1.0 . In general , how are your methods behaving for lambda* ( corresponding to S* ) such that makes all difference ( delta_i - lambda* ) positive or negative ?

- The claim of Theorem 2 in appendix B does not follow from its proof : what is proven is that the value of S ( w ) lies in an interval [ 1 - e..1+e ] with a certain probability for all w . It says nothing about a solution of an optimization problem of the form f( w ) / S ( w ) or its constrained version . Actually , the proof never makes any connection to optimization .

- What the appendix C basically claims is that it 's not possible to get an unbiased estimate of a gradient for a certain class of non - convex ratios with a finite - sum structure . This would contradict some previously established convergence results for this type of problems : Reddi et al. ( 2016 ) Stochastic Variance Reduction for Nonconvex Optimization , ICML and Wang et al. 2013 . Variance Reduction for Stochastic Gradient Optimization , NIPS . On the other hand , there seem to be no need to prove such a claim in the first claim , since the difficulty of performing self - normalized IPS on GPU should be evident , if one remembers that the normalization should run over the whole logged dataset ( while only the current mini-batch is accessible to the GPU ) .

Thank you for the detailed comments that will help us further improve the paper . We agree that we should clarify the connection and similarities to the REINFORCE baseline , and we will point out in more detail how the baseline is different in the off- policy setting we consider . First , we cannot sample new roll - outs from the current policy under consideration , which means we cannot use the standard variance - optimal baseline estimator used in REINFORCE . Second , we tried using the ( estimated ) expected loss of the learnt policy as the baseline as is commonly done in REINFORCE . As Figure 1 shows , it is between 0.130 and 0.083 ( i.e. 0 / 1 loss on the test set ) for the best policies we found . Figure 2 ( left ) shows that these baseline values are well outside of the optimum range of about [ 0.75 - 1.0 ] . Finally , the right way to modify adaptive baseline estimation procedures from REINFORCE to the off-policy setting remains an open question , and it is unclear whether gradient variance ( as opposed to variance of the ERM objective ) is really the key issue in batch learning from bandit feedback .

To clarify your comment " final objective IPS^lambda is identical to IPS with a translated loss and thus re-introduces problems of IPS " : Note that none of the individual IPS^lambda is used as an estimate , and the actual estimate we are optimizing in Equation ( 11 ) is bounded . While the SNIPS estimate can substantially reduce variance , large weights can certainly still be an issue that cannot be overcome without additional side information . And while the grid search increases training time , the empirical results ( especially Figure 2 ) show that the prediction performance is not particularly sensitive to the exact value of lambda .

Regarding your comment on " degenerate solutions for lambdas >= 1 " : The solutions are not necessarily degenerate , but they are suboptimal . This is shown in Figure 2 .

Regarding your comment on Theorem 2 : You are absolutely right , and thank you for spotting this . We should be referring to the minimizer of the true risk in the statement of the theorem , not the minimizer of the empirical risk . What the theorem is supposed to say is : limiting the search to that range is unlikely to exclude the minimizer of the true risk . We will fix this in the final version .

Regarding your comment on a " baseline system trained just on instances with top feedbacks " : We are not sure what you mean by this baseline . One possible interpretation is : collect all unique contexts , and pair each context with action that has the highest observed reward in the logged dataset . Train on this manufactured dataset using supervised learning approaches . However , we generally do n't assume that we repeatedly see that same context multiple times , so it is not clear that this is really a practical baseline .

Regarding your comment on Appendix C : Our result does not contradict recent results on mini-batch optimization of finite sums of non - convex functions \sum_i f_i . While the SNIPS objective can be written as a finite - sum of non - convex f_i , mini-batch {f_i} in these problems do not correspond to our mini-batches {delta_i , impwt_i} . Since GPU can only hold a mini-batch and the normalizer requires the entire dataset , we may be tempted to explore ways of estimating the normalizer sufficiently well using mini-batches . Appendix C shows that any such approach is always going to give a biased gradient , justifying the effort in developing the Lagrangian approach instead .

I quite liked the revival of the dual memory system ideas and the cognitive ( neuro ) science inspiration . The paper is overall well written and tackles serious modern datasets , which was impressive , even though it relies on a pre-trained , fixed ResNet ( see point below ) .

My only complaint is that I felt I could n’t understand why the model worked so well . A better motivation for some of the modelling decisions would be helpful . For instance , how much the existence ( and training ) of a BLA network really help — which is a central new part of the paper , and was n’t in my view well motivated . It would be nice to compare with a simpler baseline , such as a HC classifier network with reject option . I also do n’t really understand why the proposed pseudorehearsal works so well . Some formal reasoning , even if approximate , would be appreciated .

Some additional comments below :

- Although the paper is in general well written , it falls on the lengthy side and I found it difficult at first to understand the flow of the algorithm . I think it would be helpful to have a high - level pseudocode presentation of the main steps .

- It was somewhat buried in the details that the model actually starts with a fixed , advanced feature pre-processing stage ( the ResNet , trained on a distinct dataset , as it should ) . I ’m fine with that , but this should be discussed . Note that there is evidence that the neuronal responses in areas as early as V1 change as monkeys learn to solve discrimination tasks . It should be stressed that the model does not yet model end - to - end learning in the incremental setting .

- p. 4 , Eq. 4 , is it really necessary to add a loss for the intermediate layers , and not only for the input layer ? I think it would be clearer to define the \mathcal { L} explictily somewhere . Also , should n’t the sum start at j=0 ?


Reviewer # 2 : My only complaint is that I felt I could n’t understand why the model worked so well . A better motivation for some of the modelling decisions would be helpful . For instance , how much the existence ( and training ) of a BLA network really help — which is a central new part of the paper , and was n’t in my view well motivated . It would be nice to compare with a simpler baseline , such as a HC classifier network with reject option .

Authors : We do explore how the BLA effects FearNet performance in an ablation study shown in Table 3 . We actually tried different variants for BLA before settling on the model that we used in the paper . We have included the results of the other variants in the supplemental material to help justify our decisions .

Reviewer # 2 : I also do n’t really understand why the proposed pseudorehearsal works so well . Some formal reasoning , even if approximate , would be appreciated .

Authors : Rehearsal and psuedorehearsal are old ideas from the 1990s . We have added more justification for why they help alleviate catastrophic forgetting in Section 2 and in the discussion .

Reviewer # 2 : Although the paper is in general well written , it falls on the lengthy side and I found it difficult at first to understand the flow of the algorithm . I think it would be helpful to have a high - level pseudocode presentation of the main steps .

Authors : The high - level pseudocode for FearNet ’s train and predict functionality is a great idea . We have included this in the supplemental material of our revised version .

Reviewer # 2 : It was somewhat buried in the details that the model actually starts with a fixed , advanced feature pre-processing stage ( the ResNet , trained on a distinct dataset , as it should ) . I ’m fine with that , but this should be discussed . Note that there is evidence that the neuronal responses in areas as early as V1 change as monkeys learn to solve discrimination tasks . It should be stressed that the model does not yet model end - to - end learning in the incremental setting .

Authors : We have included the following sentence in the beginning of Section 4 , “ In this paper , we use pre-trained embeddings of the input ( e.g. , ResNet ) . ” We think representation learning is an important next step , and it will be incorporated into FearNet 2.0 , which is currently in its early planning stages .

Reviewer # 2 : p. 4 , Eq. 4 , is it really necessary to add a loss for the intermediate layers , and not only for the input layer ? I think it would be clearer to define the \mathcal { L} explictily somewhere . Also , should n’t the sum start at j=0 ?

Authors : Thank you for pointing that out . We have fixed Eq. 4 and defined the \mathcal { L} term to make it clear that we are computing the MSE loss between the output of each hidden layer and the input / output of the mPFC autoencoder . The rationale for using MSE losses at the intermediate layers stem from Valpola ( 2015 ) , where he showed that errors in deeper layers had a harder time being corrected because they were further away from the training signal ( i.e. , data layer ) . Adding the multi-layer loss forces the autoencoder to correct errors at every layer . A good autoencoder fit is important for our framework because it is directly related to the fidelity of the pseudoexamples being generated for sleep phases .


Quality : The paper presents a novel solution to an incremental classification problem based on a dual memory system . The proposed solution is inspired by the memory storage mechanism in brain .

Clarity : The problem has been clearly described and the proposed solution is described in detail . The results of numerical experiments and the real data analysis are satisfactory and clearly shows the superior performance of the method compared to the existing ones .

Originality : The solution proposed is a novel one based on a dual memory system inspired by the memory storage mechanism in brain . The memory consolidation is inspired by the mechanisms that occur during sleep . The numerical experiments showing the FearNet performance with sleep frequency also validate the comparison with the brain memory system .

Significance : The work discusses a significant problem of incremental classification . Many of the shelf deep neural net methods require storage of previous training samples too and that slows up the application to larger dataset . Further the traditional deep neural net also suffers from the catastrophic forgetting . Hence , the proposed work provides a novel and scalable solution to the existing problem .

pros : ( a ) a scalable solution to the incremental classification problem using a brain inspired dual memory system
( b ) mitigates the catastrophic forgetting problem using a memory consolidation by pseudorehearsal .
( c ) introduction of a subsystem that allows which memory system to use for the classification

cons : ( a ) How FearNet would perform if imbalanced classes are seen in more than one study sessions ?
( b ) Storage of class statistics during pseudo rehearsal could be computationally expensive . How to cope with that ?
( c ) How FearNet would handle if there are multiple data sources ?


Reviewer # 3 : How FearNet would perform if imbalanced classes are seen in more than one study sessions ?

Authors : FearNet generates a balanced number of pseudoexamples during its sleep phase and when updating BLA , so class imbalance is not an issue . To test this , we did an experiment with CIFAR - 100 where we selected a random number of samples from each class ( 20 - 500 ) so that the class distribution was imbalanced . We would expect a slight degradation in performance because we are n’t using as many samples to train FearNet as we did in the paper ( i.e. , the model does n’t generalize as well for the test set . ) The results ( \omega_{base} = 0.884 , \omega_{new} = 0.729 , and \omega_{all} = 0.897 ) indicate that FearNet is robust to imbalanced class distributions .

Reviewer # 3 : Storage of class statistics during pseudo rehearsal could be computationally expensive . How to cope with that ?

Authors : We agree that storing class statistics is a major bottleneck , but FearNet still manages to be less memory intensive than other models . In Table 5 , we show that the storage cost for FearNet is still lower than previous methods . In Table 6 , we show that FearNet still outperforms other methods when only a diagonal covariance matrix is stored for each class , while decreasing storage costs by 65 % .

Reviewer # 3 : How FearNet would handle if there are multiple data sources ?

Authors : We assume that the reviewer is referring to FearNet ’s ability to handle multiple data modalities . In Table 4 , we explored FearNet ’s ability to simultaneously learn audio and visual information . The results showed that FearNet was able to simultaneously learn datasets with very different data representations .



This paper addresses the problem of incremental class learning with brain inspired memory system . This relies on 1 / hippocampus like system relying on a temporary memory storage and probabilistic neural network classifier , 2 / a prefrontal cortex - like ladder network architecture , performing joint autoencoding and classification , 3 / an amygdala- like classifier that combines the decision of both structures . The experiments suggests that the approach performs better than state - of - the-art incremental learning approaches , and approaches offline learning .
The paper is well written . The main issue I have with the approach is the role of the number of examples stored in hippocampus and its implication for the comparison to state - of - the art approaches .
Comments :
It seems surprising to me that the network manages to outperform other approaches using such a simplistic network for hippocampus ( essentially a Euclidian distance based classifier ) . I assume that the great performance is due to the fact that a lot of examples per classes are stored in hippocampus . I could not find an investigation of the effect of this number on the performance . I assume this number corresponds to the mini-batch size ( 450 ) . I would like that the authors elaborate on how fair is the comparison to methods such as iCaRL , which store very little examples per classes according to Fig . 2 . I assume the comparison must take into account the fact that FearNet stores permanently relatively large covariance matrices for each classes .
Overall , the hippocampus structure is the weakness of the approach , as it is so simple that I would assume it cannot adapt well to increasingly complex tasks . Also , making an analogy with hippocampus for such architecture seems a bit exaggerated .



Reviewer # 1 : It seems surprising to me that the network manages to outperform other approaches using such a simplistic network for hippocampus ( essentially a Euclidian distance based classifier ) . I assume that the great performance is due to the fact that a lot of examples per classes are stored in hippocampus .

Authors : Our comparison to the state- of - the-art is valid , and we have thoroughly checked our results . The HC network is simple , and we are comparing not only it , but the entire network to the state - of - the-art . We chose to implement HC using nearest neighbor density estimation because it is able to make inferences from new data immediately without expensive loops through the training data and often works well when data is scarce ( low - shot learning ) , and it has the perfect properties for enabling us to use pseudorehearsal for consolidating information from HC to mPFC . Psuedorehearsal requires mixing the raw recently observed data with generated examples of data observed long ago . Our HC model can be thought of as a simple buffer that can enable inference to be made until the information is transferred to mPFC , at which point the HC model is erased . After “ sleeping , ” FearNet does not store old values in HC because those memories now reside in the mPFC network . FearNet uses its BLA module to determine where the memory resides . Since FearNet could erroneously predict the network where the memory resides , we do n’t believe that HC artificially inflates FearNet performance . We tested this by performing the incremental learning experiment for the 1 - nearest neighbor ( 1 - NN ) for all three datasets ( see Table 2 in the revised manuscript ) . FearNet outperformed 1 - NN because 1 - NN was unable to generalize to the test data as well as FearNet . Additionally , compared to FearNet , 1 - NN is significantly less memory efficient ( Table 5 ) and very slow at making predictions .

Reviewer # 1 : I could not find an investigation of the effect of this number on the performance . I assume this number corresponds to the mini-batch size ( 450 ) .

Authors : We did investigate FearNet performance as a function of how many classes are learned ( stored in HC ) before its sleep phase is performed ( see Fig. 5 in the discussion ) . The mini-batch is only for 1 ) the sleep phase and 2 ) updating BLA . To make this clearer , we added “ We investigate FearNet ’s performance as a function of how much data is stored in HC in Section 6.2 . ” to the end of Section 4.1.

Reviewer # 1 : I would like that the authors elaborate on how fair is the comparison to methods such as iCaRL , which store very little examples per classes according to Fig . 2 . I assume the comparison must take into account the fact that FearNet stores permanently relatively large covariance matrices for each classes .

Authors : For CIFAR - 100 , iCaRL stores 2,000 exemplars for replay . At the beginning , they are able to store most / all of the exemplars ( there are 500 per class available ) since the buffer maxes out at 2,000 . As time moves on , that number decreases as it has to make room for new classes . By the end , there are 20 exemplars per class . We have re-written the last paragraph in Section 2 to clarify this point . In comparison , our model stores the mean / covariance matrix for each class , and then generates new “ exemplars ” ( pseudoexamples ) during sleep . Note that our method still outperforms iCaRL and other methods when only a diagonal covariance is stored , as discussed in Section 6.2 ( see Table 6 ) . Using MLP type architectures for iCaRL and FearNet , we showed that storing class statistics is still more memory efficient than storing these exemplars ( see Table 5 ) . Our future work will focus on using generative models that do n’t require class statistics for pseudorehearsal .

Reviewer # 1 : Overall , the hippocampus structure is the weakness of the approach , as it is so simple that I would assume it cannot adapt well to increasingly complex tasks . Also , making an analogy with hippocampus for such architecture seems a bit exaggerated .

Authors : We agree that HC could be improved , and we included in our future work that we want to replace HC with a semi-parametric model , instead of an entirely non-parametric model . We also agree that the low - level operations that occur in the individual FearNet modules ( e.g. , HC ) are not entirely analogous to operations that occur in the brain ; and to be fair , we do n’t make that claim . Our main inspiration for FearNet is 1 ) the brain ’s dual - memory architecture for rapid acquisition of new information and long - term storage of old information , 2 ) how mammalian brains consolidate recent memories to long term storage during sleep , and 3 ) the recent and remote recall pathways that BLA uses .


== Paper Summary ==
The paper addresses the problem of balancing capacities of generator and discriminator classes in generative adversarial nets ( GANs ) from purely theoretical ( function analytical and statistical learning ) perspective . In my point of view , the main * novel * contributions are :
( a ) Conditions on function classes guaranteeing that the induced IPMs are metrics and not pseudo-metrics ( Theorem 2.2 ) . Especially I liked an argument explaining why ReLu activations could work better in discriminator that tanh .
( b ) Proving that convergence in the neural distance implies a weak convergence ( Theorem 2.5 )
( c ) Listing particular cases when the neural distance upper bounds the so-called bounded Lipschitz distance ( also know as the Fortet - Mourier distance ) and the symmetrized KL - divergence ( Corollary 2.8 and Proposition 2.9 ) .

The paper is well written ( although with * many * typos ) , the topic is clearly motivated and certainly interesting . The related literature is mainly covered well , apart from several important points listed below .

== Major comments ==
In my opinion , the authors are slightly overselling the results . Next I shortly explain why :

( 1 ) First , point ( a ) above is indeed novel , but not groundbreaking . A very similar result previously appeared in [ 1 , Theorem 5 ] . The authors may argue that the referenced result deals only with MMDs , that is IPMs specified to the function classes belonging to the Reproducing Kernel Hilbert Spaces . However , the technique used to prove the " sufficient " part of the statement is literally *identical * .

( 2 ) As discussed in the paragraph right after Theorem 2.5 , Theorem 10 of [ 2 ] presents the same result which is on one hand stronger than Theorem 2.5 of the current paper because it allows for more general divergences than the neural distance and on the other hand weaker because in [ 2 ] the authors assumes a compact input space . Overall , Theorem 2.5 of course makes a novel contribution , because the compactness assumption is not required , however conceptually it is not that novel .

( 3 ) In Section 3 the authors discuss the generalization properties of the neural network distance . One of the main messages ( emphasized several times throughout the paper ) is that surprisingly the capacity of the generator class does not enter the generalization error bound . However , this is not surprising at all as it is a consequence of the way in which the authors define the generalization . In short , the capacity of discriminators ( D ) naturally enters the picture , because the generalization error accounts for the mismatch between the true data distribution mu ( used for testing ) and its empirical version hat{mu} ( used for training ) . However , the authors assume the model distribution ( nu ) is the same both during testing and training . In practice this is not true and during testing GANs use the empirical version of nu . If the authors were to account for this mismatch , capacity of G would certainly pop up as well .

( 4 ) The error bounds of Section 3 are based on a very standard machinery ( empirical processes , Rademacher complexity ) and to the best of my knowledge do not lead to any new interesting conclusions in terms of GANs .

( 5 ) Finally , I would suggest the authors to remove Section 4 . I suggest this mainly because the authors admit in Remark 4.1 that the main result of this section ( Theorem 4.1 ) is a corollary of a stronger result appearing in [ 2 ] . Also , the main part of the paper has 13 pages , while a recommended amount is 8.

== Minor comments ==

( 1 ) There are * MANY * typos in the paper . Only few of them are listed below .
( 2 ) First paragraph of page 18 , proof of Theorem 2.2 . This part is of course well known and the authors may just cite Lemma 9.3.2. of Dudley 's " Real analysis and probability " for instance .
( 3 ) Theorem 2.5 : " Let ... "
( 4 ) Page 7 , " ...we may BE interested ... "
( 5 ) Corollary 3.2 . I doubt that in practice anyone uses discriminator with one hidden unit . The authors may want to consider using the bound on the Rademacher complexity of DNNs recently derived in [ 3 ] .
( 6 ) Page 8 , " .. is neural networK "
( 7 ) Page 9 : " ... interested IN evaluating ... "
( 8 ) Page 10 . All most ---> almost .

[ 1 ] Gretton et al. , A Kernel Two - Sample Test , JMLR 2012 .
[ 2 ] Liu et al , Approximation and Convergence Properties of Generative Adversarial Learning , 2017
[ 3 ] Bartlett et al , Spectrally - normalized margin bounds for neural networks , 2017

Reply to your minor comments :
( 1 ) We are sorry for this . We carefully corrected typos in the revised version .
( 2 ) As you suggested , we now directly cite Lemma 9.3.2. of Dudley 's " Real analysis and probability " .
( 3 ) Thank you for your suggestion . We add the new bound in Appendix A.1 in our revised version . Compared to our previous result for neural network discriminators ( Corollary 3.3 ) , the new bound gets rid of the number of parameters , which can be prohibitively large in practice . Moreover , the new bound can be directly applied to the spectral normalized GANs [ 4 ] , and may explain the empirical success of the spectral normalization technique .

[ 1 ] Liu et al , Approximation and Convergence Properties of Generative Adversarial Learning , 2017
[ 2 ] Ivo Danihelka , Balaji Lakshminarayanan , Benigno Uria , DaanWierstra , and Peter Dayan . Comparison of maximum likelihood and gan-based training of real nvps . arXiv preprint arXiv:1705.05263 , 2017 .
[ 3 ] Aditya Grover , Manik Dhar , and Stefano Ermon . Flow-gan : Bridging implicit and prescribed learning in generative models . arXiv preprint arXiv:1705.08868 , 2017 .
[ 4 ] Anonymous . Spectral normalization for generative adversarial networks . International Conference on Learning Representations , 2018 . URL https://openreview.net/forum?id=B1QRgziT-.


Thank you very much for your insightful review . Your comments help us improve the paper a lot ! Overall , all your comments are correct . The following are our replies to some details of your comments .

Major comments :
( 1 ) Yes , for the sufficient part , the proof is standard and is the same as that of the uniqueness of weak convergence . The essential idea is in fact to make use of the moment matching effect of GANs , which is “ obvious ” for neural distance but tricky for neural divergence . As long as we have moment matching , we now directly cite Lemma 9.3.2. of Dudley 's " Real analysis and probability " as you suggested .

( 2 ) Theorem 2.5 and its proof have two important differences from Theorem 10 of [ 1 ] . First , as you pointed out , it gets rid of the compactness assumption . Second , its proof ( in Appendix E ) in fact gives the convergence rate of the GAN training as the neural distance is minimized . We can see that the convergence rate depends both on the optimization error decay rate ( d_F ( \mu , \nu_n ) ) and the representation error decay rate ( \epsilon ( r ) defined in Proposition 2.7 ) . This convergence rate provides guidance to improve the training speed of GANs , by utilizing either faster optimization algorithms or more representative discriminator set . On the contrary , the existence proof in [ 2 ] does not provide an estimate of the convergence rate .

( 3 ) Yes , you are correct . In the revised version , we emphasize that only when evaluated with neural distance , generalization is guaranteed as long as the discriminator set is small enough , regardless of the size of the generator or hypothesis set . In this paper , we want to bound $ d_F ( \mu , nu_m ) $ , i.e. , the difference between the unknown target distribution \mu and the learned distribution \nu_m , instead of the testing error $ d_F ( \hat{\mu} , \hat{nu}_m ) $ . If the the testing error , the capacity of the generator set would certainly pop up , as you commented .

( 4 ) The error bound under neural distance in Section 3 ( Theorem 3.1 ) is indeed based on a very standard machinery ( empirical processes , Rademacher complexity ) . However , we think itself and its induced results are still valuable for two reasons .

First , although its derivation is standard , its meaning is very different from that in supervised learning . When the evaluation metric is taken as neural distance , the generalization error can be purely bounded by the complexity of the discriminator set . This seemingly loose bound is indeed tight * in terms of the order of sample size m* for several common cases . For example , for GANs with neural discriminators and for MMD - GANs , the bound is O ( m^{ - 1 / 2 } ) ; for Wasserstein distance , the bound is O( m^{ - 1 / d} ) ; for total variation distance , the bound is O ( 1 ) . In all these cases , the bound is indeed tight * in terms of the order of sample size m* . Of course , the bound is still very loose in other aspects , due to the ignorance of the generator set .

Second , we use our bounds between neural distance and other standard measures to derive generalization for other evaluation metrics . Especially , when the KL divergence is used as the evaluation metric , our bound ( Corollary 3.5 ) suggests that the generator and discriminator sets must be compatible in that the log density ratios of the generators and the true distributions should exist and be included inside the linear span of the discriminator set . The strong condition that log-density ratio should exist partially explains the counter- intuitive behavior of testing likelihood in flow GANs ( [ 2 ,3 ] ) .

5 . We move the neural divergence section to Appendix B , only summarizing our new contributions in discrimination properties of f- GANs in Remark 2.2 . We would like to point out that the following result is our novel contribution : a neural $ f$ - divergence is discriminative if linear span of its discriminators \emph{without the output activation function} is dense in the bounded continuous function space . Both its statement and its proof are nontrivial and cannot be found in other places .

In more detail , the analysis of the paper is as follows . Firstly , it primarily focuses on GAN objective functions which are " integral probability metrics ( IPMs ) " ; one way to define these is by way of similarity to the W-GAN , namely IPMs replace the 1 - Lipschitz functions in W-GAN with a generic set of functions F . The paper overall avoids computational issues and treats the suprema as though exactly solved by sgd or related heuristic ( the results of the paper simply state supremum , but some of the prose seems to touch on this issue ) .

The key arguments of the paper are as follows .

1 . It argues that the discriminator set should be not simply large , it should be dense in all bounded continuous functions ; as a consequence of this , the IPM is 0 iff the distributions are equal ( in the weak sense ) . Due to this assertion , it says that it suffices to use two layer neural networks as the discriminator set ( as a consequence of the " universal approximation " results well - known in the neural network literature ) .

2 . It argues the discriminator set should be small in order to mitigate small - sample effects . ( Together , points 1 and 2 mimic a standard bias -variance tradeoff in statistics . ) For this step , the paper relies upon standard Rademacher results plus a little bit of algebraic glue . Curiously , the paper chooses to argue ( and forms as a key tenet , indeed in the abstract ) that the size of the generator set is irrelevant for this , only the size of the discriminator matters .

Unfortunately , I find significant problems with the paper , in order from most severe to least severe .

A . The calculation ruling out the impact of the generator in generalization calculations in 2 above is flawed . Before pointing out the concrete bug , I note that this assertion runs completely counter to intuition , and thus should be made with more explanation ( as opposed to the fortunate magic it is presented as ) . Moreover , I 'll say that if the authors choose to " fix " this bug by adding a generator generalization term , the bound is still a remedial application of Rademacher complexity , so I 'm not exactly blown away . Anyway , the bug is as follows . The equation which drops the role of the generator in the generalization calculation is the equation ( 10 ) . The proof of this inequality is at the start of appendix E. Looking at the derivation in that appendix , everything is correct up to the second - to- last display , the one with a supremum over nu in G. First of all , this right hand side should set off alarm bells ; e.g. , if we make the generator class big , we can make this right hand side essentially as big as the IPM allows even when mu = mu_m . Now the bug itself appears when going to the next display : if the definition of d_ F is expanded , one obtains two suprema , each own over _their own_ optimization variable ( in this case the variables are discriminator functions ) . When going to the next equation , the authors accidentally made the two suprema have the same variable and invoke a fortuitous but incorrect cancellation . As stated a few sentences back , one can construct trivial counterexamples to these inequalities , for instance by making mu and mu_m arbitrarily close ( even exactly equal if you wish ) and then making nu arbitrarily far away and the discriminator set large enough to identify this .

B . The assertions in 1 , regarding sizes of discriminator sets needed to achieve the goal of the IPM being 0 iff the distributions are equal ( in the weak sense ) , are nothing more than immediate corollaries of approximation results well - known for decades in the neural network literature . It is thus hard to consider this a serious contribution .

C . I will add on a non-technical note that the paper 's assertion on what a good IPM " should be " is arguably misled . There is not only a meaning to specific function classes ( as with Lip_1 in Wasserstein_1 ) beyond simply " many functions " , but moreover there is an interplay between the size of the generator set and the size of the discriminator set . If the generator set is simple , then the discriminator set can also get away with being simple ( this is dicussed in the Arora et al 2017 ICML paper , amongst other places ) . Perhaps I am the one that is misled , but even so the paper does not appear to give a good justification of its standpoint .

I will conclude with typos and minor remarks . I found the paper to contain a vast number of small errors , to the point that I doubted a single proofread .

Abstract , first line : " a minimizing " ? general grammar issue in this sentence ; this sort of issue throughout the paper .

Abstract , " this is a mild condition " . Optimizing over a function class which is dense in all bounded measurable functions is not a mild assumption . In the particular case under discussion , the size of the network can not be bounded ( even though it has just two layers , or as the authors say is the span of single neurons ) .

Abstract , " ... regardless of the size of the generator or hypothesis set " . This really needs explanation in the abstract , it is such a bold claim . For instance , I wrote " no " in the margin while reading the abstract the first time .

Intro , first line : its -> their .

Intro , # 3 " energy - based GANs " : 'm ' clashes with sample size .

Intro , bottom of page 1 , the sentence with " irrelenvant " : I ca n't make any sense of this sentence .

Intro , bottom of page 1 , " is a much smaller discriminator set " : no , the Lip_1 functions are in general incomparable to arbitrary sets of neural nets .

From here on I 'll comment less on typos .

Middle of page 2 , point ( i ) : this is the only place it is argued / asserted that the discriminator set should contain essentially everything ? I think this needs a much more serious justification .

Section 1.1 : Lebegure -> Lebesgue .

Page 4 , vicinity of equation 5 : there should really be a mention that none of these universal approximation results give a meaningful bound on the size of the network ( the bound given by Barron 's work , while nice , is still massive ) .

Start of section 3 . To be clear , while one can argue that the Lipschitz - 1 constraint has a regularization effect , the reason it was originally imposed is to match the Kantorovich duality for Wasserstein_1 . Moreover I 'll say this is another instance of the paper treating the discriminator set as irrelevant other than how close it is to being dense in all bounded measurable functions .

Abstract , " ... regardless of the size of the generator or hypothesis set " . This really needs explanation in the abstract , it is such a bold claim . For instance , I wrote " no " in the margin while reading the abstract the first time .

Our reply : Hope that you are convinced by this * bold * claim now .

Page 4 , vicinity of equation 5 : there should really be a mention that none of these universal approximation results give a meaningful bound on the size of the network ( the bound given by Barron 's work , while nice , is still massive ) .

Our reply : In the revised version , we will mention that most early universal approximation results , e.g. , Cybenko , 1989 ; Hornik et al. , 1989 ; Hornik , 1991 ; Leshno et al. , 1993 , are qualitive results and do not give a meaningful approximation rates . Barron ( 1993 ) and Bach ( 2017 ) give the approximation rates of two - layer neural networks .

Start of section 3 . To be clear , while one can argue that the Lipschitz - 1 constraint has a regularization effect , the reason it was originally imposed is to match the Kantorovich duality for Wasserstein_1 . Moreover , I 'll say this is another instance of the paper treating the discriminator set as irrelevant other than how close it is to being dense in all bounded measurable functions .

Our reply : We agree with you on the initial motivation of the Lipschitz - 1 constraint in WGAN . We do not require that the discriminator set be dense in C_b ( X ) .
On one hand , our basic requirement is that span of the discriminator set is dense in C_b ( X ) . The larger the discriminator set is , the more discriminative the IPM will be . On the other hand , the smaller the discriminator set is , the smaller the generalization error will be . As our title indicates , there is a discrimination - generalization tradeoff in GANs . Fortunately , we show that several GANs * in practice * already chose their discriminator set at the sweet point .


Abstract , " this is a mild condition " . Optimizing over a function class which is dense in all bounded measurable functions is not a mild assumption . In the particular case under discussion , the size of the network cannot be bounded ( even though it has just two layers , or as the authors say is the span of single neurons ) .

Our reply : This is a * big misunderstanding * of our results . Our Theorem 2.2 says that it suffices to optimizing over a function class F , whose span is dense in all bounded measurable functions , to guarantee the discriminative power of GANs . The optimization is over * F* , not * span of F *.
We emphasize the * span * several times in our paper . What ’s the difference the * span* makes ?
1 . For tanh ( or sigmoid ) activation : neural networks with one hidden layer and * sufficiently many neurons * can approximate any continuous functions ; neural networks with one hidden layer and * only one neuron * are sufficient to discriminate any two distributions .
2 . For relu activation : neural networks with one hidden layer , * sufficiently many neurons * and * unbounded weights * can approximate any continuous functions ; neural networks with one hidden layer , * only one neuron * and * bounded weights * are sufficient to discriminate any two distributions .
3 . A simpler example : the span of two points ( 1 ,0 ) and ( 0 ,1 ) is dense in the whole R^2 plane !
Therefore , our condition is indeed a very mild condition .

Intro , bottom of page 1 , the sentence with " irrelenvant " : I ca n't make any sense of this sentence .

Our reply : As we listed in the first page , different GANs define their objective functions ( e.g. , Wasserstein distance and f-divergence ) with different * non - parametric * discriminator sets , while in practice they use parametric discriminator sets as surrogates , which leads to objective functions like neural distance and neural divergence . In this sentence , we say that the properties of their objective functions in mind and objective functions in practice can be fundamentally different or even irrelevant . The main goal of this paper is to close this gap , i.e. , to provide discrimination and generalization properties for practical GANs , which use parametric discriminator sets .

Intro , bottom of page 1 , " is a much smaller discriminator set " : no , the Lip_1 functions are in general incomparable to arbitrary sets of neural nets .

Our reply : WGAN is motivated to optimized over all Lip_1 functions . In practice , WGAN optimizes over neural networks with bounded parameters ( weight clipping ) . It is argued in the WGAN paper that this practical discriminator set is contained in Lip_K function class for certain K>0 . However , the set of neural networks with bounded parameters is a much smaller set compared to the Lip_K function class .
We note that different GAN variants will also use different parametric function classes . We use F_{nn} as an abstract symbol for all these parametric discriminator sets .

Middle of page 2 , point ( i ) : this is the only place it is argued / asserted that the discriminator set should contain essentially everything ? I think this needs a much more serious justification .

Our reply : We did not argue / assert that that the discriminator set should contain essentially everything . We argue that the span of the discriminator set should be dense in bounded continuous function space . As we pointed out before , the * span * makes a big difference .


Thank you for your review ! We can see that you went to some details of the paper , and we are grateful for that . However , you may have some misunderstandings of the main results as we will elaborate as below .

A . In fact , our derivations and results on generalization is correct . The flaw you found in Appendix E ( Proof of Equation 10 ) is this derivation :
|d_F ( u , v ) – d_ F( u_m , v ) | <= d_ F ( u , u_m ) := \ sup_{f\in F} E_{u} [ f ] – E_{u_m } [ f ] .
In fact , this is purely the triangle inequality of the pseudo metric d_F . It can also be proved from the definition of d_ F as follows :
d_F ( u , v ) – d_ F( u_m , v)
: = \sup_{f\in F} ( E_{u} [ f ] – E_{v} [ f ] ) - \sup_{g\in F} ( E_{u_m } [ g ] – E_{v} [ g ] )
\le \sup_{f\in F} ( E_{u} [ f ] – E_{v} [ f ] - E_{u_m } [ f ] + E_{v} [ f ] )
= \sup_{f\in F} ( E_{u} [ f ] - E_{u_m } [ f ] ) = : d_ F ( u , u_m )
Symmetrically , we can prove d_ F ( u_m , v ) – d_ F ( u , v ) \le D_F ( u_m , u ) . Therefore , we have proved |d_F ( u , v ) – d_ F ( u_m , v ) | <= d_ F ( u , u_m ) .
We can make the two variables ( i.e. , f and g ) into one variable ( i.e. , f ) because $ - \sup_{g\in F} ( E_{u_m } [ g ] – E_{v} [ g ] ) \le - E_{u_m } [ f ] – E_{v} [ f ] $ for any $ f \in F$ .

B . Our main contributions in discriminative power of GANs are :
( 1 ) We give the necessary and sufficient condition for discrimination : span ( F ) is dense in bounded continuous function space ; see Theorem 2.2.
( 2 ) For any metric space , minimizing GAN ’s objective function implies weak convergence ; see Theorem 2.6.
These two theorems have nothing to do with the universal approximation property of neural networks . Therefore , it is unfair to say that they are *immediate corollaries of approximation results well - known for decades in the neural network literature *.
Combining our Theorem 2.2 and the well - known universal approximation property of neural networks , we proved that GANs with neural works as discriminators are discriminative ; see Theorem 2.3 and Corollary 2.4.

C . From the perspective of * game theory * , there is an interplay between the generator set and the discriminator set for the existence of equilibria ; see Arora ( 2017 ) . From the perspective of * minimizing a loss function * ( e.g. , neural distance / divergence ) , the existence of a global minimum is a straightforward result from continuity of the loss function and compactness of the hypothesis set . In this paper , we analyze the properties of different loss functions , regardless of the hypothesis set . The analog in supervised learning is analyzing properties of different loss functions ( negative log-likelihood , mean square error , regularized or not ) , regardless of the hypothesis set . From this perspective of * minimizing a loss function * , the goodness of the discriminator set ( defines the loss function ) can be studied independently from the generator set .
In this paper , except our results on KL divergence ( i.e. , Proposition 2.9 and Corollary 3.5 ) , we do not make any assumptions on the generator set . This makes our results widely applicable , regardless of the user ’s choice of the generator set . Of course , given the loss function , one can study the accuracy and generalization for a specific generator set , as we commented several times in our paper .


The authors provide an insight into the discriminative and generalizable aspect of the discriminator in GANs . They show that the richer discriminator set to enhance the discrimination power of the set while reducing the generalization bound . These facts are intuitive , but they made a careful analysis of it .

The authors provide more realistic analysis of discriminators by relaxing the constraint on discriminator set to have a richer closure of linear span instead of being rich by itself which is suitable for neural networks .

They analyze the weak convergence of probability measure under neural distance and generalize it to the other distances by bounding the neural distance .

For the generalization , they follow the standard generalization procedure and techniques while carefully adapt it to their setting .

Generally , I like the way the authors advertise their results , but it might be a bit oversold , especially for readers with theory background .

The authors made a good job in clarifying what is new and what is borrowed from previous work which makes this paper more interesting and easy to read .

Since the current work is a theoretical work , being over 8 pages is acceptable , but since the section 4 is mostly based on the previous contributions , the authors might consider to have it in the appendix .

We thank for your insightful comments and your appreciation of our results .

Thank you for pointing out that we might oversell the results , especially for readers with theory background . We avoid this in the revised version in two ways . First , we simplify the proof of standard results ( the proof of the sufficient part of Theorem 2.1 and the proof of Theorem 3.1 ) and focus more on their implications on GANs . Second , to avoid misunderstanding , we emphasize that only when evaluated with neural distance , generalization is guaranteed as long as the discriminator set is small enough , regardless of the size of the generator or hypothesis set . We explain that this seemingly - surprising result is reasonable because the evaluate metric ( neural distance ) is defined by the discriminator set and is “ weak ” compared to standard metrics like BL distance and KL divergence .

We move the neural divergence section to Appendix B , only summarizing our new contributions in discrimination properties of f- GANs in Remark 2.2 . We would like to point out that the following result is our novel contribution : a neural $ f$ - divergence is discriminative if linear span of its discriminators \emph{without the output activation function} is dense in the bounded continuous function space . Both its statement and its proof are nontrivial and cannot be found in other places .

Finally , in the revised version , we add one generalization bound for GANs with DNNs as discriminators in Appendix A.1 . This bound makes use of the recent result on Rademacher complexity of DNNs in Bartlett et al . ( 2017 ) . Compared to our previous result for neural network discriminators ( Corollary 3.3 ) , the new bound gets rid of the number of parameters , which can be prohibitively large in practice . Moreover , the new bound can be directly applied to the spectral normalized GANs ( Anonymous , 2018 ) , and may explain the empirical success of the spectral normalization technique .

[ Bartlett et al. , 2017 ) ] Peter L Bartlett , Dylan J Foster , and Matus J Telgarsky . Spectrally - normalized margin bounds for neural networks . In Advances in Neural Information Processing Systems , pp. 6241–6250 , 2017 .
[ Anonymous , 2018 ] Anonymous . Spectral normalization for generative adversarial networks . International Conference on Learning Representations , 2018 . URL https://openreview.net/forum?id=B1QRgziT-.


Quality : The paper is built on solid theoretical grounds and supplemented by experimental demonstrations . Specifically , the justification for using the Sinkhorn operator is given by theorem 1 with proof given in the appendix . Because the theoretical limit is unachievable , the authors propose to truncate the Sinkhorn operator at level $ L$ . The effect of approximation for the truncation level $ L$ as well as the effect of temperature $ \tau$ are demonstrated nicely through figures 1 and 2 ( a ) . The paper also presents a nice probabilistic approach to permutation learning , where the doubly stochastic matrix arises from Gumbel matching distribution .

Clarity : The paper has a good flow , starting out with the theoretical foundation , description of how to construct the network , followed by the probabilistic formulation . However , I found some of the notation used to be a bit confusing .

1 . The notation $ l$ appears in Section 2 to denote the number of iterations of Sinkhorn operator . In Section 3 , the notation $ l$ appears as $ g_l$ , where in this case , it refers to the layers in the neural network . This led me to believe that there is one Sinkhorn operator for each layer of neural network . But after reading the paper a few times , it seemed to me that the Sinkhorn operator is used only at the end , just before the final output step ( the part where it says the truncation level was set to $ L=20 $ for all of the experiments confirmed this ) . If I 'm correct in my understanding , perhaps different notation need to be used for the layers in the NN and the Sinkhorn operator . Additionally , it would have been nice to see a figure of the entire network architecture , at least for one of the applications considered in the paper .

2 . The distinction between $ g$ and $ g_l$ was also a bit unclear . Because the input to $ M$ ( and $ S$ ) is a square matrix , the function $ g$ seems to be carrying out the task of preparing the final output of the neural network into the input formate accepted by the Sinkhorn operator . However , $ g$ is stated as " the output of the computations involving $ g_l$ " . I found this statement to be a bit unclear and did not really describe what $ g$ does ; of course my understanding may be incorrect so a clarification on this statement would be helpful .

Originality : I think there is enough novelty to warrant publication . The paper does build on a set of previous works , in particular Sinkhorn operator , which achieves continuous relaxation for permutation valued variables . However , the paper proposes how this operator can be used with standard neural network architectures for learning permutation valued latent variable . The probabilistic approach also seems novel . The applications are interesting , in particular , it is always nice to see a machine learning method applied to a unique application ; in this case from computational neuroscience .

Other comments :

1 . What are the differences between this paper and the paper by Adams and Zemel ( 2011 ) ? Adams and Zemel also seems to propose Sinkhorn operator for neural network . Although they focus only on the document ranking problem , it would be good to hear the authors ' view on what differentiates their work from Adams and Zemel .

2 . As pointed out in the paper , there is a concurrent work : DeepPermNet . Few comments regarding the difference between their work and this work would also be helpful as well .

Significance : The Sinkhorn network proposed in the paper is useful as demonstrated in the experiments . The methodology appears to be straight forward to implement using the existing software libraries , which should help increase its usability .

The significance of the paper can greatly improve if the methodology is applied to other popular machine learning applications such as document ranking , image matching , DNA sequence alignment , and etc . I wonder how difficult it is to extend this methodology to bipartite matching problem with uneven number of objects in each partition , which is the case for document ranking . And for problems such as image matching ( e.g. , matching landmark points ) , where each point is associated with a feature ( e.g. , SIFT ) , how would one formulate such problem in this setting ?


We thank the reviewer for the good evaluation of our paper , and the useful commentary .

1 ) We unintentionally overloaded the $ l$ index , and we will fix this final paper . The Sinkhorn operator is only applied to the output of the last layer of the neural network . We will include a new figure ( in the appendix ) depicting our architecture .

2 ) We also agree the notation with $ g$ and $ g_l$ currently is odd . We will make an attempt to improve exposition in the main text , by defining g as the composition of g_l . The new figure that depicts the architecture should also help .

3 ) Our work drew some inspiration from Adams and Zemel , but it has clear differences : i ) we consider different tasks , beyond ranking ; ii ) we use a shared architecture to save parameters ( permutation equivariance ) ; and iii ) in Adams and Zemel the Sinkhorn operator is used to heuristically approximate an expectation over permutations . Theorem 1 allows us to justify that approximation : by appealing to the framework of Variational Inference in exponential families [ 1 ] we can understand the Sinkhorn operator as an approximate marginal inference routine . We will briefly comment on that in the related work section and as a new last appendix ( see also response to AnonReviewer2 , point 1 ) . Also , we will improve this section in order to make these distinctions more clear .

4 ) DeepPermNet obtains results that are comparable to ours ; however , our architecture is much simpler , as argued in the results section and appendix B.2.

5 ) We agree that there are many ways in which this work could be extended , and we are actively investigating some of them . In cases of matchings between groups of different sizes , there is a simple extension : pad the cost matrix with zeros so that its row and column dimensions coincide . Also , regarding image matchings , they may be achieved by changing the architecture slightly : this is indeed explored in a simultaneous ICLR submission [ 2 ] , dealing with generative models from an optimal transportation perspective . There , a network is trained to match samples from two datasets ( the actual data and samples from the generative model ) so that minimizes a total cost functional that is the distance on some learned embedding space .


[ 1 ] Graphical Models , Exponential Families , and Variational Inference Martin J. Wainwright . and Michael I. Jordan . https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf
[ 2 ] Improving GANs Using Optimal Transport . https://openreview.net/forum?id=rkQkBnJAb



Learning latent permutations or matchings is inherently difficult because the marginalization and partition function computation problems at its core are intractable . The authors propose a new method that approximates the discrete max - weight matching by a continuous Sinkhorn operator , which looks like an analog of softmax operator on matrices . They extend the Gumbel softmax method ( Jang et al. , Maddison et al. 2016 ) to define a Gumbel - Sinkhorn method for distributions over latent matchings . Their empirical study shows that this method outperforms competitive baselines for tasks such as sorting numbers , solving jigsaw puzzles etc .

In Theorem 1 , the authors show that Sinkhorn operator solves a certain entropy - regularized problem over the Birkhoff polytope ( doubly stochastic matrices ) . As the regularization parameter or temperature \tau tends to zero , the continuous solution approaches the desired best matching or permutation . An immediate question is , can one show a convergence bound to determine a reasonable choice of \tau ?

The authors use the Gumbel trick that recasts a difficult sampling problem as an easier optimization problem . To get around non-differentiable re-parametrization under the Gumbel trick , they extend the Gumbel softmax distribution idea ( Jang et al. , Maddison et al. 2016 ) and consider Gumbel - Sinkhorn distributions . They illustrate that at low temperature \tau , Gumbel - matching and Gumbel - Sinkhorn distributions are indistinguishable . This is still not sufficient as Gumbel - matching and Gumbel - Sinkhorn distributions have intractable densities . The authors address this with variational inference ( Blei et al. , 2017 ) as discussed in detail in Section 5.4.

The empirical results do well against competitive baselines . They significantly outperform Vinyals et al . 2015 by sorting up to N = 120 uniform random numbers in [ 0 , 1 ] with great accuracy < 0.01 , as opposed to Vinyals et al . who used a more complex recurrent neural network even for N = 15 and accuracy 0.9.

The empirical study on jigsaw puzzles over MNIST , Celeba , Imagenet gives good results on Kendall tau , l1 and l2 losses , is slightly better than Cruz et al. ( arxiv 2017 ) for Kendall tau on Imagenet 3x3 but does not have a significant literature to compare against . I hope the other reviewers point out references that could make this comparison more complete and meaningful .

The third empirical study on the C. elegans neural inference problem shows significant improvement over Linderman et al. ( arxiv 2017 ) .

Overall , I feel the main idea and the experiments ( especially , the sorting and C. elegance neural inference ) merit acceptance . I am not an expert in this line of research , so I hope other reviewers can more thoroughly examine the heuristics discussed by the authors in Section 5.4 and Appendix C.3 to get around the intractable sub-problems in their approach .

We thank the reviewer for their good evaluation of our paper , and the useful commentary .

1 ) The question of convergence bounds is a quite relevant one , and we stress there is a double limit , involving tau and L. A rigorous analysis of such convergence goes beyond the scope of our work , but we point to recent convergence bounds results [ 1 ] in the more general entropy regularized OT problem , that in our case expresses in terms of optimization over the Birkhoff polytope . We believe research as in [ 1 ] is highly relevant to our work , as it suggests ways to obtain computational improvements by suitably tweaking the plain Sinkhorn iteration scheme . We plan to explore this research avenue in the future . For now , choice of tau is treated as a hyperparameter , we select it so that performance is optimal . This is discussed in the main text ( section 3 ) and the appendix C.1

2 ) We did not include more results related to jigsaw solving with neural networks as this problem is very recent in the context of neural networks . Nonetheless , we include a reference to another paper [ 2 ] that deals with jigsaw puzzles using neural networks , although comparisons are impossible since they work on a i ) different dataset ( Pascal VOC ) and ii ) their method does not scale with permutations , as it does not appeal to Sinkhorn operator but indexes each permutation as a separate entity , and limits the number of used permutations to at most 1000 .


[ 1 ] Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration Jason Altschuler∗ , Jonathan Weed∗ , and Philippe Rigollet . https://arxiv.org/pdf/1705.09634.pdf
The concrete distribution . https://arxiv.org/abs/1611.00712
[ 2 ] M. Noroozi and P. Favaro . Unsupervised learning of visual representations by solving jigsaw puzzles .https://arxiv.org/abs/1603.09246

The idea on which the paper is based - that the limit of the entropic regularisation over Birkhoff polytope is on the vertices = permutation matrices - , and the link with optimal transport , is very interesting . The core of the paper , Section 3 , is interesting and represents a valuable contribution .

I am wondering whether the paper 's approach and its Theorem 1 can be extended to other regularised versions of the optimal transport cost , such as this family ( Tsallis ) that generalises the entropic one :

https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14584/14420

Also , it would be good to keep in mind the actual proportion of errors that would make a random choice of a permutation matrix for your Jigsaws . When you look at your numbers , the expected proportion of parts wrong for a random assignment could be competitive with your results on the smallest puzzles ( typically , 2 x2 ) . Perhaps you can put the * difference * between your result and the expected result of a random permutation ; this will give a better understanding of what you gain from the non-informative baseline .
( also , it would be good to define " Prop. wrong " and " Prop. any wrong " . I think I got it but it is better to be written down )

There should also be better metrics for bigger jigsaws -- for example , I would accept bigger errors if pieces that are close in the solution tend also to be put close in the err'ed solution .

Typos :

* Rewrite definition 2 in appendix . Some notations do not really make sense .

We thank the reviewer for their thorough evaluation and thoughtful comments ..

1 ) Regarding Tsallis entropy , we recognize it is a quite interesting direction , as it allows to better understand our work in terms of information geometry ( e.g . [ 1 ] ) . In a revised version we will include a commentary on how Theorem 1 may be interpreted as a way of performing marginal in exponential families ( e.g. see [ 2 ] ) . With this , it will become clear using Tsallis entropy would yield yet another approximation . However , it is not clear that increases in computational complexity would justify the use of this type of entropy . See also response to AnonReviewer 3 for a complementary discussion .

2 ) By “ proportion wrong ” we mean the proportion of wrongly identified pieces . By “ proportion any wrong ” we consider the proportion of cases ( entire puzzles ) where there was at least one mistake . The latter was used as a performance measure in [ 3 ] , and it is much more stringent . We will clarify this in the main text .

3 ) Regarding the proportion wrong : the expected value of the proportion of errors in random guessing a permutation of n items can be shown to be ( n - 1 ) / ( n ) . In 2 x2 puzzles , this means we expect 75 % of wrong pieces at random , but in practice no errors occur with our method ( that is an easy case , though ) . We will comment on this baseline in a revised version

4 ) We agree there might be better ways to measure error . For example , if we shift one row by one and put the last element at first , then the error on that row will be 100 % . It will be also high according to the l1 and l2 norms . However , the solution still makes sense from the point of view of preserving locality constraints and e.g. still look good on vision tasks . Because of this we believe we should move towards using metrics that take into consideration the structure of local coherence between pieces . Unfortunately , that goes beyond the scope of this work .

5 ) We will correct the typo and minor inconsistencies in the main text and appendix .

We hope the reviewer will take the reviews with higher degree of confidence into consideration .

[ 1 ]S.I. Amari . Information geometry and its applications http://www.springer.com/gp/book/9784431559771
[ 2 ] Graphical Models , Exponential Families , and Variational Inference Martin J. Wainwright . and Michael I. Jordan . https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf
[ 3 ] Order Matters : Sequence to sequence for sets . Oriol Vinyals , Samy Bengio , Manjunath Kudlur]https://arxiv.org/abs/1511.06391


- The authors investigate a minimax formulation of deep network learning to increase their robustness , using projected gradient descent as the main adversary . The idea of formulating the threat model as the inner maximization problem is an old one . Many previous works on dealing with uncertain inputs in classification apply this minimax approach using robust optimization , e.g. :

https://www2.eecs.berkeley.edu/Pubs/TechRpts/2003/CSD-03-1279.pdf
http://www.jmlr.org/papers/volume13/ben-tal12a/ben-tal12a.pdf

In the case of convex uncertainty sets , many of these problems can be solved efficiently to a global minimum . Generalization bounds on the adversarial losses can also be proved . Generalizing this approach to non- convex neural network learning makes sense , even when it is hard to obtain any theoretical guarantees .

- The main novelty is the use of projected gradient descent ( PGD ) as the adversary . From the experiments it seems training with PGD is very robust against a set of adversaries including fast gradient sign method ( FGSM ) , and the method proposed in Carlini & Wagner ( CW ) . Although the empirical results are promising , in my opinion they are not sufficient to support the bold claim that PGD is a ' universal ' first order adversary ( on p2 , in the contribution list ) and provides broad security guarantee ( in the abstract ) . For example , other adversarial example generation methods such as DeepFool and Jacobian - based Saliency Map approach are missing from the comparison . Also it is not robust to generalize from two datasets MNIST and CIFAR alone .

- Another potential issue with using projected gradient descent as adversary is the quality of the adversarial example generated . The authors show empirically that PGD finds adversarial examples with very similar loss values on multiple runs . But this does not exclude the possibility that PGD with different step sizes or line search procedure , or the use of randomization strategies such as annealing , can find better adversarial examples under the same threat model . This could make the robustness of the network rather dependent on the specific implementation of PGD for the inner maximization problem .

- In Tables 3 , 4 , and 5 in the appendix , in most cases models trained with PGD are more robust than models trained with FGSM as adversary , modulo the phenomenon of label leakage when using FGSM as attack . However in the bottom right corner of Table 4 , FGSM training seems to be more robust than PGD training against black box PGD attacks . This raises the question on whether PGD is truly ' universal ' and provides broad security guarantees , once we add more first order attacks methods to the mix .



We thank the reviewer for the feedback .

We agree with the reviewer that min-max approach for robust classification have been studied before . As we mention in our paper ( right after equation 2.1 ) , such formulations go back at least to the work of Abraham Wald in the 1940s ( e.g. , see https://www.jstor.org/stable/1969022). What we view as the main contribution of our paper lies , however , not in introducing a new problem formulation but in studying if such formulation can inform training methods that lead to reliably robust deep learning models in practice .

We do not claim that training with a PGD adversary is the main novelty of our paper - prior work has already employed a variety of iterative first - order methods . Instead , our goal is to argue that training with PGD is a principled approach to adversarial robustness and to give both theoretical and empirical evidence for this view . ( See the connection via Danskin ’s theorem , and our loss function explorations in Appendix A. ) Moreover , we demonstrate that adversarial training with PGD - when done properly - leads to state - of - the- art robustness on two canonical datasets . In contrast to much other work in this area , we have also validated the robustness of our models via a public challenge in which our model underwent ( unsuccessful ) attacks by other research groups .

Regarding PGD being a " universal " first - order adversary and the " broad security guarantee " claim : first , we would like to note that on Page 2 of our paper , we state that we provide evidence for this view , not that this view is necessarily correct . Still , we believe that from the point of view of first order methods , our evaluation approach is comprehensive . Moreover , it is also worth noting that there has been increasing evidence for this view since we first published our paper : ( i ) No researcher has been able to break our released models . ( ii ) Follow - up work has used verification tools to test the PGD approaches and found that the adversarial examples found by iterative first order methods are almost as good as the adversarial examples found with a computationally expensive exhaustive search . Hence we believe that at least in the context of L_infinity robustness , viewing PGD as a universal first - order adversary has merit .

Regarding JSMA and DeepFool : JSMA is an attack that is designed to perturb as few pixels as possible ( often by a large value ) . Restricting this attack to the space of attacks we consider ( 0.3 distance from original in L_infinity norm ) leads to an attack that is very slow and , as far as we could tell , less potent than PGD . Deepfool is an attack that has been designed with an aim of computing minimum norm perturbations . For the regime we are studying , the only difference between DeepFool and the CW attack is the choice of target class to use at each step . We did n’t feel that testing against this variation was necessary ( given the length of our paper ) . Again we want to emphasize that we invited the community to attempt attacks against our published model and we did n’t receive any attacks that significantly lowered the performance of our model . Nevertheless we will perform the suggested experiments and add them to the final version .

Regarding the point about PGD step size and variations : While one needs to tune PGD to a certain degree , we found that the method is robust to reasonable changes in the choice of PGD parameters . Training against different PGD variants also leads to robust networks .

Regarding Table 4 : We emphasize that Table 4 contains results for transfer attacks , which add an additional complication due to the mismatch between the source model used to construct the attack , and the target model that we would like to attack . We do observe that training with FGSM offers more robustness against * transfer * attacks constructed using networks trained with PGD . There is an important caveat however . The larger robustness is due to the difference in the two models , not because FGSM produces inherently more robust models . We view this effect as an artifact of the transferability phenomenon rather than a fundamental shortcoming of PGD - based adversarial training . When we consider the minimum across all columns in each row , the PGD - trained target model offers significantly more robustness than the FGSM - trained model ( 64.2 % vs. 0.0 % ) .


This paper proposes to look at making neural networks resistant to adversarial loss through the framework of saddle - point problems . They show that , on MNIST , a PGD adversary fits this framework and allows the authors to train very robust models . They also show encouraging results for robust CIFAR - 10 models , but with still much room for improvement . Finally , they suggest that PGD is an optimal first order adversary , and leads to optimal robustness against any first order attack .

This paper is well written , brings new ideas and perfoms interesting experiments , but its claims are somewhat bothering me , considering that e.g. your CIFAR - 10 results are somewhat underwhelming . All you 've really proven is that PGD on MNIST seems to be the ultimate adversary . You contrast this to the fact that the optimization is non -convex , but we know for a fact that MNIST is fairly simple in that regime ; iirc a linear classifier gets something like 91 % accuracy on MNIST . So my guess is that the optimization problem on MNIST is in fact pretty convex and mostly respects the assumptions of Danskin 's theorem , but not so much for CIFAR - 10 ( maybe even less so for e.g. ImageNet , which is what Kurakin et al. seem to find ) .

Considering your CIFAR - 10 results , I do n't think anyone should " suggest that secure neural networks are within reach " , because 1 ) there is still room for improvement 2 ) it 's a safe bet that someone will always just come up with a better attack than whatever defense we have now . It has been this way in many disciplines ( crypto , security ) for centuries , I do n't see why deep learning should be exempt . Simply saying " we believe that our robust models are significant progress on the defense side " was enough , because afaik you did improve on CIFAR - 10's SOTA ; do n't overclaim .
You make these kinds of claims in a few other places in this paper , please be careful with that .

The contributions in your appendix are interesting .
Appendix A somewhat confirms one of the postulates in Goodfellow et al . ( 2014 ) : " The direction of perturbation , rather than the specific point in space , matters most . Space is not full of pockets of adversarial examples that finely tile the reals like the rational numbers " .
Appendix B and C are not extremely novel in my mind , but definitely add more evidence .
Appendix E is quite nice since it gives an insight into what actually makes the model resistant to adversarial examples .


Remarks :
- The update for PGD should be using \nabla_{x_t} L( \theta , x_t , y ) , ( rather than only \nabla_x ) ?
- In table 2 , attacking a with 20 - step PGD is doing better than 7 - step . When you say " other hyperparameter choices did n’t offer a significant decrease in accuracy " , does that include the number of steps ? If not why stop there ? What happens for more steps ? ( or is it too computationally intensive ? )
- You only seem to consider adversarial examples created from your dataset + adv. noise . What about rubbish class examples ? ( e.g. rgb noise )


We thanks the reviewer for providing feedback .

Regarding " secure networks are within reach " claim : We definitely agree that there is ( large ) room for improvement on CIFAR10 . Our claim , however , comes from the fact that ( to the best of our knowledge ) the classifiers we trained were the first ones to robustly classify any non-trivial fraction of the test set . We indeed believe ( and provide experimental evidence for it ) that no attack will significantly decrease the accuracy of our classifier . We view our results as a baseline that shows that classification robustness is indeed achievable ( against a well - defined class of adversaries at least ) . We agree that our claims ended up sounding too strong though . We will update our paper to tone them down .

Regarding CIFAR10 results : The reviewer points out that the optimization landscape for MNIST is much simpler than that of CIFAR10 and that this would explain the difference in the performance of the resulting classifiers . We want to point out , however , that our performance on CIFAR10 is due to poor generalization and not the difficulty of the training problem itself . As can be seen in Figure 1b , we are able to train a perfectly robust classifier with 100 % adversarial accuracy on the training set . This shows that the optimization landscape of the problem is still tractable with a PGD adversary .

Regarding CIFAR10 attack parameters : We did n’t explore additional parameters due to the computational constraints at the time . Overall we have observed that the number of PGD steps does not change the resulting accuracy by more than a few percent . For instance , if we retrain the ( non - wide version of the ) CIFAR10 network with a 5 - step PGD adversary we get the following accuracies when testing against PGD : 5 steps -> 45.00 % , 10 steps -> 43.02 % , 20 steps -> 42.65 % , 100 steps -> 42.21 %.

Regarding rubbish class examples : We agree that rubbish class examples are an important class to consider . However it is unclear how to rigorously define them . As we discuss in our paper ( 3rd paragraph of page 3 ) , providing any kind of robustness guarantees requires a precise definitions of the allowed adversarial perturbations .


This paper consolidates and builds on recent work on adversarial examples and adversarial training for image classification . Its contributions :

- Making the connection between adversarial training and robust optimization more explicit .

- Empirical evidence that :
* Projected gradient descent ( PGD ) ( as proposed by Kurakin et al . ( 2016 ) ) reasonably approximates the optimal attack against deep convolutional neural networks
* PGD finds better adversarial examples , and training with it yields more robust models , compared to FGSM

- Additional empirical analysis :
* Comparison of weights in robust and non-robust MNIST classifiers
* Vulnerability of L_infty -robust models to to L_2 -bounded attacks

The evidence that PGD consistently finds good examples is fairly compelling -- when initialized from 10,000 random points near the example to be disguised , it usually finds examples of similar quality . The remaining variance that 's present in those distributions should n't hurt learning much , as long as a significant fraction of the adversarial examples are close enough to optimal .

Given the consistent effectiveness of PGD , using PGD for adversarial training should yield models that are reliably robust ( for a specific definition of robustness , such as bounded L_infinity norm ) . This is an improvement over purely heuristic approaches , which are often less robust than claimed .

The comparison to R+ FGSM is interesting , and could be extended in a few small ways . What would R+FGSM look like with 10,000 restarts ? The distribution should be much broader , which would further demonstrate how PGD works better on these models . Also , when generating adversarial examples for testing , how well would R+ FGSM work if you took the best of 2,000 random restarts ? This would match the number of gradient computations required by PGD with 100 steps and 20 restarts . Again , I expect that PGD would be better , but this would make that point clearer . I think this analysis would make the paper stronger , but I do n't think it 's required for acceptance , especially since R+ FGSM itself is such a recent development .

One thing not discussed is the high computational cost : performing a 40 - step optimization of each training example will be ~ 40 times slower than standard stochastic gradient descent . I suspect this is the reason why there are results on MNIST and CIFAR , but not ImageNet . It would be very helpful to add some discussion of this .

The title seems unnecessarily vague , since many papers have been written with the same goal -- make deep learning models resistant to adversarial attacks . ( This comment does not affect my opinion about whether or not the paper should be accepted , and is merely a suggestion for the authors . )

Also , much of the paper 's content is in the appendices . This reads like a journal article where the references were put in the middle . I do n't know if that 's fixable , given conference constraints .

We thank the reviewer for the positive feedback .

Regarding R+FGSM : We evaluated our robust networks against R+ FGSM with multiple restarts and got the following results .
- MNIST . PGD - 40 : 93.2 % , R+FGSM x 40 : 92.2 % , R+FGSM x 2000 : 90.51 %.
- CIFAR10 ( non-wide ) . PGD - 10 : 43.02 % , R+FGSM x 10 : 50.17 % , R+FGSM x 2000 : 48.66 %.
These experiments suggest that for evaluation purposes R+ FGSM is qualitatively similar to PGD ( at least for adversarially trained networks ) . Still if one attempts to adversarially * train * using R+ FGSM , the resulting classifier overfits to the R+FGSM perturbations and while achieving high training and test accuracy against R+ FGSM , it is completely vulnerable to PGD .

We also created loss histograms to compare PGD and R+ FGSM with the results plotted at https://ibb.co/gcJuxG ( final loss value frequency over 10,000 random restarts for 5 random examples ) . We observe that R+ FGSM exhibits a similar concentration phenomenon to that observed for PGD in Appendix A . We will include these experiments in the final paper version . We agree that further investigating the difference between PGD and R+ FGSM is worth exploring in subsequent research .

Regarding the computational cost of robust optimization : It is indeed true that training against a PGD adversary increases the training time by a factor that is roughly equal to the number of PGD steps . This is a drawback of this method that we hope will be addressed in future research . But , at this point , getting sufficient robustness indeed leads to a running time overhead . We will add a brief discussion of this in the final version of the paper .

Regarding title choice : Our intention was to convey that there exist robustness baselines that current techniques can achieve . Still , we agree that the title might be too vague and we will revisit our choice .

This paper is well - written and easy follow . I did n't find serious concern and therefore suggest an acceptance .

Pros
Methodology
1 . inductive ability : can generalize to unseen nodes without any further training
2 . personalized ranking : the model uses natural ranking that embeddings of closer nodes ( considers node pairs of any distance ) should be closer in the embedding space , which is more general than prevailing first and second order proximity
3 . sampling strategy : the proposed node - anchored sampling method gives unbiased estimates of loss function and successfully reduces the time complexity

Experiment
1 . Evaluation tasks including link prediction and node classification are conducted across multiple datasets with additional parameter sensitivity and missing - link robustness experiments
2 . Compared with various baselines with diverse model designs such as GCN and node2vec as well as compared with naive baseline ( using original node attributes as model inputs )
3 . Demonstrated the model captures uncertainties and the learned uncertainties can be used to infer latent dimensions
Related Works
The survey of related work is sufficiently wide and complete .

Cons
Authors should include which kind of model is used to do the link prediction task given embedding vectors from different models as inputs .

Thank you for your review and comments .

Regarding the model used to do the link prediction task we adopt the exact same approach as described in the respective original methods of each of the competitors ( e.g we use the dot product of the embeddings ) . For Graph2 Gauss the negative energy ( - E_ij ) is used for ranking candidate links . Note that the two metrics AUC and AP do not need a binary decision ( edge / non-edge ) , but rather a ( possibly unnormalized ) score indicating how likely is the edge . We now include these details in the uploaded revised version .

The paper proposes to learn Gaussian embeddings for directed attributed graph nodes . Each node is associated to a Gaussian representation ( mean and diagonal covariance matrix ) . The mean and diagonal representations for a node are learned as functions of the node attributes . The algorithm is unsupervised and optimizes a ranking loss : nodes at distance 1 in the graph are closer than nodes at distance 2 , etc . Distance between nodes representation is measured via KL divergence . The ranking loss is a square exponential loss proposed in energy based models . In order to limit the complexity , the authors propose the use of a sampling scheme and show the convergence in expectation of this strategy towards the initial loss . Experiments are performed on two tasks : link prediction and node classification . Baselines are unsupervised projection methods and a ( supervised ) logistic regression . An analysis of the algorithm behavior is then proposed .
The paper reads well . Using a ranking loss based on the node distance together with Gaussian embeddings is probably new , even if the novelty is not that big . The comparisons with unsupervised methods shows that the algorithm learns relevant representations .
Do you have a motivation for using this specific loss Eq. ( 1 ) , or is it a simple heuristic choice ? Did you try other ranking losses ?
For the link prediction experiments , it is not indicated how you rank candidate links for the different methods and how you proceed with the logistic . Did you compare with a more complex supervised model than the logistic ? Fort the classification tasks , it would be interesting to compare to supervised / semi-supervised embedding methods . The performance of unsupervised embeddings for graph node classification is usually much lower than supervised / semi-supervised methods . Having a measure of the performance gap on the different tasks would be informative . Concerning the analysis of uncertainity , discovering that uncertainty is higher for nodes with neighbors of distinct classes is interesting . In your setting this might simply be caused by the difference in the node attributes . I was not so convinced by the conclusions on the dimensionality of the hidden representation space . An immediate conclusion of this experiment would be that only a small dimensional latent space is needed . Did you experiment with this ?
Detailed comments :
The title of the paper is “ Deep … ” . There is nothing Deep in the proposed model since the NN are simple one layer MLPs . This is not a criticism , but the title should be changed .
There is a typo in KL definition ( d should be replaced by the dimension of the embeddings ) . Probably another typo : the energy should be + D_KL and not – D_KL . The paragraph below eq ( 1 ) should be modified accordingly .
All the figures are too small to see anything and should be enlarged .
Overall the paper brings some new ideas . The experiments are fine , but not so conclusive .


Thank you for your review and comments . We provide answers to all your questions .

1 ) Eq. ( 1 ) motivation :
Three types of loss functions are typically considered in the ranking literature : pointwise , pairwise and listwise . We employ the pairwise approach since it usually outperforms the pointwise approach and compared to the listwise approach it is more amenable to stochastic training . The listwise approach is also computationally more expensive and early experiments did not show any benefits of using it . Regarding the pairwise loss function we indeed considered several forms typically used in energy - based learning , including the square - exponential , the hinge loss , LVQ2 and others . They performed comparatively . The final choice of the square - exponential is because compared to e.g. the hinge loss and LVQ2 we do n't have the need for tuning a hyperparameter such as the margin .

2 ) Link prediction :
Note that the two metrics AUC and AP do not need a binary decision ( edge / non-edge ) , but rather a ( possibly unnormalized ) score indicating how likely is the edge . To rank candidate links ( i.e. obtain the score ) we adopt the exact same approach as described in the respective original methods of each of the competitors ( e.g we use the dot product of the embeddings ) . For Graph2 Gauss the negative energy ( - E_ij ) is used for ranking candidate links . We now include these details in the uploaded revised version .

3 ) Logistic regression :
We used the logistic regression as a supervised model since this is a common choice used in almost all previous node embedding papers .

4 ) Supervised / semi-supervised method :
It is expected that the performance of supervised / semi-supervised method would be stronger , especially on the node classification task . However , as we already state in the related work section the focus of this paper is on unsupervised learning . While additional comparison with different supervised / semi-supervised methods would be beneficial , we feel this would distract the reader from the main goal : " unsupervised learning of node embeddings " . Furthermore , it would be straightforward to extend Graph2 Gauss to the semi-supervised setting by including a supervised component in the loss , and we leave this for future work .

5 ) Uncertainty / dimensionality :
The conclusion that only a small dimensional latent space is needed is correct and we indeed experimented with this : the sensitivity analysis in Figures 1a ) and 1 b ) shows that increasing the latent dimensionality beyond some small ( dataset -specific ) number does n't give significant increase in performance and the performance flattens out . The benefit of the uncertainty analysis is that for a new dataset we would not need to do train multiple models with different latent dimensions such as in Figures 1a ) and 1 b ) to determine what is the minimum number of dimensions for good performance . We could instead train the model with a large latent dimension and perform analysis similar to the one in Figure 4 c ) .

6 ) Deep :
We used " Deep " in the title , since the general architecture is conceived with multiple layers in mind . However , in our experiments single hidden layers proved to be enough to reach good performance . We could certainly change the the title to reflect this .

7 ) KL / Energy typo :
It is true , the KL definition and energy have a typo . We have already fixed this in the uploaded revised version . This question was asked earlier and we also answer it in more details in the comment below .

8 ) Readability :
We agree that readability is important and we and we will enlarge the figures .

This paper proposes Graph2 Gauss ( G2G ) , a node embedding method that embeds nodes in attributed graphs ( can work w/ o attributes as well ) into Gaussian distributions rather than conventionally latent vectors . By doing so , G2G can reflect the uncertainty of a node 's embedding . The authors then use these Gaussian distributions and neighborhood ranking constraints to obtain the final node embeddings . Experiments on link prediction and node classification showed improved performance over several strong embedding methods . Overall , the paper is well - written and the contributions are remarkable . The reason I am giving a less possible rating is that some statements are questionable and can severely affect the conclusions claimed in this paper , which therefore requires the authors ' detailed response . I am certainly willing to change my rating if the authors clarify my questions .

Major concern 1 : Is the latent vector dimension L really the same for G2G and other compared methods ?
In the first paragraph of Section 4 , it is stated that " in all experiments if the competing techniques use an embedding of
dimensionality L , G2G ’s embedding is actually only half of this dimensionality so that the overall number of ’ parameters ’ per node ( mean vector + variance terms ) matches L . " This setting can be wrong since the degree of freedom of a L-dim Gaussian distribution should be L+L ( L - 1 ) / 2 , where the first term corresponds to the mean and the second term corresponds to the covariance . If I understand it correctly , when any compared embedding method used an L-dim vector , the authors used the dimension of L / 2 . But this setting is wrong if one wants the overall number of ’ parameters ’ per node ( mean vector + variance terms ) matches L , as stated by the authors . Fixing L , the equivalent dimension L_G2G for G2G should be set such that L_G2G + L_G2G ( L_G2G - 1 ) / 2=L , not 2*L_G2G=L. Since this setting is universal to the follow - up analysis and may severely degrade the performance of GSG due to less embedding dimensions , I hope the authors can clarify this point .

Major concern 2 : The claim on inductive learning
Inductive learning is one of the major contributions claimed in this paper . The authors claim G2G can learn an embedding of an unseen node solely based on their attributes . However , is it not clear why this can be done . In the learning stage of Sec. 3.3 , the attributes do not seem to play a role in the energy function . Also , since no algorithm descriptions are available , it 's not clear how using only an unseen node 's attributes can yield a good embedding under G2G work ( so does Sec. 4.5 ) .
Moreover , how does it compare to directly using raw user attributes for these tasks ?

Minor concern / suggestions : The " similarity " measure in section 3.1 using KL divergence should be better rephased by " dissimilarity " measure . Otherwise , one has a similarity measure $ Delta $ and wants it to increase as the hop distance k decreases ( closer nodes are more similar ) . But the ranking constraints are somewhat counter- intuitive because you want $ Delta$ to be small if nodes are closer . There is nothing wrong with the ranking condition , but rather an inconsistency between the use of " similarity " measure for KL divergence .


Thank you for your review and comments . We provide clarification for all your concerns .

1 ) Number of parameters :
Yes the latent dimension is indeed the same for G2G and the other compared methods . As mentioned in Sections 3.2 and 4.4 we always use ** diagonal * * covariance matrices which only have free parameters on the diagonal ( and zeros everywhere else ) . Thus , an L-dimensional Gaussian with a diagonal covariance has L + L free parameters ( mean + variance terms ) and using only half of the competitors ' dimensionality is a fair comparison .

You are correct - in general , an L-dimensional Gaussian distribution has L+L( L - 1 ) / 2 free parameters , but only if we use a ** full * * covariance matrix . Our choice of diagonal covariances leads not only to fewer parameters but it also has computational advantages ( e.g. it 's easy to invert a diagonal matrix ) . We now highlight this choice one more time in the evaluation section for increased clarity .

2.1 ) Inductive learning :
To see why the attributes play a role in the energy function notice that \mu_i and \sigma_i are not free parameters ( i.e. to be updated by gradient descent ) , but they are rather the output of a parametric function that takes the node 's attributes x_i as input . More specifically , as mentioned in Section 3.2 ( and also in the appendix ) \mu_i and \sigma_i are the outputs of a feed - forward neural network that takes the attributes of the node as input . During learning we do not directly update \mu_i and \sigma_i , but rather we update the weights of the neural network that produces \mu_i and \sigma_i as output .

As mention in the discussion ( Section 3.4 ) during learning we need both the attributes and the network structure ( since the loss depends on the network structure ) . However , once the learning concludes , we essentially have a learned function f_theta ( x_i ) that only needs the attributes ( x_i ) of the node as input to produce the embedding ( \mu_i and \ sigma_i ) as output . This is precisely what enables G2G to be inductive .

2.2 ) Raw attributes :
We do indeed already compare the performance when using the raw attributes . The " Logistic Regression " method shown in Table 1 and 2 , as well as Figures 1 and 2 , is trained using only the raw attributes and it actually shows strong performance as a baseline . However , on the inductive learning task specifically , as we see in Table 2 , G2G has significantly better performance compared to the logistic regression method that uses only the raw attributes .

3 ) Similarity / Dissimilarity :
We agree with your comment w.r.t. similarity / dissimilarity and the KL divergence , this is essentially a typo and is already fixed in the uploaded revised version . This question was also asked earlier and we answer it in more details in the comment below .

Update : Based on the discussions and the revisions , I have improved my rating . However I still feel like the novelty is somewhat limited , hence the recommendation .

======================

The paper introduces a system to estimate a floor - level via their mobile device 's sensor data using an LSTM to determine when a smartphone enters or exits a building , then using the change in barometric pressure from the entrance of the building to indoor location . Overall the methodology is a fairly simple application of existing methods to a problem , and there remain some methodological issues ( see below ) .

General Comments
- The claim that the bmp280 device is in most smartphones today does n’t seem to be backed up by the “ comScore ” reference ( a simple ranking of manufacturers ) . Please provide the original source for this information .
- Almost all exciting results based on RNNs are achieved with LSTMs , so calling an RNN with LSTM hidden units a new name IOLSTM seems rather strange - this is simply an LSTM .
- There exist models for modelling multiple levels of abstraction , such as the contextual LSTM of [ 1 ] . This would be much more satisfying that the two level approach taken here , would likely perform better , would replace the need for the clustering method , and would solve issues such as the user being on the roof . The only caveat is that it may require an encoding of the building ( through a one - hot encoding ) to ensure that the relationship between the floor height and barometric pressure is learnt . For unseen buildings a background class could be used , the estimators as used before , or aggregation of the other buildings by turning the whole vector on .
- It ’s not clear if a bias of 1 was added to the forget gate of the LSTM or not . This has been shown to improve results [ 2 ] .
- Overall the whole pipeline feels very ad-hoc , with many hand - tuned parameters . Notwithstanding the network architecture , here I ’m referring to the window for the barometric pressure , the Jaccard distance threshold , the binary mask lengths , and the time window for selecting p0 .
- Are there plans to release the data and / or the code for the experiments ? Currently the results would be impossible to reproduce .
- The typo of accuracy given by the authors is somewhat worrying , given that the result is repeated several times in the paper .

Typographical Issues
- Page 1 : ” floor - level accuracy ” back ticks
- Page 4 : Figure 4.1 → Figure 1 ; Nawarathne et al Nawarathne et al . → Nawarathne et al .
- Page 6 : ” carpet to carpet ” back ticks
- Table 2 : What does - 4 + mean ?
- References . The references should have capitalisation where appropriate . For example , Iodetector → IODetector , wi-fi→Wi - Fi , apple → Apple , iphone→ iPhone , i→ I etc .

[ 1 ] Shalini Ghosh , Oriol Vinyals , Brian Strope , Scott Roy , Tom Dean , and LarryHeck . Contextual LSTM ( CLSTM ) models for large scale NLP tasks . arXivpreprint arXiv:1602.06291 , 2016 .
[ 2 ] Rafal Jozefowicz , Wojciech Zaremba , and Ilya Sutskever . An empirical exploration of recurrent network architectures . InProceedings of the 32nd International Conference on Machine Learning ( ICML - 15 ) , pages 2342–2350,2015

Can you explain why in the table 1 in the revision from 29th October the validation and test accuracy of the LSTM are 0.949 and 0.911 and in the most recent version they have dropped to 0.935 and 0.898 ( worse than the baselines ) ?

Also I agree with the statement by reviewer 2 :

" The RNN model for Indoor / Outdoor determination is compared to several baseline classifiers . However these are not the right methods to compare to -- at least , it is not clear how you set up the vector input to these non-auto- regressive classifiers . You need to compare your model to a time series method that includes auto-regressive terms , or other state space methods like Markov models or HMMs . "

It seems like no changes have been made to address this .


Thank you so much for your valuable feedback ! I want to preface the breakdown below by letting you know that we added time - distributed dropout which helped our model 's accuracy . The new accuracy is 100 % with no margin of error in the floor number .

1 . As of June 2017 the market share of phones in the US is 44.9 % Apple and 29.1 % Samsung [ 1 ]. 74 % are iPhone 6 or newer [ 2 ] . The iPhone 6 has a barometer [ 3 ] . Models after the 6 still continue to have a barometer .
For the Samsung phones , the Galaxy s5 is the most popular [ 4 ] , and has a barometer [ 5 ] .


[ 1 ] https://www.prnewswire.com/news-releases/comscore-reports-june-2017-us-smartphone-subscriber-market-share-300498296.html
[ 2 ] https://s3.amazonaws.com/open-source-william-falcon/911/2017_US_Cross_Platform_Future_in_Focus.pdf
[ 3 ] https://support.apple.com/kb/sp705?locale=en_US
[ 4 ] https://deviceatlas.com/blog/most-popular-smartphones-2016
[ 5 ] https://news.samsung.com/global/10-sensors-of-galaxy-s5-heart-rate-finger-scanner-and-more

2 . Makes sense , we separated it for the non deep learning audience trying to understand it . However , happy to update everything to say LSTM .
3 . Thanks for this great suggestion . We had experimented with end - to - end models but decided against it . We did have a seq2seq model that attempted to turn the sequence of readings into a sequence of meter offsets . It did not fully work , but we 're still experimenting with it . This model does not however get rid of the clustering step .

An additional benefit of separating this step from the rest of the model is that it can be used as a stand - alone indoor / outdoor classifier .

I 'll address your concerns one at a time :
a . In which task would it perform better ? The indoor-outdoor classification task or the floor prediction task ?
c . What about this model would solve the issue of the user being on the roof ?
d . Just to make sure I understand , the one - hot encoding suggestion aims to learn a mapping between the floor height and the barometric pressure which in turn removes the need for clustering ?
e . This sounds like an interesting approach , but seems to fall outside of the constraint of having a self - contained model which did not need prior knowledge . Generating a one - hot encoding for every building in the world without a central repository of building plans makes this intractable .

4 . We used the bias ( tensorflow LSTM cell ) . https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell
5 . Happy to add explanations for why the " ad-hoc " parameters were chosen :
a . Jaccard window , binary mask lengths , and window length were chosen via grid search .
b . Will add those details to the paper .

6 . Yes ! All the data + code will be made public after reviews . However , if you feel strongly about having it before , we can make it available sooner through an anonymous repository . In addition , we 're planning on releasing a basic iOS app which you 'll be able to download from the app store to run the model on your phone and see how it works on any arbitrary building for you .

7 . Yes , many typos . Apologize for that . We did a last minute typo review too close to the deadline and missed those issues . This is in fact going to change now that we 've increased the model accuracy to 100 % with no floor margin of error .

We 're updating the paper now and will submit a revised version in the coming weeks

The paper proposes a two -step method to determine which floor a mobile phone is on inside a tall building .
An LSTM RNN classifier analyzes the changes / fading in GPS signals to determine whether a user has entered a building . Using the entrance point 's barometer reading as a reference , the method calculates the relative floor the user has moved to using a well known relationship between heights and barometric readings .

The paper builds on a simple but useful idea and is able to develop it into a basic method for the goal . The method has minimal dependence on prior knowledge and is thus expected to have wide applicability , and is found to be sufficiently successful on data collected from a real world context . The authors present some additional explorations on the cases when the method may run into complications .

The paper could use some reorganization . The ideas are presented often out of order and are repeated in cycles , with some critical details that are needed to understand the method revealed only in the later cycles . Most importantly , it should be stated upfront that the outdoor- indoor transition is determined using the loss of GPS signals . Instead , the paper elaborates much on the neural net model but delays until the middle of p.4 to state this critical fact . However once this fact is stated , it is obvious that the neural net model is not the only solution .

The RNN model for Indoor / Outdoor determination is compared to several baseline classifiers . However these are not the right methods to compare to -- at least , it is not clear how you set up the vector input to these non-auto- regressive classifiers . You need to compare your model to a time series method that includes auto-regressive terms , or other state space methods like Markov models or HMMs .

Other questions :

p.2 , Which channel 's RSSI is the one included in the data sample per second ?

p.4 , k=3 , what is k?

Do you assume that the entrance is always at the lowest floor ? What about basements or higher floor entrances ? Also , you may continue to see good GPS signals in elevators that are mounted outside a building , and by the time they fade out , you can be on any floor reached by those elevators .

How does each choice of your training parameters affect the performance ? e.g. number of epoches , batch size , learning rate . What are the other architectures considered ? What did you learn about which architecture works and which does not ? Why ?

As soon as you start to use clustering to help in floor estimation , you are exploiting prior knowledge about previous visits to the building . This goes somewhat against the starting assumption and claim .

Thank you for your feedback ! We 're working on adding your suggestions and will post an update in the next few weeks .

Wanted to let you know we 've improved the results from 91 % to 100 % by adjusting our regularization mechanism in the LSTM . We 'll make the appropriate changes to the paper .

" The paper could use some reorganization "
1 . Agreed and the updated draft will have :
- Cleaner organization
- Upfront clarification about the GPS signal
- Shortened discussion about the neural net model

" The RNN model for Indoor / Outdoor determination is compared to several baseline classifiers . "
2 . The problem is reduced to classification by creating a fixed window of width k ( in our case , k=3 ) where the middle point is what we 're trying to classify as indoors / outdoors .
- Happy to add the HMM comparison .
- Happy to add a time series comparison .

" p.2 , Which channel 's RSSI is the one included in the data sample per second ?
"
3 . We get the RSSI strength as proxied by the iPhone status bar . Unfortunately , the API to access the details of that signal is private . Therefore , we do n't have that detailed information . However , happy to add clarification about how exactly we 're getting that signal ( also available in the sensory app code ) .


4 . k is the window size . Will clarify this .

" Do you assume that the entrance is always at the lowest floor ? What about basements or higher floor entrances ? "

5 . We actually do n't assume the entrance is on the lower floors . In fact , one of the buildings that we test in has entrances 4 stories appart . This is where the clustering method shines . As soon as the user enters the building through one of those lower entrances , the floor- level indexes will update because it will detect another cluster .


" Also , you may continue to see good GPS signals in elevators that are mounted outside a building , and by the time they fade out , you can be on any floor reached by those elevators . "
6 . Yup , this is true . Unfortunately this method does heavily rely on the indoor / outdoor classifier .
- We 'll add a brief discussion to highlight this issue .


" How does each choice of your training parameters affect the performance ? e.g. number of epoches , batch size , learning rate . What are the other architectures considered ? What did you learn about which architecture works and which does not ? Why ?
"
7 . We can add a more thorough description about this and provide training logs in the code that give visibility into the parameters for each experiment and the results .
- The window choice ( k ) actually might be the most critical hyperparameter ( next to learning rate ) . The general pattern is that a longer window did not help much .
- The fully connected network actually does surprisingly well but the RNN generalizes slightly better . A 1 - layer RNN did not provide much modeling power . It was the multi-layer model that added the needed complexity to capture these relationships . We also tried bi-directional but it failed to perform well .

" As soon as you start to use clustering to help in floor estimation , you are exploiting prior knowledge about previous visits to the building . This goes somewhat against the starting assumption and claim .
"
8 . Fair point . We provide a prior for each situation will will get you pretty close to the correct floor- level . However , it 's impossible to get more accurate without building plans , beacons or some sort of learning . We consider the clustering method more of the learning approach : It updates the estimated floor heights as either the same user or other users walk in that building . In the case where the implementer of the system ( ie. A company ) , only wants to use a single - user 's information and keep it 100 % on their device , the clustering system will still work using that user 's repeated visits . In the case where a central database might aggregate this data , the clusters for each building will develop a lot faster and converge on the true distribution of floor heights in a buillding .

Hello . We 've added the HMM baseline . We apologize for the delay , we wanted to make sure we set the HMM baseline as rigorous as possible .

The code is also available for your review .

Thank you once again for your feedback !


The authors motivate the problem of floor level estimation and tackle it with a RNN . The results are good . The models the authors compare to are well chosen . As the paper foremost provides application ( and combination ) of existing methods it would be benefitial to know something about the limitations of their approach and about the observed prequesits .

Thank you for your valuable feedback !
In Appendix A , section B we provide a lengthy discussion about potential pitfalls of our system in a real - world scenario and offer potential solutions .

Was there something in addition to this that you 'd like to see ?

Dear reviewer ,

We 've released a main update listed above . Please let us know if there 's anything we can help clarify !

Thank you once again for your feedback !

I have read authors ' reply . In response to authors ' comprehensive reply and feedback . I upgrade my score to 6 .

-----------------------------

This paper presents a novel approach to calibrate classifiers for out of distribution samples . In additional to the original cross entropy loss , the “ confidence loss ” was proposed to guarantee the out of distribution points have low confidence in the classifier . As out of distribution samples are hard to obtain , authors also propose to use GAN generating “ boundary ” samples as out of distribution samples .

The problem setting is new and objective ( 1 ) is interesting and reasonable . However , I am not very convinced that objective ( 3 ) will generate boundary samples . Suppose that theta is set appropriately so that p_theta ( y|x ) gives a uniform distribution over labels for out of distribution samples . Because of the construction of U( y ) , which uniformly assign labels to generated out of distribution samples , the conditional probability p_g ( y|x ) should always be uniform so p_g ( y|x ) divided by p_theta ( y|x ) is almost always 1 . The KL divergence in ( a ) of ( 3 ) should always be approximately 0 no matter what samples are generated .

I also have a few other concerns :
1 . There seems to be a related work :
[ 1 ] Perello - Nieto et al. , Background Check : A general technique to build more reliable and versatile classifiers , ICDM 2016 ,
Where authors constructed a classifier , which output K + 1 labels and the K+1 -th label is the “ background noise ” label for this classification problem . Is the method in [ 1 ] applicable to this paper ’s setting ? Moreover , [ 1 ] did not seem to generate any out of distribution samples .

2 . I am not so sure that how the actual out of distribution detection was done ( did I miss something here ? ) . Authors repeatedly mentioned “ maximum prediction values ” , but it was not defined throughout the paper .
Algorithm 1. is called “ minimization for detection and generating out of distribution ( samples ) ” , but this is only gradient descent , right ? I do not see a detection procedure . Given the title also contains “ detecting ” , I feel authors should write explicitly how the detection is done in the main body .


We very much appreciate your valuable comments , efforts and times on our paper . We provide our responses for all questions below . Revised parts in the new draft are colored by blue .

Q1 : " I am not very convinced that objective ( 3 ) will generate boundary samples . "

A1 : As you pointed out , the KL divergence term ( a ) of ( 3 ) is approximately 0 no matter how out - of-distribution samples are generated . However , if the samples are far away from " boundary " ( here , we assume the high - density area of in -distribution is a closed set ) , the GAN loss ( b ) of ( 3 ) should be high , i.e. , the GAN loss forces having samples being not too far from the in-distribution space . This is the primary reason why ( 3 ) will generate out - of-distribution samples from the low - density boundary of the in-distribution space . We provided more explanations in the revision ( please see updated Section 2.2 for details ) .

Q2 : " There seems to be a related work :
[ 1 ] Perello - Nieto et al. , Background Check : A general technique to build more reliable and versatile classifiers , ICDM 2016 , Where authors constructed a classifier , which output K + 1 labels and the K+1 - th label is the " background noise " label for this classification problem . Is the method in [ 1 ] applicable to this paper 's setting ? Moreover , [ 1 ] did not seem to generate any out of distribution samples . "

A2 : As you mentioned , the Background Check ( BC ) proposed by [ 1 ] can be applied to our setting , i.e. , one can consider background distribution in [ 1 ] as out -of-distribution . The authors propose two methods called discriminative approach ( BCD ) and familiarity approach ( BCF ) . First , BCD requires out -of-distribution samples , but they mentioned generating artificial background data is hard and did not try in their experiments . This is what we resolve in this paper . On the other hand , BCF does not require out -of-distribution samples , and instead it uses a density estimator of P_{in} ( x ) such as one - class support vector machine ( OCSVM ) for modeling in - distributions . Such an additional model not only increases the detection complexity , but also is not clear to perform well in the high -dimensional datasets used in our paper . For example , one can try complex density estimators such as PixelCNN [ 2 ] , but they are quite difficult to train and should be chosen depending on data characteristics ( this hurts the generality of our work ) . Our method is much simpler and easier to use . In addition , please note that the authors [ 1 ] did not report any neural network experiments at all .

Q3 : " Authors repeatedly mentioned " maximum prediction values " , but it was not defined throughout the paper . "

A3 : The " maximum prediction value " corresponds to a maximum value of the predictive distribution , i.e. , \max_y P ( y|x ) . We formally defined this in the revision ( Section 2.1 ) .

Q4 : " I am not so sure that how the actual out of distribution detection was done ( did I miss something here ? ) . Algorithm 1. is called " minimization for detection and generating out of distribution ( samples ) " , but this is only gradient descent , right ? I do not see a detection procedure . Given the title also contains " detecting " , I feel authors should write explicitly how the detection is done in the main body . "

A4 : Our goal is to develop a new training method ( Algorithm 1 is the training algorithm ) , which works with a simple detection method . For the actual detecting procedure , one can apply any known inference algorithms [ 3 , 4 , 5 ] on a trained model . Here , we remark that the performance of these detectors highly depends how the classifier is trained , e.g. , as shown in Table 1 and Figure 4 , the detection performance of prior inference algorithms can be dramatically improved under a confident classifier trained by our method . We explained the detection procedure more precisely in the revision , as mentioned in the beginning of Section 2 and more formally defined in the Appendix A . Thank you for your suggestion .

[ 1 ] Perello - Nieto et al. , Background Check : A general technique to build more reliable and versatile classifiers , ICDM 2016
[ 2 ] Oord , A.V.D. , Kalchbrenner , N. , Vinyals , O. , Espeholt , L. , Graves , A. and Kavukcuoglu , K. Conditional Image Generation with PixelCNN Decoders . In NIPS , 2016 .
[ 3 ] Shiyu Liang , Yixuan Li , and R Srikant . Principled detection of out -of-distribution examples in neural networks . arXiv preprint arXiv:1706.02690 , 2017 .
[ 4 ] Chuan Guo , Geoff Pleiss , Yu Sun , and Kilian Q Weinberger . On calibration of modern neural networks . In ICML , 2017 .
[ 5 ] Dan Hendrycks and Kevin Gimpel . A baseline for detecting misclassified and out -of-distribution examples in neural networks . In ICLR , 2017 .

Thanks ,
Authors .

The manuscript proposes a generative approach to detect which samples are within vs . out of the sample space of the training distribution . This distribution is used to adjust the classifier so it makes confident predictions within sample , and less confident predictions out of sample , where presumably it is prone to mistakes . Evaluation on several datasets suggests that accounting for the within-sample distribution in this way can often actually improve evaluation performance , and can help the model detect outliers .

The manuscript is reasonably well written overall , though some of the writing could be improved e.g. a clearer description of the cost function in section 2 . However , equation 4 and algorithm 1 were very helpful in clarifying the cost function . The manuscript also does a good job giving pointers to related prior work . The problem of interest is timely and important , and the provided solution seems reasonable and is well evaluated .

Looking at the cost function and the intuition , the difference in figure 1 seems to be primarily due to the relative number of samples used during optimization -- and not to anything inherent about the distribution as is claimed . In particular , if a proportional number of samples is generated for the 50x50 case , I would expect the plots to be similar . I suggest the authors modify the claim of figure 1 accordingly .

Along those lines , it would be interesting if instead of the uniform distribution , a model that explicitly models within vs . out of sample might perform better ? Though this is partially canceled out by the other terms in the optimization .

Finally , the authors claim that the PT is approximately equal to entropy . The cited reference ( Zhao et. al. 2017 ) does not justify the claim . I suggest the authors remove this claim or correctly justify it .

Questions :
- Could the authors comment on cases where such a strong within-sample assumption may adversely affect performance ?
- Could the authors comment on how the modifications affect prediction score calibration ?
- Could the authors comment on whether they think the proposed approach may be more resilient to adversarial attacks ?

Minor issues :
- Figure 1 is unclear using dots . Perhaps the authors can try plotting a smoothed decision boundary to clarify the idea ?

We very much appreciate your valuable comments , efforts and times on our paper . We provide our responses for all questions below . Revised parts in the new draft are colored by blue .

Q1 : " About the difference in figure 1 . "

A1 : First , we emphasize that we use the same number ( i.e. , 100 ) of training out -of-distribution samples for Figure 1 ( a ) / ( b ) and 1 ( c ) / ( d ) . As you pointed out , if one increases the number of training out -of-distribution samples for the 50x50 case , Figure 1 ( b ) is expected to be similar to Figure 1 ( d ) . In other words , one needs more samples in order to train confidence classifier if samples are generated from the entire space , i.e. , 50x50 . However , as we mentioned , this might be impossible and not efficient since the number of out - of-distribution training samples might be almost infinite to cover its entire , high - dimensional data space . Therefore , instead , we suggest to sample out - of-distribution close to in - distribution , which could be more effective ( given the fixed sampling complexity ) . The difference in Figure 1 confirms such intuition . We clarified this more in the revision ( Section 2.1 ) .

Q2 : " Justification of PT . "

A2 : We agree with you that Zhao et al. did not justify the claim . But , the PT corresponds to the squared cosine similarity of generated samples , and intuitively one can expect the effect of increasing the entropy by minimizing it . In the recent work [ 1 ] , the authors also used PT to maximize the entropy . However , as we mentioned in A3 for Reviewer_1 , after our submission , we actually verified that PT helps , but its gains are relatively marginal in overall . Since PT increases the training complexity , we decided to remove the PT in the revision and have updated all experimental results without using PT . Finally , for interested readers , we also report the effects of PT in the Appendix D . We really appreciate your valuable comments . We updated Section 2.2 and 2.3 . Figure 3 , 4 and 5 . Appendix D. accordingly .

Q3 : " About cases where such a strong within-sample assumption may adversely affect performance . "

A3 : As shown in Table 1 and Table 2 ( in Appendix C ) , splitting in - and out -of-distributions and optimizing the confidence loss ( 1 ) does not adversely affect the classification accuracy due to the high expressive power of deep neural networks in all our experiments . We have n't found a case where our proposed method ( based on this assumption ) leads to adverse performance . However , more theoretical investigation on whether this assumption guarantees a good performance or whether there is a counterexample would be an interesting future work .

Q4 : " How do the modifications affect prediction score calibration ? "

A4 : Thank you for your great suggestion . After our submission , we actually verified that our method can improve the prediction score calibration . For example , the expected calibration error ( ECE ) [ 2 ] of a classifier trained by our method is lower than that of a classifier trained by the standard cross entropy loss . For interested readers , we reported the corresponding experimental results in the revision ( see Appendix C.2 ) .

Q5 : " Whether the proposed approach may be more resilient to adversarial attacks . "

A5 : This is a very interesting question . We believe our method has some potential for being more resilient to adversarial attacks . This is because adversarial examples are special types of out -of-distribution samples . We believe that this should be an interesting future direction to explore .

[ 1 ] Shiyu Liang , Yixuan Li , and R Srikant . Principled detection of out -of-distribution examples in neural networks . arXiv preprint arXiv:1706.02690 , 2017 . ( https://arxiv.org/abs/1706.02690)
[ 2 ] Chuan Guo , Geoff Pleiss , Yu Sun , and Kilian Q Weinberger . On calibration of modern neural networks . In ICML , 2017 . ( https://arxiv.org/abs/1706.04599)

Thanks ,
Authors .

This paper proposes a new method of detecting in vs . out of distribution samples . Most existing approaches for this deal with detecting out of distributions at * test time * by augmenting input data and or temperature scaling the softmax and applying a simple classification rule based on the output . This paper proposes a different approach ( with could be combined with these methods ) based on a new training procedure .

The authors propose to train a generator network in combination with the classifier and an adversarial discriminator . The generator is trained to produce images that ( 1 ) fools a standard GAN discriminator and ( 2 ) has high entropy ( as enforced with the pull - away term from the EBGAN ) . Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples .

The model is evaluated on CIFAR - 10 and SVNH , where several out of distribution datasets are used in each case . Performance gains are clear with respect to the baseline methods .

This paper is clearly written , proposes a simple model and seems to outperform current methods . One thing missing is a discussion of how this approach is related to semi-supervised learning approaches using GANS where a generative model produces extra data points for the classifier / discriminator .

I have some clarifying questions below :
- Figure 4 is unclear : does " Confidence loss with original GAN " refer to the method where the classifier is pretrained and then " Joint confidence loss " is with joint training ? What does " Confidence loss ( KL on SVHN / CIFAR - 10 ) " refer to ?

- Why does the join training improve the ability of the model to generalize to out - of-distribution datasets not seen during training ?

- Why is the pull away term necessary and how does the model perform without it ? Most GAN models are able to stably train without such explicit terms such as the pull away or batch discrimination . Is the proposed model unstable without the pull - away term ?

- How does this compare with a method whereby instead of pushing the fake sample 's softmax distribution to be uniform , the model is simply a trained to classify them as an additional " out of distribution " class ? This exact approach has been used to do semi supervised learning with GANS [ 1 ] [ 2 ] . More generally , could the authors comment on how this approach is related to these semi-supervised approaches ?

- Did you try combining the classifier and discriminator into one model as in [ 1 ] [ 2 ] ?

[ 1 ] Semi-Supervised Learning with Generative Adversarial Networks ( https://arxiv.org/abs/1606.01583)
[ 2 ] Good Semi-supervised Learning that Requires a Bad GAN ( https://arxiv.org/abs/1705.09783)

We very much appreciate your valuable comments , efforts and times on our paper . We provide our responses for all questions below . Revised parts in the new draft are colored by blue .

Q1 : " Figure 4 is unclear . "

A1 : First , " Confidence loss with original GAN " corresponds to a variant of confidence loss ( 1 ) which trains a classifier by optimizing the KL divergence term using samples from a pre-trained original / standard GAN , i.e. , GAN generates in -distribution samples . Next , " Joint confidence loss " is the proposed loss ( 4 ) optimized by Algorithm 1 . Here , we remark that only " Joint confidence loss " optimizes the KL divergence terms using implicit samples from the proposed GAN , i.e. , GAN generates " boundary " samples in the low - density area of in -distribution . Finally , " Confidence loss ( KL on SVHN / CIFAR - 10 ) " corresponds to the confidence loss ( 1 ) using explicit out -of-distribution samples ( SVHN or CIFAR - 10 ) . For example , " Confidence loss ( KL on SVHN ) " refers to the method where the KL divergence term in the confidence loss ( 1 ) is optimized using SVHN training data . In the revision , we clarified the notations such that the KL divergence term is optimized on samples indicated in the parentheses , i.e. , " Confidence loss with original GAN " and " Confidence loss ( KL on SVHN / CIFAR - 10 ) " were revised to " Confidence loss ( samples from original GAN ) " and " Confidence loss ( SVHN / CIFAR - 10 ) " , respectively . We updated Figure 2 and Figure 4 accordingly .

Q2 : " Why does the joint training improve the ability of the model to generalize to out - of-distribution datasets not seen during training ? "

A2 : It is explained in Section 2.3 . In Section 2.1 , we suggest to use out - of-distribution samples for training a confident classifier . Conversely , in Section 2.2. , we suggest to use a confident classifier for training a GAN generating out - of-distribution samples . Namely , two models can be used for improving each other . Hence , this naturally suggests a joint training scheme in Section 2.3 for confident classifier and the proposed GAN , where both improve as the training proceeds . We emphasize the effect of joint training again in the revision . Please see our revision of Section 2.3 for details .

Q3 : " Why is the pull away term necessary and how does the model perform without it ? "

A3 : We really appreciate your valuable comments .

The pull away term ( PT ) is not related to " stability . " Our intuition was that the entropy of out - of-distribution is expected to be much higher compared to that of in - distribution since the out -of-distribution is typically on a much larger space than the in-distribution . Consequently , we expected that optimizing the PT term is useful for generating better out - of-distribution samples .
We also note that the PT was recently used [ 2 ] for a similar purpose as ours .

However , since we suggest to generate out - of-distribution samples nearby in -distribution ( for efficient sampling purpose ) , its entropy might be not that high and the effect of PT is not clear . After our submission , we actually verified that PT sometimes helps ( but not always ) , and its gains are relatively marginal in overall . Since PT increases the training complexity , we decided to remove the PT in the revision and have updated all experimental results without using PT . Still , for interested readers , we also report the effects of PT in the Appendix D . We updated Section 2.2 and 2.3 , Figure 3 , 4 and 5 , and Appendix D , accordingly .

Q4 : " How is this approach related to the semi-supervised approaches in [ 1 ] [ 2 ] ? Did you try combining the classifier and discriminator into one model as in [ 1 ] [ 2 ] ? "

A4 : As briefly mentioned in Section 4 , we expect that our proposed GAN might be useful for semi-supervised settings . Also , we actually thought about combining the classifier and discriminator into one model , i.e. , adding K + 1 class . However , we choose a more " conservative " way to design network architectures so that the original classification performance does not degrade . Extension to semi-supervised learning should be an interesting future direction to explore .

[ 1 ] Odena , A. Semi-supervised learning with generative adversarial networks . In NIPS , 2016 . ( https://arxiv.org/abs/1606.01583)
[ 2 ] Dai , Z. , Yang , Z. , Yang , F. , Cohen , W.W. and Salakhutdinov , R. Good Semi-supervised Learning that Requires a Bad GAN . In NIPS , 2017 . ( https://arxiv.org/abs/1705.09783)

Thanks ,
Authors

This paper proposes active question answering via a reinforcement learning approach that can learn to rephrase the original questions in a way that can provide the best possible answers . Evaluation on the SearchQA dataset shows significant improvement over the state - of - the- art model that uses the original questions .

In general , the paper is well - written ( although there are a lot of typos and grammatical errors that need to be corrected ) , and the main ideas are clear . It would have been useful to provide some more details and carry out additional experiments to strengthen the merit of the proposed model .

Especially , in Section 4.2 , more details about the quality of paraphrasing after training with the multilingual , monolingual , and refined models would be helpful . Which evaluation metrics were used to evaluate the quality ? Also , more monolingual experiments could have been conducted with state - of - the- art neural paraphrasing models on WikiQA and Quora datasets ( e.g. see https://arxiv.org/pdf/1610.03098.pdf and https://arxiv.org/pdf/1709.05074.pdf).

More details with examples should be provided about the variants of AQA along with the oracle model . Especially , step -by-step examples ( for all alternative models ) from input ( original question ) to question reformulations to output ( answer / candidate answers ) would be useful to understand how each module / variation is having an impact towards the best possible answer / ground truth .

Although experiments on SearchQA demonstrate good results , I think it would be also interesting to see the results on additional datasets e.g. MS MARCO ( Nguyen et al. , 2016 ) , which is very similar to the SearchQA dataset , in order to confirm the generalizability of the proposed approach .

---------------------------------------------
Thanks for revising the paper , I am happy to update my scores .

Thanks for your review and suggestions ! We address each point below :

Other datasets
We agree that it will be important to extend the empirical evaluation on new datasets . Our current experimental setup cannot be straightforwardly applied to MsMarco , unfortunately . Our environment ( the BiDAF QA system ) is an extractive QA system . However , MsMarco contains many answers ( 55 % ) that are not substrings of the context ; even after text normalization , 36 % are missing . We plan to investigate the use of generative answer models for the environment with which we could extend AQA to this data .

Paraphrasing quality
Regarding stand - alone evaluation of the paraphrasing quality of our models , we ran several additional experiments inspired by the suggested work .
We focused on the relation between paraphrasing quality and QA quality . To tease apart the relationship between paraphrasing and reformulation for QA we evaluated 3 variants of the reformulator :

Base - NMT : this is the model used to initialize RL training of the agent . Trained first on the multilingual U.N. corpus , then on the Paralex corpus .
Base-NMT -NoParalex : is the model above trained solely on the multilingual U.N. corpus , without the Paralex monolingual corpus .
Base-NMT + Quora : is the same as Base - NMT , additionally trained on the Quora duplicate question dataset .

Following Prakash et al. ( 2016 ) we evaluated all models on MSCOCO , selecting one out of five captions at random from Val2014 and using the other 4 as references . We use beam search , as in the paper , to compute the top hypothesis and report uncased , moses - tokenized BLEU using John Clark 's multeval . [ github.com/jhclark/multeval ]
The Base- NMT model performs at 11.4 BLEU ( see Table 1 for the QA eval numbers ) . Base-NMT -NoParalex performs poorly at 5.0 BLEU . Limiting training to the multilingual data alone also degrades QA performance : the scores of the Top Hypothesis are at least 5 points lower in all metrics , CNN scores are 2 - 3 points lower for all metrics .
By training on additional monolingual data , the Base- NMT + Quora model BLEU score improves marginally to 11.6 . End-to -end QA performance also improves marginally , the maximum delta with respect to Base - NMT under all conditions is + 0.5 points , but the difference is not statistically significant . Thus , adding the Quora training does not have a significant effect . This might be due to the fact that most of the improvement is captured by training on the larger Paralex data set .

Improving raw paraphrasing quality as well as reformulation fluency help AQA up to a point . However , they are only partially aligned with the main task , which is QA performance . The AQA - QR reformulator has a BLEU score of 8.6 , well below both Base - NMT models trained on monolingual data . AQA - QR significantly outperforms all others in the QA task . Training the agent starting from the Base- NMT + Quora model yielded identical results as starting from Base-NMT .

Examples
We have updated Appendix A in the paper with the answers corresponding to all queries , together with their F1 scores . We also added a few examples ( Appendix B ) where the agent is not able to identify the correct candidate reformulation , even if present in the candidate set . We also added an appendix ( C ) with example paraphrases from MSCOCO from the different models .

Presentation
We spelled and grammar checked the manuscript .


This paper formulates the Jeopardy QA as a query reformulation task that leverages a search engine . In particular , a user will try a sequence of alternative queries based on the original question in order to find the answer . The RL formulation essentially tries to mimic this process . Although this is an interesting formulation , as promoted by some recent work , this paper does not provide compelling reasons why it 's a good formulation . The lack of serious comparisons to baseline methods makes it hard to judge the value of this work .

Detailed comments / questions :
1 . I am actually quite confused on why it 's a good RL setting . For a human user , having a series of queries to search for the right answer is a natural process , but it 's not natural for a computer program . For instance , each query can be viewed as different formulation of the same question and can be issued concurrently . Although formulated as an RL problem , it is not clear to me whether the search result after each episode has been used as the immediate environment feedback . As a result , the dependency between actions seems rather weak .
2 . I also feel that the comparisons to other baselines ( not just the variation of the proposed system ) are not entirely fair . For instance , the baseline BiDAF model has only one shot , namely using the original question as query . In this case , AQA should be allowed to use the same budget -- only one query . Another more realistic baseline is to follow the existing work on query formulation in the IR community . For example , 20 shorter queries generated by methods like [ 1 ] can be used to compare the queries created by AQA .

[ 1 ] Kumaran & Carvalho . " Reducing Long Queries Using Query Quality Predictors " . SIGIR - 09

Pros :
1 . An interesting RL formulation for query reformulation

Cons :
1 . The use of RL is not properly justified
2 . The empirical result is not convincing that the proposed method is indeed advantageous

---------------------------------------

After reading the author response and checking the revised paper , I 'm both delighted and surprised that the authors improved the submission substantially and presented stronger results . I believe the updated version has reached the bar and recommend accepting this paper .

Thanks for your review , questions , and suggestions which we address below :

1 - RL formulation
We require RL ( policy gradient ) because ( a ) the reward function is non-differentiable , and ( b ) we are optimizing against a black box environment using only queries , i.e. no supervised query transformation data ( query to query that works better for a particular QA system ) is available .
Without RL we could not optimize these reformulations against the black - box environment to maximize expected answer quality ( F1 score ) .

Regarding the training process you are correct : in this work , the reformulations of the initial query are indeed issued concurrently , as shown in Figure 1 . We note this when we introduce the agent in the first paragraph of Section 2 ; we say “ The agent then generates a * set* of reformulations {q_i} ” rather than a sequence .

In the last line of the conclusion , we comment that we plan to extend AQA to sequential reformulations which would then depend on the previous questions / answers also .

2 - Baseline comparisons
We computed an IR baseline following [ Kumaran & Carvalho , 2009 ] as suggested . We implemented the candidate generation method ( Section 4.3 ) of their system to generate subquery reformulations of the original query . We choose the reformulations from the term - level subsequences of length 3 to 6 . We associate each reformulation with a graph , where the vertices are the terms and the edges are the mutual information between terms . We rank the reformulations by the average edge weights of the Maximum Spanning Trees of the corresponding graphs . We keep the top 20 reformulations , the same number as we keep for the AQA agent . Then , we train a CNN to score these reformulations to identify those with above - average F1 , in exactly the same way we do for the AQA agent . As suggested , we then compare this method both in terms of choosing the single top hypothesis ( 1 shot ) , and ensemble prediction ( choose from 20 queries ) .
We additionally compare AQA to the Base - NMT system in the same way . This is the pre-trained monolingual seq2seq model used to initialize the RL training . We evaluate the Base - NMT model 's top hypothesis ( 1 shot ) and in ensemble mode .

We find that the AQA agent outperforms all other methods both in 1 - shot prediction ( top hypothesis ) and using CNNs to pick a hypothesis from 20 . To verify that the difference in performance is statistically significant we ran a statistical test . The null hypothesis is always rejected ( p < 0.00001 ) .
All results are summarized and discussed in the paper .

PS - After reviewing the suggested paper , and related IR literature we took the opportunity to add an IR query quality metric , QueryClarity , to our qualitative analysis at the end of the paper , in the box plot . Query Clarity contributes to our conclusion . showing that the AQA agent learns to transform the initial reformulations ( Base - NMT ) into ones that have higher QueryClarity , in addition to having better tf -idf and worse fluency .

We have summarized the comparison of the AQA agent in different modes versus the baselines in Table 1 .

This article clearly describes how they designed and actively trained 2 models for question reformulation and answer selection during question answering episodes . The reformulation component is trained using a policy gradient over a sequence - to- sequence model ( original vs. reformulated questions ) . The model is first pre-trained using a bidirectional LSTM on multilingual pairs of sentences . A small monolingual bitext corpus is the uses to improve the quality of the results . A CNN binary classifier performs answer selection .

The paper is well written and the approach is well described . I was first skeptical by the use of this technique but as the authors mention in their paper , it seems that the sequence - to- sequence translation model generate sequence of words that enables the black box environment to find meaningful answers , even though the questions are not semantically correct . Experimental clearly indicates that training both selection and reformulation components with the proposed active scheme clearly improves the performance of the Q&A system .

We thank Reviewer 1 for the encouraging feedback !

Thanks for addressing most of the issues . I changed my given score from 3 to 6 .

Summary :
This work explores the use of learned compressed image representation for solving 2 computer vision tasks without employing a decoding step .

The paper claims to be more computationally and memory efficient compared to the use of original or the decompressed images . Results are presented on 2 datasets " Imagenet " and " PASCAL VOC 2012 " . They also jointly train the compression and classification together and empirically shows it can improve both classification and compression together .

Pros :
+ The idea of learning from a compressed representation is a very interesting and beneficial idea for large - scale image understanding tasks .

Cons :
- The paper is too long ( 13 pages + 2 pages of references ) . The suggested standard number of pages is 8 pages + 1 page of references . There are many parts that are unnecessary in the paper and can be summarized . Summarizing and rewording them makes the paper more consistent and easier to read :
( 1 . A very long introduction about the benefits of inferring from the compressed images and examples .
2 . A large part of the intro and Related work can get merged .
3 . Experimental setup part is long but not well - explained and is not self - contained particularly for the evaluation metrics .
“ Please briefly explain what MS- SSIM , SSIM , and PSNR stand for ” . There is a reference to the Agustsson et al 2017 paper
“ scalar quantization ” , which is not well explained in the paper . It is better to remove this part if it is not an important part or just briefly but clearly explain it .
4 . Fig. 4 is not necessary . 4.3 contains extra information and could be summarized in a more consistent way .
5 . Hyperparameters that are applied can be summarized in a small table or just explain the difference between the
architectures that are used . )

- There are parts of the papers which are confusing or not well - written . It is better to keep the sentences short and consistent :
E.g : subsection 3.2 , page 5 : “ To adapt the ResNet … where k is the number of … layers of the network ” can be changed to 3 shorter sentences , which is easier to follow .
There are some typos : e.g : part 3.1 , fever ---> fewer ,

- As it is mentioned in the paper , solving a Vision problem directly from a compressed image , is not a novel method ( e.g : DCT coefficients were used for both vision and audio data to solve a task without any decompression ) . However , applying a deep representation for the compression and then directly solving a vision task ( classification and segmentation ) can be considered as a novel idea .

- In the last part of the paper , both compression and classification parts are jointly trained , and it is empirically presented that both results improved by jointly training them . However , to me , it is not clear if the trained compression model on this specific dataset and for the task of classification can work well for other datasets or other tasks .
The experimental setup and the figures are not well explained and well written .



Thank you for your your review , we have considered your comments in the revised version of the paper . Given the improved paper and positive perspective of the other reviews , we hope you reconsider your rating .

For specific points :

Regarding paper length : since this is a study paper , we felt it benefited from verbosity . However , we have managed to shorten the paper to 9.5 pages , while keeping the original story intact . We followed most of your suggestions : ( 1 - 2 ) We shortened the introduction and related work ; ( 4 ) We made Section 4.3 ( now Section 4.4 ) much more concise and moved Figure 4 to the appendix as it did not contain core results of our work . ( 3 ) We added a better description of the compression metrics to the experiments section . However , we also moved the compression results to the appendix , and added a more detailed explanation of the metrics there . ( 5 ) We also fixed wording in the paper as you suggested and moved hyperparameter settings and details to the appendix , as we felt these details distract from the main message of the paper . In addition to this , we refined presentation of joint training results using plots rather then presenting them in text .

As we mention in the paper , learning from DCT ( of JPEG ) has been done before . However , our setting of using features from learned compression networks is significantly different . The DCT of JPEG is simply a linear transform over 8 x8 patches , whereas the compressed representation is a feature map from a deep convolutional neural network . This opens directions such as joint learning of compression and inference ( see Section 6 ) and warrants a full study of the problem .

To show that the improvement of joint training generalizes to another task , we added an experiment : We take the ( jointly trained ) classification network and finetune it for segmentation . The results are shown in Figure 7 , where the resulting network significantly outperforms the separately trained network - achieving a significant performance boost of 1.1- 1.8 % higher mIoU depending on the compression operating point . See Figure 7 and discussion in Section 6.2 of the revised paper for more details .
We emphasize that this generalization is also occurring across datasets , from ILSVRC2012 ( classification ) to PASCAL VOC ( segmentation ) .

Finally , we made an effort to better clarify and describe the experiments .

Neural - net based image compression is a field which is about to get hot , and this paper asks the obvious question : can we design a neural - net based image compression algorithm such that the features it produces are useful for classification & segmentation ?

The fact that it 's an obvious question does not mean that it 's a question that 's worthless . In fact , I am glad someone asked this question and tried to answer it .

Pros :
- Clear presentation , easy to follow .
- Very interesting , but obvious , question is explored .
- The paper is very clear , and uses building blocks which have been analyzed before , which leaves the authors free to explore their interactions rather than each individual building block 's property .
- Results are shown on two tasks ( classification / segmentation ) rather than just one ( the obvious one would have been to only discuss results on classification ) , and relatively intuitive results are shown ( i.e. , more bits = better performance ) . What is perhaps not obvious is how much impact does doubling the bandwidth have ( i.e. , initially it means more , then later on it plateaus , but much earlier than expected ) .
- Joint training of compression + other tasks . As far as I know this is the first paper to talk about this particular scenario .
- I like the fact that classical codecs were not completely discarded ( there 's a comparison with JPEG 2K ) .
- The discussion section is of particular interest , discussing openly the pros / cons of the method ( I wish more papers would be as straightforward as this one ) .

Cons :
- I would have liked to have a discussion on the effect of the encoder network . Only one architecture / variant was used .
- For PSNR , SSIM and MS-SSIM I would like a bit more clarity whether these were done channel - wise , or on the grayscale channel .
- While runtime is given as pro , it would be nice for those not familiar with the methods to provide some runtime numbers ( i.e. , breakdown how much time does it take to encode and how much time does it take to classify or segment , but in seconds , not flops ) . For example , Figure 6 could be augmented with actual runtime in seconds .
- I wish the authors did a ctrl + F for " ?? " and fixed all the occurrences .
- One of the things that would be cool to add later on but I wished to have beeyn covered is whether it 's possible to learn not only to compress , but also downscale . In particular , the input to ResNet et al for classification is fixed sized , so the question is -- would it be possible to produced a compact representation to be used for classification given arbitrary image resolutions , and if yes , would it have any benefit ?

General comments :
- The classification bits are all open source , which is very good . However , there are very few neural net compression methods which are open sourced . Would you be inclined to open source the code for your implementation ? It would be a great service to the community if yes ( and I realize that it could already be open sourced -- feel free to not answer if it may lead to break anonymity , but please take this into consideration ) .


Thank you for your positive feedback . As for your comments , we have addressed them as follows :
1 ) The reason for not exploring more encoders is twofold . First , few of the state - of - the- art compression variants of neural networks are open-source which makes the implementation of different architectures very time consuming and difficult . We picked this particular encoder since it has been used in at least 2 different works , Theis et al. and Agustsson et al . Second , the autoencoder compression methods also all have a similar structure and thus one could expect the performance to be similar for different encoders . This is nevertheless an interesting direction of research worth pursuing .
2 ) These were done channel - wise ( RGB ) .
3 ) We have not yet had time to do this as the code for segmentation , classification and compression networks have different levels of optimization ( e.g. , NCHW vs NHWC tensor layout ) , so doing a fair comparison is time consuming and involves careful engineering . We however can do this before camera ready , if the paper is accepted . We also note that the architectures used for compression and inference are very similar , convolutional networks with residual blocks , and therefore FLOPs should be a good proxy metric .
4 ) This has been fixed in the revised edition of the paper
5 ) We agree that it would be interesting to learn the downscaling as well , after all it is just an anti-aliasing kernel ( i.e. a convolution with a fixed kernel ) followed by a subsampling operation and can be learned as well . However , processing the images in full - resolution during training would quite significantly increase training times , which are already pushing our limits in terms of compute resources . Another challenge is that hyperparameters of the ResNet architecture would likely need to be re-tuned . We chose to adhere to the standard setting of the classification literature , so that we could use the hyperparameter settings and training schedules which have already been optimized extensively .

This is a well - written and quite clear work about how a previous work on image compression using deep neural networks can be extended to train representations which are also valid for semantic understanding . IN particular , the authors tackle the classic and well - known problems of image classification and segmentation .

The work evolves around defining a loss function which initially considers only a trade - off between reconstruction error and total bit-rate . The representations trained with the loss function , at three different operational points , are used as inputs for variations of ResNet ( image classification ) and DeepLab ( segmentation ) . The results obtained are similar to a ResNet trained directly over the RGB images , and actually with a slight increase of performance in segmentation . The most interesting part is a joint training for both compression and image classification .

PROS
P.1 Joint training for both compression and classification . First time to the authors knowledge .
P.2 Performance on classification and segmentation tasks are very similar when compared to the non-compressed case with state - of - the-art ResNet architectures .
P.3 Text is very clear .
P.4 Experimentation is exhaustive and well - reported .

CONS
C1 . The authors fail into providing a better background regarding the metrics MS- SSIM and SSIM ( and PSNR , as well ) and their relation to the MSE used for training the network . Also , I missed an explanation about whether high or low values for them are beneficial , as actually results compared to JPEG and JPEG - 2000 differ depending on the experiment .
C2 . The main problem is of the work is that , while the whole argument is that in an indexing system it would not be necessary to decompress the representation coded with a DNN , in terms of computation JPEG2000 ( and probably JPEG ) are much lighter that coding with DNN , even if considering both the compression and decompression . The authors already point at another work where they explore the efficient compression with GPUs , but this point is the weakest one for the adoption of the proposed scheme .
C3 . The paper exceeds the recommendation of 8 pages and expands up to 13 pages , plus references . An effort of compression would be advisable , moving some of the non-core results to the appendixes .

QUESTIONS
Q1 . Do you have any explanation for the big jumps on the plots of Figure 5 ?
Q2 . Did you try a joint training for the segmentation task as well ?
Q3 . Why are the dots connected in Figure 10 , but not in Figure 11 .
Q4 . Actually results in Figure 10 do not seem good ... or maybe I am not understanding them properly . This is related to C1 .
Q5 . There is a broken reference in Section 5.3 . Please fix .

Thank you for your review . We considered your comments as follows .
C1 We have added definitions and clarification on the metrics to the paper . For all of them high means good .
C2 We gave an indexing system as an example application , but the main goal of the paper is to study the interplay between learned compression and inference in general . As mentioned in the paper , classical compression can be much faster than learned one - and our computational gains are relative to a system using learned compression . While falling back to classical codecs would be cheaper in terms of compute ( since classical encoder + decoder is more efficient than the learned encoder ) , the story is not so simple , since this would come at the expense of transmitting more data for a given target image quality . This can be crucial , since for mobile devices , data transmission ( I /O ) is responsible for most energy usage in common applications ( Pathak et. al , 2012 ) . Since learned compression is still at its infancy , we expect the gap in terms of compression performance between learned methods and classical ones to grow . Furthermore , with the increasing availability of dedicated neural network processing units on devices , deep image compression methods could become as fast as traditional ones .
C3 We have condensed the paper to remove redundant text and also moved some non- core results to the appendix .

Q1 . Yes , the standard learning rate schedule for training the ResNet classification architectures is a constant learning rate divided by factor 10 at fixed points in the training ( every 8 epochs for our setting ) . At the point when the learning rate is lowered the validation accuracy increases rapidly , and our validation curves show these jumps clearly .
The jumps / difference between operating points is due to more detail being present in images at higher bitrate ( higher bpp ) and therefore doing inference on them is easier .
Q2 . Yes we also did experiments for joint training with segmentation that are detailed in the revised version of the paper . In short , we do not train jointly on the segmentation task but we take the jointly trained classification network ( that improves classification ) and use that as a starting point for segmentation , showing significant improvement for segmentation . These results are shown in Figure 7 .
Q3 . We have made the style of the plots consistent , connecting the dots for both .
Q4 . Figure 10 ( also Figure 10 in the revised edition ) shows how the compression metrics change when training jointly compared to training only the compression network . It can be seen that training jointly improves the perceptual metrics MS - SSIM and SSIM slightly while PSNR gets slightly worse ( higher is better for all metrics ) . Figure 10 ’s main point is that the image compression metrics do not get worse in two out of three metrics as we do joint training . At the same time , Figure 7 shows that the inference performance ( both segmentation and classification ) significantly improves . See Section 6.2 for a thorough discussion in the revised edition . As this is not a core result it was moved to the appendix .
Q5 . We have fixed this in the revised edition of the paper .

( Pathak , A. , Hu , Y. C. , & Zhang , M. ( 2012 , April ) . Where is the energy spent inside my app ? : fine grained energy accounting on smartphones with eprof . In Proceedings of the 7th ACM european conference on Computer Systems ( pp. 29 - 42 ) . ACM . )

The paper offers a formal proof that gradient descent on the logistic
loss converges very slowly to the hard SVM solution in the case where
the data are linearly separable . This result should be viewed in the
context of recent attempts at trying to understand the generalization
ability of neural networks , which have turned to trying to understand
the implicit regularization bias that comes from the choice of
optimizer . Since we do not even understand the regularization bias of
optimizers for the simpler case of linear models , I consider the paper 's
topic very interesting and timely .

The overall discussion of the paper is well written , but on a more
detailed level the paper gives an unpolished impression , and has many
technical issues . Although I suspect that most ( or even all ) of these
issues can be resolved , they interfere with checking the correctness of
the results . Unfortunately , in its current state I therefore do not
consider the paper ready for publication .


Technical Issues :

The statement of Lemma 5 has a trivial part and for the other part the
proof is incorrect : Let x_u = ||nabla L( w ( u ) ) ||^2.
- Then the statement sum_{u=0 }^t x_u < infinity is trivial , because
it follows directly from ||nabla L ( w ( u ) ) | |^2 < infinity for all u . I
would expect the intended statement to be sum_{u=0 }^infinity x_u <
infinity , which actually follows from the proof of the lemma .
- The proof of the claim that t*x_t -> 0 is incorrect : sum_{u=0 }^t x_u
< infinity does not in itself imply that t*x_t -> 0 , as claimed . For
instance , we might have x_t = 1 / i^2 when t=2 ^i for i = 1 ,2 , ... and
x_t = 0 for all other t.

Definition of tilde{w} in Theorem 4 :
- Why would tilde{w} be unique ? In particular , if the support vectors
do not span the space , because all data lie in the same
lower-dimensional hyperplane , then this is not the case .
- The KKT conditions do not rule out the case that \hat{w}^top x_n =
1 , but alpha_n = 0 ( i.e. a support vector that touches the margin ,
but does not exert force against it ) . Such n are then included in
cal { S} , but lead to problems in ( 2.7 ) , because they would require
tilde{w}^top x_n = infinity , which is not possible .

In the proof of Lemma 6 , case 2 . at the bottom of p.14 :
- After the first inequality , C_0^2 t^{ - 1.5 epsilon_ + } should be
C_0^2 t^{-epsilon_+}
- After the second inequality the part between brackets is missing an
additional term C_0^2 t^{-\epsilon_+}.
- In addition , the label ( 1 ) should be on the previous inequality and
it should be mentioned that e^{ - x} <= 1 - x + x^2 is applied for x >= 0
( otherwise it might be false ) .
In the proof of Lemma 6 , case 2 in the middle of p.15 :
- In the line of inequality ( 1 ) there is a t^{ -epsilon_ - } missing . In
the next line there is a factor t^{-epsilon_ - } too much .
- In addition , the inequality e^x >= 1 + x holds for all x , so no need
to mention that x > 0.

In Lemma 1 :
- claim ( 3 ) should be lim_{t \to \infty } w( t ) ^\top x_n = infinity
- In the proof : w( t ) ^top x_n > 0 only holds for large enough t.

Remarks :

p.4 The claim that " we can expect the population ( or test )
misclassification error of w( t ) to improve " because " the margin of w( t )
keeps improving " is worded a little too strongly , because it presumes
that the maximum margin solution will always have the best
generalization error .

In the proof sketch ( p.3 ) :
- Why does the fact that the limit is dominated by gradients that are
a linear combination of support vectors imply that w_infinity will
also be a non-negative linear combination of support vectors ?
- " converges to some limit " . Mention that you call this limit
w_infinity


Minor Issues :

In ( 2.4 ) : add " for all n " .

p.10 , footnote : Should n't " P_1 = X_s X_s^ + " be something like " P_1 =
( X_s^top X_s ) ^ + " ?

A.9 : ell should be ell '

The paper needs a round of copy editing . For instance :
- top of p.4 : " where tilde{w } A is the unique "
- p.10 : " the solution tilde{w} to TO eq . A.2 "
- p.10 : " might BOT be unique "
- p.10 : " penrose - moorse pseudo inverse " -> " Moore -Penrose
pseudoinverse "

In the bibliography , Kingma and Ba is cited twice , with different years .


We thank the reviewer for acknowledging the significance of our results , and for investing significant efforts in improving the quality of this manuscript . We uploaded a revised version in which all the reviewer comments were addressed , and the appendix was further polished . Notably ,

[ Lemma 5 in appdendix ]

- Indeed , the upper limit of the sum over x_u should be ' infinity ' instead of 't ' .

- It should be ' x_t -> 0 ' , not ' t * x_t -> 0'.

[ Definition of tilde{w} Theorem 4 ]

- tilde{w} is indeed unique , given the initial conditions . We clarified this in Theorem 4 and its proof .

- alpha_n=0 for the support vectors is only true for a measure zero of all datasets ( we added a proof of this in appendix F ) . Thus , we clarified in the revision that our results hold for almost every dataset ( and so , they are true with probability 1 for any data drawn from a continuous - valued distribution ) .

[ Why does the fact that the limit is dominated by gradients that are a linear combination of support vectors imply that w_infinity will also be a non-negative linear combination of support vectors ? ]

We clarified in the revision : “ ... The negative gradient would then asymptotically become a non-negative linear combination of support vectors . The limit w_{\infinity } will then be dominated by these gradients , since any initial conditions become negligible as ||w ( t ) ||->infinity ( from Lemma 1 ) ” .

Paper focuses on characterising behaviour of the log loss minimisation on the linearly separable data . As we know , optimisation like this does not converge in a strict mathematical sense , as the norm of the model will grow to infinity . However , one can still hope for a convergence of normalised solution ( or equivalently - convergence in term of separator angle , rather than parametrisation ) . This paper shows that indeed , log-loss ( and some other similar losses ) , minimised with gradient descent , leads to convergence ( in the above sense ) to the max- margin solution . On one hand it is an interesting property of model we train in practice , and on the other - provides nice link between two separate learning theories .

Pros :
- easy to follow line of argument
- very interesting result of mapping " solution " of unregularised logistic regression ( under gradient descent optimisation ) onto hard max margin one

Cons :
- it is not clear in the abstract , and beginning of the paper what " convergence " means , as in the strict sense logistic regression optimisation never converges on separable data . It would be beneficial for the clarity if authors define what they mean by convergence ( normalised weight vector , angle , whichever path seems most natural ) as early in the paper as possible .

We thank the reviewer for the positive review and for the helpful comment . We uploaded a revised version in which clarified in the abstract that the weights converge “ in direction ” to the L2 max margin solution .

( a ) Significance
The main contribution of this paper is to characterize the implicit bias introduced by gradient descent on separable data . The authors show the exact form of this bias ( L_2 maximum margin separator ) , which is independent of the initialization and step size . The corresponding slow convergence rate explains the phenomenon that the predictor can continue to improve even when the training loss is already small . The result of this paper can inspire the study of the implicit bias introduced by gradient descent variants or other optimization methods , such as coordinate descent . In addition , the proposed analytic framework seems promising since it may be extended to analyze other models , like neural networks .

( b ) Originality
This is the first work to give the detailed characterizations of the implicit bias of gradient descent on separable data . The proposed assumptions are reasonable , but it seems to limit to the loss function with exponential tail . I ’m curious whether the result in this paper can be applied to other loss functions , such as hinge loss .

( c ) Clarity & Quality
The presentation of this paper is OK . However , there are some places can be improved in this paper . For example , in Lemma 1 , results ( 3 ) and ( 4 ) can be combined together . It is better for the authors to use another section to illustrate experimental settings instead of writing them in the caption of Figure 3.1.

Minor comments :
1 . In Lemma 1 ( 4 ) , w^ T ( t ) - >w ( t ) ^ T
2 . In the proof of Lemma 1 , it ’s better to use vector 0 for the gradient L ( w )
3 . In Theorem 4 , the authors should specify eta
4 . In appendix A , page 11 , beta is double used
5 . In appendix D , equation ( D.5 ) has an extra period


We thank the reviewer for the positive review and for the helpful comments . We uploaded a revised version in which all the reviewer comments were addressed .

[ “ I ’m curious whether the result in this paper can be applied to other loss functions , such as hinge loss . ” ]

We believe our results could be extended to many other types of loss functions ( in fact , we are currently working on such extensions ) . However , for the hinge loss ( without regularization ) , gradient descent on separable data can converge to a finite solution which is not to the max margin vector . For example , if there is a single data point x= ( 1, 0 ) , and we start with a weight vector w= ( 2 , 2 ) , the hinge loss and its gradient are both equal to zero . Therefore , no weight updates are performed , and we do not converge to the direction of the L2 max margin classifier : w= ( 1,0 ) .

[ “ It is better for the authors to use another section to illustrate experimental settings instead of writing them in the caption of Figure 3.1 . “ ]

We felt it is easier to read if all details are summarized in the figure , and wanted to save space to fit the main paper into 8 pages . However , we can change this if required .

This paper examines a distributed Deep RL system in which experiences , rather than gradients , are shared between the parallel workers and the centralized learner . The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers . Using this system , the authors are able to harness much more compute to learn very high quality policies in little time . The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow .

It ’s hard to take issue with a paper that has such overwhelmingly convincing experimental results . However , there are a couple additional experiments that would be quite nice :
• In order to understand the best way for training a distributed RL agent , it would be nice to see a side - by-side comparison of systems for distributed gradient sharing ( e.g. Gorila ) versus experience sharing ( e.g. Ape-X ) .
• It would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen , rather than just wall - clock time . For example , in Table 1 , is Ape-X at 200M frames doing better than Rainbow at 200M frames ?

Pros :
• Well written and clear .
• Very impressive results .
• It ’s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm .

Cons :
• Hard to replicate experiments without the deep computational pockets of DeepMind .


Thank you very much for the review . This is a good summary of the paper .

Q1 : on side - by-side comparison of systems for distributed gradient sharing ( e.g. Gorila ) versus experience sharing ( e.g. Ape-X ) .

A thorough exploration and comparison of these approaches would be valuable , but we believe that fairly and rigorously investigating this large space of possible designs is likely to be a complex topic unto itself , and it would not be possible to do it justice in this paper . Performance comparisons of such systems will likely depend on practical factors such as network latency ( due to stale gradients or straggling workers ) as well as model size and the size of the observation data ( since this will affect the throughput across the distributed system ) . Ultimately we believe that distributed gradient sharing and distributed experience sharing will prove complementary , but that the nuances of how to optimally combine them will therefore depend on not only the domain but also the nature and distribution of the available computational resources .

Q2 : on how Ape-X performs as a function of the number of frames it has seen , rather than just wall - clock time .

In case you missed it , Figure 10 in the Appendix includes plots of performance against number of frames seen for the first billion frames , with comparisons against Rainbow and DQN . Note , however , that in all of these algorithms , the amount of experience replay per environment step may be varied , and this factor can have a significant effect on such results .


A parallel aproach to DQN training is proposed , based on the idea of having multiple actors collecting data in parallel , while a single learner trains the model from experiences sampled from a central replay memory . Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines .

The core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well , when implemented efficiently over a large cluster ( which is not trivial ) . I also appreciate the various experiments to analyze the impact of several settings ( instead of just reporting a new SOTA ) . Overall I believe this is definitely a solid contribution that will benefit both practitioners and researchers ... as long as they got the computational resources to do so !

There are essentially two more things I would have really liked to see in this paper ( maybe for future work ? ) :
- Using all Rainbow components
- Using multiple learners ( with actors cycling between them for instance )
Sharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus !

Minor points :
- Figure 1 does not seem to be referenced in the text
- « In principle , Q-learning variants are off - policy methods » => not with multi-step unless you do some kind of correction ! I think it is important to mention it even if it works well in practice ( just saying « furthermore we are using a multi-step return » is too vague )
- When comparing the Gt targets for DQN vs DPG it strikes me that DPG uses the delayed weights phi - to select the action , while DQN uses current weights theta . I am curious to know if there is a good motivation for this and what impact this can have on the training dynamics .
- In caption of Fig. 5 25 K should be 250 K
- In appendix A why duplicate memory data instead of just using a smaller memory size ?
- In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN . Why use a different scheme ?
- Why store rewards and gamma ’s at each time step in memory instead of just the total discounted reward ?
- It would have been better to re-use the same colors as in Fig . 2 for plots in the appendix
- Would Fig. 10 be more interesting with the full plot and a log scale on the x axis ?

Thank you for the thorough review ! This is a good summary of the paper .

Q1 : on using all Rainbow components and on using multiple learners .

These are both interesting directions which we agree may help to boost performance even further . For this paper , we felt that adding extra components would distract from the finding that it is possible to improve results significantly by scaling up , even with a relatively simple algorithm .

Q2 : on sharing the custom Tensorflow implementation of prioritized experience replay .

We would love to share an implementation , as we have found prioritization to be a consistently helpful component and would like to see it more widely used , but when we looked into this we realized that the current version depends on a library that would require a significant amount of engineering work to open source - so unfortunately we ca n’t commit to it at this time . However , we will bear this in mind for future versions .

Q3 : on multi-step Q-learning not being off-policy .

We ’ll try to clarify this .

Q4 : on which weights to use for action selection in DQN vs DPG target computations .

Interesting observation - setting aside the multi-step modification , our Ape-X DQN targets follow the approach described in [ 1 ] directly , whilst the Ape-X DPG targets are the same as those described in [ 2 ] . For the sake of simplicity in this paper , we were motivated not to deviate from the previously described update rules , in order to focus primarily on the improvements that could be obtained by our modifications to the method of generating and selecting the training data .

However , to answer your question on a more technical level , in [ 2 ] , the motivation given for the use of the target network weights to select the action when computing target values is that “ the target values are constrained to change slowly , greatly improving the stability of learning ” - the authors of [ 2 ] further note that they “ found that having both a target µ’ and Q’ was required to have stable targets y_i in order to consistently train the critic without divergence . This may slow learning , since the target network delays the propagation of value estimations . However , in practice we found this was greatly outweighed by the stability of learning ” .

In [ 1 ] , the update is modified in order to reduce overestimation resulting from the maximization step in Q-learning ; they note that “ the selection of the action , in the argmax , is still due to the online weights θ_t . This means that , as in Q-learning , we are still estimating the value of the greedy policy according to the current values , as defined by θ_t . However , we use the second set of weights θ’_t to fairly evaluate the value of this policy ” .

We have not yet re-evaluated these choices to determine whether the conclusions still hold in our new system . However , note that in DDPG ( and thus also in Ape-X DDPG ) there is no maximization step in the critic update , since we are using temporal - difference learning to update the critic instead of Q-learning - so the decoupling of action selection from evaluation used in Double Q Learning does not apply directly anyway .

We do not claim that these combinations of learning rules and target networks are necessarily the optimal ones , but we hope that this helps to explain the rationale behind the choices used in this paper .

[ 1 ] https://arxiv.org/abs/1509.06461
[ 2 ] https://arxiv.org/abs/1509.02971

Q5 : in appendix A why duplicate memory data instead of just using a smaller memory size ?

Conceptually , it would be indeed be sufficient to use a smaller memory to investigate this effect ; in fact our results in Figure 5 begin to do this - but we wanted to corroborate the finding by also measuring it in a different way . For implementation reasons , the two approaches are not guaranteed to be equivalent : for example , duplicating the data that each actor adds increases the computational load on the replay server , whereas using a smaller memory size does not . During development we noticed that in very extreme cases , many actors adding large volumes of data to the replay memory could overwhelm it , causing a slowdown in sampling which would affect the performance of the learner and thus the overall results .

In our experiments in Appendix A where we sought to determine whether recency of data was the reason for our observed scalability results , we wanted to make certain that the load on the replay server in the duplicated - data experiments would be the same as in the experiments with the corresponding numbers of real actors , to ensure a fair comparison . In practice , we did not find that we were running into any such contention issues in these experiments , and the results from Figure 5 do agree with those in Appendix A. However , we felt that it was still helpful to include both of the results in order to cover this aspect thoroughly . We will add a note explaining this .




Q6 : In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN . Why use a different scheme ?

We believe that this prioritized removal scheme may improve upon the usual FIFO removal approach , since it allows high priority data to remain in memory for longer . We have not yet re-run the Atari experiments with this newer modification , due to the significant resource requirements - we apologize for the discrepancy and we will add some explanation to make this more explicit .

Q7 : Why store rewards and gamma ’s at each time step in memory instead of just the total discounted reward ?

To clarify , we are storing the sum of discounted rewards accumulated across each multi-step transition , and the product of gammas across each multi-step transition . While this is not the only way to do it , these are cheap to compute online on each actor worker , and are sufficient to be able to compute up - to- date target values easily on the learner . We will make this more explicit in the implementation section in the appendix .

Q8 : Would Fig. 10 be more interesting with the full plot and a log scale on the x axis ?

We tried this but decided it was too difficult to read that way , unfortunately ... Since the data is from the same experiment as Figure 9 and the rate of data generation is approximately constant , the information that would be available in a full plot can largely be inferred from Figure 9 , though .

Q9 : on the other minor points ( Fig 1 reference , Fig 5 caption , and Fig 2 plot colors )

Thanks ! We ’ll fix these oversights .

This paper proposes a distributed architecture for deep reinforcement learning at scale , specifically , focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework . It has a very nice introduction and literature review of Prioritized experience replay and also suggested to parallelize the actor algorithm by simply adding more actors to execute in parallel , so that the experience replay can obtain more data for the learner to sample and learn . Not surprisingly , as this framework is able to learn from way more data ( e.g. in Atari ) , it outperforms the baselines , and Figure 4 clearly shows the more actors we have the better performance we will have .

While the strength of this paper is clearly the good writing as well as rigorous experimentation , the main concern I have with this paper is novelty . It is in my opinion a somewhat trivial extension of the previous work of Prioritized experience replay in literature ; hence the challenge of the work is not quite clear . Hence , I feel adding some practical learnings of setting up such infrastructure might add more flavor to this paper , for example .

Thank you for your comments and for this helpful suggestion . Our work is indeed closely related to the previous work on Prioritized Experience Replay . In achieving our reported results in practice , there was considerable challenge in two aspects : firstly , in the engineering work necessary to run the algorithm at a large scale , and secondly , in the discovery through empirical experimentation of a ) the necessary algorithmic extensions to the prior work upon which we built , and b ) the best way in which to combine them in practice . The point is well taken that the difficulty of this may not have been evident from our description , since in the paper we opted to focus more on our final architecture and results , believing this to be of greater interest to most readers . Indeed , we covered the implementation only briefly in the Appendix , and , as you note , we did not discuss our practical learnings in much depth . We are happy to hear that this is also of interest and we will gladly expand upon this section to provide further information and advice .

[ =========================== REVISION ===============================================================]
I am satisfied with the answers to my questions . The paper still needs some work on clarity , and authors defer the changes to the next version ( but as I understood , they did no changes for this paper as of now ) , which is a bit frustrating . However I am fine accepting it .
[ ============================== END OF REVISION =====================================================]

This paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label ( gaussian features , label generated using a particular function that should be learnable with neural net ) . Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss ( due to apparently bad local optima ) and suggest two ways of addressing it , on top of the known way of dealing with this problem ( which is overparameterization ) . First is to use a new activation function , the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD

Overall the paper is well written . The authors first introduce their suggested loss function and then go into details about what inspired its creation . I do find interesting the formulation of population risk in terms of tensor decomposition , this is insightful

My issues with the paper are as follows :
- The loss function designed seems overly complicated . On top of that authors notice that to learn with this loss efficiently , much larger batches had to be used . I wonder how applicable this in practice - I frankly did n't see insights here that I can apply to other problems that do n't fit into this particular narrowly defined framework
- I do find it somewhat strange that no insight to the actual problem is provided ( e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima ) , but authors are concerned with developing new loss function that has provable properties about global optima . Since it is all empirical , the first fix ( activation function ) seems sufficient to me and new loss is very far-fetched .
- It seems that changing activation function from relu to their proposed one fixes the problem without their new loss , so i wonder whether it is a problem with relu itself and may be other activations funcs , like sigmoids will not suffer from the same problem
- No comparison with overparameterization in experiments results is given , which makes me wonder why their method is better .

Minor : fix margins in formula 2.7.




Thanks for the review and comments .

Response to your questions :

--- " empirical fix seems sufficient " : We do consider the proposal of the empirical fix as part of the contribution of the paper . It 's driven by the theoretical analysis of the squared loss ( more intuition below ) , and it 's novel as far as we know .

That said , we think it 's also valuable to pursue a provably nice loss function without bad local minima . Note that there is no known method to empirically verify whether a function has no bad local minima , and such statements can only be established by proofs . Admittedly our first - cut design of the loss function is complicated and sub-optimal in terms of sample complexity , we do hope that our technique can inspire better loss function and model design in the future .

--- " does sigmoid suffer from the same problem ? " : we did experiment with the sigmoid activation and found that the sigmoid activation function also has bad local minima . We will add this experiment to the next version of the paper . We conjecture that the activation h_2 + h_4 or the activation 1 / 2 |z| has no spurious local minima .

--- " actual insights about the landscape " : Our insights is that the squared loss objective function is trying to perform an infinite number of tensor decomposition problems simultaneously , and the mixing of all of these problems very likely creates bad local minima , as we empirically observed . The intuition behind the empirical fix is that removing some of these tensor decomposition problems would make the landscape simpler and nicer .

--- " comparison with over - parameterization " : over- parameterization is indeed a powerful way to remove the bad local minima , and it gives models with good prediction . But it does n't recover the parameters of the true model because the training parameters space is larger . Our method is guaranteed to recover the true parameters of the model , which in turns guarantees a ``complete " generalization to any unseen examples , even including e.g. , adversarial examples or test examples drawn from another distribution . In this sense , our approaches ( both the empirical fix and theoretical one ) return solutions with stronger guarantees than over - parameterization can provide .

Of course , it 's also a very important open problem to understand better other alternatives to landscape design such as over - parametrization .


Thanks for the review again !

We apologize that we did n't know that the paper was expected to be updated . We just added the results for sigmoid that answers the question " does sigmoid suffer from the same problem ? " as we claimed in the response before . Please see page 9 , figure 2 in the current version .

We will revise the paper with more intuitions / explanations as promised in the previous response as soon as possible .

This paper studies the problem of learning one-hidden layer neural networks and is a theory paper . A well - known problem is that without good initialization , it is not easy to learn the hidden parameters via gradient descent . This paper establishes an interesting connection between least squares population loss and Hermite polynomials . Following from this connection authors propose a new loss function . Interestingly , they are able to show that the loss function globally converges to the hidden weight matrix . Simulations confirm the findings .

Overall , pretty interesting result and solid contribution . The paper also raises good questions for future works . For instance , is designing alternative loss function useful in practice ? In summary , I recommend acceptance . The paper seems rushed to me so authors should polish up the paper and fix typos .

Two questions :
1 ) Authors do not require a^* to recover B^ * . Is that because B^ * is assumed to have unit length rows ? If so they should clarify this otherwise it confuses the reader a bit .
2 ) What can be said about rate of convergence in terms of network parameters ? Currently a generic bound is employed which is not very insightful in my opinion .



Thanks for the comments .
Regarding the questions :

1 ) Yes , since we mostly focus on the ReLU activation , we assume that the rows of B^ * have unit norms . For ReLU activation , the row norms are inherently unidentifiable .

2 ) The technical version of the landscape analysis in Theorem B.1 specifies the precise dependencies of the landscape properties on the dimension , etc . To get a convergence rate , one can combine our Theorem B.1 with an analysis of gradient descent or other algorithms on non - convex functions . The best - known analysis for SGD in Ge et al . 2015 does not specify the precise polynomial dependencies . Since developing stochastic algorithms ( beyond SGD ) with lower iteration complexity is an active area of research , the best - known convergence rate is constantly changing .


This paper proposes a tensor factorization - type method for learning one hidden - layer neural network . The most interesting part is the Hermite polynomial expansion of the activation function . Such a decomposition allows them to convert the population risk function as a fourth - order orthogonal tensor factorization problem . They further redesign a new formulation for the tensor decomposition problem , and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015 . At last , they also establish the sample complexity for recovery .

The organization and presentation of the paper need some improvement . For example , the authors defer many technical details . To make the paper accessible to the readers , they could provide more intuitions in the first 9 pages .

There are also some typos : For example , the dimension of a is inconsistent . In the abstract , a is an m-dimensional vector , and on Page 2 , a is a d-dimensional vector . On Page 8 , P ( B ) should be a degree - 4 polynomial of B.

The paper does not contains any experimental results on real data .

Thanks for the comments . We will add more intuitions in the paper and fix the typos .

The high -level intuition is that the squared loss objective function is trying to perform an infinite number of tensor decomposition problems simultaneously and the mixing of all of these problems very likely creates bad local minima , as we empirically observed . Thus we design an objective function that selects only two of these tensor decompositions problems , which empirically removes the bad local minima . Finally , we design an objective function that resembles more a 4th order tensor decomposition objective in Ge et al. ( 2015 ) , which are known to be good .

The authors propose a method for performing transfer learning and domain adaptation via a clustering approach . The primary contribution is the introduction of a Learnable Clustering Objective ( LCO ) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class . Once the LCO is trained , it is applied to the unlabeled target data and effectively serves to provide " soft labels " for whether or not pairs of target data belong to the same class . A separate model can then be trained to assign target data to clusters while satisfying these soft labels , thereby ensuring that clusters are made up of similar data points .

The proposed LCO is novel and seems sound , serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest . The authors also demonstrate a variety of extensions , such as how to handle the case when the number of target categories is unknown , as well as how the model can make use of labeled source data in the setting where the source and target share the same task .

The way the method is presented is quite confusing , and required many more reads than normal to understand exactly what is going on . To point out one such problem point , Section 4 introduces f , a network that classifies each data instance into one of k clusters . However , f seems to be mentioned only in a few times by name , despite seeming like a crucial part of the method . Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model . Likewise , the introduction of G during the explanation of the LCO is rather abrupt , and the intuition of what purpose G serves and why it must be learned from data is unclear . Additionally , because G is introduced alongside the LCO , I was initially misled into understanding was that G was optimized to minimize the LCO . Further text explaining intuitively what G accomplishes ( soft labels transferred from the auxiliary dataset to the target dataset ) and perhaps a general diagram of what portions of the model are trained on what datasets ( G is trained on A , CCN is trained on T and optionally S ' ) would serve the method section greatly and provide a better overview of how the model works .

The experimental evaluation is very thorough , spanning a variety of tasks and settings . Strong results in multiple settings indicate that the proposed method is effective and generalizable . Further details are provided in a very comprehensive appendix , which provides a mix of discussion and analysis of the provided results . It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works . I 'm also curious how well the model works if , you do not make use of the labeled source data in the cross-domain setting , thereby mimicking the cross-task setup .

At times , the experimental details are a little unclear . Consistent use of the A , T , and S ' dataset abbreviations would help . Also , the results section seems to switch off between calling the method CCN and LCO interchangeably . Finally , a few of the experimental settings differ from their baselines in nontrivial ways . For the Office experiment , the LCO appears to be trained on ImageNet data . While this seems similar in nature to initializing from a network pre-trained on ImageNet , it 's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model , as opposed to other baselines which merely initialize weights and then fine- tune exclusively on the Office data . Similarly , the evaluation on SVHN -MNIST makes use of auxiliary Omniglot data , which makes the results hard to compare to the existing literature , since they generally do not use additional training data in this setting . In addition to the existing comparison , perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature .

Overall , the paper seems to have both a novel contribution and strong technical merit . However , the presentation of the method is lacking , and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained . I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission .

=========================
Update after author rebuttal :
=========================
I have read the author 's response and have looked at the changes to the manuscript . I am satisfied with the improvements to the paper and have changed my review to ' accept ' .

We greatly appreciate your constructive suggestions for enhancing the clarity . The suggestions make sense and can be done with minor refinement . We adopt them in our updated version . Thank you so much for your contribution .

The experimental settings you mentioned are also very interesting to us . Although we believe the current experimental settings serve well the purpose of supporting the main claim that transferring predictive similarity is an effective and generic way of transfer learning , the settings you described will help exploring other dimensions of the framework . For example , learning the similarity with only limited semantic categories , i.e. , only use T’ instead of the large A . We would like to include the aspects you mentioned in a future work .

For the discussion about the Office - 31 experiment , we are pleased to explain why the ImageNet data ( or A ) does n’t need to be presented during the training with T . The community uses the weights pretrained with ImageNet as a generic initialization due to its training on a large number of categories . We leverage a similar idea and argue that a semantic similarity learned with ImageNet -scale probably generalize well to unseen classes . Our cross-task Image Net experiment shows support to this idea . Thereby we augment the transferring scheme from only transferring the weights to transferring both weights and the learned similarity prediction . In other words , once the similarity prediction network is learned on ImageNet , it can be applied directly to other image classification datasets without access to Image Net dataset , just like how we use the pre-trained weights ( features ) .


( Summary )
This paper tackles the cross-task and cross-domain transfer and adaptation problems . The authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close ( in KL divergence ) and the distributions from dissimilar pairs of data to be farther apart ( in KL divergence ) . What 's similar vs dissimilar is trained with a binary classifier .

( Pros )
1 . The citations and related works cover fairly comprehensive and up - to-date literatures on domain adaptation and transfer learning .
2 . Learning to output the k class membership probability and the loss in eqn 5 seems novel .

( Cons )
1 . The authors overclaim to be state of the art . For example , table 2 does n't compare against two recent methods which report results exactly on the same dataset . I checked the numbers in table 2 and the numbers are n't on par with the recent methods . 1 ) Unsupervised Pixel - Level Domain Adaptation with Generative Adversarial Networks , Bousmalis et al. CVPR17 , and 2 ) Learning Transferrable Representations for Unsupervised Domain Adaptation , Sener et al . NIPS16 . Authors selectively cite and compare Sener et al. only in SVHN-MNIST experiment in sec 5.2.3 but not in the Office - 31 experiments in sec 5.2.2.
2 . There are some typos in the related works section and the inferece procedure is n't clearly explained . Perhaps the authors can clear this up in the text after sec 4.3.

( Assessment )
Borderline . Refer to the Cons section above .

Thank you for the detailed comments . We would like to respond to the cons you mentioned .

1 . First , we discuss several points about the cross domain transfer experiment . We separate them in the following :

( 1 ) Thank you so much for referring us to Bousmalis ’s CVPR work . It is a great work and we added the citation in our updated version . However , that paper does n’t include the experiments on Office - 31 nor SVHN-MNIST . Could you please share the mentioned results with us ?

( 2 ) Sener ’s paper is another great work . However , to make the numbers be comparable , the backbone networks ( e.g. , AlexNet , VGG , ResNet … ) , training configurations , and image preprocessing must be the same . Our table 2 for Office - 31 makes sure the setup is comparable . Sener ’s work has a different setup and has not released the code yet . Therefore a fair comparison on Office - 31 dataset is not viable at this date . For the SVHN-to-MNIST experiment , we use another way to compare the results . In table 10 , it shows the relative performance gain against the baselines ( source - only ) of each original paper . In such way we do n’t need the code of the referred approaches be available to make the absolute numbers comparable .

( 3 ) Based on above explanation , our statement about the performance achievement is still valid .

( 4 ) We would like to heavily emphasize that in the context of domain adaptation , our work should be considered as a complementary strategy to current mainstream domain adaptation approaches since the proposed LCO does not minimize the domain discrepancy . Instead of beating the benchmark , the main purpose of our paper is showing that transferring more information is an effective strategy for transfer learning across both tasks and domains . In fact , we show improved results using one of the domain adaptation methods ( DANN ) but we anticipate that incorporating later advances in domain adaptation using discrepancy losses will improve this . Furthermore , the performance of the framework is largely dependent on the accuracy of the learned similarity prediction . We use a naive implementation of a similarity prediction network ( SPN ) to obtain the presented numbers . The performance can be improved further when a more powerful similarity learning algorithm is available .

2 . The inference procedure for cross task transfer applies forward propagation on the network in figure 4 and uses the outputs after the cluster assignment layer . For cross domain transfer , the network is in figure 5 and the inference outputs is from the classification layer . We included the description in the updated version . Thank you so much for the feedback to improve the clarity .


pros :
This is a great paper - I enjoyed reading it . The authors lay down a general method for addressing various transfer learning problems : transferring across domains and tasks and in a unsupervised fashion . The paper is clearly written and easy to understand . Even though the method combines the previous general learning frameworks , the proposed algorithm for LEARNABLE CLUSTERING OBJECTIVE ( LCO ) is novel , and fits very well in this framework . Experimental evaluation is performed on several benchmark datasets - the proposed approach outperforms state - of - the- art for specific tasks in most cases .

cons / suggestions :
- the authors should discuss in more detail the limitations of their approach : it is clear that when there is a high discrepancy between source and target domains , that the similarity prediction network can fail . How to deal with these cases , or better , how to detect these before deploying this method ?
- the pair- wise similarity prediction network can become very dense : how to deal with extreme cases ?

Thank you for your comments and for recognizing the work . We are pleased to have a discussion of the limitations here and we added part of it into the paper .

As you pointed out , the performance of the similarity prediction is crucial , and the amount of performance gain on the target task is proportional to the accuracy of the similarity . One idea for enhancement is applying domain adaptation to learn the similarity prediction network ( SPN ) . Any existing adaptation method can be used to train the SPN ; as long as that adaptation method can deal with the high discrepancy between source and target , our framework will directly benefit from improvements in the learned SPN . With our proposed learning framework , the meta-knowledge carried by the SPN can then be transferred across either tasks or domains .

An extreme case involving the density of pairs is when there is a large number of categories in the target dataset . If there are 100 categories and we only randomly sample 32 instances for a mini-batch , there could be no similar pairs in a mini-batch since the instances are all from different categories . The LCO might not work well in this case since it has a form of contrastive loss . There are two ways to address this . The first is enlarge the mini-batch size , so that the number of sampled similar pairs increases . This method is limited by the memory size . The second way is to obtain dense similarity predictions offline . Then a mini-batch is sampled based on the pre-calculated dense similarity to ensure similar pairs are presented .


This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation . This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations . These schemes have generally achieved close-to- SOTA accuracy for small networks on datasets such as MNIST and CIFAR - 10 . However , for larger networks ( ResNET , Vgg , etc ) on large dataset such as ImageNET , a significant accuracy drop are reported . In this work , the authors show that a careful implementation of mixed - precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets . Using a INT16 ( as opposed to FP16 ) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16 .

The reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs . However , I was hoping to see a direct comparison between FP16 and INT16 .

The paper is written clearly and the English is fine .

Thanks a lot for your comments .

We do indeed have a Proof of Concept implementation for ResNet - 50 on KNM 72c , 1.5 GHz part with 16GB MCDRAM .
On this part a FP32 implementation using MKLDNN on Intel Caffe achieves 152 img /s while our POC ( also using Intel Caffe + MKLDNN interface ( but not MKLDNN code ) ) achieved 275 img /s while achieving SOTA . This is a ~ 1.8 x speedup over FP32 . We also believe that there is scope for further improvements . If the PC / Reviewers permit we can add these results to the paper .

Also the results in the paper are obtained using QVNNI - 16 kernels on a 32 node KNM cluster as mentioned in Section 5 .

We do admit that the performance and overflow related discussion has room for improvement . Specifically the statement you point out pertains to the fact that we can always have the following sequence of instructions : QVNNI - 16 , cvtepi32 ps ( convert INT32 to FP32 ) , fmaddps ( scale and accumulate FP32 results ) which will almost never overflow . Unfortunately the above mentioned sequence is ~ 3 x slower than pure QVNNI - 16 ( as it has 3 x more instructions ) . Therefore we select a compromise point between number of sufficient number of QVNNI - 16 instructions followed by the convert and accumulate sequence , which optimizes performance without compromising numerics .

I hope this clarifies things a little more . We will rewrite this Section 4.3 to clarify more .

Again as per the breakup of performance lost per component of the mixed precision training methodology , if the reviewers / PC permits we can provide more details in the paper .

We will update the submission shortly for a bunch of typographical and grammar issues we have identified at our end , and other edits discussed here .


This paper is about low - precision training for ConvNets . It proposed a " dynamic fixed point " scheme that shares the exponent part for a tensor , and developed procedures to do NN computing with this format . The proposed method is shown to achieve matching performance against their FP32 counter - parts with the same number of training iterations on several state - of - the-art ConvNets architectures on Imagenet -1K . According to the paper , this is the first time such kind of performance are demonstrated for limited precision training .

Potential improvements :

- Please define the terms like FPROP and WTGRAD at the first occurance .
- For reference , please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline ( default FP32 training ) .

We would like to thank the reviewer for the comments .

We will shortly update the manuscript to fix the missing definitions for the terms pointed out and also a number of other minor typographical errors that we have identified since submission .

We intend to also include a more detailed discussion on performance ( described in the comment below ) , in which we also include the baseline FP32 performance , along with a comparison with the INT16 variant in terms of various system aspects ( memory footprint , performance profile ... )

This work presents a CNN training setup that uses half precision implementation that can get 2X speedup for training . The work is clearly presented and the evaluations seem convincing . The presented implementations are competitive in terms of accuracy , when compared to the FP32 representation . I 'm not an expert in this area but the contribution seems relevant to me , and enough for being published .

We would like to thank the reviewer for the comments .

After a first manuscript that needed majors edits , the revised version
offers an interesting GAN approach based the scattering transform .

Approach is well motivated with proper references to the recent literature .

Experiments are not state of the art but clearly demonstrate that the
proposed approach does provide meaningful results .

Remark : Please do not miss the thank you comment ( above ) for all the reviews .

We rewrote large parts of the paper to make it as clear as possible . Hopefully , as expressed in the thank you comment , the changes we made will make things clearer .

The authors introduce scattering transforms as image generative models in the context of Generative Adversarial Networks and suggest why they could be seen as Gaussianization transforms with controlled information loss and invertibility .
Writing is suggestive and experimental results are interesting , so I clearly recommend acceptation .

I would appreciate more intuition on some claims ( e.g. relation between Lipschitz continuity and wavelets ) but they refer to the appropriate reference to Mallat , so this is not a major problem for the interested reader .

However , related to the above non-intuitive claim , here is a question on a related Gaussianization transform missed by the authors that ( I feel ) fulfils the conditions defined in the paper but it is not obviously related to wavelets . Authors cite Chen & Gopinath ( 2000 ) and critizise that their approach suffers from the curse of dimensionality because of the ICA stage . However , other people [ Laparra et al . Iterative Gaussianization from ICA to random rotations IEEE Trans . Neural Nets 2011 ] proved that the ICA stage is not required ( but only marginal operations followed by even random rotations ) . That transform seems to be Lipschitz continuous as well - since it is smooth and derivable - . In fact it has been also used for image synthesis . However , it is not obviously related to wavelets ... Any comment ?

Another relation to previous literature : in the end , the proposed analysis ( or Gaussianization ) transform is basically a wavelet transform where the different scale filters are applied in a cascade ( fig 1 ) . This is similar to Gaussian Scale Mixture models for texture analysis [ Portilla & Simoncelli Int. J. Comp. Vis. 2000 ] in which after wavelet transform , local division is performed to obtain Gaussian variables , and these can be used to synthesize the learned textures . That is similar to Divisive Normalization models of visual neuroscience that perform similar normalization alfter wavelets to factorize the PDF ( e.g. [ Lyu&Simoncelli Radial Gaussianization Neur.Comput. 2009 ] , or [ Malo et al . Neur. Comput. 2010 ] ) .

Minor notation issues : authors use a notation for functions that seems confusing ( to me ) since it looks like linear products . For instance : GZ for G ( Z ) [ 1st page ] and phiX for phi( X ) [ 2nd page ] Sx for S ( x ) [ in page 5 ] ...


Remark : Please do not miss the thank you comment ( above ) for all the reviews .

Question : " I would appreciate more intuition on some claims ( e.g. relation between Lipschitz continuity and wavelets ) but they refer to the appropriate reference to Mallat , so this is not a major problem for the interested reader . "

We have included now the definition of Lipschitz continuity to deformations in order to specify more clearly what it means , and we give a short , intuitive explanation of what is required at the beginning of section 3.1 . Going beyond would be too long , so we referred to the paper of Mallat ( 2012 ) .

Question : " However , related to the above non-intuitive claim , here is a question on a related Gaussianization transform missed by the authors that ( I feel ) fulfils the conditions defined in the paper but it is not obviously related to wavelets . Authors cite Chen \& Gopinath ( 2000 ) and critizise that their approach suffers from the curse of dimensionality because of the ICA stage . However , other people [ Laparra et al . Iterative Gaussianization from ICA to random rotations IEEE Trans . Neural Nets 2011 ] proved that the ICA stage is not required ( but only marginal operations followed by even random rotations ) . That transform seems to be Lipschitz continuous as well - since it is smooth and derivable - . In fact it has been also used for image synthesis . However , it is not obviously related to wavelets ... Any comment ? "

These transforms are Lipschitz in the sense that a small additive modification of the input yields a small modification of the output . The Lipschitz continuity to deformations means that a small modification of the form of a dilation yields a small modification of the Euclidean norm of the resulting vector . However , a small dilation can induce a large displacement of high frequencies . To avoid this , it is necessary to separate the frequencies in different packets , which is done by wavelets , and map them back to lower frequencies , which is done by the modulus and averaging ( a rectifier could replace the modulus ) . We now explain these points at the beginning of section 3.1.

For this reason , it is very unlikely that an Iterative Gaussianization produces an operator that is stable to deformations unless they take into account this issue , but in [ Laparra et al . IEEE Trans . Neural Nets 2011 ] there is no mention of this fact . While it is true that they also synthesize images , they do not show results on the nature of the interpolations between them .

Question : " Another relation to previous literature : in the end , the proposed analysis ( or Gaussianization ) transform is basically a wavelet transform where the different scale filters are applied in a cascade ( fig 1 ) . This is similar to Gaussian Scale Mixture models for texture analysis [ Portilla \& Simoncelli Int. J. Comp. Vis. 2000 ] in which after wavelet transform , local division is performed to obtain Gaussian variables , and these can be used to synthesize the learned textures . That is similar to Divisive Normalization models of visual neuroscience that perform similar normalization alfter wavelets to factorize the PDF ( e.g. [ Lyu\&Simoncelli Radial Gaussianization Neur.Comput. 2009 ] , or [ Malo et al . Neur. Comput. 2010 ] ) . "

Yes , there are similarities between Portilla and Simoncelli representations and scattering representations , and we have included them in the references . Portilla and Simoncelli also use the modulus of wavelet coefficients . At the second order , they use a covariance operator ; this may create a problem because covariance operators are not entirely stable to deformations ( this is better explained in the paper of Mallat ( 2012 ) ) . One may indeed define embedding operators , different from the scattering transform , which could lead to as good and maybe better results . We now emphasize this critical point in the introduction and at the beginning of Section 3.1 and refer to all these papers .

Question : " Minor notation issues : authors use a notation for functions that seems confusing ( to me ) since it looks like linear products . For instance : GZ for G ( Z ) [ 1st page ] and phiX for phi( X ) [ 2nd page ] Sx for S ( x ) [ in page 5 ] ... "

This was modified .


The paper proposes a generative model for images that does no require to learn a discriminator ( as in GAN ’s ) or learned embedding . The proposed generator is obtained by learning an inverse operator for a scattering transform .

The paper is well written and clear . The main contribution of the work is to show that one can design an embedding with some desirable properties and recover , to a good degree , most of the interesting aspects of generative models . However , the model does n’t seem to be able to produce high quality samples . In my view , having a learned pseudo- inverse for scattering coefficients is interesting on its own right . The authors should show more clearly the generalization capabilities to test samples . Is the network able to invert images that follow the train distribution but are not in the training set ?

As the authors point out , the representation is non-invertible . It seems that using an L2 loss in pixel space for training the generator would necessarily lead to blurred reconstructions ( and samples ) ( as it produces a point estimate ) . Unless the generator overfits the training data , but then it would not generalize . The reason being that many images would lie in the level set for a given feature vector , and the generator cannot deterministically disambiguate which one to match .

The sampling method described in Section 3.2 does not suffer from this problem , although as the authors point out , a good initialization is required . Would it make sense to combine the two ? Use the generator network to produce a good initial condition and then refine it with the iterative procedure .

This property is exploited in the conditional generation setting in :

Bruna , J. et al " Super-resolution with deep convolutional sufficient statistics . " arXiv preprint arXiv:1511.05666 ( 2015 ) .

The samples produced by the model are of poorer quality than those obtained with GAN ’s . Clearly the model is assigning mass to regions of the space where there are not valid images . ( similar effect that suffer models train with MLE ) . Could you please comment on this point ?

The title is a bit misleading in my view . “ Analyzing GANs ” suggests analyzing the model in general , this is , its architecture and training method ( e.g. loss functions etc ) . However the analysis concentrates in the structure of the generator and the particular case of inverting scattering coefficients .

However , I do find very interesting the analysis provided in Section 3.2 . The idea of using meaningful intermediate ( and stable ) targets for the first two layers seems like a very good idea . Are there any practical differences in terms of quality of the results ? This might show in more complex datasets .

Could you please provide details on what is the dimensionality of the scattering representation at different scales ? Say , how many coefficients are in S_5 ?

In Figure 3 , it would be good to show some interpolation results for test images as well , to have a visual reference .

The authors mention that considering the network as a memory storage would allow to better recover known faces from unknown faces . It seems that it would be known from unknown images . Meaning , it is not clear why this method would generalize to novel image from the same individuals . Also , the memory would be quite rigid , as adding a new image would require adapting the generator .

Other minor points :

Last paragraph of page 1 , “ Th inverse \ Phi … ” is missing the ‘ e ’.

Some references ( to figures or citations ) seem to be missing , e.g. at the end of page 4 , at the beginning of page 5 , before equation ( 6 ) .

Also , some citations should be corrected , for instance , at the end of the first paragraph of Section 3.1 :

“ … wavelet filters Malat ( 2016 ) . ”

Sould be :

“ ... wavelet filters ( Malat , 2016 ) . ”

First paragraph of Section 3.3 . The word generator is repeated .


Remark : Please do not miss the thank you comment ( above ) for all the reviews .

Question : " In my view , having a learned pseudo- inverse for scattering coefficients is interesting on its own right . The authors should show more clearly the generalization capabilities to test samples . Is the network able to invert images that follow the train distribution but are not in the training set ? "

We have reorganized the paper so that this very important generalization point appears immediately , in Section 2.1 of the new version . We have thus inverted two sections . Figure 4 shows that the network is indeed able to generalize in the sense that it can invert test images that follow the distributions but which are not in the training set . The precision of this recovery depends upon the image complexity , which is quantified by Table 1 .

Question : " As the authors point out , the representation is non-invertible . It seems that using an L2 loss in pixel space for training the generator would necessarily lead to blurred reconstructions ( and samples ) ( as it produces a point estimate ) . Unless the generator overfits the training data , but then it would not generalize . The reason being that many images would lie in the level set for a given feature vector , and the generator cannot deterministically disambiguate which one to match . "

The blur is in fact not due to the non-invertibility of the embedding . We checked this by using an invertible embedding , by reducing the scattering scale $ 2^J $ from $ 2^5 $ in the original paper to $ 2^4 $ in this version . Figure 2 ( a , b ) shows that this embedding operator is nearly invertible . However , as we now further emphasize in Section 2.1 , the convolutional network does not invert exactly the embedding operator ( here the scattering ) . It makes a regularized inversion on the training images . It is the regularization induced by the convolutional network structure which allows the generator to build random models of complex images . Figure 2 ( c , d ) shows that it recovers much better images than what would have obtained by inverting the scattering embedding operator , this is also now explained in Section 2.1.

The regularized inversion is based on some form of memorization of information in the training samples , which does not seem to be sufficient for complex images . The blur is indeed smaller for images of polygons . It may also be that the blur is partly due to instabilities in the optimization . We now explain this point in the experiments section .

Question : " The sampling method described in Section 3.2 does not suffer from this problem , although as the authors point out , a good initialization is required . Would it make sense to combine the two ? Use the generator network to produce a good initial condition and then refine it with the iterative procedure . This property is exploited in the conditional generation setting in : Bruna , J. et al " Super-resolution with deep convolutional sufficient statistics . " arXiv preprint arXiv:1511.05666 ( 2015 ) . "

As previously mentioned the goal is not to invert the scattering transform because it does not build good image models , as shown by the Figure 2 . If we incorporate iterations from the inverse scattering transform , it degrades the model because the convolutional network generator becomes an inverse scattering transform .

Question : " The samples produced by the model are of poorer quality than those obtained with GAN ’s . Clearly the model is assigning mass to regions of the space where there are not valid images . ( similar effect that suffer models train with MLE ) . Could you please comment on this point ? "

We have reduced this effect by choosing a smaller maximum scattering scale $ 2 ^J $ with $ J = 4 $ as opposed to $ J = 5 $ . GANs suffer from a diversity issue which means that they sample a limited part of the space . As the reviewer says , it seems that we do the opposite , the model assigns mass to regions where the images are not valid . We conjecture that there is a trade - off between image quality and diversity , this trade - off comes from a limited memory capacity of the network . However , understanding the generalization and memory capabilities of these models remains an open question .

Question : " The title is a bit misleading in my view . “ Analyzing GANs ” suggests analyzing the model in general , this is , its architecture and training method ( e.g. loss functions etc ) . However the analysis concentrates in the structure of the generator and the particular case of inverting scattering coefficients . "

We fully agree with this point . We thus propose to change the title to " Generative networks as inverse problems with scattering transforms " to emphasize our inverse problem approach , which is indeed different from GANs . This is the title of the new version .

Question : " However , I do find very interesting the analysis provided in Section 3.2 . The idea of using meaningful intermediate ( and stable ) targets for the first two layers seems like a very good idea . Are there any practical differences in terms of quality of the results ? This might show in more complex datasets . "

There were slight numerical differences but no visual difference in the quality of the training and test images . To address the previous point of the reviewer , in this new version , we reduced the scattering scale from $ 2^5 $ to $ 2^4 $ , which also improved image qualities . We thus do not distinguish the invertibility from the non-invertibility range of the scattering , which also simplifies explanations . We are thus now using a single global architecture which is the same as the DCGAN generator ( Radford et al. 2016 ) . We are currently exploring better the idea of using meaningful intermediate targets along the generative network . However , these results extend the paper considerably ; therefore , they will be better explained in future work .

Question " Could you please provide details on what is the dimensionality of the scattering representation at different scales ? Say , how many coefficients are in $ S_5 $ ? "

We gave the ratio $ \alpha_j$ between the number of image coefficients and the size of each layer $ S_j$ . For $ j = 5 $ then $ \alpha_j = 0.66 $ which means that $ S_5 $ has about twice less coefficients than the number of pixels in the image , which is $ 64^2 $ . If $j = 4 $ then $ \alpha_j = 1.63 $ and $ S_4 ( x ) $ thus has more coefficients than the image $ x$ which explains why it is invertible . We now give these numbers .

Question : " In Figure 3 , it would be good to show some interpolation results for test images as well , to have a visual reference . "

This is done in Figure 6 in the new version .

Question : " The authors mention that considering the network as a memory storage would allow to better recover known faces from unknown faces . It seems that it would be known from unknown images . Meaning , it is not clear why this method would generalize to novel image from the same individuals . Also , the memory would be quite rigid , as adding a new image would require adapting the generator . "

The network has some form of memory since it recovers high dimensional images from lower dimensional input vectors . It can be considered as an associative memory in the sense that it is content addressable . From an image $ x$ sampled from $ X$ , we can compute an address $ z = \ Sigma_d^{ - 1 / 2 } ( S_J ( x ) - \mu ) $ from which we can reconstruct an approximation of $ x$ . This is what is usually called an associative memory ( e.g. , Hopfield networks ) . Indeed , it clearly depends upon the generalization capabilities of the network .

The memory is indeed rigid in the sense that it requires modifying all coefficients to add a single image , but this is the case of any distributed associative memory such as Hopfield networks . We agree that the ability to add a new face easily in the network is key to have an effective memory .

We have included all the minor points of the reviewer .

The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries ( e.g. , CuDNN ) can not exploit sparsity when it is presented and because a number of works have proposed to sparsify / prune RNNs so as to be able to run on devices with limited compute power ( e.g. , smartphones ) . Unfortunately , due to the low - level and GPU specific nature of the work , I would think that this work will be better critiqued in a more GPU - centric conference . Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity , these are not contrasted by presenting the loss in accuracy caused by introducing sparsity ( in the main portion of the paper ) . It may be the case by reducing density to 1 % we can speedup by N fold but this observation may not have any value if the accuracy becomes abysmal .

Pros :
- Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs
- Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well

Cons :
- This work may be better reviewed at a more GPU - centric conference
- Experiments ( in main paper ) only show speedups and do not show loss of accuracy due to sparsity

Thank you for your comments and observations . Let us first address the critical importance of network accuracy after pruning . We completely agree that large speed improvements are a moot point if the accuracy does not hold up . However , an exhaustive study of the sparsity / accuracy tradeoff is out of the scope of this paper . Instead , we refer to several other published results that show good accuracy results for recurrent networks around the 10 % density point [ Han et al. 2016 ( a , b ) , Narang et al. 2017 , See et al. 2016 , Anonymous 2018 ] . So , we centered our experiments around this density and swept from 1 % to 30 % to cover a wider range . After submission , densities down to 3 % have been recently used to achieve state of the art results on some workloads ( https://blog.openai.com/block-sparse-gpu-kernels/), and we show good speedups for higher densities , especially when the layer size is too large for a dense persistent kernel . Finally , we provided more accuracy vs . sparsity vs. speed results in the appendix to show why our technique is important . We 'll gladly move this analysis into the main paper if extending beyond 8 pages is preferable to merely including references to other works with accuracy results .

We feel that this work is relevant to the ICLR audience . As you noted , sparsity is not regularly accelerated by deep learning libraries . More importantly , as we show in our appendix , some recurrent layers are actually better off staying dense and using a persistent approach if possible ( without our sparse persistent technique ) . Simply increasing accuracy for the same number of effective parameters is not sufficient to claim success ; the network 's speed may not increase over a dense network ! Thus , one of the fundamental benefits of sparsity is tempered in some cases . Our main contribution shifts this balance back in favor of pruning for recurrent layers .

This paper introduces sparse persistent RNNs , a mechanism to add pruning to the existing work of stashing RNN weights on a chip . The paper describes the use additional mechanisms for synchronization and memory loading .

The evaluation in the main paper is largely on synthetic workloads ( i.e. large layers with artificial sparsity ) . With evaluation largely over layers instead of applications , I was left wondering whether there is an actual benefit on real workloads . Furthermore , the benefit over dense persistent RNNs for OpenNMT application ( of absolute 0.3- 0.5s over dense persistent rnns ? ) did not appear significant unless you can convince me otherwise .

Storing weights persistent on chip should give a sharp benefit when all weights fit on the chip . One suggestion I have to strengthen the paper is to claim that due to pruning , now you can support a larger number of methods or method configurations and to provide examples of those .

To summarize , the paper adds the ability to support pruning over persistent RNNs . However , Narang et. al. , 2017 already explore this idea , although briefly . Furthermore , the gains from the sparsity appear rather limited over real applications . I would encourage the authors to put the NMT evaluation in the main paper ( and perhaps add other workloads ) . Furthermore , a host of techniques are discussed ( Lamport timestamps , memory layouts ) and implementing them on GPUs is not trivial . However , these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized .

Thank you comments and suggestions . It is fair to wonder about the performance on real workloads ; we decided to show the performance of our technique over a wide range of synthetic workloads so that practitioners can look to see where their application lives in the space and judge the relative performance accordingly . Our appendix shows the performance of recurrent layers of one particular application .

With respect to the speedup over the dense persistent LSTMs in the OpenNMT network , 0.3- 0.5s ( looking at layers of the same size ) is not the proper comparison . Instead , we think that the comparison should be between networks of the same accuracy . In this case , the improvement is up to 0.7 ms ( from 1.26 ms for a BLEU score of 24.62 to 0.55 ms for a BLEU score of 24.60 ) . Also , this is a per-layer improvement ; a full network will be composed of several such layers leading to a larger absolute improvement for the network as a whole . More important than absolute speedup for a single iteration , however , is the potential speedup for training networks . This absolute 0.7 ms reduces the run time to 44 % of the previous time , roughly halving the time needed to train the network to a given accuracy . We 'll make this clear in the final text . Finally , it 's worth noting that without our contributions , the benefit of sparsity would be negative : existing sparse methods are worse than persistent kernels for a given accuracy or speed target on the workloads we studied .

We have a question about your suggestion to claim support for a larger number of methods . We do claim this : Figure 3 shows that we can support larger layers in a persistent approach than existing methods . Please let us know if we 've misunderstood ; we welcome opportunities to strengthen this paper !

We will certainly move the NMT evaluation into the main paper if the reviewers think it warrants the extra space . We agree that it naturally belongs there .

We 're also willing to emphasize the non-trivial aspects of the optimizations we used , as opposed to the brief mention in past work you point out . It is exactly these optimizations which take the bulk of the main paper ; was there something in particular you suggest adding ?

The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels . The authors further propose model - pruning optimizations which are aware of the persistent implementation .

It 's not clear if the paper is relevant to the ICLR audience due to its emphasize on low - level optimization which has little insight in learning representations . The exposition in the paper is also not well - suited for people without a systems background , although I 'll admit I 'm mostly using myself as a proxy for the average machine learning researcher here . For instance , the authors could do more to explain Lamport Timestamps than a 1974 citation .

Modulo problems of relevance and expected audience , the paper is well - written and presents useful improvements in performance of large RNNs , and the work has potential for impact in industrial applications of RNNs . The work is clearly novel , and the contributions are clear and well - justified using experiments and ablations .

Thank you for your time and comments . With respect to showing the relevance to ICLR , we think the results of our work are very important . Let us try to clarify this relevance by presenting the results of the appendix _without_ the context of the main paper : " For the recurrent layers of the network we studied , there 's no need to prune the weights . A dense persistent implementation of the network is faster for the same accuracy as a pruned network , or more accurate at a given speed target . "

There has been significant interest in model pruning , mostly for the purposes of increasing performance . However , realizing increased performance often requires some type of structured pruning , such as pruning filters or channels from convolutional networks , or leaving dense blocks in recurrent networks . ( As Narang et al. noted in their 2017 work at ICLR , cuSPARSE achieves limited speedup for unstructured sparsity , especially for large batch sizes . ) However , imposing structure on the sparsity reduces the degrees of freedom ; unstructured sparsity can represent a proper superset of the patterns that any structured sparse layer can represent . Therefore , it is preferable from a model 's point of view to use sparsity without any structure ( if sparsity is to be used at all , and second - order regularization effects of imposed structure notwithstanding ) . So , we are motivated to find the efficient method , presented in the main section , to accelerate recurrent layers with unstructured sparsity .

However , presenting an efficient method is only half the story ; to start filling in the pieces , we included our appendix ( as an appendix , in order to stay within the suggested page limit ) . We show how both accuracy and speed change with sparsity . In particular , without our method , unstructured sparsity ( preferred by the model ) is inferior to a dense network . Dense persistent kernels are faster and more accurate than their pruned cuSPARSE counterparts for the model we studied . We will make these points more clear in the next version of the text -- as well as spending some more space on describing Lamport Timestamps !

The paper is written well and clear . The core contribution of the paper is the illustration that : under the assumption of flat , or curved decision boundaries with positive curvature small universal adversarial perturbations exist .

Pros : the intuition and geometry is rather clearly presented .

Cons :
References to " CaffeNet " and " LeNet " ( even though the latter is well - known ) are missing . In the experimental section used to validate the main hypothesis that the deep networks have positive curvature decision boundaries , there is no description of how these networks were trained .

It is not clear why the authors have decided to use out - dated 5 - layer " LeNet " and NiN ( Network in network ) architectures instead of more recent and much better performing architectures ( and less complex than NiN architectures ) . It would be nice to see how the behavior and boundaries look in these cases .

The conclusion is speculative :
" Our analysis hence shows that to construct classifiers that are robust to universal perturbations , it
is key to suppress this subspace of shared positive directions , which can possibly be done through
regularization of the objective function . This will be the subject of future works . "

It is clear that regularization should play a significant role in shaping the decision boundaries . Unfortunately , the paper does not provide details at the basic level , which algorithms , architectures , hyper - parameters or regularization terms are used . All these factors should play a very significant role in the experimental validation of their hypothesis .

Notes : I did not check the proofs of the theorems in detail .


We thank the reviewer for the comments that helped improve the manuscript . Please see clarifications below .

1 . We have updated the manuscript with references for the networks , as well as a description of how these networks were trained as requested by the reviewer .

2 . To address the Reviewer concern , we have conducted new experiments on more architectures , in particular ResNet - 18 , VGG - 16 for CIFAR - 10 and ResNet - 152 for ImageNet ; all confirm and validate our results . Specifically ,

* Fig. 5 and 6 were updated with decision boundaries of ResNet - 18 for CIFAR - 10 and ResNet - 152 for ImageNet .
* We have conducted the same experiment as in Fig. 7 ( b ) for VGG - 16 and ResNet - 18 architectures . Please see Appendix C for the figures .
* As also requested by Reviewer 1 , we have shown visual examples on ImageNet of the universal perturbations computed using the curvature - based proposed approach . Please see Fig. 8.

The new experiments confirm that our conclusions hold equally well on modern architectures ; in particular , these new results confirm that the existence of universal perturbations is due to the existence of shared positively curved directions in the decision boundary of deep networks .

3 . While our conclusion is indeed speculative , we believe that our analysis ( in particular the fact that universal perturbations are random vectors in subspace S_c ) can be leveraged to improve the robustness to universal perturbations . Other authors have actually already used our analysis to counter universal perturbations in a very recent paper [ Anonymous , 2017 ] *. The authors specifically eliminate universal perturbations through random sampling from this subspace , and training a " denoising " module to effectively project on the orthogonal of this subspace . This is indeed a very simple way of using the proposed analysis , and we believe that such analysis will lead to more ways to counter universal perturbations .

* : We anonymized this paper , as it is citing a technical report of ours and might violate the double blind policy .


This paper discusses universal perturbations - perturbations that can mislead a trained classifier if added to most of input data points . The main results are two fold : if the decision boundary are flat ( such as linear classifiers ) , then the classifiers tend to be vulnerable to universal perturbations when the decision boundaries are correlated . If the decision boundary are curved , then vulnerability to universal perturbations is directly resulted from existence of shared direction along with the decision boundary positively curved . The authors also conducted experiments to show that deep nets produces decision boundary that satisfies the curved model .

The main issue I am having is what are the applicable insight from the analysis :

1 . Why is universal perturbation an important topic ( as opposed to adversarial perturbation ) .
2 . Does the result implies that we should make the decision boundary more flat , or curved but on different directions ? And how to achieve that ? It might be my mis-understanding but from my reading a prescriptive procedure for universal perturbation seems not attained from the results presented .

We thank the reviewer for the comments . Please see clarifications below .

1 . Universal perturbations are static images that can be used by adversaries to fool a classifier ( no need to run an optimization procedure to fool each new image ) ; classifiers hence need to be robust to this excessively simple perturbation model . Adversarial perturbations are image-specific and do not generalize well across different images .
The existence of universal perturbations is also informative for the geometry of the classification boundaries , which is one step towards better understanding the fundamental properties of deep networks .

2 . The goal of the paper is not ( yet ) to improve the design of classifiers , but to gain insight through their analysis . It is beyond the scope of a single paper to prescribe procedures to improve robustness by modifying the curvature of classification regions .
Nevertheless , we should mention that our analysis ( in particular , the fact that universal perturbations are random vectors in subspace S_c ) has already been used by others to provide a constructive procedure to combat universal perturbations [ Anonymous , 2017 ] *. The authors specifically eliminate universal perturbations through random sampling from this subspace , and training a " denoising " module to effectively project on the orthogonal of this subspace .
This is indeed a very simple way of using the proposed analysis , and we believe that such analysis will inspire more ways to counter universal perturbations .

* : We anonymized this paper , as it is citing a technical report of ours and might violate the double blind policy .


The paper develops models which attempt to explain the existence of universal perturbations which fool neural networks — i.e. , the existence of a single perturbation which causes a network to misclassify most inputs . The paper develops two models for the decision boundary :

( a ) A locally flat model in which the decision boundary is modeled with a hyperplane and the normals two the hyperplanes are assumed to lie near a low - dimensional linear subspace .

( b ) A locally positively curved model , in which there is a positively curved outer bound for the collection of points which are assigned a given label .

The paper works out a probabilistic analysis arguing that when either of these conditions obtains , there exists a fooling perturbation which affects most of the data .

The theoretical analysis in the paper is straightforward , in some sense following from the definition . The contribution of the paper is to posit these two conditions which can predict the existence of universal fooling perturbations , argue experimentally that they occur in ( some ) neural networks of practical interest .

One challenge in assessing the experimental claims is that practical neural networks are nonsmooth ; the quadratic model developed from the hessian is only valid very locally . This can be seen in some of the illustrative examples in Figure 5 : there *is * a coarse - scale positive curvature , but this would not necessarily come through in a quadratic model fit using the hessian . The best experimental evidence for the authors ’ perspective seems to be the fact that random perturbations from S_c misclassify more points than random perturbations constructed with the previous method .

I find the topic of universal perturbations interesting , because it potentially tells us something structural ( class - independent ) about the decision boundaries constructed by artificial neural networks . To my knowledge , the explanation of universal perturbations in terms of positive curvature is novel . The paper would be much stronger if it provided an explanation of * why * there exists this common subspace of universal fooling perturbations , or even what it means geometrically that positive curvature obtains at every data point .

Visually , these perturbations seem to have strong , oriented local high - frequency content — perhaps they cause very large responses in specific filters in the lower layers of a network , and conventional architectures are not robust to this ?

It would also be nice to see some visual representations of images perturbed with the new perturbations , to confirm that they remain visually similar to the original images .


We thank the reviewer for the comments . Please see clarifications below .

- As requested , we have added visual representations of images perturbed with the new perturbation ( see Fig . 8 ) .

- We agree curvature is indeed only informative of the local structure of the decision boundary , but a first step to understand it . In the experiments , we have looked at coarse scale second - order information through a finite difference of gradients . This is indeed inevitable as state - of - the art networks using ReLU have theoretically vanishing Hessian almost everywhere .

- Visual appearance of universal perturbations : That is an interesting question . Our focus in this paper was more oriented towards explaining the existence of universal perturbations through an investigation of the geometry of the decision boundary . Interpreting the visual appearance of universal perturbations requires to draw a link between the weights in the lower layers with the curvature of the decision boundary . This would definitely be a fascinating connection , that we would like to work on in the future .

The paper studies learning in deep neural networks with hard activation functions , e.g. step functions like sign ( x ) . Of course , backpropagation is difficult to adapt to such networks , so prior work has considered different approaches . Arguably the most popular is straight - through estimation ( Hinton 2012 , Bengio et al. 2013 ) , in which the activation functions are simply treated as identity functions during backpropagation . More recently , a new type of straight - through estimation , saturated STE ( Hubara et al. , 2016 ) uses 1 [ |z | < 1 ] as the derivative of sign ( z ) .

The paper generalizes saturated STE by recognizing that other discrete targets of each activation layer can be chosen . Deciding on these targets is formulated as a combinatorial optimization problem . Once the targets are chosen , updating the weights of each layer to minimize the loss on those targets is a convex optimization . The targets are heuristically updated through the layers , starting out the output using the proposed feasibility target propagation . At each layer , the targets can be chosen using a variety of search algorithms such as beam search .

Experiments show that FTP often outperforms saturated STE on CIFAR and ImageNet with sign and quantized activation functions , reaching levels of performance closer to the full - precision activation networks .

This paper 's ideas are very interesting , exploring an alternative training method to backpropagation that supports hard - threshold activation functions . The experimental results are encouraging , though I have a few questions below that prevent me for now from rating the paper higher .

Comments and questions :

1 ) How computationally expensive is FTP ? The experiments using ResNet indicate it is not prohibitively expensive , but I am eager for more details .

2 ) Does ( Hubara et al. , 2016 ) actually compare their proposed saturated STE with the orignal STE on any tasks ? I do not see a comparison . If that is so , should this paper also compare with STE ? How do we know if generalizing saturated STE is more worthwhile than generalizing STE ?

3 ) It took me a while to understand the authors ' subtle comparison with target propagation , where they say " Our framework can be viewed as an instance of target propagation that uses combinatorial optimization to set discrete targets , whereas previous approaches employed continuous optimization . " It seems that the difference is greater than explicitly stated , that prior target propagation used continuous optimization to set * continuous targets * . ( One could imagine using continuous optimization to set discrete targets such as a convex relaxation of a constraint satisfaction problem . ) Focusing on discrete targets gains the benefits of quantized networks . If I am understanding the novelty correctly , it would strengthen the paper to make this difference clear .

4 ) On a related note , if feasible target propagation generalizes saturated straight through estimation , is there a connection between ( continuous ) target propagation and the original type of straight through estimation ?

5 ) In Table 1 , the significance of the last two columns is unclear . It seems that ReLU and Saturated ReLU are included to show the performance of networks with full - precision activation functions ( which is good ) . I am unclear though on why they are compared against each other ( bolding one or the other ) and if there is some correspondence between those two columns and the other pairs , i.e. , is ReLU some kind of analog of SSTE and Saturated ReLU corresponds to FTP -SH somehow ?

Thank you for your review . We respond to each of your questions below .

1 ) FTP - SH is no more expensive than backprop ( in the same way that SSTE is n’t either , and SSTE is a special case of FTPROP - MB ) . The only added cost is that the soft hinge loss requires computing an exponential , which is slower than a max ( i.e. , the cost of computing a sigmoid vs . a ReLU ) , but this is a minor difference in compute time .

2 ) In the experiments , Hubara et al. ( 2016 ) does not compare SSTE and STE directly , but in the text of the paper they report that “ Not [ saturating ] the gradient when [ the input ] is too large significantly worsens performance . ” This is also what we found in preliminary experiments , where the unsaturated STE is significantly worse than STE . Note , however , that STE is also a special case of our framework where the loss function is just loss ( z , t ) = - zt , so we generalize that as well ( and pretty much any type of STE can be obtained by choosing different losses in our framework ) .

3 ) Yes , this is a good point and correct . We will update the paper to make this fact more clear . Thank you .

4 ) It ’s possible , although if so it ’s not an obvious connection , and we have n’t studied this issue in detail yet .

5 ) Yes , good point . This is somewhat confusing , and we will clarify it in the paper and remove the bolding , since the goal is n’t really to compare them against each other ( although it is mildly interesting that saturating the ReLU improves performance in some cases ) . There is no correspondence between those two columns and the other pairs ; the formatting of the table is just unclear .

The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it . The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy . The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems . However since the complexity of such a problem is exponential , the authors propose a collection of heuristics / approximations to solve the problem . These include , a heuristic for setting the targets at each layer , using a soft hinge loss , mini-batch training and such . Using these modifications the authors propose an algorithm ( Algorithm 2 in appendix ) to train such models efficiently . They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight - through -estimator ( SSTE ) on a couple of datasets , namely , CIFAR - 10 and ImageNet . They show superiority of their algorithm over SSTE .

I thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds . The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting . The results are moderately convincing in favor of the proposed approach . Though a disclaimer here is that I 'm not 100 % sure that SSTE is the state of the art for this problem . Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community .

There are a few flaws / weaknesses in the paper though , making it somewhat lose .
- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1 . Realizing the limitations of the proposed algorithm , given the assumptions under which it was conceived in , the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees , such as checks for feasibility , discussed earlier .
- The result of this is another algorithm ( I guess the main result of the paper ) , which is strangely presented in the appendix as opposed to the main text , which has no such guarantees .
- There is no theoretical proof that the heuristic for setting the target is a good one , other than a rough intuition
- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach . The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit .


Thank you for your review . We respond to each of your questions and comments below .

Based on the quantization literature and other hard - threshold papers that we looked at and cited , SSTE is ( by far ) the most widely used method . It ’s true that there are many variations of the straight- through estimator ( STE ) , but we compare to the main one ( SSTE ) , and do n’t know of any that outperform SSTE . Note that neither STE nor SSTE has convergence guarantees ( they ’re biased estimators ) but SSTE at least works well in practice .

While we agree that it would be nice to have better guarantees for FTPROP - MB , it is typical in AI and combinatorial search ( as you likely know ) to start from a theoretically - justified approach and then use that to define a more heuristic approach that sacrifices those guarantees in favor of efficiently achieving the desired property ( i.e. , feasibility ) , as we do here . Since the problem we are solving is NP - complete and ( most likely ) hard to approximate , heuristics are unavoidable . By using the ( soft ) hinge loss at each layer , FTPROP - MB is implicitly trying to maximize “ soft feasibility ” of the network because of the correspondence between the hinge loss and margin maximization .

Further , while feasibility is important for understanding the solution we propose , giving it up is necessary to avoid overfitting . This is similar to the linear- separability property of the perceptron where the robust method for learning a perceptron is to use a hinge loss instead of the perceptron criterion . We intend to further study the properties of FTPROP and ( soft ) feasibility in the future .

We did not put the FTPROP - MB pseudocode in the main paper because it ’s such a simple algorithm and we were running short on space , but we can move it to the main body .

Space limitations also precluded further discussions of generalization ability . We used standard approaches to avoid overfitting ( L2 regularization , mini-batching , hinge vs. perceptron criterion , etc. ) , which we mention in the paper ( but can make more clear ) and which account for the good generalization performance .

This is a dense , rich , and impressive paper on rapid meta-learning . It is already highly polished , so I have mostly minor comments .

Related work : I think there is a distinction between continual and life- long learning , and I think that your proposed setup is a form of continual learning ( see Ring ‘ 94 / ‘97 ) . Given the proliferation of terminology for very related setups , I ’d encourage you to reuse the old term .

Terminology : I find it confusing which bits are “ meta ” and which are not , and the paper could gain clarity by making this consistent . In particular , it would be good to explicitly name the “ meta-loss ” ( currently the unnamed triple expectation in ( 3 ) ) . By definition , then , the “ meta-gradient ” is the gradient of the meta-loss -- and not the one in ( 2 ) , which is the gradient of the regular loss .

Notation : there ’s redundancy / inconsistency in the reward definition : pick either R_T or \bold{r} , not both , and maybe include R_T in the task tuple definition ? It is also confusing that \mathcal { R} is a loss , not a reward ( and is minimized ) -- maybe use another symbol ?

A question about the importance sampling correction : given that this spans multiple ( long ) trajectories , do n’t the correction weights become really small in practice ? Do you have some ballpark numbers ?

Typos :
- “ event their learning ”
- “ in such setting ”
- “ experience to for ”


Thank you for carefully reading the paper and the thoughtful comments . We answer the questions below :

Related work :
We agree that continuous adaptation is indeed a variation of continual learning . The updated version of the paper now points this out .

Terminology :
Thank you for suggestions . We have improved our terminology and notation throughout the paper , explicitly named the meta-loss , and renamed the inner loop gradient update ( as given in Eq. 2 ) from “ meta-update ” to “ adaptation update ” .

Notation :
Initially , \mathcal{R}_T was standing for the risk ( i.e. , the expected loss , as commonly used in machine learning literature ) . This notation was indeed a bit confusing in the RL context where R is often used for rewards , so we have altered it .

Importance sampling :
Good point . Eq. 9 gives a general form of the estimator for \phi . In practice , the adaptation gradient ( i.e. , the gradient of L_{T_{i- 1 }} as now given in Eq. 9 ) decouples into a sum over time steps , so we compute importance weights for each time step ( i.e. , for each action ) separately . The effective sample size in our experiments was no less than 20 % of the given sample size . ( Also , see our answer to a similar question asked by R2 . )

This paper proposed a gradient - based meta-learning approach for continuous adaptation in nonstationary and adversarial environment . The idea is to treat a nonstationary task as a sequence of stationary tasks and train agents to exploit the dependencies between consecutive tasks such that they can deal with nonstationarities at test time . The proposed method was evaluated based on a nonstationary locomotion and within a competitive multi agent setting . For the later , this paper specifically designed the RomoSumo environment and defined iterated adaptation games to test various aspect of adaptation strategies . The empirical results in both cases demonstrate the efficacy of the proposed meta-learned adaptation rules over the baselines in the few-short regime . The superiority of meta-learners is further justified on a population level .

The paper addressed a very important problem for general AI and it is well - written . Careful experiment designs , and thorough comparisons make the results conniving . I

Further comments :

1 . In the experiment the trajectory number seems very small , I wonder if directly using importance weight as shown in ( 9 ) will cause high variance in the performance ?

2 . One of the assumption in this work is that trajectories from T_i contain some information about T_{i+1} , I wonder what will happen if the mutually information is very small between them ( The extreme case is that two tasks are independent ) , will current method still perform well ?

P7 , For the RL^2 policy , the authors mentioned that “ … with a given environment ( or an opponent ) , reset the state once the latter changes ” How does the agent know when an environment ( or opponent ) changes ?

P10 , “ This suggests that it meta-learned a particular … ” This sentence need to be rewritten .

P10 , ELO is undefined


Thank you for carefully reading the paper and the thoughtful comments . We answer the questions below :

1 . Good point , the estimator in Eq. 9 may cause high variance in \phi . Three points to note :
( i ) We used importance weight correction only at execution time ( indeed , the variance of the estimator hindered learning in such a regime ; see footnote 2 ) .
( ii ) Even though Eq. 9 shows the sum over K episodes , each episode consists of multiple time steps ( typically 100 - 500 time steps , depending on the experiment ) each of which is treated as a separate sample and gets an importance weight . Even with a limited number of episodes , we get quite a substantial number of time steps ( with 3 episodes of 500 steps each , we get 1,500 time steps ) . In our experiments , the effective sample size was always reasonable ( more than 20 % ) , which worked at execution time ( but not for learning ) .
( iii ) To compute adaptation updates using Eq. 9 in practice , meta-learners not only used the immediate past episode , but multiple previous episodes ( see section 5.1 , paragraph 2 ) , which increased the number of samples and further helped to reduce the variance of the estimator .

2 . No , the method is not designed to work in the regime with no mutual information between T_i and T_{i+1} . Our meta-learning approach targets to solve a zero-shot problem ( i.e. , do well at T_{i+1 } without previous interaction experience with that particular task ) knowing that tasks are sequentially dependent . If the tasks are independent , having some initial interaction with each new task is perhaps the only way to solve the problem .

3 . In our setup , the number of episodes after which the environment / opponent changes is fixed . Moreover , we assume that the agent knows a priori the number of episodes or rounds after which the environment or opponent changes . This information is directly used by RL^2 .

---- Summary ----
This paper addresses the problem of learning to operate in non-stationary environments , represented as a Markov chain of distinct tasks . The goal is to meta-learn updates that are optimal with respect to transitions between pairs of tasks , allowing for few - shot execution time adaptation that does not degrade as the environment diverges ever further from the training time task set .

During learning , an inner loop iterates iterates over consecutive task pairs . For each pair , ( T_i , T_{i+1 } ) trajectories sampled from T_i are used to construct a local policy that is then used to sample trajectories from T_{i+1} . By calculating the outer- loop policy gradient with respect to expectations of the trajectories sampled from T_i , and the trajectories sampled from T_{i+1 } using the locally optimal inner- loop policy , the approach learns updates that are optimal with respect to the Markovian transitions between pairs of consecutive tasks .

The training time optimization algorithm requires multiple passes through a given sequence of tasks . Since this is not feasible at execution time , the trajectories calculated while solving task T_i are used to calculate updates for task T_{i+1 } and these updates are importance weighted w.r.t the sampled trajectories ' expectation under the final training - time policy .

The approach is evaluated on a pair of tasks . In the locomotion task , a six legged agent has to adapt to deal with an increasing inhibition to a pair of its legs . In the new RoboSumo task , agents have to adapt to effectively compete with increasingly competent components , that have been trained for longer periods of time via self - play .

It is clear that , in the locomotion task , the meta learning strategy maintains performance much more consistently than approaches that adapt through PPO - tracking , or implicitly by maintaining state in the RL^2 approach . This behaviour is less visible in the RoboSumo task ( Fig 5 . ) but it does seem to present . Further experiments show that when the adaptation approaches are forced to fight against each other in 100 round iterated adaptation games , the meta learning strategy is dominant . However , the authors also do point out that this behaviour is highly dependent on the number of episodes allowed in each game , and when the agent can accumulate a large amount of evidence in a given environment the meta learning approach falls behind adaptation through tracking . The bias that allows the agent to learn effectively from few examples precludes it from effectively using many examples .

---- Questions for author ----
Updates are performed from \theta to \ phi_{i+ 1 } rather than from \phi_i to \ phi_{i + 1} . Footnote 2 states that this was due to empirical observations of instability but it also necessitates the importance weight correction during execution time . I would like to know how the authors expect the sample in Eqn 9 to behave in much longer running scenarios , when \pi_{\phi} starts to diverge drastically from \pi_{\theta} but very few trajectories are available .

The spider - spider results in Fig. 6 do not support the argument that meta learning is better than PPO tracking in the few -shot regime . Do you have any idea of why this is ?

---- Nits ----
There is a slight muddiness of notation around the use of \tau in lines 7 & 9 in of Algorithm 1 . I think it should be edited to line up with the definition given in Eqn . 8.

The figures in this paper depend excessively and unnecessarily on color . They should be made more printer , and colorblind , friendly .

---- Conclusion ----
I think this paper would be a very worthy contribution to ICLR . Learning to adapt on the basis of few observations is an important prerequisite for real world agents , and this paper presents a reasonable approach backed up by a suite of informative evaluations . The quality of the writing is high , and the contributions are significant . However , this topic is very much outside of my realm of expertise and I am unfamiliar with the related work , so I am assigning my review a low confidence .

Thank you for carefully reading the paper and thoughtful questions . We have improved the notation and the color-coding in the figures . We answer the questions below :

- When \pi_{\phi} significantly diverges from \pi_{\theta} , the estimate given Eq. 9 would become of very high variance . In our setup , \pi_{\phi} was always at most a few gradient steps away from \pi_{\theta} in the parameter space . This gave difference in behaviors while keeping the effective sample size reasonable ( always more than 20 % ) . Much longer running scenarios may require a better estimator ( i.e. , of lower variance ) which should also take into account the sequential structure of the tasks ( e.g. , a particle filter ) .

- Good point , different methods yielded similar performance in the spider- spider experiments . This is because the agents tended to learn very similar behaviors regardless of the algorithm . The spectrum of behaviors learned by the agent highly depends on the morphology . From videos , we noticed that spiders always picked up a very particular fighting style , using front legs to kick the opponent and back legs to stabilize the posture , and never altered it during adaptation . This could be due to , perhaps , optimality of such behavior , but we did not further quantify this effect .


SUMMARY

The paper considers the problem of using cycle GANs to decipher text encrypted with historical ciphers . Also it presents some theory to address the problem that discriminating between the discrete data and continuous prediction is too simple . The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables .
The log loss of the GAN is replaced by a quadratic loss and a regularization of the Jacobian of the discriminator . Experiments show that the method is very effective .

REVIEW

The paper considers an interesting and fairly original problem and the overall discussion of ciphers is quite nice . Unfortunately , my understanding is that the theory proposed in section 2 does not correspond to the scheme used in the experiments ( contrarily to what the conclusion suggest and contrarily to what the discussion of the end of section 3 , which says that using embedding is assumed to have an equivalent effect to using the methodology considered in the theoretical part ) . Another important concern is with the proof : there seems to be an unmotivated additional assumption that appears in the middle of the proof of Proposition 1 + some steps need to be clarified ( see comment 16 below ) .
The experiments do not have any simple baseline , which is somewhat unfortunate .


DETAILED COMMENTS :

1 - The paper makes a few bold and debatable statements :

line 9 of section 1
" Such hand - crafted features have fallen out of favor ( Goodfellow et al. , 2016 ) as a
result of their demonstrated inferiority to features learned directly from data in end - to - end learning
frameworks such as neural networks "

This is certainly an overstatement and although it might be true for specific types of inputs it is not universally true , most deep architectures rely on a human-in- the-loop and there are number of areas where human crafted feature are arguably still relevant , if only to specify what is the input of a deep network : there are many domains where the notion of raw data does not make sense , and , when it does , it is usually associated with a sensing device that has been designed by a human and which implicitly imposes what the data is based on human expertise .

2 - In the last paragraph of the introduction , the paper says that previous work has only worked on vocabularies of 26 characters while the current paper tackles word level ciphers with 200 words . But , is n't this just a matter of scalability and only possible with very large amounts of text ? Is it really because of an intrinsic limitation or lack of scalability of previous approaches or just because the authors of the corresponding papers did not care to present larger scale experiments ?


3 - The discussion at the top of page 5 is difficult to follow . What do you mean when you say " this motivates the benefits of having strong curvature globally , as opposed to linearly between etc "
Which curvature are we talking about ? and what how does the " as opposed to linearly " mean ? Should we understand " as opposed to having curvature linearly interpolated between etc " or " as opposed to having a linear function " ? Please clarify .

4 - In the same paragraph : what does " a region that has not seen the Jacobian norm applied to it " mean ? How is a norm applied to a region ? I guess that what you mean is that the generator G might creates samples in a part of the space where the function F has not yet been learned and is essentially close to 0 . Is this what you mean ?

5 - I do not understand why the paper introduces WGAN since in the end it does not use them but uses a quadratic loss , introduced in the first display of section 4.3.

6 - The paper makes a theoretical contribution which supports replacing the sample y by a sample drawn from a region around y . But it seems that this is not used in the experiment and that the authors consider that the introduction of the embedding is a substitution for this . Indeed , in the last paragraph of section 3.1 , the paper says " we make the assumption that the training of the embedding vectors approximates random sampling similar to what is described in Proposition 1 " . This does not make any sense to me because the embedding vectors map each y deterministically to a single point , and so the distribution on the corresponding vectors is still a fixed discrete distribution . This gives me this impression that the proposed theory does not match what is used in the experiments .
( The last sentence of section 3.1 , which is commenting on this and could perhaps clarify the situation is ill formed with two verbs . )

7 - In the definitions : " A discriminator is said to perform uninformative discrimination " etc . -> It seems that the choice of the word uninformative would be misleading : an uninformative discrimination would be a discrimination that completely fails , while what the condition is saying it that it cannot perform perfect discrimination . I would thus suggest to call this " imperfect discrimination " .


8 - It seems that the same embedding is used in X space and in Y space ( from equations 6 and 7 ) . Is there any reason for that ? I would seem more natural to me to introduce two different embeddings since the objects are a priori different ...
Actually I do n't understand how the embeddings can be the same in the Vignere code case since time taken into account one one side .

9 - On the 5th line after equation ( 7 ) , the paper says " the embeddings ... are trained to minimize L_GAN and L_cyc , meaning ... and are easy to discriminate " -> This last part of the sentence seems wrong to me . The discriminator is trying to maximize L_GAN and so minimizing w.r.t. to the embedding is precisely trying to prevent to the discriminator to tell apart too easily the true elements from the estimated ones .
In fact the regularization of the Jacobian that will be preventing the discriminator to vary too quickly in space is more likely to explain the fact that the discrimination is not too easy to do between the true and mapped embeddings . This might be connected to the discussion at the top of page 5 . Since there are no experiments with alpha different than the default value = 10 , this is difficult to assess .

10 - The Vigenere cipher is explained again at the end of section 4.2 when it has already been presented in section 1.1

11 - Concerning results in Table 2 : I do not see why it would not be possible to compare the performance of the method with classical frequency analysis , at least for the character case .

12 - At the beginning of section 4.3 , the text says that the log loss was replaced with the quadratic loss , but without giving any reason . Could you explain why .

13 - The only comparison of results with and without embeddings is presented in the curves of figure 3 , for Brown- W with a vocabulary of 200 words . In that case it helps . Could the authors report systematically results about all cases ? ( I guess this might however be the only hard case ... )

14 - It would be useful to have a brief reminder of the architecture of the neural network ( right now the reader is just refered to Zhu et al. , 2017 ) : how many layers , how many convolution layers etc .
The same comment applies for the way the position of the letter / word in the text appear is in encoded in a feature that is provided as input to the neural network : it would be nice if the paper could provide a few details here and be more self contained . ( The fact that the engineering of the time feature can " dramatically " improve the performance of the network should be an argument to convince the authors that hand - crafted feature have not fallen out of favor completely yet ... )

15 - I disagree with the statement made in the conclusion that the proposed work " empirically confirms [ ...] that the use of continuous relaxation of discrete variable facilitates [ ...] and prevents [ ... ] " because for me the proposed implementation does not use at all the theoretical idea of continuous relaxation proposed in the paper , unless there is a major point that I am missing .


16 - I have two issues with the proof in the appendix

a ) after the first display of the last page the paper makes an additional assumption which is not announced in the statement of the theorem , which is that two specific inequality hold ...
Unless I am mistaken this assumption is never proven ( later or earlier ) . Given that this inequality is just " the right inequality to get the proof go through " and given that there are no explanation for why this assumption is reasonable , to me this invalidates the proof . The step of going from G ( S_y ) to S_ ( G ( y ) ) seems delicate ...

b ) If we accept these inequalities , the determinant of the Jacobian ( the notation is not defined ) of F at ( x_bar ) disappears from the equations , as if it could be assumed to be greater than one . If this is indeed the case , please provide a justification of this step .

17 - A way to address the issue of trivial discrimination in GANs with discrete data has been proposed in

Luc , P. , Couprie , C. , Chintala , S. , & Verbeek , J. ( 2016 ) . Semantic segmentation using adversarial networks . arXiv preprint arXiv:1611.08408.
The authors should probably reference this paper .


18 - Clarification of the Jacobian regularization : in equation ( 3 ) , the Jacobian computed seems to be w.r.t D composed with F while in equation ( 8 ) it is only the Jacobian of D. Which equation is the correct one ?

TYPOS :

Proposition 1 : the if - then statement is broken into two sentences separated by a full point and a carriage return .

sec. 4.3 line 10 we use a cycle loss * with a regularization coefficient * lambda=1 ( a piece of the sentence is missing )

sec. 4.3 lines 12 - 13 the learning rates given are the same at startup and after " warming up " ...

In the appendix :
3rd line of proof of prop 1 : I don ' understand " countably infinite finite sequences of vectors lying in the vertices of the simplex " -> what is countable infinite here ? The vertices ?













We would like to thank Reviewer 1 for providing such a high quality and clear review that has allowed us to greatly improve our paper . We hope the new draft of the paper and the clarifications and improvements made below serve to increase your rating of our work .

We address each point below :

1 . We completely agree that this was an overstatement and have replaced the line with the following " Across a number of domains , the use of hand - crafted features has often been replaced by automatic feature extraction directly from data using end - to - end learning frameworks ( Goodfellow et al. , 2016 ) . " We qualify the statement , restricting it to ‘a number of ’ domains of application , while acknowledging that automated feature extraction is not ubiquitous and removing any notion of superiority / inferiority of techniques .

2 . The issue of scalability is indeed an important one ; as past algorithms have scaled exponentially in the length of key and size of vocabulary . Prior work has generally relied upon ngram frequencies whose space grows exponentially with the vocabulary size rapidly sparsifying occurrences ; leading to rapidly decreasing information in these statistics . The other facet of increasing vocabulary size is the applicability to more modern techniques such as block ciphers where the vocabulary is expanded to hundreds or thousands of elements . We intended to show that our method does n’t completely collapse as vocab space increases ( while frequency analysis rapidly does , as we show in new baseline comparisons ) . We feel this is a valuable and important feature of the work .

3 . We clarified the discussion making things a little less verbose and trying to improve the flow of ideas . The curvature we refer to is that of the discriminator output with respect to its inputs ( we ’ve tried to clarify this ) ; the curvature of this region is important since it represents the strength of the training signal received by the generator . We were trying to make the point that WGAN ’s method of regularizing between the generated data and the true data may miss regions of the simplex that our model regularly traverses . We also note that others have pointed this out and have found benefits of regularizing more ‘ globally ’ or more ‘ broadly ’ across the simplex ( by this we mean other than exclusively between generated data and true data ; we have tried to make this clearer in the paper ) . We hope the changes are an improvement and that it reads more intelligibly . Thank you for raising this concern .

4 . Thank you for catching this , that was indeed a typo . We ’ve corrected to clarify that it is the curvature regularization being applied ( i.e. the WGAN regularization technique of forcing the norm of the Jacobian to 1 )

5 . We introduce WGAN because it is the inspiration for the last term of our L_GAN loss . The key insight we draw from WGAN is their use of a Jacobian norm regularization term ( we also refer to it as ‘ curvature regularization ’ since it is clearer ) .

6 . Thank you for pointing this out ; we ’ve done our best to make the precise use in our paper clear . The embeddings are not fixed during training , instead , they are parameters that are tuned throughout training . It is the stochasticity of these points that leads us to the suggestion that these points estimate random samples around fixed points . We came to this conclusion after observing that , as training progresses , the embeddings appear to ‘ settle ’ and remain bound within a tight region , yet are still moving . Perhaps an analogy to Hamiltonian MCMC or Metropolis - adjusted Langevin sampling as a comparison between noisy gradient updates and gradient - based sampling would improve the argument ? We ’ve updated the paper to include a clearer motivation of why we suggest jointly - trained embedding vectors might approximate sampling about fixed points . We ’ve updated the last sentence of section 3.1 to clarify precisely how we arrived at our conclusion .

7 . We refer to this behaviour of the discriminator as uninformative since it says : if we can re-discretize an element to the correct token , but the discriminator evaluates it as incorrect , then the discriminator is not informing on the underlying task when acting on the continuous space . We do n’t mean to say that it is ‘ entirely ’ uninformative of task , only that it demonstrates uninformative behaviour . We are willing to update the name to ‘ partially uninformative discrimination ’ if the reviewer feels this is an abuse of language ?

Continued in next comment .

8 . Indeed , we do use the same space for X and Y ( it ’s only the distribution over the spaces that changes ) . This is possible since in shift and Vigenere we replace each token of the input space with a different token from the same space . I.e. “ ABC ” -> “ GHI ” for a shift or “ ABC ” -> “ GGI ” for Vigenere with key “ 656 ” . This is why we use the same embeddings for both the plaintext data space and ciphertext space . Perhaps we should note that we did experiment with separating the embedding spaces and found little improvement ?

9 . We have corrected to ‘ maximizing ’ L_GAN . We analyze the effects of the Jacobian regularization effects in Section 2 as well as in Figure 3 ( right ) . You are correct that Jacobian regularization certainly helps with the problem and we cite three papers which mention this ; but in our experiments ( see Figure 3 comparison between embeddings and softmax ) we found that Jacobian regularization was complemented by our relaxed sampling technique .

10 . Thank you for pointing this out , we have removed Section 4.2

11 . We completely agree that comparison to standard frequency analysis should be shown and have added this to the table . As is made clear , CipherGAN out - performs frequency analysis by a large margin ( > 20 % ) . CipherGAN was able to crack all ciphers to nearly flawless accuracy ( save Vigenere Brown 200 , which is an extremely difficult case we use to stress test the technique ) .

12 . Absolutely ; we have added some of the details from Mao et al .

13 . You are correct , the only relevant experiment was on Vigenere with Brown 200 since it challenged the network ’s ability the most and exposed the divergence in performance between the two techniques .

14 . We have added a full description of the architecture in the Appendix .

15 . Hopefully the previous clarification resolves this critique .

16 . Both your points are correct , the previous version of the paper had a proof that was ‘ in between ’ two directions . One being an analogue to Ian ’s proof in the original GAN paper , and the other being an asymptotic argument that ended up being more elegant and easy to follow . In the updated paper we hope you find the new proof clearly articulated and thoroughly justified . Your point in a ) about G ( S_y ) to S_ ( G ( y ) ) appears in Lemma 1 ; the note about inequalities is clarified using Corollary 2 ; the note about the Jacobian is now stated in the premises of the proposition and we now only require the Jacobian to be near 1 and show that as it approaches 1 the upper and lower bounds squeeze to the same maximal value in the same place .

17 . Thank you , we have added a citation in the discrete gan section .

18 . Very good catch , thank you . We have correct Eq. 3

We have also addressed the typos pointed out by the reviewer .

The reviewer ’s principal concern seems to stem from the assumption of embeddings approximating sampling . We hope our clarification that our embeddings are non-fixed points and that experiments with Concrete samples produce nearly indistinguishable results give the review confidence in our methods . Additionally we hope the new proof convinces the reviewer and address the previous concerns ( which arose from the proof being incomplete at the time of submission ) . We hope that the reviewer finds confidence in both the theoretical contributions and the success of the experiments in order to raise the rating to one of acceptance .

Again , we sincerely appreciate such a detailed and exemplary critique of our work . Please inform us of any other changes that would improve our work .

The paper shows an application of GANs to deciphering text . The goal is to arrive at a ``` " hands free " approach to this problem ; i.e. , an approach that does not require any knowledge of the language being deciphered such as letter frequency and such . The authors start from a CycleGAN architecture , which may be used to learn mapping between two probability spaces . They point out that using GANs for discrete distributions is a challenging problem since it can lead to uninformative discriminants . They propose to resolve this issue by using a continuous embedding space to approximate ( or convert ) the discrete random variables into continuous random variables . The new proposed algorithm , called CipherGAN , is then shown to be stable and achieve deciphering of substitution ciphers and Vigenere ciphers .

I did not completely understand how the embedding was performed , so perhaps the authors could elaborate on that a bit more . Apart from that , the paper is well written and well motivated . It used some recent ideas in deep learning such as Cycle GANs and shows how to tweak them to make them work for discrete problems and also make them more stable . One comment would be that the paper is decidedly an applied paper ( and not much theory ) since certain steps in the algorithm ( such as training the discriminator loss along with the Lipschitz conditioning term ) are included because it was experimentally observed to lead to stability .

We thank Reviewer 2 for their helpful comments .

Thank you for pointing out the lack of clarity w.r.t. the embeddings . We have added more discussion about precisely how we treat the embeddings .

We hope that the reviewer will be able to assign more confidence to their review given the changes . Please inform us of any further changes that would improve the paper .

Again , we sincerely thank the reviewer for their time and support .


The paper proposed to replace the 2 - dim convolutions in CycleGAN by one dimension variant and reduce the filter sizes to 1 , while leave the generator convex embedding and using L2 loss function .

The proposed simple change help with the dealing of discrete GAN . The benefit of increased stability by adding Jacobian norm regularization term to the discriminator 's loss is nice .

The paper is well written . A few minor ones to improve :
* The original GAN was proposed / stated as min_max , while in Equation 1 did n't defined F and was not clear about min_ { F} . Similar for Equations 2 and 3.
* Define abbreviation when first appear , e.g. WGAN ( Wasserstein ... ) .
* Clarify x - and y- axis label in Figure 3.

We thank Reviewer 3 for their suggested improvements .

We have incorporated all three suggestions into the latest draft of the paper . Please inform us of any changes that would further improve the work .

Thank you again for your review .


This paper proposes an interesting model of self - play where one agent learns to propose tasks that are easy for her but difficult for an opponent . This creates a moving target of self - play objectives and learning curriculum .

The idea is certainly elegant and clearly described .
I do n't really feel qualified to comment on the novelty , since this paper is somewhat out of my area of expertise , but I did notice that the authors ' own description of Baranes and Oudeyer ( 2013 ) seems very close to the proposal in this paper . Given the existence of similar forms of self - play the key issue with paper I see is that there is no strong self - play baseline in the experimental evaluation . It is hard to tell whether this neat idea is really an improvement .

Is progress guaranteed ? Is it not possible for Alice to imemdiately find an easy task for her where Bob times out , gets no reward signal , and therefore is unable to learn anything ? Then repeating that task will loop forever without progress . This suggests that the adversarial setting is quite brittle .

I also find that the paper is a little light on the technical side .

We thank the reviewer for the constructive review . However , we would like to address several points raised :

“ Baranes and Oudeyer ( 2013 ) seems very close to the proposal in this paper . Given the existence of similar forms of self - play the key issue with paper I see is that there is no strong self - play baseline in the experimental evaluation ” .
- In B & O , one needs to construct a set of all possible tasks in the environment , and parameterize this set in such a way that it can be reasonably partitioned and sampled . It is not obvious how to do this in our problems without using extra domain knowledge . In our approach , however , tasks are discovered by an agent acting in the environment , thus eliminating the need of domain knowledge about the environment . For this reason , we cannot directly compare to the approach of B & O and no other forms of self - play exist , as far as we are aware .
Also note that it is not clear how to obtain the same sorts of guarantees we get in the tabular setting ( and the related intuitions about what the learning protocol is achieving ) with their method .

“ Is it not possible for Alice to immediately find an easy task for her where Bob times out , gets no reward signal , and therefore is unable to learn anything ? Then repeating that task will loop forever without progress . This suggests that the adversarial setting is quite brittle ” .
- An easy task means it only requires few actions for Alice to succeed . In repeat self - play , this means that Bob would only need a few actions to succeed also . So it is unlikely that Bob will keep failing on such easy tasks since even taking random actions would sometimes yield success on such easy tasks . The same is true for the reverse self - play because of the reversibility assumption ( Bob just needs to perform the opposite of Alice 's actions in reverse order ) .
In general however , our adversarial setting does assume that Alice and Bob are trained in sync . Similar to generative adversarial networks , if one of them gets too far ahead of the other , then it can impede training . However , our experiments demonstrate that the two of them can be successfully training is possible on non-trivial problems .

“ I also find that the paper is a little light on the technical side ” .
- We will add further technical details in the final revision .

The paper presents a method for learning a curriculum for reinforcement learning tasks . The approach revolves around splitting the personality of the agent into two parts . The first personality learns to generate goals for other personality for which the second agent is just barely capable -- much in the same way a teacher always pushes just past the frontier of a student ’s ability . The second personality attempts to achieve the objectives set by the first as well as achieve the original RL task .

The novelty of the proposed method is introduction of a teacher that learns to generate a curriculum for the agent . The formulation is simple and elegant as the teacher is incentivised to widen the gap between bob but pays a price for the time it takes which balances the adversarial behavior .

Prior and concurrent work on learning curriculum and intrinsic motivation in RL rely on GANs ( e.g. , automatic goal generation by Held et al. ) , adversarial agents ( e.g. , RARL by Pinto et al. ) , or algorithmic / heuristic methods ( e.g. , reverse curriculum by Florensa et al. and HER Andrychowicz et al . ) . In the context of this work , the contribution is the insight that an agent can be learned to explore the immediate reachable space but that is just within the capabilities of the agent . HER and goal generation share the core insight on training to reach goals . However , HER does generate goals beyond the reachable it instead relies on training on existing reached states or explicitly consider the capabilities of the agent on reaching a goal . Goal generation while learning to sample from the achievable frontier does not ensure the goal is reachable and may not be as stable to train .

As noted by the authors the above mentioned prior work is closely related to the proposed approach . However , the paper only briefly mentions this corpus of work . A more thorough comparison with these techniques should be provided even if somewhat concurrent with the proposed method . The authors should consider additional experiments on the same domains of this prior work to contrast performance .

Questions :
Do the plots track the combined iterations that both Alice and Bob are in control of the environment or just for Bob ?


“ Do the plots track the combined iterations that both Alice and Bob are in control of the environment or just for Bob ? ”
- The plots track the iterations / steps of Bob during target task episodes where a supervision from the environment given as a reward signal . The paradigm we consider is the RL equivalent of semi-supervised learning , with self - play being the unsupervised learning component . In this context , what matters is the number of labeled examples ( analogously : target task episodes ) used , rather than the number of unlabeled points ( i.e. self - play episodes ) . This RL paradigm was introduced in Finn et al . 2016 https://arxiv.org/abs/1612.00429 and we note that they also use this convention . We will clarify this in the final version .

The authors propose a framework for interactive language learning , called Mechanical Turker Descent ( MTD ) . Over multiple iterations , Turkers provide training examples for a language grounding task , and they are incentivized to provide new training examples that quickly improve generalization . The framework is straightforward and makes few assumptions about the task , making it applicable to potentially more than grounded language . Unlike recent works on " grounded language " using synthetic templates , this work operates over real language while maintaining interactivity .

Result show that the interactive learning outperforms the static learning baseline , but there are potential problems with the way the test set is collected . In MTD , the same users inherently provide both training and test examples . In the collaborative - only baseline , it is possible to ensure that the train and test sets are never annotated by the same user ( which is ideal for testing generalization ) . If train and test sets are split this way , it would give an unfair advantage to MTD . Additionally , there is potentially a different distribution of language for gamified and non-gamified settings . By aggregating the test set over 3 MTD scenarios and 1 static scenario , the test set could be skewed towards gamified language , again making it unfair to the baseline . I would like to see the results over the different test subsets , allowing us to verify whether MTD outperforms the baseline for the baseline 's test data .

- How is the test set broken down and what are the results in each part ?
The breakdown of the test set into the three portions ( MTD limit , Baseline Test and Pilot Study ) is shown in Table 5 . MTD ( with AC - Seq2Seq ) outperforms the baseline on each portion ( but by different amounts ) . Because MTD emphasizes a curriculum the data is generally more difficult than the baseline data which is why the MTD - trained models are better at predicting longer sequences ( see Table 4 ) , which is why performance of all models is worse on that data compared to the baseline data , and the gap between the models is bigger .


TL ; DR of paper : Improved human-in-the-loop data collection using crowdsourcing . The basic gist is that on every round , N mechanical turkers will create their own dataset . Each turker gets a copy of a base model which is trained on their own dataset , and each trained model is evaluated on all the other turker datasets . The top-performing models get a cash bonus , incentivizing turkers to provide high quality training data . A new base model is trained on the pooled - together data of all the turkers , and a new round begins . The results indicate an improvement over static data collection .

This idea of HITL dataset creation is interesting , because the competitive aspect incentivizes turkers to produce high quality data . Judging by the feedback given by turkers in the appendix , the workers seem to enjoy the competitive aspect , which would hopefully lead to better data . The results seem to suggest that MTD provides an improvement over non-HITL methods .

The authors repeatedly emphasize the " collaborative " aspect of MTD , saying that the turkers have to collaborate to produce similar dataset distributions , but this is misleading because the turkers do n't get to see other datasets . MTD is mostly competitive , and the authors should reduce the emphasis on a stretched definition of collaboration .

One questionable aspect of MTD is that the turkers somehow have to anticipate what are the best examples for the model to train with . That is , the turkers have to essentially perform the example selection process in active learning with relatively little interaction with the training model . While the turkers are provided immediate feedback when the model already correctly classifies the proposed training example , it seems difficult for turkers to anticipate when an example is too hard , because they have no idea about the learning process .

My biggest criticism is that MTD seems more like an NLP paper rather than an ICLR paper . I gave a 7 because I like the idea , but I would n't be upset if the AC recommends submitting to an NLP conference instead .

- Collaborative aspect of MTD
MTD is collaborative in the sense that the human players are building a shared model over multiple iterations , which is important for learning a good model . More specifically , the turkers collaborate in two ways . First , after each round , the data of all Turkers are merged to train a single model . It is clear that having 30 different models would make any of those models worse than a single model built with the collaborative baseline . Second , a Turker in the current round benefits from other Turkers in previous rounds , which ensures that worse - off Turkers from previous rounds can still compete . As we point out in the paper , this is related to the publication model of the research community , where researchers collaborate by using others ’ results to build research for the next conference ( in MTD , it is the same , where those results are in the form of data and models , rather than papers ) . We have indeed thought about showing the Turker ’s samples of the other datasets , and even implemented that at one point , but decided against in our experiments as it introduced unnecessary complexity . We will add text soon to the paper discussing these issues further .

- It is hard for Turkers to anticipate the correct curriculum .
Yes , currently the best feedback they get is the immediate output from the model when they type an example , they know whether the model can already do it or not , which we think is pretty good feedback . We also experimented with giving the predictions of the model ( rather than just a message of whether it could do it or not ) . We thought this would be good for expert labelers , but we decided against using it in our experiments because we thought it would be too complicated for casual Turkers . One could also show examples that the model is currently good or bad at from the last round , as mentioned in the previous point , but again this would add complexity to our experiments for this paper . However , we believe MTD is extensible in many ways . Note that these points are also already discussed in Appendix I .

- MTD seems more like an NLP paper rather than an ICLR paper
The core of our paper is about a method for learning representations for language grounding , which includes a pipeline of interaction with humans , a learning environment and an embodied agent which performs the learning . Although we evaluated on a language task , the same method could be used on many other tasks . We believe that both the method and the task that we chose are of interest to the ICLR audience , and all of the reviewers appear to agree that it is interesting ( and we as ML researchers like it too ! ) . Past ICLR conferences have had papers utilizing language , vision , speech , etc . In the call for papers it is also written that the following are relevant topics : “ applications in vision , audio , speech , natural language processing , robotics , neuroscience , or any other field ” . Hence , we think ICLR is one of the most suitable conferences for this type of work .

The paper provides an interesting data collection scheme that improves upon standard collection of static databases that have multiple shortcomings -- End of Section 3 clearly summarizes the advantages of the proposed algorithm . The paper is easy to follow and the evaluation is meaningful .

In MTD , both data collection and training the model are intertwined and so , the quality of the data can be limited by the learning capacity of the model . It is possible that after some iterations , the data distribution is similar to previous rounds in which case , the dataset becomes similar to static data collection ( albeit at a much higher cost and effort ) . Is this observed ? Further , is it possible to construct MTD variants that lead to constantly improving datasets by being agnostic to the actual model choice ? For example , utilizing only the priors of the D_{train_all} , mixing model and other humans ' predictions , etc .





- Q : Could the quality of the data be limited by the model ? Is this observed ?
We did not observe this yet , but it is possible -- the data is optimized for the model , it might not be optimal for e.g. a higher capacity model . On the other hand , since we optimize hyperparameters of the model each round , it can increase its capacity on the fly , which would mitigate this effect to some extent . If a high - capacity model cannot fit some complex data , however , e.g. due to optimization challenges , it is possible that the data distribution would gradually become static . In this case , the bottleneck is actually our optimization algorithms and models , rather than the data collection paradigm ; i.e. , MTD is doing its best in terms of coordinating the training data distribution to provide a good curriculum . Empirically , in Fig 3 we show learning curves for different models and approaches , which have not saturated after 5 rounds .

- Q : Is it possible to construct MTD variants that lead to constantly improving datasets by being agnostic to the actual model choice ?
We ’re not clear on how to do that , but if you have ideas then we ’d love to hear them ! The model is used to score the human ’s data , so you would need to replace it with a model - agnostic automatic scoring function somehow . The benefit of using a model in the loop , as we do , is that you are actually optimizing for what your model can do ( the human teacher is optimizing the curriculum for the model ) .


Post rebuttal phase ( see below for original comments )
================================================================================
I thank the authors for revising the manuscript . The methods makes sense now , and I think its quite interesting . While I do have some concerns ( e.g. choice of eta , batching may not produce a consistent gradient estimator etc. ) , I think the paper should be accepted . I have revised my score accordingly .

That said , the presentation ( esp in Section 2 ) needs to be improved . The main problem is that many symbols have been used without being defined . e.g. phi , q_phi , \pi , and a few more . While the authors might assume that this is obvious , it can be tricky to a reader - esp . someone like me who is not familiar with GANs . In addition , the derivation of the estimator in Section 3 was also sloppy . There are neater ways to derive this using RKHS theory without doing this on a d' dimensional space .

Revised summary : The authors present a method for estimating the gradient of some training objective for generative models used to sample data , such as GANs . The idea is that this can be used in a training procedure . The idea is based off the Stein 's identity , for which the authors propose a kernelized solution . The key insight comes from rewriting the variational lower bound so that we are left with having to compute the gradients w.r.t a random variable and then applying Stein 's identity . The authors present applications in Bayesian NNs and GANs .


Summary
================================================================
The authors present a method for estimating the gradient of some training objective
for generative models used to sample data , such as GANs . The idea is that this can be
used in a training procedure . The idea is based off the Stein 's identity , for which the
authors propose a kernelized solution . The authors present applications in Bayesian NNs
and GANs .




Detailed Reviews
================================================================

My main concern is what I raised via a comment , for which I have not received a response
as yet . It seems that you want the gradients w.r.t the parameters phi in ( 3 ) . But the
line immediately after claims that you need the gradients w.r.t the domain of a random
variable z and the subsequent sections focus on the gradients of the log density with
respect to the domain . I am not quite following the connection here .

Also , it does n't help that many of the symbols on page 2 which elucidates the set up
have not been defined . What are the quantities phi , q , q_phi , epsilon , and pi ?

Presentation
- Bottom row in Figure 1 needs to be labeled . I eventually figured that the colors
correspond to the figures above , but a reader is easily confused .
- As someone who is not familiar with BNNs , I found the description in Section 4.2
inadequate .

Some practical concerns :
- The fact that we need to construct a kernel matrix is concerning . Have you tried
batch verstions of these estimator which update the gradients with a few data points ?
- How is the parameter \eta chosen in practice ? Can you comment on the values that you
used and how it compared to the eigenvalues of the kernel matrix ?

Minor
- What is the purpose behind sections 3.1 and 3.2 ? They do n't seem pertinent to the rest
of the exposition . Same goes for section 3.5 ? I do n't see the authors using the
gradient estimators for out - of - sample points ?

I am giving an indifferent score mostly because I did not follow most of the details .

( We have revised the paper to make the presentation clearer . Please consider it and we would welcome your feedback . )

Thank you for your time for reviewing the paper . Again we are sorry that the presentation is not very clear in the first version of the manuscript . We have revised the paper according to your comments and added a brief introduction to Bayesian neural networks in the appendix .

We believe that our paper is highly novel and contains significant contributions ( as reviewer 3 commented ) . The paper is based on an important observation that an accurate gradient approximation method would be very helpful in many learning tasks that involve fitting an implicit distribution . As the other two reviewers pointed out , the proposed Stein gradient estimator is highly novel , and the experiments consider novel tasks that have not been considered in the literature , e.g. meta-learning for approximate inference , and entropy regularisation methods for GANs .

Now for your detailed comments :

1 . notations of phi , pi , etc .
We are sorry again for unclear presentation in the first version . In the latest version of the manuscript , we have explicitly defined them and provided a detailed derivation of the entropy gradient in eq ( 3 ) . Please let us know if it is still unclear .

2 . computing kernel matrix .
In section 4.3 we performed mini-batch training , and this means we only need to compute the gradient of log q on the mini-batch data . We found that with mini-batch size K=100 ( which is typical for deep learning tasks ) the computational cost is quite cheap , see the revised paper for a report of running time .

3 . choice of \eta .
Indeed for kernel methods , \eta needs to be tuned . However , our empirical observation indicates that for better performance of the Stein approach , small \eta is often preferred than large ones . Apparently , matrix inversion has numerical issues , so in our tests , we set \eta to be some small value but large enough to ensure numerical stability .

4 . purpose of 3.1 and 3.2 ( in the first version ) .
Since the Stein gradient estimator is kernel - based , we need to compare to existing kernel - based gradient estimator . Therefore we introduce them in 3.1 and 3.2 ( in the first version of the paper ) .

5 . purpose of 3.5 ( in the first version ) .
Our experiment 4.1 actually needs predictive estimators , since we want the particles of parallel chains to be independent of each other . The estimator derived in section 3.3 ( of the first version ) introduces correlations between the estimates of the score function at different locations .

Also in an on-going work , we apply the proposed Stein gradient estimator to training implicit generative models , which also requires predicting the gradient values . We already have some success on MNIST data , and now we are incorporating kernel learning techniques to scale it to massive data .

Thank you again for reading the feedback , and we look forward to hearing from you again .

Thank you for the positive review . We will think about how to revise the paper . Now on your further comments :

1 . consistency
We did not claim the proposed Stein gradient estimator is unbiased . This is because : 1 ) we used the V-statistics of KSD , 2 ) the fixed point of the MC approximated objective is not necessary the fixed point of the KSD . Similar things apply to the KDE and Score matching estimators . However , asymtotic consistency results have been proved for KDE , and Score matching the proof requires the kernel machine hypothesis set to contain the ground truth . This is not always the case , and our proposal might be prefered here because it is non-parametric .

We are currently working on establishing similar asymtotic consistency results for the Stein gradient estimator .

2 . preference of the RKHS story
Indeed if we directly start to talk about kernels then I would rather prefer the derivation of section 3.2 ( in the current version ) . However for people ( like engineers ) who are not familiar with the RKHS theory , the explanation of section 3.1 might be more intuitive , and that 's why I decided to include both of them . This is in similar spirit as to derive linear regression equations in many statistics textbooks : we first write down the solutions , and then notice that we can use the kernel trick to address the d' >> K problem .

Thank you for your feedback again and do let us know what we can do to improve the paper .

In this paper , the authors proposed the Stein gradient estimator , which directly estimates the score function of the implicit distribution . Direct estimation of gradient is crucial in the context of GAN because it could potentially lead to more accurate updates . Motivated by the Stein ’s identity , the authors proposed to estimate the gradient term by replacing expectation with the empirical counterpart and then turn the resulting formulation into a regularized regression problem . They also showed that the traditional score matching estimator ( Hyvarinen 2005 ) can be obtained as a special case of their estimator . Moreover , they also showed that their estimator can be obtained by minimizing the kernelized Stein discrepancy ( KSD ) which has been used in goodness - of - fit test . In the experiments , the proposed method is evaluated on few tasks including Hamiltonian flow with approximate gradients , meta-learning of approximate posterior samplers , and GANs using entropy regularization .

The novelty of this work consists of an approach based on score matching and Stein ’s identity to estimate the gradient directly and the empirical results of the proposed method on meta-learning for approximate inference and entropy regularized GANs . The proposed method is new and technically sound . The authors also demonstrated through several experiments that the proposed technique can be applied in a wide range of applications .

Nevertheless , I suspect that the drawback of this method compared to existing ones is computational cost . If it takes significantly longer to compute the gradient using proposed estimator compared to existing methods , the gain in terms of accuracy is questionable . By spending the same amount of time , we may obtain an equally accurate estimate using other methods . For example , the authors claimed in Section 4.3 that the Stein gradient estimator is faster than other methods , but it is not clear as to why this is the case . Hence , the comparison in terms of computational cost should also be included either in the text or in the experiment section .

While the proposed Stein gradient estimator is technically interesting , the experimental results do not seem to evident that it significantly outperforms existing techniques . In Section 4.2 , the authors only consider four datasets ( out of six UCI datasets ) . Also , in Section 4.3 , it is not clear what the point of this experiment is : whether to show that entropy regularization helps or the Stein gradient estimator outperforms other estimators .

Some comments :

- Perhaps , it is better to move Section 3.3 before Section 3.2 to emphasize the main contribution of this work , i.e. , using Stein ’s identity to derive an estimate of the gradient of the score function .
- Stein gradient estimator vs KDE : What if the kernel is not translation invariant ?
- In Section 4.3 , why did you consider the entropy regularizer ? How does it help answer the main hypothesis of this paper ?
- The experiments in Section 4.3 seems to be a bit out of context .


( We have revised the paper to make the presentation clearer . Please consider it and we would welcome your feedback . )

Thank you for your time for reviewing the paper . We appreciate your positive comment that the paper contains significant contributions to the community . The diversity of the experimental tasks show that gradient estimation is fundamental to many machine learning tasks , so we believe the proposed estimator is widely applicable as you pointed out .

Also , we would like to thank you for the suggestions on making the paper clearer . We have re-organised the presentation to emphasise the contribution of the Stein gradient estimator .

Now on your comments :

1 . Computation cost
We added two paragraphs in the manuscript for further discussions on this . In short , we discussed :

Comparisons between kernel methods and other ideas . It is also known that the denoising auto-encoder ( DAE ) , when trained with infinitesimal noise , also provides a score function estimator . However , this requires training the DAE , and depending on the neural network architecture , it can take significantly much more time compared to the kernel - based estimators which often have analytical solutions .

For the three kernel - based methods mentioned in the paper , both Score and Stein method require inverting a K* K matrix ( O ( K^3 ) time ) . All three methods require computing the kernel matrix ( O ( K^2 * d ) time ) . However in the BNN and GAN experiments , since d >> K , the cost is dominated by the kernel matrix computation , meaning that all three methods have similar computational costs . Indeed we reported the running times for the GAN experiments which are almost identical . Also adding the entropy regularisation only resulted in 1s / epoch more time compared to vanilla BEGAN , which is actually quite cheap .

2 . BNN experiment .
We have clearly shown that the Stein approach is significantly better than the other two gradient estimators . SGLD with small step -size is known to work well , and the Stein method works equally well in this case . To our knowledge , this is the first attempt of meta-learning for approximate samplers , and our results demonstrate that this direction is worth investigation . We strongly believe that with a better neural network structure our method can be improved .

Regarding the scale of the experiment : UCI datasets are standard benchmarks for Bayesian neural networks ( e.g. see the PBP paper , Hernandez -Lobato and Adams 2015 ) , and for datasets of this scale , we know that point estimates work worse . The size of the network is of the same scale as reported in Fig 5 ( left ) of ( Andrychowicz et al. 2016 ) .

3 . the GAN experiment in 4.3
The purpose of section 4.3 is to show the application of gradient estimation methods to tasks other than approximate inference ( 4.1 and 4.2 ) . Our goal here is to show : ( i ) by adding entropy regulariser it can help address the mode collapse problem , and ( ii ) the resulting diversity measure also reflects the approximation accuracy of the entropy gradient . In this experiment , we showed that the Stein approach works considerably better .

Indeed our ultimate goal of developing gradient estimation methods is to use them for training implicit generative models , and if successful , it can serve as an alternative to GAN - like approaches . In an on-going work , we already have some success on MNIST data . We are now working on incorporating kernel learning techniques to scale it to massive data .

4 . non translation invariant kernel case .
To our knowledge , it is rare for KDE methods to use non translation invariant kernels . And we have never seen consistency results proved for KDE gradient estimator in this case . But indeed connections between Stein and KDE methods is still a research question when using non translation invariant kernels .

Thank you again for reading the feedback and we look forward to hearing from you again .

This paper deals with the estimation of the score function , i.e. , the derivative of the log likelihood . Some methods were introduced and a new method using Stein identity was proposed . The setup of the trasnductive learning was introduced to add the prediction power to the proposed method . The method was used to several applications .

This is an interesting approach to estimate the score function for location models in a non-parametric way . I have a couple of minor comments below .

- Stein identity is the formula that holds for the class of ellipsoidal distribution including Gaussian distribution . I 'm not sure the term " Stein identity " is appropriate to express the equation ( 8 ) .
- Some boundary condition should be assumed to assure that integration by parts works properly . Describing an explicit boundary condition to guarantee the proper estimation would be nice .

( We have revised the paper to make the presentation clearer . Please consider it and we would welcome your feedback . )

Thank you for your time for reviewing the paper . We appreciate your comment that the proposed approach is interesting .

We would like to emphasise that our work is highly novel ( as both reviewers 2 and 3 pointed out ) .

1 . The Stein gradient estimator is a novel score function estimator , which , as you mentioned , generalises the score matching estimator . To our knowledge , this is the first ** non - parametric * * direct estimator : the KDE method , although also non- parametric , is an ** indirect * * method as it first estimates the density then takes the gradient .

2 . We applied the gradient estimation methods to a wide range of novel applications . To our knowledge , before our development , no paper has considered meta-learning tasks for approximate inference . Also , the entropy regularisation idea for GANs is novel , which cannot be done without an efficient gradient estimation method .

In an on-going work , we have applied the Stein gradient estimator to training implicit generative models , and small - scale experiments have shown promising results .

Now on your comments :

1 . Yes as you pointed out , the original Stein 's identity ( Stein 1972 , 1981 ) are for Gaussian distributions . However , the identity has been generalised to more general case . In equation ( 6 ) of the revised manuscript , we explicitly write down the integration by part derivations with the boundary condition assumed . Indeed for distributions with Gaussian - like tails almost any test function will satisfy the boundary condition .

2 . If you would like to see an counterexample : if q( x ) is Cauchy , then h( x ) should be less or equal than order of x^2 . But in practice , since the kernel in use often has decaying tails , it is generally the case that the boundary condition is satisfied .

Thank you again for reading the feedback and we look forward to hearing from you again .



The paper introduces a new memory mechanism specifically tailored for agent navigation in 2D environments . The memory consists of a 2D array and includes trainable read / write mechanisms . The RL agent 's policy is a function of the context read , read , and next step write vectors ( which are functions of the observation ) . The effectiveness of the proposed architecture is evaluated via reinforcement learning ( % of mazes solved ) . The evaluation included 1000 test mazes -- which sets a good precedent for evaluation in this subfield .

My main concern is the lack of experiments to test whether the agent really learned to localize and plan routes using it 's memory architecture . The downsampling experiment in Section 5.1 seems to indicate the contrary : downsampling the memory should lead to position aliasing which seems to indicate that the agent is not using its memory to store the map and its own location . I 'm concerned whether the proposed agent is actually employing a navigation strategy , as seems to be suggested , or is simply a good agent architecture for this task ( e.g. for optimization reasons ) . The short experiment in Appendix E seems to try and answer this question , but it 's results are anecdotal at best .

If good RL performance on navigation tasks is the ultimate goal then one can imagine an agent that directly copies the raw map observation ( world centric ) into memory and use something like a value iteration network or shortest path planning to plan routes . My point is that there are classical algorithms to solve navigation even in partially observable 2D grid worlds , why bother with deep RL here ?

Based on the author 's rebuttal I have revised the score to a 7 .

Dear Reviewer 1 ,

We thank you for your valuable comments and feedback . With respect to the concern over the lack of experiments , we have run experiments on 4 different memory - based environments and on each environment shown that the Neural Map exceeds the performance of previous baseline models , including LSTMs and Memory Networks . We think this has sufficiently demonstrated that the Neural Map demonstrates a performance improvement on memory - based navigation tasks .

We have also added additional results in appendix E demonstrating more episodic examples of the context - based retrieval on 3D tasks , including both egocentric and allocentric versions of the Neural Map . From these results , we can see that the Neural Map uses its context operator to mostly retrieve states around the starting position where the indicator is in full view . In addition , we further demonstrated that the indicator identity could be inferred with 100 % accuracy from the memory map using just a logistic regression model . To explore whether the Neural Map used its memory to accurately plan routes , we measured its ability to do backtracking . We showed that the egocentric variant of the Neural Map explores on average around 10 % more of the test mazes compared to an LSTM baseline .

With respect to the downsampling experiment in Section 5.1 , each wall in the environment takes one ' pixel ' in the map , so the reduction to 8x8 is only aliasing on average 2 positions compared to the 15x15 map . We argue that this is not a significant enough reduction in spatial resolution to cause a large decrease in performance , and the Neural Map can still navigate at this slightly larger spatial scale . The fact that , comparatively , the 6x6 map decreases significantly in performance due to larger aliasing ( aliasing up to 3x3 positions ) provides evidence that the Neural Map does utilize spatial information to navigate , but is robust to some small noise .

With respect to the point about motivating the use of Deep RL , we believe the " Repeating " environment shows the added capability of using memory - based Deep RL over using only traditional navigation algorithms . In this Repeating environment the indicator always changes to red after the first goal entry , meaning an agent that just writes / maps observations from its current position would not be capable of remembering the original indicator color after the first goal entry ( as on being reset to the initial position after the first goal entry , its observation would be overwritten with a potentially incorrect indicator color ) .

Similar to the repeating environment , we can envision many other applications of Deep RL within dynamic environments , where the environment is continuously changing . For example , an office environment where objects are constantly being moved and misplaced . In such an environment , a navigation system on a map of past observations might by itself not be sufficient , and a differentiable memory that writes its own features into memory could potentially learn things such as " if object X is not at Y , it is likely to be at Z " in an end - to - end manner without pre-specification by an expert .


This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments , assuming near perfect * relative localization provided by velocity . The model is more general than the cognitive maps ( Gupta et al , 2017 ) and builds on the NTM / DNC or related architectures ( Graves et al , 2014 , 2016 , Rae et al , 2017 ) thanks to the 2D spatial structure of the associative memory . Basically , it consists of a 2D - indexed grid of features ( the map ) M_t that can be summarized at each time point into read vector r_t , and used for extracting a context c_t for the current agent state s_t , compute ( thanks to an LSTM / GRU ) an updated write vector w_{t + 1}^{x , y} at the current position and update the map using that write vector . The position {x , y} is a binned representation of discrete or continuous coordinates . The absolute coordinate map can be replaced by a relative ego-centric map that is shifted ( just like in Gupta et al , 2017 ) as the agent moves .

The experiments are exhaustive and include remembering the goal location with or without cues ( similarly to Mirowski et al , 2017 , not cited ) in simple mazes of size 4 x4 up to 8 x8 in the 3D Doom environment . The most important aspect is the capability to build a feature map of previously unseen environments .

This paper , showing excellent and important work , has already been published on arXiv 9 months ago and widely cited . It has been improved since , through different sets of experiments and apparently a clearer presentation , but the ideas are the same . I wonder how it is possible that the paper has not been accepted at ICML or NIPS ( assuming that it was actually submitted there ) . What are the motivations of the reviewers who rejected the paper - are they trying to slow down competing research , or are they ignorant , and is the peer review system broken ? I quite like the formulation of the NIPS ratings : " if this paper does not get accepted , I am considering boycotting the conference " .

* The noise model experiment in Appendix D is commendable , but the noise model is somewhat unrealistic ( very small variance , zero mean Gaussian ) and assumes only drift in x and y , not along the orientation . While this makes sense in grid world environments or rectilinear mazes , it does not correspond to realistic robotic navigation scenarios with wheel skid , missing measurements , etc ... Perhaps showing examples of trajectories with drift added would help convince the reader ( there is no space restriction in the appendix ) .

Dear Reviewer 3 ,

Thank you for the strong support and for your comments and feedback . We understand that the noise model is to some extent simplistic compared to those found in robotics applications , but we argue that it does at least demonstrate that the Neural Map is robust to some degree of drift / aliasing in its position estimate . We have added a figure in Appendix D showing example trajectories from the noisy model .

We have also added an analysis of the memory in the appendix where we demonstrate that the context operator is mainly used to address the positions near the starting state , where the indicator color is in full view . We also demonstrate the improved ability of the Neural Map to explore the test mazes , with the egocentric Neural Map variant exploring on average 10 % more than an LSTM baseline .


# Summary
This paper presents a new external - memory - based neural network ( Neural Map ) for handling partial observability in reinforcement learning . The proposed memory architecture is spatially - structured so that the agent can read / write from / to specific positions in the memory . The results on several memory - related tasks in 2D and 3D environments show that the proposed method outperforms existing baselines such as LSTM and MQN / FRMQN .

[ Pros ]
- The overall direction toward more flexible / scalable memory is an important research direction in RL.
- The proposed memory architecture is new .
- The paper is well - written .

[ Cons ]
- The proposed memory architecture is new but a bit limited to 2D/3D navigation tasks .
- Lack of analysis of the learned memory behavior .

# Novelty and Significance
The proposed idea is novel in general . Though [ Gupta et al . ] proposed an ego-centric neural memory in the RL context , the proposed memory architecture is still new in that read / write operations are flexible enough for the agent to write any information to the memory , whereas [ Gupta et al . ] designed the memory specifically for predicting free space . On the other hand , the proposed method is also specific to navigation tasks in 2D or 3D environment , which is hard to apply to more general memory - related tasks in non-spatial environments . But , it is still interesting to see that the ego-centric neural memory works well on challenging tasks in a 3D environment .

# Quality
The experiment does not show any analysis of the learned memory read / write behavior especially for ego-centric neural map and the 3D environment . It is hard to understand how the agent utilizes the external memory without such an analysis .

# Clarity
The paper is overall clear and easy - to- follow except for the following . In the introduction section , the paper claims that " the expert must set M to a value that is larger than the time horizon of the currently considered task " when mentioning the limitation of the previous work . In some sense , however , Neural Map also requires an expert to specify the proper size of the memory based on prior knowledge about the task .

Dear Reviewer 2 ,

We thank you for your valuable comments and feedback .

We have added an analysis of the memory in the appendix E where we demonstrate more episodic examples of the context - based retrieval on 3D tasks , including both egocentric and allocentric versions of the Neural Map . From these results , we can see that the Neural Map uses its context operator to mostly retrieve states around the starting position where the indicator is in full view . In addition , we further demonstrated that the indicator identity could be inferred with 100 % accuracy from the memory map using just a logistic regression model .
To explore whether the Neural Map used its memory to accurately plan routes , we measured its ability to do backtracking . We demonstrated the improved ability of the Neural Map to explore the test mazes , with the egocentric Neural Map variant exploring on average 10 % more than an LSTM baseline .

As for setting the memory size , we argue that in many cases it can be easier for an agent designer to specify spatial distances than the time horizon of a task . For example , you could have an agent operating within a household where the agent designer only has to set the spatial extent of the map to represent an area at least as large as the house . On the other hand , estimating how long in time it might take an agent to do a task such as object collection would require knowing things such as e.g. how fast the robot navigates the house , how long it takes to grasp the object , etc .
Although the memory architecture is limited to 2D/3D environments , we argue that those environments encompass a large portion of real world applications of Deep RL . The Neural Map could potentially be generalized to a memory over graphs but we leave this extension to future work .


Overall , the idea of this paper is simple but interesting . Via weighted mean NLL over retrieved neighbors , one can update parameters of output network for a given query input . The MAP interpretation provides a flexible Bayesian explanation about this MbPA .

The paper is written well , and the proposed method is evaluated on a number of relevant applications ( e.g. , continuing learning , incremental learning , unbalanced data , and domain shifts . )

Here are some comments :
1 MbPA is built upon memory . How large should it be ? Is it efficient to retrieve neighbors for a given query ?
2 For each test , how many steps of MbPA do we need in general ? Furthermore , it is a bit unfair for me to retrain deep model , based on test inputs . It seems that , you are implicitly using test data to fit model .


Thank you for your review . Please find below our response and clarifications .

1 ) MbPA is built upon memory . How large should it be ?

* The optimal memory size is task dependent , but in general the larger the memory the better . However , performance saturates at a given point .
* A nice property of the model ( as shown in the continual and incremental learning setups ) is that performance degrades gracefully as memory size decreases . For continual learning , even storing 1 % of data seen on a task boosts performance significantly .
* One important aspect to note is that the smaller the memory the more important it becomes to add regularization to prevent overfitting to the local context , as explained in Section 2.1 . This is the case of the language modeling experiments .
* For the Image Net experiments we show how performance varies with memory size in Fig 6 ( Appendix ) . We will include a similar evaluation for continual learning and language modeling tasks .

2 ) Is it efficient to retrieve neighbors for a given query ?

* In this work it is the cost of an exact nearest neighbour search . Which is linear in memory size . We see that the cost of retrieving neighbours is negligible compared to the rest of the model ( eg. the inner optimisation ) . For eg. on PTB language modeling with a cache size of 5000 , the content based lookup is about ~ 20us , and each step of optimization is ~ 1 ms on one GPU .
* Fast approximate knn search can be used , but performance could degrade depending on the recall of the approximate search . This would be a nice direction for future work .
* One of the advantages of not querying the memory at training time , is that we avoid this cost .

3 ) For each test , how many steps of MbPA do we need in general ?

* This is a hyper- parameter of the model . Across all tasks , we observed that a small number of iteration is sufficient , between 5 and 20 . However , we see noticeable gains with even 1 step .

4 ) Furthermore , it is a bit unfair for me to retrain deep model , based on test inputs . It seems that , you are implicitly using test data to fit model .

* Many algorithms have a clean split between train and test . They are unable to adapt to shifts in distribution . We are interested specifically in studying algorithms that are capable of adapting to domain shift . Or , to leverage the temporal correlation during an evaluation episode .
* We only do this in the language model example , which deals with quickly adapting to a change in the data distribution at test time . The effect of online adaptation during test time has been long studied in this task , solutions dating back to Dynamic Evaluation ( A. Graves ’ thesis ) . Naturally , all these approaches use the test data in a causal way ( as in online learning ) , meaning , only the examples that have been processed are available for training .
* Note that we ’re comparing with many models that also use the observed test samples to adapt their predictions . The data seen at each test example is thus consistent across all baselines .

We will update the text to take into account all clarifications above .


This paper proposes a non-parametric episodic memory that can be used for the rapid acquisition of new knowledge while preserving the old ones . More specially , it locally adapts the parameters of a network using the episodic memory structure .

Strength :
+ The paper works on a relevant and interesting problem .
+ The experiment sections are very thorough and I like the fact that the authors selected different tasks to compare their models with .
+ The paper is well - written except for sections 2 and 3.
Weakness and Questions :
- Even though the paper addresses the interesting and challenging problem of slow adaption when distribution shifts , their episodic memory is quite similar ( if not same as ) to the Pritzel et al. , 2017 .
- In addition , as the author mentioned in the text , their model is also similar to the Kirkpatrick et al. , 2017 , Finn et al. , 2017 , Krause et al. , 2017 . That would be great if the author can list " explicitly " the contribution of the paper with comparing with those . Right now , the text mentioned some of the similarity but it spreads across different sections and parts .
- The proposed model does adaption during the test time , but other papers such as Li & Hoiem , 2016 handles the shift across domain in the train time . Can authors say sth about the motivation behind adaptation during test time vs . training time ?
- There are some inconsistencies in the text about the parameters and formulations :
-- what is second subscript in {v_i}_i ? ( page 2 , 3rd paragraph )
-- in Equation 4 , what is the difference between x_c and x ?
-- What happened to $ x $ in Eq 5 ?
-- The " − " in Eq. 7 does n't make sense .
- Section 2.2 , after equation 7 , the text is not that clear .
- Paper is well beyond the 8 - page limit and should be fitted to be 8 pages .
- In order to make the experiments reproducible , the paper needs to contain full details ( in the appendix ) about the setup and hyperparameters of the experiments .

Others :
Do the authors plan to release the codes ?


------------------------------------
------------------------------------
Update after rebuttal :
Thanks for the revised version and answering my concerns .
In the revised version , the writing has been improved and the contribution of the paper is more obvious .
Given the authors ' responses and the changes , I have increased my review score .

A couple of comments and questions :
1 . Can you explain how / why $ x_c$ is replaced by $ h_k$ in eq_7 ?
2 . In the same equation ( 7 ) , how $ \log p( v_k | h_k , \theta_x , x ) $ will be calculated ? I have some intuition but not sure . Can you please explain ?
3. in equation ( 8 ) , what happened to $ x $ in log p( .. ) ?
4 . How figure 2 is plotted ? based on a real experiment ? if yes , what was the setting ? if not , how ?
5 . It 'd be very useful to the community if the authors decide to release their codes .



Thank you for your review . Please find below our response and clarifications . The comment has been split into two to ensure we are under the comment character limit .

1 ) Even though the paper addresses the interesting and challenging problem of slow adaptation when distribution shifts , their episodic memory is quite similar ( if not same as ) to the Pritzel et al. , 2017 .

* Our memory module is indeed essentially the same as that of Pritzel et al , 2017 , differing only on how the keys are obtained . The keys are embeddings computed from our parametric model ( embedding + output networks ) trained directly on the target task instead of relying on gradients through the memory . Note that several other works ( cited in the manuscript ) use very similar memory architectures . We do not claim the memory as one of our contributions , instead , the novelty lies in the use of the memory as a way of enhancing powerful parametric models . We will further clarify this in the text .

2 ) In addition , as the author mentioned in the text , their model is also similar to the Kirkpatrick et al. , 2017 , Finn et al. , 2017 , Krause et al. , 2017 . That would be great if the author can list " explicitly " the contribution of the paper with comparing with those . Right now , the text mentioned some of the similarity but it spreads across different sections and parts .

* We will include a detailed description of our contributions , and concentrate ( and expand ) the relation with previous work in Section 3.
* The contributions of our work are : ( i ) proposing an architecture for enhancing powerful parametric models to include a fast adaptation mechanism to cope with changes in the task at hand ( ii ) we establish connections of our method with attention mechanisms frequently used for querying memories ( iii ) we present a bayesian interpretation of the method allowing a principled form regularization ( iv ) we evaluate the method in a range of different tasks : continual learning ( pmnist ) , incremental learning ( imagenet ) and data distribution shifts ( language ) , obtaining promising results .
* The only similarity with Krause et al . is that we too use a memory buffer in the context of language modelling . Their method of using the memory is via a mixture of experts system to deal with recent words for language models . We do compare to this baseline for our LM experiments , however their method does not deal with the problem of distributional shifts and cannot be applied to continual or incremental learning set ups .
* Finn et al. devises MAML - a way of doing meta-learning over a distribution of tasks . Both of our methods extend the classic fine - tuning technique used in domain adaptation type of ideas ( e.g. fit a given neural network to a small set of new data ) . Their algorithm aims at learning an easily adaptable set of weights , such that given a small amount of training data for a given task following the training distribution , the fine-tuning procedure would effectively adapt the weights to this particular task . Their work does not use any memory or per-example adaptation and is not based on a continual ( life-long ) learning setting . In contrast , our work , aims at augmenting a powerful neural network with a fine-tuning procedure that is used at inference only . The idea is to enhance the performance of the parametric model while maintaining it 's full training .
* EWC , developed in Kirkpatrick et al . 2017 is powerful method of doing continual learning across tasks . The algorithm works by learning a new task with an additional loss forcing the model to stay close to the solution found on the previous task . This method makes no use of memory or local adaptation , requiring instead the storing of weights and fisher matrices for each task seen . We compare to this method for our continual learning tasks as a very competitive baseline . MbPA does not rely on storing past weights or fisher matrices . We show comparable performance with even 100 examples stored per task and show how these methods are orthogonal and can be combined . One similarity we do note is that adding a regularization term to the local loss of MbPA can be seen as a local version or approximation of the EWC loss term - i.e. forcing the model to stay close to the solution found at training time .



3 ) The proposed model does adaptation during the test time , but other papers such as Li & Hoiem , 2016 handles the shift across domain in the train time . Can authors say sth about the motivation behind adaptation during test time vs . training time ?

* The work “ Learning without forgetting ” by Li & Hoiem is a simple and effective method for avoiding catastrophic forgetting . However , in our view , it does n’t guarantee that the internal representations would be preserved and does n’t show any evidence in this direction .
* Our motivation is two -fold . First , we want our model to be able to consolidate the knowledge and be able to perform well without relying on the memory content . The memory then serves to boost performance by focusing the weights on memory relevant to the prediction at hand . Second , adapting the model during training is computationally very demanding ( e.g. language modeling , imagenet ) .
* Further , adaptation at test time for language modelling has strong established baselines such as Krause et al 2017 . We thus wanted a comparable setting to the reported baselines .

4 ) Paper is well beyond the 8 - page limit and should be fitted to be 8 pages .

* ICLR has a soft page limit . We are aware that the text is long , but we did n’t want to leave out details on the experimental settings . We will take this comment into account and edit the text as needed after the clarifications mentioned here are added in , in an attempt to reduce the length of the paper by moving a few things into an appendix .

5 ) In order to make the experiments reproducible , the paper needs to contain full details ( in the appendix ) about the setup and hyperparameters of the experiments .

* We currently include details on the hyper- parameter selection procedure , and provide the best performing options . We will further clarify if anything is missing and add details to the appendix .

Further , thank you for pointing out typos and inconsistencies .

* We will correct this in the paper and clarify subscripts and the text in section 2.2.
* The negative sign in eq 7 is a typo .
* x is the input being regressed or classified whereas x_c ( " c " is context ; we will clarify this ) is the input that was used to create embedding h_c stored in memory ( with value v_c ) .

We will update the text to take into account all clarifications above .

This article introduces a new method to improve neural network performances on tasks ranging from continual learning ( non-stationary target distribution , appearance of new classes , adaptation to new tasks , etc ) to better handling of class imbalance , via a hybrid architecture between nearest neighbours and neural net .
After an introduction summarizing their goal , the authors introduce their Model -based parameter adaptation : this hybrid architecture enriches classical deep architectures with a non-parametric “ episodic ” memory , which is filled at training time with ( possibly learned ) encodings of training examples and then polled at inference time to refine the neural network parameters with a few steps of gradient in a direction determined by the closest neighbours in memory to the input being processed . The authors justify this inference - time SGD update with three different interpretations : one linked in Maximum A Posteriori optimization , another to Elastic Weight Regularisation ( the current state of the art in continual learning ) , and one generalising attention mechanisms ( although to be honest that later was more elusive to this reviewer ) . The mandatory literature review on the abundant recent uses of memory in neural networks is then followed by experiments on continual learning tasks involving permuted MNIST tasks , ImageNET incremental inclusion of classes , Image Net unbalanced , and two language modeling tasks .

This is an overall very interesting idea , which has the merit of being rather simple in its execution and can be combined with many other methods : it is fully compatible with any optimiser ( e.g. ADAM ) and can be tacked on top of EWC ( which the authors do ) . The justification is clear , the examples reasonably thorough . It is a very solid paper , which this reviewer believes to be of real interest to the ICLR community .


The following important clarifications from the authors could make it even better :
* Algorithm 1 in its current form seems to imply an infinite memory , which the experiments make clear is not the case . Therefore : how does the algorithm decide what entries to discard when the memory fills up ?
* In most non-trivial settings , the parameter $ gamma$ of the encoding is learned , and therefore older entries in the memory lose any ability to be compared to more recent encodings . How do the authors handle this obsolescence of the memory , other than the trivial scheme of relying on KNN to only match recent entries ?
* Because gamma needs to be “ recent ” , this means “ theta ” is also recent : could the authors give a good intuition on how the two sets of parameters can evolve at different enough timescales to really make the episodic memory relevant ? Is it anything else than relying on the fact that the lower levels of a neural net converge before the upper levels ?
* Table 1 : could the authors explain why the pre-trained Parametric ( and then Mixture ) models have the best AUC in the low - data regime , whereas MbPA was designed very much to be superior in such regimes ?
* Paragraph below equation ( 5 ) , page 3 : why not including the regularisation term , whereas the authors just went to great pain to explain it ? Rationale ? Not including it is also akin to using an improper non-information prior on theta^x independent of theta , which is quite a strong choice to be made “ by default ” .
* The extra complexity of choosing the learning rate alpha_ M and the number of MpAB steps is worrying this reviewer somewhat . In practice , in Section 4.1 the authors explain using grid search to tune the parameters . Is this reviewer correct in understanding that this search is done across all tasks , as opposed to only the first task ? And if so , does n’t this grid search introduce an information leak by bringing information from the whole pre-determined set of task , therefore undermining the very “ continuous learning ” aim ? How do the algorithm performs if the grid search is done only on the first task ?
* Figure 3 : the text could clarify that the accuracy is measured across all tasks seen so far . It would be interesting to add a figure ( in the Appendix ) showing the evolution of the accuracy * per task * , not just the aggregated accuracy .
* In the related works linking neural networks to encoded episodic memory , the authors might want to include the stream of research on HMAX of Anselmi et al 2014 ( https://arxiv.org/pdf/1311.4158.pdf) , Leibo et al 2015 ( https://arxiv.org/abs/1512.08457), and Blundell et al 2016 ( https://arxiv.org/pdf/1606.04460.pdf ).

Minor typos :
* Figure 4 : the title of the key says “ New / Old ” but then the lines read , in order , “ Old ” then “ New ” -- it would be nicer to have them in the same order .
* Section 5 : missing period between " ephemeral gradient modifications " and " Further " .
* Section 4.2 , parenthesis should be " perform well across all 1000 classes " , not " all 100 classes " .

With the above clarifications , this article could become a very remarked contribution .


Dear Authors and AC

Thank you for your detailed answers -- having to split in two comments due to length shows how seriously you take it :)
Between them and the fact that my mind kept wandering back to the ideas in this paper during the holidays , I am happy to maintain my score of 8 - Top 50 % papers .

Thank you for your review . Please find below our response and clarifications . The responses have been split to ensure we are under the ICLR comment character limit .

1 ) Algorithm 1 in its current form seems to imply an infinite memory , which the experiments make clear is not the case . Therefore : how does the algorithm decide what entries to discard when the memory fills up ?

* In the current implementation we simply treat the memory as a circular buffer , in which we overwrite the oldest data as the memory gets full . We will clarify this on the text .
* Deciding what to store ( or overwrite ) is indeed a very interesting question that we did not explore and will address in future work . We evaluated a few heuristics ( e.g. storing only examples with high training loss ) that did not perform better than the circular buffer described above .

2 ) In most non-trivial settings , the parameter $ gamma$ of the encoding is learned , and therefore older entries in the memory lose any ability to be compared to more recent encodings . How do the authors handle this obsolescence of the memory , other than the trivial scheme of relying on KNN to only match recent entries ?

* Having a stable ( or slow changing ) network is important for being able to have long term recall . This could be justified ( as the reviewer mentions ) by the fact that lower level parameters converge faster than those in the higher part of the network . Hence , it is inevitable some memory obsolescence in the beginning of training . This is also the case on humans as infant amnesia could be explained as memories stored with an old ( not consolidated ) network that can not be recovered later in life . We will include a short comment further clarifying this important point .
* An alternative approach would be to rely on replay of raw data ( e.g. store the input images from pixels ) . A downside is that , unlike internal activations ( embeddings ) , replaying raw data requires a large amount of storage . However many artificial systems do it ( e.g. DQN for RL ) . If we store raw data , we could still base our look - ups on a distance in the embedding space in order to obtain a semantic ( more relevant ) metric . We would replay the memories to prevent catastrophic forgetting and periodically recompute the embeddings to keep them up to date . We did not implement this variant .

3 ) Table 1 : could the authors explain why the pre-trained Parametric ( and then Mixture ) models have the best AUC in the low - data regime , whereas MbPA was designed very much to be superior in such regimes ?

* Note that this happens only for the classes that were used during pre-training . The result makes sense : the initial parametric model performs very well on the classes that was pre-trained on . The memory is initially empty , so adapting the predictions of parametric model ( via MbPA or the mixture model ) using few examples slightly degrades its performance in the beginning . This quickly changes as more examples are collected .
* On the other hand , for the new classes , relying on the memories massively improves performance even when few examples have been stored .

4 ) Paragraph below equation ( 5 ) , page 3 : why not including the regularisation term , whereas the authors just went to great pain to explain it ? Rationale ? Not including it is also akin to using an improper non-information prior on theta^x independent of theta , which is quite a strong choice to be made “ by default ” .

* We wrote it in this way for ease of explanation and developed later in Section 2.1 , as we only talk about the bayesian interpretation then . We will change the text accordingly .



5 ) The extra complexity of choosing the learning rate alpha_ M and the number of MbPA steps is worrying this reviewer somewhat . In practice , in Section 4.1 the authors explain using grid search to tune the parameters . Is this reviewer correct in understanding that this search is done across all tasks , as opposed to only the first task ? And if so , does n’t this grid search introduce an information leak by bringing information from the whole pre-determined set of task , therefore undermining the very “ continuous learning ” aim ? How do the algorithm performs if the grid search is done only on the first task ?

* We agree with the reviewer : setting the hyper- parameters would leak information from the future tasks . We do not do this in our experiments .
* The hyper-parameters were obtained using different variants of permuted MNIST , following the standard practice for continual learning .
* It is worth noting that we empirically found that MbPA is not very sensitive to the choice other parameters such as inner learning rate or number of steps ( especially when combined with the regularization term or EWC ) . The tuning was required more for the EWC baseline where there is a tradeoff between learning new tasks and remembering old ones based on the weighting of the loss . For MbPA for CL we found any number of steps between 5 - 10 worked well with high learning rates between 0.1 and 1.0.
* For MbPA , we reported several hyper- parameters ( i.e memory size ) to give a feel of the sensitivity of the algorithm .

6 ) Figure 3 : the text could clarify that the accuracy is measured across all tasks seen so far . It would be interesting to add a figure ( in the Appendix ) showing the evolution of the accuracy * per task * , not just the aggregated accuracy .

* We will include this figure and clarify this in the text .
* For the EWC baseline we find that ( as mentioned above ) the per-task curves are very different based on which tasks you care about more ( e.g trivially setting the EWC penalty to a high value would give near perfect accuracy on the first task and no learning on the others ) . The only way to tune is to in fact , look at final average accuracy on ( another , validation set ) of permuted pixels and then apply that to the final test version . For MbPA , we found it shows gradual forgetting across tasks based on how many examples are stored per task .

7 ) In the related works linking neural networks to encoded episodic memory , the authors might want to include the stream of research on HMAX of Anselmi et al 2014 ( https://arxiv.org/pdf/1311.4158.pdf) , Leibo et al 2015 ( https://arxiv.org/abs/1512.08457), and Blundell et al 2016 ( https://arxiv.org/pdf/1606.04460.pdf ).

* Thank you for the links to relevant work - we will include all these references .

We will update the text to take into account all clarifications above and typos mentioned .

The paper proposes to combine two approaches to compress deep neural networks - distillation and quantization . The authors proposed two methods , one largely relying on the distillation loss idea then followed by a quantization step , and another one that also learns the location of the quantization points . Somewhat surprisingly , nobody has combined the two approaches before , which makes this paper interesting . Experiments show that both methods work well in compressing large deep neural network models for applications where resources are limited , like on mobile devices .

Overall I am mostly OK with this paper but not impressed by it . Detailed comments below .

1 . Quantizing with respect to the distillation loss seems to do better than with the normal loss - this needs more discussion .
2 . The idea of using the gradient with respect to the quantization points to learn them is interesting but not entirely new ( see , e.g. , " Matrix Recovery from Quantized and Corrupted Measurements " , ICASSP 2014 and " OrdRec : An Ordinal Model for Predicting Personalized Item Rating Distributions " , RecSys 2011 , although in a different context ) . I also wonder if it would work better if you can also allow the weights to move a little bit ( it seems to me from Algorithm 2 that you only update the quantization points ) . How about learning them altogether ? Also this differentiable quantization method does not really depend on distillation , which is kind of confusing given the title .
3 . I am a little bit confused by how the bits are redistributed in the second method , as in the end it seems to use more than the proposed number of bits shown in the table ( as recognized in section 4.2 ) . This makes the comparison a little bit unfair ( especially for the CIFAR 100 case , where the " 2 bits " differentiable quantization is actually using 3.23 bits ) . This needs more clarification .
4 . The writing can be improved . For example , the concepts of " teacher " and " student " is not clear at all in the abstract - consider putting the first sentence of Section 3 in there instead . Also , the first sentence of the paper reads as " ... have showed tremendous performance " , which is not proper English . At the top of page 3 I found " we will restrict our attention to uniform and non-uniform quantization " . What are you not restricting to , then ?

Slightly increased my rating after reading the rebuttal and the revision .

1 . The importance of distillation loss :
Across all our experiments , using distillation loss in quantizing was superior to using ‘ normal ’ loss . This is one of our main findings , and is extremely consistent across experiments . We have given two sets of experiments to illustrate this , but we will add more examples .
2 . Differentiation w.r.t. quantization points :
We will discuss the connection with the RecSys 2011 and ICASSP papers in the next revision .
Alternating optimization w.r.t. weights and locations of quantization points is a neat idea , which we are experimenting with ; we will present results on it in the next revision .
Distillation is actually present in Differentiable Quantization ( DQ ) , since we are starting from a distilled full - precision model . It is true however that DQ can be applied independently from distillation . We will clarify this point .
3 . Re-distribution of bits :
Fractional numbers appear since there are a couple of different techniques being concurrently : 1 ) we preferentially re-distribute bits proportionally to the gradient norm ; 2 ) we perform Huffman encoding to compute the “ optimal ” resulting compression score per layer ; 3 ) we do bucketing , which slightly increases the bit cost due to storing the scaling factor .
We will add a detailed procedure on how these costs were obtained , and explain why they can exceed the baseline bit width .
4 . Writing inconsistencies :
We thank the reviewer for the detailed comments , which we will fully address in the next version .

Thank you for your patience . Please see the reply above and the new revision of the paper .

This paper proposes to learn small and low - cost models by combining distillation and quantization . Two strategies are proposed and the ideas are reasonable and clearly introduced . Experiments on various datasets are conducted to show the effectiveness of the proposed method .

Pros :
( 1 ) The paper is well written , the review of distillation and quantization is clear .
( 2 ) Extensive experiments on vision and neural machine translation are conducted .
( 3 ) Detailed discussions about implementations are provided .

Cons :
( 1 ) The differentiable quantization strategy seems not to be consistently better than the straightforward quantized distillation which may need more research .
( 2 ) The actual speedup is not clearly calculated . The authors claim that the inference times of 2 xResNet18 and ResNet18 are similar which seems to be unreasonable . And it seems to need a lot of more work to make the idea really practical .

Finally , I am curious whether the idea will work on object detection task .


We acknowledge the reviewer ’s comments regarding experiments on larger datasets and more accurate baseline models . We chose to run on CIFAR - 10 and OpenNMT since they have reasonable iteration times . This allows us to carefully study the limits of the methods , and the trade - offs between bit width , network depth , and network layer width , given a fixed model by performing several experiments at each data point in reasonable time . To address the reviewer ’s point , we are extending our experiments to training
1 ) quantized ResNet students , e.g. ResNet18 , from larger ResNet teachers , e.g. ResNet50 , on ImageNet , comparing against the full - precision and existing quantized state - of - the- art baselines .
2 ) quantized state - of - the -art students for CIFAR - 10 and CIFAR - 100 tasks from the full - precision baselines , and comparing them against best performing published quantized versions .

It would be very helpful if the reviewer could be more precise with respect to what they consider as a good baseline .

Regarding the performance of differentiable quantization ( DQ ) with respect to quantized distillation ( QD ) , we point out that
1 ) in constrained settings ( e.g. 2 bit quantization ) DQ can be significantly more accurate than QD . See for example the CIFAR - 100 2 - bit experiment . We will add more examples here .
2 ) DQ usually converges earlier than QD to similar accuracy , which can be useful in settings where reducing number of iterations is important .

Thank you for your patience . Please see the reply above and the new revision of the paper .

This paper presents a framework of using the teacher model to help the compression for the deep learning model in the context of model compression . It proposed both the quantized distillation and also the differentiable quantization . The quantized distillation method just simply adapt the distillation work for the task of model compression , and give good results to the baseline method . While the differentiable quantization optimise the quantization function in a unified back - propagation framework . It is interesting to see the performance improvements by using the one-step optimisation method .

I like this paper very much as it is in good motivation to utilize the distillation framework for the task of model compression . The starting point is quite interesting and reasonable . The information from the teacher network is useful for constructing a better compressed model . I believe this idea is quite similar to the idea of Learning using Privileged Information , in which the information on teacher model is only used during training , but is not utilised during testing .

Some minor comments :
In table 3 , it seems that the results for 2 bits are not stable , and are there any explanations ?
What will be the results if the student model performs the same with the teacher model ( e.g. , use the teacher model as the student model to do the compression ) or even better ( reverse the settings ) ?
What will be the prediction speed for each of models ? We can also get the time of speedup for the compressed model .

It will be better if the authors could discuss the connections between distillation and the recent work for the Learning using Privileged Information setting :
Vladimir Vapnik , Rauf Izmailov :
Learning using privileged information : similarity control and knowledge transfer . Journal of Machine Learning Research 16 : 2023-2049 ( 2015 )
Xinxing Xu , Joey Tianyi Zhou , IvorW . Tsang , Zheng Qin , Rick Siow Mong Goh , Yong Liu : Simple and Efficient Learning using Privileged Information . BeyondLabeler : Human is More Than a Labeler , Workshop of the 25th International Joint Conference on Artificial Intelligence ( IJCAI - 16 ) . New York City , USA . July , 2016 .


1 . Divergence of 2 bit variants :
Indeed , the 2 bit version can diverge for some parameter settings . Our interpretation is that through trimming and quantization we reduce the capacity of the student , which might no longer have enough capacity to mimic the teacher model , and diverges .
2 . Swapping student and teacher model :
That is an interesting question . We focused on larger teachers and smaller students for compressing a fixed model . However , it is highly probable that quantizing a larger CNN via distillation from a smaller one will work as well . We will add experiments for this case .
3 . Inference speed is linear in network depth and bit width . In our experiments , the speedup we get on inference is proportional to the reduction in depth . This is mentioned in passing in the Conclusions section , but we will add some exact numbers .
4 . Connection to “ Learning using Privileged Information ” :
This is an excellent point , we will discuss this connection in the next revision .

Thank you for your patience . Please see the reply above and the new revision of the paper .

The paper proposes to use a deep neural network to embed probability distributions in a vector space , where the Euclidean distance in that space matches the Wasserstein distance in the original space of probability distributions . A dataset of pairs of probability distributions and their Wasserstein distance is collected , and serves as a target to be predicted by the deep network .

The method is straightforward , and clearly explained . Two analyses based on Wasserstein distances ( computing barycenters , and performing geodesic analysis ) are then performed directly in the embedded space .

The authors claim that the proposed method produces sharper barycenters than those learned using the standard ( smooth ) Wasserstein distance . It is unclear from the paper whether the advantage comes from the ability of the method to scale better and use more examples , or to be able to use the non-smooth Wasserstein distance , or finally , whether the learning of a deep embedding yields improved extrapolation properties . A short discussion could be added . It would also be interesting to provide some guidance on what is a good structure for the encoder ( e.g. should it include spatial pooling layers ? )

The term “ Wasserstein deep learning ” is probably too broad , “ deep Wasserstein embedding ” could be more appropriate .

The last line of future work in the conclusion seems to describe the experiment of Table 1 .

Indeed the first of line of future work is concerned with transferability issue of a learned mapping toward a new
dataset . In the paper we have examined if the mapping was transferable and we observed that it is mostly data dependent . In a future line of work , we would like to see if we can ‘ transfer ’ an already learnt embedding to work on a different dataset ( as would work a domain adaptation technique ) . We have rephrased the text to state this idea more clearly .


This is a difficult question . The Wasserstein distance cares about spatial location , hence adding spatial pooling in our network may coarser the embedding . For bigger images , we may consider strided convolutions instead of max-pooling . This is currently under examination as we are working with larger images , but with no definitive answer for the moment .


The main interest of the method is to be able to compute a fast and accurate approximation of the true Wasserstein distance ( and not the regularized one ) , but the embedding could also be learned to reflect a regularized version of W if needed by the application . The sharper quality of barycenters mostly comes with the fact that we are handling true Wasserstein distances and not regularized ones


The paper presents a simple idea to reduce the computational cost of computing Wasserstein distance between a pair of histograms . Specifically , the paper proposes learning an embedding on the original histograms into a new space where Euclidean distance in the latter relates to the Wasserstein distance in the original space . Despite simplicity of the idea , I think it can potentially be useful practical tool , as it allows for very fast approximation of Wasserstein distance . The empirical results show that embeddings learned by the proposed model indeed provide a good approximation to the actual Wasserstein distances .

The paper is well - written and is easy to follow and understand . There are some grammar / spelling issues that can be fixed by a careful proofreading . Overall , I find the paper simple and interesting .

My biggest concern however is the applicability of this approach to high - dimensional data . The experiments in the paper are performed on 2D histograms ( images ) . However , the number of cells in the histogram grows exponentially in dimension . This may turn this approach impractical even in a moderate-sized dimensionality , because the input to the learning scheme requires explicit representation of the histogram , and the proposed method may quickly run into memory problems . In contrast , if one uses the non-learning based approach ( standard LP formulation of Wasserstein distance ) , at least in case of W_1 , one can avoid memory issues caused by the dimensionality by switching to the dual form of the LP . I believe that is an important property that has made computation of Wasserstein distance practical in high dimensional settings , but seems inapplicable to the learning scheme . If there is a workaround , please specify .


Indeed we agree with the reviewer that the input dimension of our embedding network scales linearly in terms of bins in the histograms . Note however that dual ( or semi-dual ) approaches require the computation of Kantorovich potentials that are scalar functions of the dimension of ambient ( input ) space , that turns to be of same size as the number of bins of the histogram . Hence both views require to process the data through networks that have the same input size and might suffer from the same problem of high dimensionality . If considering 2D , 3D or 4D tensors , note however that neural networks architecture are known to accommodate well to such dimensions ( generally through convolution and pooling layers ) . We also note that in high dimensions , even computing a single Wasserstein distance is difficult , and a recent analysis [ 1 ] shows also the impact of dimensionality in estimating accurately the Wasserstein distance .

[ 1 ] J. Weed , F. Bach . Sharp asymptotic and finite - sample rates of convergence of empirical measures in Wasserstein distance . Technical Report , Arxiv-1707.00087 , 2017

This paper proposes approximating the Wasserstein distance between normalized greyscale images based on a learnable approximately isometric embedding of images into Euclidean space . The paper is well written with clear and generally thorough prose . It presents a novel , straightforward and practical solution to efficiently computing Wasserstein distances and performing related image manipulations .

Major comments :

It sounds like the same image may be present in the training set and eval set . This is methodologically suspect , since the embedding may well work better for images seen during training . This affects all experimental results .

I was pleased to see a comparison between using exact and approximate Wasserstein distances for image manipulation in Figure 5 , since that 's a crucial aspect of whether the method is useful in practice . However the exact computation ( OT LP ) appears to be quite poor . Please explain why the approximation is better than the exact Wasserstein difference for interpolation . Relatedly , please summarize the argument in Cuturi and Peyre that is cited ( " as already explained in " ) .

Minor comments :

In section 3.1 and 4.1 , " histogram " is used to mean normalized - to- sum-to - 1 images , which is not the conventional meaning .

It would help to pick one of " Wasserstein Deep Learning " and " Deep Wasserstein Embedding " and use it and the acronym consistently throughout .

" Disposing of a decoder network " in section 3.1 should be " using a decoder network " ?

In section 4.1 , the architectural details could be clarified . What size are the input images ? What type of padding for the convolutions ? Was there any reason behind the chosen architecture ? In particular the use of a dense layers followed by convolutional layers seems peculiar .

It would be helpful to say explicitly what " quadratic ground metric " means ( i.e. W_2 , I presume ) in section 4.2 and elsewhere .

It would be helpful to give a sense of scale for the numbers in Table 1 , e.g. give the 95th percentile Wasserstein distance . Perhaps use the L2 distance passed through a 1D - to-1D learned warping as a baseline .

Mention that OT stands for optimal transport in section 4.3.

Suggest mentioning " there is no reason for a Wasserstein barycenter to be a realistic sample " in the main text when first discussing barycenters .

With our settings , it may be possible to have some redundancy between the training and the test set . However , we ensure that statistically , it is highly unlikely to have redundant couples of images between the training and test set . Eventually , the higher the number of images to compute pairwise Wasserstein distance is , the lower is the probability of sharing images between the training and test set : especially when N > sqrt ( 100,000 ) . We ensure this condition for every dataset ( N ( mnist ) =50000 , N( face ) =161666 , N ( crab ) = 126930 , N( cat ) = 123202 ) .
Regarding our experiments on Principal Geodesic Analysis and Barycenter ’s estimation , those have been done on test images independent from the training set .

However , to clear any doubt regarding the efficiency of our method , we update Figure 2 , Figure 9 and Table 1 : we tested the pairwise Wasserstein distance with test images independent from the training set . Our results remain almost unchanged .


We are referring to Figures 3.1 and 3.2 in the paper ‘ A smoothed dual approach for variational Wasserstein problems ’ from Cuturi and Peyré , that show how the exact solution of the linear program corresponding to an interpolation in the Wasserstein sense of two Gaussians can lead to a staircase effect in the interpolated Gaussian , that is mainly due to discretization . We believe that the reconstructed images in our case suffer from the same discretization effect .

We provide details about the architecture we used :

Encoder :
- input size ( 1 , 28 , 28 )
- a convolutional layer : 20 filters of kernel size 3 by 3 , with zero padding and ReLu activation
- a convolutional layer : 10 filters of kernel size 3 by 3 , with zero padding and ReLu activation
- a convolutional layer : 5 filters of kernel size 5 by 5 , with zero padding and ReLu activation
- a fully connected layer with 100 output neurons and ReLu activation
- a fully connected layer with 50 output neurons , Relu activation . The output is our embedding .

Decoder :
- input size ( 50 , )
- a fully connected layer with 100 output neurons and ReLu activation
- a fully connected layer with 5*28 * 28 output neurons and ReLu activation
- a reshape layer of target size ( 5 , 28 , 28 )
- a convolutional layer : 10 filters of kernel size 5 by 5 , with zero padding and ReLu activation
- a convolutional layer : 20 filters of kernel size 3 by 3 , with zero padding and ReLu activation
- a convolutional layer : 1 filter of kernel size 3 by 3 , with zero padding and ReLu activation
- a Softmax layer whose output is the image reconstruction .

All weights are initialized with Glorot ’s rule .
In the encoder , there is no dense layer followed by a convolutional layer . However without max-pooling , we need dense layers at the end of the encoder to control the size of the embedding . Hence to mimic the inversion of each layer of the encoder , we indeed add dense layers followed by convolutional layers .
We also plan to publish a version of our code on GitHub .


First of all , thanks for the reviewer for helping us to improve our manuscript .

Regarding the Quadratic ground metric , it refers to squared Euclidean distance . As it was indeed not clear in the paper , we changed this notation in the revised version .

Regarding the scale of table 1 , the theoretical maximum distance is 1458 ( all mass between pixels in opposite corners ) , in average the pairwise wasserstein distance if of the order 12 for MNIST and 15 for CAT , CRAB and FACES but with relative MSE of order 1e - 3 ( see Table 1 for the exact values ) for which is quite large with respect to the quadratic mean error reported in the tables .

In this paper , the authors propose a method of compressing network by means of weight ternarization . The network weights ternatization is formulated in the form of loss - aware quantization , which originally proposed by Hou et al . ( 2017 ) .

To this reviewer ’s understanding , the proposed method can be regarded as the extension of the previous work of LAB and TWN , which can be the main contribution of the work .

While the proposed method achieved promising results compared to the competing methods , it is still necessary to compare their computational complexity , which is one of the main concerns in network compression .

It would be appreciated to have discussion on the results in Table 2 , which tells that the performance of quantized networks is better than the full - precision network .


Thanks for your review and suggestions .

1 . " compare their computational complexity "

- For space ( assuming that the weight values are stored in 32 bits for full - precision networks ) , the memory required by ternarized networks are 16 times smaller than the the full - precision network ; while m-bit networks are 32 / m times smaller .
- For time , consider the product WX between a rxs weight matrix W and sxn input matrix X. For full - precision networks , the cost of WX is ( M +A ) rsn , where M and A are the computation costs of 32 - bit floating - point multiplication and addition respectively . With the proposed ternarization , WX is computed by steps 3 - 5 in Algorithm 3 . For illustration , we use the approximate solver ( Algorithm 2 ) to compute the scaling parameter \alpha and ternarized value b ( invoked in Step 3 of Algorithm 3 ) . With fixed b , computing \alpha takes 2rs multiplications and 2rs additions . With fixed \alpha , computing b takes rs comparisons . Assume that alternating minimization is run for k steps ( empirically , k< =10 ) , the computation cost of ternarization using Algorithm 2 is k( 2M +2A + U ) rs , where U is the computation cost of 32 - bit floating - point comparison . Moreover , Steps 4 and 5 of Algorithm 3 take sn multiplications and rsn additions respectively . Thus the total cost for the product is Arsn + Msn + k ( 2M +2A +U ) rs , and the speedup ratio is S = ( ( M+A ) rsn ) / ( Arsn + Msn + k ( 2M+2A + U ) rs ) , which is approximately ( M+A ) / A ( some terms can be omitted as usually r >> 1 and n >>1 , and k is very small ) . Following ( Hubara et al , 2016 ) , we consider the implication on power in 45 nm technology ( with A = 0.9J , and M=3.7J ) . Substituting into the ratio above , the energy reduction is then approximately 5 . For the other ternarization algorithms such as TWN and TTQ , they also need at least sn multiplications and rsn additions for the product of WX , and thus the computation cost is similar to the proposed LAT_a .
- Details and complexity analysis for the other models will be provided in the final version of the paper .

2 . " discussion on the results in Table 2 "

- The quantized LSTM performs better than full - precision network because deep networks often have larger - than - needed capacities , and so are less affected by the limited expressiveness of quantized weights . Besides , low - bit quantization acts as regularization , and so contributes positively to the performance . We will add the discussion in the final version of the paper .


This paper proposes a new method to train DNNs with quantized weights , by including the quantization as a constraint in a proximal quasi- Newton algorithm , which simultaneously learns a scaling for the quantized values ( possibly different for positive and negative weights ) .

The paper is very clearly written , and the proposal is very well placed in the context of previous methods for the same purpose . The experiments are very clearly presented and solidly designed .

In fact , the paper is a somewhat simple extension of the method proposed by Hou , Yao , and Kwok ( 2017 ) , which is where the novelty resides . Consequently , there is not a great degree of novelty in terms of the proposed method , and the results are only slightly better than those of previous methods .

Finally , in terms of analysis of the algorithm , the authors simply invoke a theorem from Hou , Yao , and Kwok ( 2017 ) , which claims convergence of the proposed algorithm . However , what is shown in that paper is that the sequence of loss function values converges , which does not imply that the sequence of weight estimates also converges , because of the presence of a non-convex constraint ( $ b_j^t \in Q^{n_l} $ ) . This may not be relevant for the practical results , but to be accurate , it ca n't be simply stated that the algorithm converges , without a more careful analysis .

Thanks for your review and suggestions .

1 . " the paper is a somewhat simple extension of the method proposed by Hou , Yao , and Kwok ( 2017 ) , which is where the novelty resides . Consequently , there is not a great degree of novelty in terms of the proposed method "

- Please see our reply to reviewer 1 above .

2 . " the results are only slightly better than those of previous methods "

- The testing errors on these data sets are often only a few percent , and so the improvements may appear small . To have a clearer comparison , we added the percentage degradation of classification error as compared to the full - precision network in Table 1 ( https://www.dropbox.com/s/miquko7qhff9kns/iclr2018_rebuttal.pdf?dl=0).
- As can be seen , among the weight - ternarized networks , the proposed LAT and its variants achieve much smaller performance degradation on all four data sets . Existing methods often have large degradation , while ours has < 3 % degradation on MNIST and < 1 % on the other three data sets . On CIFAR - 100 , the proposed LAT and its variants achieve even better results than the full - precision network .
- For recurrent networks , we similarly added the percentage degradation of cross-entropy in Table 2 . As can be seen , the proposed weight ternarization is the only method that performs even better than the full - precision counterpart on all three data sets . On the Linux Kernel and Penn Treebank data sets , the proposed LAT and its variants even have > 5 % performance gain . On the War and Peace dataset , the proposed LAT and its variants are the only methods that achieve significantly better results than the full - precision network .

3 . " it ca n't be simply stated that the algorithm converges "

- On the theory side , we can only show convergence of the objective value . We will clarify this in the final version of the paper .
- Empirically , the quantized weight also converges , as can be seen from the convergence of $ \alpha$ in Figure 1 ( b ) .


This paper extends the loss -aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments .

Review :

Pros
This paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm . They extend the scheme to allow the use of different scaling parameters and to m-bit quantization . Experiments demonstrate the proposed scheme outperforms the state - of - the- art methods .

The experiments are complete and the writing is good .

Cons
Although the work seems convincing , it is a little bit straight - forward derived from the original binarization scheme ( Hou et al. , 2017 ) to tenarization or m-bit since there are some analogous extension ideas ( Lin et al. , 2016 b , Li & Liu , 2016 b ) . Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary .


Thanks for your review and suggestions .

1 . " it is a little bit straight - forward derived from the original binarization scheme ( Hou et al. , 2017 ) to ternarization or m-bit "

- While the idea of extending from 1 - bit ( binarization ) to more bits is straightforward , the difficulty and novelty are in the mathematical derivations . In Hou et al. ( 2017 ) , the optimal closed - form solution for loss - aware binarization can be derived easily . However , for ternarization , the optimal \alpha and b ( in Proposition 3.2 ) can not be easily solved . A straightforward solution would require combinatorial search . Instead , we proposed an exact solver ( Algorithm 1 ) which relies only on sorting . This can be further simplified to an efficient alternating minimization procedure ( Algorithm 2 ) . The same situation applies to m-bit quantization .

2 . " there are some analogous extension ideas ( Lin et al. , 2016 b , Li & Liu , 2016 b ) . Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary "

- While analogous extension ideas have been proposed , their weight solutions obtained are not rigorously derived . Specifically , ternary - connect ( Lin et al. , 2016 b ) performs simple stochastic quantization , but does not relate that to any quality measure ( e.g. , the loss , or distance between the quantized and full - precision weights ) . In TWN ( Li & Liu , 2016 b ) , obtaining the theoretical optimal solution is time - consuming and so they used a heuristic instead . In this paper , we explicitly consider the quantization effect to the loss ( as in Hou et al ( 2017 ) ) . However , the resultant optimization problem is much more difficult than theirs as explained above .


Update : I read the other reviews and the authors ' rebuttal . Thanks to the authors for clarifying some details . I 'm still against the paper being accepted . But I do n't have a strong opinion and will not argue against so if other reviewers are willing .

------

The authors propose Kernel Implicit VI , an algorithm allowing implicit distributions as the posterior approximation by employing kernel ridge regression to estimate a density ratio . Unlike current approaches with adversarial training , the authors argue this avoids the problems of noisy ratio estimation , as well as potentially high - dimensional inputs to the discriminator . The work has interesting ideas . Unfortunately , I 'm not convinced that the method overcomes these difficulties as they argue in Sec 3.2.

An obvious difficulty with kernel ridge regression in practice is that its complete inaccuracy to estimate high - dimensional density ratios . This is especially the case given a limited number of samples from both p and q ( which is the same problem as previous methods ) as well as the RBF kernel . While the RBF kernel still takes the same high - dimensional inputs and does not involve learning massive sets of parameters , it also does not scale well at all for accurate estimation . This is the same problem as related approaches with Stein variational gradient descent ; namely , it avoids minimax problems as in adversarial training by implicitly integrating over the discriminator function space using the kernel trick .

This flaw has rather deep implications . For example , my understanding of the implicit VI on the Bayesian neural network in Sec 4 is that it ends up as cross-entropy minimization subject to a poorly estimated KL regularizer . I 'd like to see just how much entropy the implicit approximation has instead of concnetrating toward a point ; or more directly , what the implicit posterior approximation looks like compared to a true posterior inferred by , say , HMC as the ground truth . This approach also faces difficulties that the naive Gaussian approximation applied to Bayesian neural nets does not : implicit approximations cannot exploit the local reparameterization trick and are therefore limited to specific architectures that does not involve sampling very large weight matrices .

The authors report variational lower bounds , which I 'm not sure is really a lower bound . Namely , the bias incurred by the ratio estimation makes it difficult to compare numbers . An obvious but very illustrative experiment I 'd like to see would be the accuracy of the KL estimator on problems where we can compute it tractably , or where we can Monte Carlo estimate it very well under complicated but tractable densities . I also suggest the authors perform the experiment suggested above with HMC as ground truth on a non- toy problem such as a fairly large Bayesian neural net .

Thank you for the insightful comments and we have included further experiments to investigate the questions raised . We have revised the paper to include the analysis .

Q1 : Inaccuracy of kernel regression in high dimensions & not convinced that KIVI overcomes the difficulties :

First , we have to emphasize that implicit VI is surely a much harder problem than VI with a common variational posterior ( e.g. , Gaussian ) , due to the lack of a tractable density for variational posterior q. Given the limited number of samples from q per iteration , if no additional knowledge is available , almost all implicit VI methods as well as nonparametric methods ( e.g. , SVGD ) suffer to some degree in high dimensions , as agreed by the reviewer . However , as we extensively investigated in experiments , though not fully addressed all the challenges , KIVI can outperform existing strong competitors to get state - of - the- art performance . We think this is a valuable contribution to variational inference .

Below , we further clarify our contributions . We have also revised the two challenges in Section 2 and the statements of contributions in the paper to make them clearer .

1 ) For the noisy estimation , we focused on the variance introduced in discriminator - based methods . In fact , existing discriminator - based methods have been identified to have high variance ( noisy ) , i.e. , samples from the two distributions are easily discriminated , which indicates overfitting ( Mescheder et al. , 2017 ) . This phenomenon is like the case when you push $ \lambda$ in KIVI to 0 . We are not claiming high accuracy for estimation in high - dimensional spaces ( In fact no implicit VI method can claim that with limited samples per iteration , as explained above ) . One main contribution of KIVI is to provide an explicit trade - off between bias and variance , since there was no principled way of doing so in discriminator - based methods . As a result , our algorithm can be rather stable ( see Fig.2 , right ) . It ’s true that bringing down the variance requires to pay some bias in the gradient in general . However , as empirically shown in the experiments and also in the investigation of the learned posteriors ( see the answer to Q3 below ) , we found that we still gain over previous VI methods , both in terms of accuracy and also the quality of uncertainty , which is highly non-trivial .

2 ) For high - dimensional latent variables , the argument mainly focused on computation issues . The other main contribution of KIVI is to make implicit VI computationally FEASIBLE for models like moderate-sized BNNs . In the classification case , the weights are of tens of thousands of dimensions and can hardly be fed into neural nets , which renders discriminator - based approaches infeasible .

Finally , we ’d like to add a point that KIVI opens up the door for improving implicit VI methods . The view of kernel regression at least brings two possible directions : One is pointed out by the reviewer , the RBF kernel could be replaced by other kernels that are more suitable to the model here . The other is to improve the regression problem to utilize the geometry of the distribution . And the latter is actually an ongoing work of us .

Q2 : Accuracy of the KL estimator on problems where we can compute it tractably :
Thanks for the suggestion . We added Appendix F.4 to compare the true KL term with the estimated KL term . We used normalizing flow there as the “ complicated but tractable densities ” . We can see that the KL estimates closely track the ground truth , and are more accurate as the variational approximation improves over time .

Q3 : Quality of posterior approximation & comparison to HMC :
We added Appendix F.3 to visualize the posterior approximation by KIVI and compare with HMC and the VI with naive Gaussian posteriors . The quantitative results and settings of HMC are described in Appendix F.2 . The main conclusion is that the VI with naive Gaussian posteriors leads to over - pruning problems . KIVI does n’t have the problem , and retains a good amount of uncertainty compared to HMC .

Q4 : Can not use local reparameterization trick :
This is a valid point . But the problem exists as long as we want to go beyond tractable variational posteriors ( e.g. , Gaussian ) . The results by naive Gaussian posteriors have been shown above , which has significant over - pruning problems . New difficulty introduced should n’t be the reason that we stick to the naive Gaussian approximation .

Q5 : Bias of lower bounds :
There are two places where we report lower bounds . In Figure 2 ( right ) the lower bounds are used only to show the stability of training . In Figure 3 ( b ) lower bounds are plotted to show the overfitting problems . We argue that though the lower bounds have bias , their relative gap ( the training / test gap ) should be comparable . Moreover , in this case we have also evaluated the test log likelihoods using golden truths estimated by Annealed Importance Sampling ( AIS ) . The results by AIS confirmed the conclusion that the KIVI - trained VAE less overfits .

This paper presents Kernel Implicit Variational Inference ( KIVI ) , a novel class of implicit variational distributions . KIVI relies on a kernel approximation to directly estimate the density ratio . Importantly , the optimal kernel approximation in KIVI has closed - form solution , which allows for faster training since it avoids gradient ascent steps that may soon get " outdated " as the optimization over the variational distribution runs . The paper presents experiments on a variety of scenarios to show the performance of KIVI .

Up to my knowledge , the idea of estimating the density ratio using kernels is novel . I found it interesting , specially since there is a closed - form solution for this estimate . The closed form solution involves a matrix inversion , but this should n't be an issue , as the matrix size is controlled by the number of samples , which is a parameter that the practitioner can choose . I also found interesting the implicit MMNN architecture proposed in Section 4 .

The experiments seem convincing too , although I believe the paper could probably be improved by comparing with other implicit VI methods , such as [ Liu & Feng ] , [ Tran et al . ] , or others .

My major criticism with the paper is the quality of the writing . I found quite a few errors in every page , which significantly affects readability . I strongly encourage the authors to carefully review the entire paper and search for typos , grammatical errors , unclear sentences , etc .

Please find below some further comments broken down by section .

Section 1 : In the introduction , it is unclear to me what " protect these models " means . Also , in the second paragraph , the authors talk about " often leads to biased inference " . The concept to " biased inference " is unclear . Finally , the sentence " the variational posterior we get in this way does not admit a tractable likelihood " makes no sense to me ; how can a posterior admit ( or not admit ) a likelihood ?

Section 3 : The first paragraph of the KIVI section is also unclear to me . In Section 3.1 , it looks like the cost function L ( \hat ( r ) ) is different from the loss in Eq. 1 , so it should have a different notation . In Eq. 4 , I found it confusing whether L ( r ) = J ( r ) . Also , it would be nice to include a brief description of why the expectation in Eq. 4 is taken w.r.t. p( z ) instead of q( z ) , for those readers who are less familiar with [ Kanamori et al . ] . Finally , the motivation behind the " reverse ratio trick " was unclear to me ( the trick is clear , but I did n't fully understand why it 's needed ) .

Section 4 : The first paragraph of the example can be improved with a brief discussion of why the methods of [ Mescheder et al . ] and [ Song et al . ] " are nor applicable " . Also , the paragraph above Eq. 11 ( " When modeling a matrix ... " ) was unclear to me .

Section 6 : In Figure 1 ( a ) , I think there must be something wrong , because it is well - known that VI tends to cover one of the modes of the posterior only due to the form of the KL divergence ( in contrast to EP , which should look like the curve in the figure ) . Additionally , Figure 3 ( a ) ( and the explanation in the text ) was unclear to me . Finally , I disagree with the discussion regarding overfitting in Figure 3 ( b ) : that plot does n't show overfitting because it is a plot of the training loss ( and overfitting occurs on test ) ; instead it looks like an optimization issue that makes the bound decrease .


**** EDITS AFTER AUTHORS ' REBUTTAL ****

I increased the rating to 7 after reading the revised version .


Thank you for the detailed comments . We apologize for the typos and errors . We have corrected them and revised the unclear sentences . Below , we address the individual concerns .

Q1 : Comparisons with other implicit VI methods , such as [ Liu & Feng ] , [ Tran et al . ] , or others :
Thanks for the suggestion . In the revision , we added the comparison with ( Liu & Feng , 2016 ) in Appendix F.2 . Their approach is to directly minimize the kernel Stein discrepancy ( KSD ) between the variational posterior and the true posterior . Since KSD has been shown to be the magnitude of a functional gradient of KL divergence ( Liu & Wang , 2016 ) , all saddle points in the original problem of optimizing KL divergence will become local optima when optimizing KSD . In experiments we also found that KSD VI soon converges to local minima , where the performance is unsatisfying .

For ( Tran et al. , 2017 ) , as it investigates both implicit models and implicit inference , the technique used is the joint-contrastive method , which is beyond our scope ( only meaningful to use joint - contrastive when the model is also implicit ) . So the comparison is infeasible since we are only focusing on implicit inference . We have compared to other discriminator - based approaches in our experiments ( e.g. , prior-contrastive , AVB ) .

Q2 : Detailed comments by section :
Section 1 : We revised all the unclear statements . “ biased inference ” means the true posterior is far from the variational family when the family only includes factorized distributions . “ admit a tractable likelihood ” should be “ have a tractable density ” .

Section 3 : We revised the unclear statements . In Section 3.1 , we cleaned the notations and added the description of why the expectation in Eq.4 is taken w.r.t. p( z ) . We also revised the reverse ratio trick part . A comparison between estimation with and without the trick is added to Appendix F.1.

Section 4 : The implicit distributions introduced in [ Mescheder et al . ] and [ Song et al . ] are not applicable because they are based on traditional fully - connected neural networks , which cannot afford a very large output space . However , this is indeed the case of the distribution over weights in a normal - size BNN . We made it clearer in the paper . The paragraph above the original Eq. ( 11 ) has been revised .

Section 6 : Thanks for pointing out the error in Figure 1 ( a ) . We have corrected it . VI with normal posterior indeed converges to a single mode . For Figure 3 ( a ) , we made it clearer and added more detailed descriptions to the posterior . Figure 3 ( b ) did show overfitting , where we have plotted both the training and the test loss . The smaller their gap is , the less the model overfits . We added more descriptions in Section 6.3.

Thank you for the feedback , and I think many of my concerns have been addressed .

I think the paper should be accepted .

==== original review ====

Thank you for an interesting read .

Approximate inference with implicit distribution has been a recent focus of the research since late 2016 . I have seen several papers simultaneously proposing the density ratio estimation idea using GAN approach . This paper , although still doing density ratio estimation , uses kernel estimators instead and thus avoids the usage of discriminators .

Furthermore , the paper proposed a new type of implicit posterior approximation which uses intuitions from matrix factorisation . I do think that another big challenge that we need to address is the construction of good implicit approximations , which is not well studied in previous literature ( although this is a very new topic ) . This paper provides a good start in this direction .

However several points need to be clarified and improved :
1 . There are other ways to do implicit posterior inference such as amortising deterministic / stochastic dynamics , and approximating the gradient updates of VI . Please check the literature .
2 . For kernel based density ratio estimation methods , you probably need to cite a bunch of Sugiyama papers besides ( Kanamori et al. 2009 ) .
3 . Why do you need to introduce both regression under p and q ( the reverse ratio trick ) ? I did n't see if you have comparisons between the two . From my perspective the reverse ratio trick version is naturally more suitable to VI .
4 . Do you have any speed and numerical issues on differentiating through alpha ( which requires differentiating K^{ -1 } ) ?
5 . For kernel methods , kernel parameters and lambda are key to performances . How did you tune them ?
6 . For the celeb A part , can you compute some quantitative metric , e.g inception score ?


Thank you for the positive feedback . We address the individual questions below .

Q1 : Related works :
Thanks for the suggestion . We have cited a paper on amortizing the deterministic dynamics of SVGD ( Liu & Feng , 2016 ) . In the revision , we added two more recent papers on amortized MCMC ( Li et al. , 2017 ) and gradient estimators of implicit models ( Li & Turner , 2017 ) in Section 5 . We also added more content there to highlight the contributions that Sugiyama and his collaborators has made to density ratio estimation .

Q2 : On the reverse ratio trick :
In fact , we did n’t do regression under q . We only adopted the regression under p ( the reverse ratio trick ) in our experiments ( See Algo . 1 ) . And we have explained why the reverse ratio version is more suitable for VI in Section 3.1 . In the revision , we further added a comparison between the two using the 2-D Bayesian logistic regression example in Appendix F.1 , which shows that the trick is very essential for KIVI to work well .

Q3 : Speed and numerical issues on differentiating through alpha :
Because K is of size n_p x n_p ( n_p is the number of samples ) , which is usually of tens or a hundred , the cost of differentiating through K^{ - 1 } is not high . And we used the automatic differentiation in Tensorflow . We did n’t observe any numerical issues , as long as the regularization parameter is n’t extremely small , say , less than 1e - 7 .

Q4 : Tuning parameters :
As mentioned in Section 3.1 , we selected the kernel bandwidth by the commonly used median heuristic , i.e. , the kernel bandwidth is chosen as the median of pairwise distances between the samples .

As for lambda , it has clear meaning , which controls the balance between bias and variance . So a good criterion would be tuning it to achieve a good trade - off between the aggressiveness of the estimate and stability of training . In the toy experiments , we tuned lambda so that optimizing only the KL term will make the posterior samples more disperse like the prior . In most other experiments , lambda is set at 0.001 which has good performance , though it could be improved by cross-validation .

Q5 : Quantitative evaluation for CelebA :
Thanks for the suggestion . In fact , inception score is only suitable to natural image datasets like Cifar10 and ImageNet . Instead , we adopted a recently developed quantitative measure named Fréchet Inception Distance ( FID ) ( Heusel et al. , 2017 ) , which improved the Inception score to use the statistics of real world samples . The scores achieved at epoch 25 by AVB and KIVI are 160 and 41 ( smaller is better ) , respectively . We added these results in Section 6.3.

( Summary )
This paper is about learning discriminative features for the target domain in unsupervised DA problem . The key idea is to use a critic which randomly drops the activations in the logit and maximizes the sensitivity between two versions of discriminators .

( Pros )
The approach proposed in section 3.2 uses dropout logits and the sensitivity criterion between two softmax probability distributions which seems novel .

( Cons )
1 . By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims " achieved state of the art results on three datasets . " in sec5 . 1 ) Unsupervised Pixel - Level Domain Adaptation with Generative Adversarial Networks , Bousmalis et al. CVPR17 , and 2 ) Learning Transferrable Representations for Unsupervised Domain Adaptation , Sener et al . NIPS16 . Does the proposed method outperform these state of the art methods using the same network architectures ?
2 . I suggest the authors to rewrite the method section 3.2 so that the loss function depends on the optimization variables G , C . In the current draft , it 's not immediately clear how the loss functions depend on the optimization variables . For example , in eqns 2,3 ,5 , the minimization is over G , C but G , C do not appear anywhere in the equation .
3 . For the digits experiments , appendix B states " we used exactly the same network architecture " . Well , which architecture was it ?
4 . It 's not clear what exactly the " ENT " baseline is . The text says " ( ENT ) obtained by modifying ( Springenberg 2015 ) " . I 'd encourage the authors to make this part more explicit and self - explanatory .

( Assessment )
Borderline . The method section is not very well written and the authors avoid comparing the method against the state of the art methods in unsupervised DA .

We uploaded an updated version of the paper with changes highlighted in blue .

To Reviewer 1
1 . By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims " achieved state of the art results on three datasets . " in sec5 . 1 ) Unsupervised Pixel - Level Domain Adaptation with Generative Adversarial Networks , Bousmalis et al. CVPR17 , and 2 ) Learning Transferrable Representations for Unsupervised Domain Adaptation , Sener et al . NIPS16 . Does the proposed method outperform these state of the art methods using the same network architectures ?

In the updated version of our paper , we added new experimental results following the same setting as Bousmalis did ( Table 1 ) . Ours is slightly better on MNIST -> USPS , but Bousmalis et al . do n’t report on more difficult shifts where we achieve state of the art , as such SVHN ->MNIST . In addition , we compared our method with Sener et al . NIPS16 in Table 1 .

Changes in the Paper
In Table 1 , We added Sener NIPS16 , for SVHN to MNIST . We also added results on MNIST to USPS to compare with Bousmalis CVPR 2016 . Results of our method changed in the adaptation using USPS because we found a bug in preprocessing of USPS . According to the change , we replaced the graph of Fig4 ( a ) ( b ) and we changed the relevant sentences .

2 . I suggest the authors to rewrite the method section 3.2 so that the loss function depends on the optimization variables G , C . In the current draft , it 's not immediately clear how the loss functions depend on the optimization variables . For example , in eqns 2,3 ,5 , the minimization is over G , C but G , C do not appear anywhere in the equation .

We clarified notation of Eqns 2,3,5 .

Change of paper
Change notation of Eqns 2,3,5 .

3 . For the digits experiments , appendix B states " we used exactly the same network architecture " . Well , which architecture was it ?

We wanted to say that , for our baseline method , we used the same network architecture as our proposed method . We added this explanation .

Change of paper .
Add sentence in the last of our appendix section ( Digits Classification Training Detail ) .

4 . It 's not clear what exactly the " ENT " baseline is . The text says " ( ENT ) obtained by modifying ( Springenberg 2015 ) " . I 'd encourage the authors to make this part more explicit and self - explanatory .

We did explain it in the appendix , but we added sentences to make the method clearer .

Change of paper
Add sentence in Section 2 , Section 4.2.




Unsupervised Domain adaptation is the problem of training a classifier without labels in some target domain if we have labeled data from a ( hopefully ) similar dataset with labels . For example , training a classifier using simulated rendered images with labels , to work on real images .
Learning discriminative features for the target domain is a fundamental problem for unsupervised domain adaptation . The problem is challenging ( and potentially ill - posed ) when no labeled examples are given in the target domain . This paper proposes a new training technique called ADR , which tries to learn discriminative features for the target domain . The key idea of this technique is to move the target - domain features away from the source - domain decision boundary . ADR achieves this goal by encouraging the learned features to be robust to the dropout noise applied to the classifier .

My main concern about this paper is that the idea of " placing the target - domain features far away from the source - domain decision boundary " does not necessarily lead to * discriminative features * for the target domain . In fact , it is easy to come up with a counter - example : the target - domain features are far from the * source-domain * decision boundary , but they are all ( both the positive and negative examples ) on the same side of the boundary , which leads to poor target classification accuracy . The loss function ( Equations 2 - 5 ) proposed in the paper does not prevent the occurrence of this counter - example .

Another concern comes from using the proposed idea in training a GAN ( Section 4.3 ) . Generating fake images that are far away from the boundary ( as forced by the first term of Equation 9 ) is somewhat opposite to the objective of GAN training , which aims at aligning distributions of real and fake images . Although the second term of Equation 9 tries to make the generated and the real images similar , the paper does not explain how to properly balance the two terms of Equation 9 . As a result , I am worried that the proposed method may lead to more mode- collapsing for GAN .

The experimental evaluation seems solid for domain adaptation . The semi-supervised GANs part seemed significantly less developed and might be weakening rather than strengthening the paper .

Overall the performance of the proposed method is quite well done and the results are encouraging , despite the lack of theoretical foundations for this method .


We uploaded an updated version of the paper with changes highlighted in blue .

To Reviewer 2
1 . , My main concern about this paper is that the idea of " placing the target - domain features far away from the source - domain decision boundary " does not necessarily lead to * discriminative features * for the target domain . In fact , it is easy to come up with a counter - example : the target - domain features are far from the * source-domain * decision boundary , but they are all ( both the positive and negative examples ) on the same side of the boundary , which leads to poor target classification accuracy . The loss function ( Equations 2 - 5 ) proposed in the paper does not prevent the occurrence of this counter - example .

Yes , we understand that there can be such a counter - example with our method . Note that we add a term that discourages target examples from being placed on one side of the boundary . However it is possible in theory that positive and negative examples switch labels , but we find that this does not occur in practice , and our method works well based on our experimental results .

2 . , Another concern comes from using the proposed idea in training a GAN ( Section 4.3 ) . Generating fake images that are far away from the boundary ( as forced by the first term of Equation 9 ) is somewhat opposite to the objective of GAN training , which aims at aligning distributions of real and fake images . Although the second term of Equation 9 tries to make the generated and the real images similar , the paper does not explain how to properly balance the two terms of Equation 9 . As a result , I am worried that the proposed method may lead to more mode- collapsing for GAN .
The experimental evaluation seems solid for domain adaptation . The semi-supervised GANs part seemed significantly less developed and might be weakening rather than strengthening the paper .

If the goal is to train a GAN to mimic a distribution only , then our additional objective may not help , but if the goal is to learn features for semi-supervised learning , then our objective helps by forcing the GAN to not generate fake images near the boundary ( ambiguous features ) .


I think the paper was mostly well - written , the idea was simple and great . I 'm still wrapping my head around it and it took me a while to feel convinced that this idea helps with domain adaptation . A better explanation of the intuition would help other readers . The experiments were extensive and show that this is a solid new method for trying out for any adaptation problem . This also shows how to better utilize task models associated with GANs and domain adversarial training , as used eg. by Bousmalis et al. , CVPR 2017 , or Ganin et al , ICML 2015 , Ghifary et al , ECCV 2016 , etc .

I think important work was missing in related work for domain adaptation . I think it 's particularly important to talk about pixel / image - level adaptations eg CycleGAN / DiscoGAN etc and specifically as those were used for domain adaptation such as Domain Transfer Networks , PixelDA , etc . Other works like Ghifary et al , 2016 , Bousmalis et al . 2016 could also be cited in the list of matching distributions in hidden layers of a CNN .

Some specific comments :

Sect . 3 paragraph 2 should be much clearer , it was hard to understand .

In Sect. 3.1 you mention that each node of the network is removed with some probability ; this is not true . it 's each node within a layer associated with dropout ( unless you have dropout on every layer in the network ) . It also was n't clear to me whether C_1 and C_2 are always different . If so , is the symmetric KL divergence still valid if it 's minimizing the divergence of distributions that are different in every iteration ? ( Nit : capitalize Kullback Leibler )

Eq.3 I think the minus should be a plus ?

Fig.3 should be improved , it was n't well presented and a few labels as to what everything is could help the reader significantly . It also seems that neuron 3 does all the work here , which was a bit confusing to me . Could you explain that ?

On p.6 you discuss that you do n't use a target validation set as in Saito et al . Is one really better than the other and why ? In other words , how do you obtain these fixed hyperparameters that you use ?

On p. 9 you claim that the unlabeled images should be distributed uniformly among the classes . Why is that ?

We uploaded an updated version of the paper with changes highlighted in blue .

To Reviewer 3
1 , I think important work was missing in related work for domain adaptation . I think it 's particularly important to talk about pixel / image - level adaptations eg CycleGAN / DiscoGAN etc and specifically as those were used for domain adaptation such as Domain Transfer Networks , PixelDA , etc . Other works like Ghifary et al , 2016 , Bousmalis et al . 2016 could also be cited in the list of matching distributions in hidden layers of a CNN .

We will refer to such methods and compare with PixelDA as possible as we can . ( Same question as Reviewer1 , 1 )

2 . Sect. 3 paragraph 2 should be much clearer , it was hard to understand .

We changed paragraph 2 of section 3.

3 . In Sect. 3.1 you mention that each node of the network is removed with some probability ; this is not true . it 's each node within a layer associated with dropout ( unless you have dropout on every layer in the network ) . It also was n't clear to me whether C_1 and C_2 are always different . If so , is the symmetric KL divergence still valid if it 's minimizing the divergence of distributions that are different in every iteration ? ( Nit : capitalize Kullback Leibler )
” It also was n't clear to me whether C_1 and C_2 are always different ”

→ C_1 and C_2 are not necessarily always different . C_1 and C_2 can be the same classifier . However , it rarely happens .
“ If so , is the symmetric KL divergence still valid if it 's minimizing the divergence of distributions that are different in every iteration ? ”
→ Yes , we think it is valid . The generator tries to minimize the divergence . The divergence means the sensitivity to noise caused by dropout . The goal of minimizing it is to generate features that are insensitive to the dropout noise . We minimize the divergence of distributions that are different in almost every iteration .

4 . Eq.3 I think the minus should be a plus ?

No . In Eq.3 , we aim to maximize the sensitivity for classifiers . In this phase , the classifiers should be trained to be sensitive to the noise caused by dropout . Thus , the minus should be a minus .

5 . Fig.3 should be improved , it was n't well presented and a few labels as to what everything is could help the reader significantly . It also seems that neuron 3 does all the work here , which was a bit confusing to me . Could you explain that ?

We improved the presentation . Neuron 3 seems to be dominant in bottom row ( our method . However , when comparing Neuron 3 and Column 6 , the shape of boundary looks a little different because of the effect of other neurons . What we wanted to show here is that each neurons will learn different features by our method . We will improve our presentation .

Change of paper
Add notation in Figure 3 , add caption .

6 . , On p.6 you discuss that you do n't use a target validation set as in Saito et al . Is one really better than the other and why ? In other words , how do you obtain these fixed hyperparameters that you use ?


The main hyperparameter in our method is n , which indicates how many times to repeat Step 3 in our method . We set 4 in our experiments . Although we did not show in our experimental results , we tried other number such as 1 ,2,3 . Through the experiment , we found that 4 works well in most settings . With regard to other hyperparameters , such as batch -size , learning rate , we used the ones that are common in other papers on domain adaptation .
If one uses a target val set ( as in Saito et al. ) , then one assumes access to training labels on target , which we do n’t want to assume in our setting .

7 . On p. 9 you claim that the unlabeled images should be distributed uniformly among the classes . Why is that ?

We assumed that it is not desirable if unlabeled images are aligned with one class . We add this term following “ Unsupervised and semi-supervised learning with categorical generative adversarial networks ” .


This paper proposes a formulation for discovering subtasks in Linearly - solvable MDPs . The idea is to decompose the optimal value function into a fixed set of sub value functions ( each corresponding to a subtask ) in a way that they best approximate ( e.g. in a KL - divergence sense ) the original value .

Automatically discovering hierarchies in planning / RL problems is an important problem that may provide important benefits especially in multi-task environments . In that sense , this paper makes a reasonable contribution to that goal for multitask LMDPs . The simulations also show that the discovered hierarchy can be interpreted . Although the contribution is a methodological one , from an empirical standpoint , it may be interesting to provide further evidence of the benefits of the proposed approach . Overall , it would also be useful to provide a short paragraph about similarities to the literature on discovering hierarchies in MDPs .

A few other comments and questions :

- This may be a fairly naive question but given your text I 'm under the impression that the goal in LMDPs is to find z( s ) for all states ( and Z in the multitask formulation ) . Then , your formulation for discovery subtasks seems to assume that Z is given . Does that mean that the LMDPs must first be solved and only then can subtasks be discovered ? ( The first sentence in the introduction seems to imply that there 's hope of faster learning by doing hierarchical decomposition ) .

- You motivate your approach ( Section 3 ) using a max-variance criterion ( as in PCA ) , yet your formulation actually uses the KL - divergence . Are these equivalent objectives in this case ?


Other ( minor ) comments :

- In Section it would be good to define V ( s ) as well as ' i ' in q_i ( it 's easy to mistake it for an index ) .

We would like to thank the reviewer for their efforts and insightful comments .

Similarities to other hierarchical discovery methods :
Where most other approaches have been used to learn a single level of hierarchy . Our method is distinctive mainly in being able to be iterated repeatedly , forming deep hierarchies . We have extended our discussion of this point in the paper .

The paper assumes that the multitask Z matrix is given :
We assume that Z is given for a basis set of tasks , not for all possible tasks . There are a number reasons that we believe this is not a limiting assumption :
1 . The basis set of tasks can be a tiny fraction of the set of possible tasks in the space . As an example , suppose we consider tasks with boundary rewards at any of two separate locations in an N dimensional world such that there are N-choose - 2 possible tasks ( corresponding to tasks like “ navigate to point A or B ” ) . We require only an N-dimensional Z matrix containing tasks to navigate to each point individually . The resulting subtasks we uncover will aid in solving all of these N-choose - 2 tasks . More generally we might consider tasks in which boundary rewards are placed at three or more locations , etc . To know Z therefore means to know an optimal policy to achieve N of ~ 2^N tasks in a space .
2 . While we assume knowledge of Z in the paper , we need n’t have a full N-task Z matrix for the method to applied as is . Suppose we had a smaller Z_hat matrix corresponding to M< N tasks . The method would nevertheless find a compressed representation for those M tasks and in so doing uncover useful subtasks . We assume the full Z matrix in the paper so that uncovered subtasks are intuitive decompositions of the full state space . If we consider Z_hat with tasks drawn only from some subsection of the state space , our method would uncover a compressed representation of just the subspace ( in this way our method can be said to be task dependent ) . Similarly if we consider Z_hat with tasks drawn uniformly over the state space we uncover similar decompositions to those presented in the paper .
3 . Ultimately , in practice we would like to obtain estimates for Z online from experience in a domain . This could be done either directly through Z-iteration ( an off - policy value iteration - like update ) , or by first building a state transition model . Methods to achieve this sort of estimate are well understood . We believe the results presented in this work are a necessary precursor : before tackling the joint estimation of Z and the hierarchy , we wanted to focus solely on inferring the hierarchy , which is in our view a critically challenging aspect of the problem . Just as LMDPs were first solved in batch mode as an eigenvalue problem before developing online z-iteration , we wanted to formulate and solve the computational problem in the batch setting before turning to online learning . Online learning thus is beyond the scope of this paper , though it is a focus of our current work , and we are excited to see this in the near future .
We have revised the paper to make this point more strongly .

Equivalence of maximum - variance and kl-divergence in the matrix factorization :
We do not believe that the maximum - variance ( \beta=2 ) and the kl-divergence ( \beta=1 ) are mathematically equivalent . Instead the intuition for the decomposition scheme came from a maximum variance like argument , but the kl-divergence cost was ultimately chosen in practice to align with the RL objective cost for LMDPs . In practice the method does not appear to be overly sensitive to the choice of \beta in the range [ 1 , 2 ] . For extreme values of \beta outside this range results degrade .

The empirical value of our method :
The fact that a task hierarchy can yield efficiency improvements in the multitask setting was shown in ( Saxe et al. , 2017 , ICML ) . In this instance the hierarchy was , however , hand crafted . More generally , when any ‘ good ’ hierarchy is provided ( one in which new tasks case be represented well within the hierarchy ) , the learning jump -start is observed .
A full investigation into the empirical value of the method in the online setting is very interesting , but it is beyond the scope of the present submission .

The paper builds upon the work of Saxe et al on multitask LMDP and studies how to automatically discover useful subtasks . The key idea is to perform nonnegative matrix factorization on the desirability matrix Z to uncover the task basis .

The paper does a good job in illustrating step by step how the proposed algorithms work in simple problems . In my opinion , however , the paper falls short on two particular aspects that needs further development :

( 1 ) As far as I can tell , in all the experiments the matrix Z is computed from the MDP specification . If we adopt the proposed algorithm in an actual RL setting , however , we will need to estimate Z from data since the MDP specification is not available . I would like to see a detailed discussion on how this matrix can be estimated and also see some RL experiment results .

( 2 ) If I understand correctly , the row dimension of Z is equal to the size of the state space , so the algorithm can only be applied to tabular problem as -is . I think it is important to come up with variants of the algorithm that can scale to large state spaces .

In addition , I would encourage the authors to discuss connection to Machado et al . Despite the very different theoretical foundations , both papers deal with subtask discovery in HRL and appeal to matrix factorization techniques . I would also like to point out that this other paper is in a more complete form as it clears the issues ( 1 ) and ( 2 ) I raised above . I believe the current paper should also make further development in these two aspects before it is published .

Minor problems :
- Pg 2 , " ... can simply be taken as the resulting Markov chain under a uniformly random policy " . This statement seems problematic . The LMDP framework requires that the agent can choose any next-state distribution that has finite KL divergence from the passive dynamics , while in a standard MDP , the possible next-state distribution is always a convex combination of the transition distribution of different actions .

References
Machado et al . ICML 2017 . A Laplacian Framework for Option Discovery in Reinforcement Learning .

We would like to thank the reviewer for their efforts and insightful comments .

How can we estimate Z from data :
Estimating Z from data in an online RL setting is an important question and is the focus of our current research efforts . The simplest approach is to apply online z-iteration , an off-policy form of value iteration . Z-iteration is a state - based scheme , and because it is off - policy , all tasks in the Z matrix can be updated regardless of which task is currently being executed . An alternative approach is to obtain estimates for the transition model , and then solve for Z. However , we emphasize that the RL setting is not the only possible application of our method : even without online RL , our method allows the automatic discovery of hierarchy and its use in planning in a batch or offline setting . We note that other prior methods like the Bayesian estimation approach of Solway et al. , 2014 ( “ Optimal behavioral hierarchy , ” PLoS Comp Bio ) or the information theoretic approach of McNamee et al. , ( “ Efficient state - space modularization for planning , ”
NIPS 2016 ) do not operate in the online RL setting , require full knowledge of the state space and transition model , and operate only in tabular representations . These algorithms have still been fundamental in specifying the computational problem to be solved . Our method goes beyond this prior work most significantly by being able to learn multiple levels of hierarchy , and being more computationally efficient . We believe these features make this work significant for the offline setting . Given that the online RL setting introduces additional variables , we have elected to take a more gradual approach to the development of the method - first ensuring that the new subtask discovery concepts are robust in tabular batch settings before tackling the joint estimation of Z and the hierarchy in high dimensional problems . We are very keen to see an empirical demonstration of online learning in this new framework soon , but it is beyond the scope of the present submission .

Variants of the algorithm to allow for problems with non- tabular state representations :
This will certainly be an important extension of the method , and again , is the focus of current work . As it stands , current approaches to deep RL use function approximation over the state space , but keep a tabular representation of tasks . Conversely , our approach is , at present , a tabular representation over states but function approximation over tasks .
When the number of tasks one wishes to perform in some space is significantly smaller than the state space , current approaches seem sensible . On the other hand , when the number of tasks we wish to perform is much greater than the number of states , current approaches appear unlikely to scale well . We believe filling in this possibility — demonstrating how function approximation can be safely used to perform many tasks organized hierarchically — is an important contribution . Ultimately , we must find a way to combine the best of both worlds and do function approximation both in the state and task space , but this is beyond the scope of the present submission .

Discuss comparisons to Machado et al .:
Thank you for the important pointer , we now include a discussion comparing our approach with that in Machado et al ( 2017 ) . As noted , while both papers are concerned with options discovery , and utilize matrix factorization tools to achieve this , they have different theoretical foundations and yield different results in practice . Their approach to extend the core concepts therein to a linear function approximation scheme is instructive , and will be useful to our current work .
There are several notable differences between these methods . Most importantly , our method can be recursively applied to generate arbitrarily deep control hierarchies , while it is not immediately clear ( and there has been no empirical demonstration of ) how the approach taken in Machado et al . might achieve a deeper hierarchical architecture , or enable immediate generalization to novel tasks .
The methods appear to some extent to be orthogonal ( with one supporting function approximation techniques in the state space , and the other supporting deep hierarchies and function approximation in the task space ) , and thus could potentially be profitably combined .

Taking the passive dynamics to be the uniform random policy :
A uniform random policy for the passive dynamics is a common choice in LMDPs which is suitable for a variety of tasks ( spatial navigation , trajectory planning , Tower of Hanoi , etc ) . This is , however , a modeling assumption and there is no requirement that the passive dynamics be derived from the uniform random policy . Choosing some alternate reference policy is possible , and may be suitable for specific tasks . More generally , Todorov ( 2009 ) provides a general way of approximating MDPs using LMDPs . We have added citations to a variety of works which show how standard domains have been modeled in the LMDP framework .

The present paper extends a previous work by Saxe et al ( 2017 ) that considered multitask learning in RL and proposed a hierarchical learner based on concurrent execution of many actions in parallel . That framework made heavy use of the framework of linearly solvable Markov decision process ( LMDP ) proposed by Todorov , which allows for closed form solutions of the control due to the linearity of the Bellman optimality equations . The simple form of the solutions allow them to be composed naturally , and to form deep hierarchies through iteration . The framework is restricted to domains where the transitions are fixed but the rewards may change between tasks . A key role is played in the formalism by the so-called ‘ free dynamics ’ that serves to regularize the action selected .

The present paper goes beyond Saxe et al. in several ways . First , it renders the process of deep hierarchy formation automatic , by letting the algorithm determine the new passive dynamics at each stage , as well as the subtasks themselves . The process of subtask discovery is done via non- negative matrix factorization , whereby the matrix of desirability functions , determined by the solution of the LMDPs with exponentiated reward . Since the matrix is non-negative , the authors propose a non-negative factorization into a product of non - negative low rank matrices that capture its structure at a more abstract level of detail . A family of optimization criteria for this process are suggested , based on a subclass if Bregman divergences . Interestingly , the subtasks discovered correspond to distributions over states , rather than single states as in many previous approaches . The authors present several demonstrations of the intuitive decomposition achieved . A nice feature of the present framework is that a fully autonomous scheme ( given some assumed parameter values ) is demonstrated for constructing the full hierarchical decomposition .

I found this to be an interesting approach to hierarchical multitask learning , augmenting a previous approach with several steps leading to increased autonomy , an essential agent for any learning agent . Both the intuition behind the construction and the application to test problem reveal novel insight . The utilization on the analytic framework of LMDP facilitates understanding and efficient algorithms .

I would appreciate the authors ’ clarification of several issues . First , the LMDP does not seem to be completely general , so I would appreciate a description of the limitations of this framework . The description of the elbow - joint behavior around eq . ( 4 ) was not clear to me , please expand . The authors do not state any direct or indirect extensions – please do so . Please specify how many free parameters the algorithm requires , and what is a reasonable way to select them . Finally , it would be instructive to understand where the algorithm may fail .



We would like to thank the reviewer for their efforts and insightful comments .

Limitations of the LMDP framework :
The LMDP framework on the surface appears very different from the standard MDP setting , and the question of its limitations arises frequently . In our view the LMDP framework is in fact quite general , and can be used to solve non- navigational and conceptual tasks such as the TAXI domain , and the Towers of Hanoi problem . For more on the generality of the LMDP see ( Saxe et al. , ICML 2017 supplementary material ) , which describes ways in which a variety of tasks have been modeled as LMDPs . The initial work of Todorov , 2009 , for instance , gives a method for approximating any MDP with an LMDP . The main limitation of the LMDP framework , so far as we understand it , is that actions must incur costs : the transition cost in LMDPs necessarily has a KL divergence term with respect to the passive dynamics , which is non-negative . Hence , for instance , it must be costly to move from one position in a grid to the next ( more precisely , to deviate from the passive dynamics ) . The LMDP would struggle to model a situation in which actions have strong rewards , eg , where the goal is to take the most circuitous path to a destination . We do not view this as a strong limitation , however , since nearly all domains have a principle of efficient action and it is common to place costs on each action taken in a traditional MDP . Indeed , we would argue that the LMDP exploits this shared structure in nearly all real - world tasks to allow more efficient solutions .

Parsing the phrase “ elbow - joint behavior ” :
One of the hyper parameters in our method is the number of nodes / subtasks at each level of the hierarchy . This corresponds to the rank of the decomposition . This choice is akin to choosing the number of neurons at different layers of a NN . Nevertheless we make an observation that may provide a way to establish a good value for even this parameter choice from data .
The key idea is that by increasing the rank of the decomposition we monotonically improve the approximation to Z , as the error ||Z- DW| | tends to zero . For some domains there is an obvious inflection point at which increasing the rank of the decomposition only slightly improves the approximation . This suggests a natural trade - off between expressiveness of the hierarchy , and the additional computational effort required to support additional subtasks . When we plot the quality of the approximation , ||Z-DW|| , against the decompositions factor k , the observed inflection point is described as exhibiting “ elbow - joint ” behavior . We have clarified this point in the text .

Extensions and future work :
We view this work as being an important stepping stone on the path towards a method for fully online learning of a deep control hierarchy . In that vein , there are a number of natural extensions ( a few of which were rightly called out by other reviewers ) . Some of the major items are :
1 . Estimating Z from data ( either directly or by learning a transition model ) , so that the agent can operate completely online
2 . Introducing standard notions of function approximation and compressed state representations to allow the method to scale to high dimensional state spaces
3 . Introducing some concept of regularized nonlinear composition ; allowing more complex behavior to be approximated by the hierarchy Many of these items are the focus of our current research efforts .

Free parameters and how to specify them :
The number of hyper parameters introduced by our method is minimal .
1 . The number of nodes / subtasks at each level of the hierarchy
◦ This is a common set of hyper parameters for many deep learning applications
◦ The elbow joint behavior provides one possible path to estimate efficient values here from data
◦ In practice we choose the number of nodes at layer ( l + 1 ) to be approximately log ( | S^l| ) , where | S^l| is the number of states at the preceding layer . This also determines the number of layers
2 . The subtask transition matrix Pt contains a scaling parameter such that Pt = \alpha W. Here \alpha controls how frequently the agent will consult the hierarchy for guidance . In practice we chose \alpha ~ 0.2 . The intuition here is that it is important that the agent to be able to consult the hierarchy sufficiently frequently that it influences its behavior ; but overly frequent access wastes computational resources .
3 . The choice of \beta in the cost function for the matrix decomposition . In our experiments we have typically chosen \beta = 1 ( KL ) or \beta = 2 ( Maximum Variance ) . All of our experiments suggest that the method is not overly sensitive to choices for \beta in the range [ 1 , 2 ] . For extreme values of \beta outside this range results degrade .


Where does the algorithm fail :
This is a great question . We believe the method fails most obviously in domains in which there is no latent structure to abstract . For example , if the passive dynamics ( at any level ) are fully connected and uniform , then the decomposition delivers no value . While such a problem is degenerate in the base case , it is not yet clear to us under what conditions the recursive iteration of the hierarchical abstraction might at some point yield such a uniform structure ( rendering further recursion useless ) .


This paper proposes a novel scalable method for incorporating uncertainty estimate in neural networks , in addition to existing methods using , for example , variational inference and expectation propagation . The novelty is in extending the Laplace approximation introduced in MacKay ( 1992 ) using a Kronecker - factor approximation of the Hessian . The paper is well written and easy to follow . It provides extensive references to related works , and supports its claims with convincing experiments from different domains .

Pros :
- A novel method in an important and interesting direction .
- It is a prediction method , so can be applied on existing trained neural networks ( however , see the first con ) .
- Well -written with high clarity .
- Extensive and convincing experiments .

Cons :
- Although it is a predictive method , it 's still worth discussing how this method relates to training . For example , I suspect it works better when the model is trained with second - order method , as the resulting Taylor approximation ( eq. 2 ) of the log-likelihood function might have higher quality when both terms are explicitly used in optimisation .
- The difference between using KFAC and KFRA is unclear , or should be better explained if they are identical in this context . Botev et al . 2017 reports they are slightly different in approximating the Gaussian Newton matrix .
- Acronyms , even well - known , are better defined before using ( e.g. , EP , PSD ) .
- Need more details of the optimisation method used in experiments , especially the last one .

Thank you very much for your positive review , we have updated the manuscript to introduce all acronyms before using them and added details regarding the hyperparameters of the last experiment to the appendix .

Regarding how the optimisation method affects the Laplace approximation is a question that we believe is closely related to how the optimisation method affects generalisation . We therefore decided to simply go with an optimiser that is commonly used in practice to make our results relevant to those who might use our method , however we are definitely open to adding an empirical comparison with different optimisation methods to a camera-ready version of the paper . Answering this question in full generality seems like a very interesting , but challenging open research problem .

This paper proposes a Laplace approximation to approximate the posterior distribution over the parameters of deep networks .

The idea is interesting and the realization of the paper is good . The idea builds upon previous work in scalable Gauss - Newton methods for optimization in deep networks , notably Botev et al. , ICML 2017 . In this respect , I think that the novelty in the current submission is limited , as the approximation is essentially what proposed in Botev et al. , ICML 2017 . The Laplace approximation requires the Hessian of the posterior , so techniques developed for Gauss - Newton optimization can straightforwardly be applied to construct Laplace approximations .

Having said that , the experimental evaluation is quite interesting and in - depth . I think it would have been interesting to report comparisons with factorized variational inference ( Graves , 2011 ) as it is a fairly standard and widely adopted in Bayesian deep learning . This would have been an interesting way to support the claims on the poor approximation offered by standard variational inference .

I believe that the independence assumption across layers is a limiting factor of the proposed approximation strategy . Intuitively , changes in the weights in a given layer should affect the weights in other layers , so I would expect the posterior distribution over all the weights to reflect this through correlations across layers . I wonder how these results can be generalized to relax the independence assumption .



Thank you very much for your comments and your review . We will address a few specific points that you raised in the following :


> In this respect , I think that the novelty in the current submission is limited , as the approximation is essentially what proposed in Botev et al. , ICML 2017 . The Laplace approximation requires the Hessian of the posterior , so techniques developed for Gauss - Newton optimization can straightforwardly be applied to construct Laplace approximations .

We fully agree that , from a techincal perspective , the approximation to the Hessian is not new and that once the two Kronecker factors are calculated it is relatively straightforward ( in terms of implementation ) to calculate the approximate predicive mean for a network . However , we do think that introducing these ideas from the optimisation literature to the Bayesian deep learning community , demonstrating how the Laplace approximation can be scaled to neural networks , is indeed a novel and valuable contribution ( since the diagonal approximation is not sufficient as shown in our experiments and the full approximation is not feasible ) . The Laplace approximation fundamentelly differs from the currently popular variational inference approaches in not requiring a modification to the training procedure , which is extremely useful for practictioners as they can simply apply it to their exisiting networks / do not need to do a full new hyperparameter search for optimising the parameters of an approximate posterior .


> I think it would have been interesting to report comparisons with factorized variational inference ( Graves , 2011 ) as it is a fairly standard and widely adopted in Bayesian deep learning . This would have been an interesting way to support the claims on the poor approximation offered by standard variational inference .

We have added this baseline to the 2nd and 3rd experiment , as this was also requested by Reviewer 1 ( our original aim was to have a " clean " comparison that is independent of the optimisation objective / procedure by focusing on different prediction methods for an identical network ) .


> I believe that the independence assumption across layers is a limiting factor of the proposed approximation strategy . Intuitively , changes in the weights in a given layer should affect the weights in other layers , so I would expect the posterior distribution over all the weights to reflect this through correlations across layers . I wonder how these results can be generalized to relax the independence assumption .

Thank you for this suggestion . Indeed , the layerwise blocks of the Fisher and Gauss - Newton are all Kronecker factored , so it should be possible to include the covariance of e.g. neighbouring layers in a computationally efficient way . In their work on KFAC , Martens & Grosse investigated such a tri-diagonal block approximation of the Fisher , however this only gave a minor improvment in performance over the block -diagonal approximation . Yet , since optimisation is a lot more time - critical , this could be worth investigating in the future for the Laplace approximation .

This paper uses recent progress in the understanding and approximation of curvature matrices in neural networks to revisit a venerable area - that of Laplace approximations to neural network posteriors . The Laplace method requires two stages - 1 ) obtaining a point estimate of the parameters followed by 2 ) estimation of the curvature . Since 1 ) is close to common practice it raises the appealing possibility of adding 2 ) after the fact , although the prior may be difficult to interpret in this case . A pitfall is that the method needs the point estimate to fall in a locally quadratic bowl or to add regularisation to make this true . The necessary amount of regularisation can be large as reported in section 5.4.

The paper is generally well written . In particular the mathematical exposition attains good clarity . Much of the mathematical treatment of the curvature was already discussed by Martens and Grosse and Botev et al in previous works . The paper is generally well referenced .

Given the complexity of the method , I think it would have helped to submit the code in anonymized form at this point . There are also some experiments not there that would improve the contribution . Figure 1 should include a comparison to Hamiltonian Monte Carlo and the full Laplace approximation ( It is not sufficient to point to experiments in Hernandez -Lobato and Adams 2015 with a different model / prior ) . The size of model and data would not be prohibitive for either of these methods in this instance . All that figure 1 shows at the moment is that the proposed approximation has smaller predictive variance than the fully diagonal variant of the method .

It would be interesting ( but perhaps not essential ) to compare the Laplace approximation to other scalable methods from the literature such as that of Louizos and Welling 2016 which uses also used matrix normal distributions . It is good that the paper includes a modern architecture with a more challenging dataset . It is a shame the method does not work better in this instance but the authors should not be penalized for reporting this . I think a paper on a probabilistic method should at some point evaluate log likelihood in a case where the test distribution is the same as the training distribution . This complements experiments where there is dataset shift and we wish to show robustness . I would be very interested to know how useful the implied marginal likelihoods of the approximation where , as suggested for further work in the conclusion .


Thank you very much for your thoughts and suggestions .


> Given the complexity of the method , I think it would have helped to submit the code in anonymized form at this point .

We will make the code available after the review period for ICLR . It is unfortunately spread out across multiple repositories , which we have n't open sourced yet , in particular we have integrated the calculation of the curvature factors that would also be needed for KFAC / KFRA into a currently internal version of Lasagne , so it would have been tricky to ensure that everything is fully anonymised .


> There are also some experiments not there that would improve the contribution . Figure 1 should include a comparison to Hamiltonian Monte Carlo and the full Laplace approximation

Thank you for pointing this out , we have added the corresponding figures to the manuscript and expanded the section . We have moved the figures of the unregularised Laplace approximations into the appendix and put figures for the regularised one into the main text , as they give a better fit to the HMC posterior .


> It would be interesting ( but perhaps not essential ) to compare the Laplace approximation to other scalable methods from the literature such as that of Louizos and Welling 2016 which uses also used matrix normal distributions .

We have added a comparison to a fully factorised Gaussian approximation as in Graves ( 2011 ) and Blundell et al. ( 2015 ) as this was also suggested by Reviewer 2 . We attempted to train a network with an approximate matrix normal posterior as in Louizos & Welling ( 2016 ) by parameterising the Cholesky factors of the two covariance matrices , as this would most closely correspond to how the posterior is approximated by our Laplace approximation . However , this lead to poor classification accuracies and the authors confirmed that this approach was n't successful for them either . They stated that instead the pseudo data ideas from the GP literature were crucial for the success of their method .

# Update after the rebuttal
Thank you for the rebuttal .
The authors claim that the source of objective mismatch comes from n-step Q-learning , and their method is well - justified in 1 - step Q-learning . However , there is still a mismatch even with 1 - step Q-learning because the bootstrapped target is also computed from the TreeQN . More specifically , there can be a mismatch between the optimal action sequences computed from TreeQN at time t and t+1 if the depth of TreeQN is equal or greater than 2 . Thus , the author 's response is still not convincing to me .
I like the overall idea of using a tree-structured neural network which internally performs planning as an abstraction of Q-function , which makes implementation simpler compared to VPN . However , the particular method ( TreeQN ) proposed in this paper introduces a mismatch in the model learning as mentioned above . One could argue that TreeQN is learning an " abstract " planning rather than " grounded " planning . However , the fact that reward prediction loss is used to train TreeQN significantly weakens this claim , and there is no such an evidence in the paper .
In conclusion , I think the research direction is worth pursuing , but the proposed modification from VPN is not well - justified .

# Summary
This paper proposes TreeQN and ATreeC which perform look - ahead planning using neural networks . Tree QN simulates the future by predicting rewards / values of the future states and performs tree backup to construct Q-values . ATreeC is an actor-critic architecture that uses a softmax over TreeQN . The architecture is trained through n-step Q-learning with reward prediction loss . The proposed methods outperform DQN baseline on 2D Box Pushing domain and outperforms VPN on Atari games .

[ Pros ]
- The paper is easy to follow .
- The application to actor-critic setting ( ATreeC ) is novel , though the underlying idea was proposed by [ O' Donoghue et al. , Schulman et al . ].

[ Cons ]
- The proposed method has a technical issue .
- The proposed idea ( TreeQN ) and underlying motivation are almost same as those of VPN [ Oh et al . ] , but there is no in - depth discussion that shows why TreeQN is potentially better than VPN .
- Comparison to VPN on Atari is not much convincing .

# Novelty and Significance
- The underlying motivation ( planning without predicting observations ) , the architecture ( transition / reward / value functions applied to the latent state space ) , and the algorithm ( n- step Q-learning with reward prediction loss ) are same as those of VPN . But , the paper does not provide in - depth discussion on this . The following is the differences that I found from this paper , so it would be important to discuss why such differences are important .

1 ) The paper emphasizes the " fully - differentiable tree planning " aspect in contrast to VPN that back - propagates only through " non-branching " trajectories during training . However , differentiating TreeQN also amounts to back - propagating through a " single " trajectory in the tree that gives the maximum Q-value . Thus , the only difference between TreeQN and VPN is that TreeQN follows the best ( estimated ) action sequence , while VPN follows the chosen action sequence in retrospect during back - propagation . Can you justify why following the best estimated action sequence is better than following the chosen action sequence during back - propagation ( see Technical Soundness section for discussion ) ?

2 ) TreeQN only sets targets for the final Q-value after tree backup , whereas VPN sets targets for all intermediate value predictions in the tree . Why is TreeQN 's approach better than VPN 's approach ?

- The application to actor-critic setting ( ATreeC ) is novel , though the underlying idea of combining Q-learning with policy gradient was proposed by [ O' Donoghue et al . ] and [ Schulman et al . ].

# Technical Soundness
- The proposed idea of setting targets for the final Q-value after tree backup can potentially make the temporal credit assignment difficult , because the best estimated actions during tree planning does not necessarily match with the chosen actions . Suppose that TreeQN estimated " up-right-right " as the best future action sequence the during 3 - step tree planning , while the agent actually ended up with choosing " up - left-left " ( this is possible because the agent re-plans at every step and follows epsilon - greedy policy ) . Following n-step Q-learning procedure , we end up with setting target Q-value based on on-policy action sequence " up - left-left " , while back - propagating through " up-right-right " action sequence in the TreeQN 's plan . This causes a wrong temporal credit assignment , because TreeQN can potentially increase / decrease value estimates in the wrong direction due to the mismatch between the planned actions and chosen actions . So , it is unclear why the proposed algorithm is technically correct or better than VPN 's approach ( i.e. , back - propagating through the chosen actions in the search tree ) .

# Quality
- Comparison to VPN on Atari is not convincing because TreeQN - 1 is actually ( almost ) equivalent to VPN - 1 , but the results show that TreeQN - 1 performs much better than VPN on many games . Since the authors took the numbers from [ Oh et al . ] rather than replicating VPN , it is possible that the gap comes from implementation details ( e.g. , hyperparameter ) .

# Clarity
- The paper is overall easy to follow and the description of the proposed method is clear .

Thank you for your feedback .

Regarding the soundness of n-step Q-learning targets :
As you point out , there is a mismatch between the n-step targets , which include an on-policy component , and our model ’s estimates of the optimal Q-function . However , this mismatch appears for * any * model estimating the optimal Q* with partially on - policy bootstraps . The weakly grounded internal temporal semantics of our architecture do not exacerbate this problem , but simply render more explicit the mechanism for estimating Q*.
In 1 - step Q-learning , or when using policy gradients , this model - objective mismatch does not appear . In practice , n-step Q-learning targets help to stabilise and speed learning despite the unusual combination of on - and off - policy components in the targets . However , it is true that 1 - step Q-learning , or policy gradients , provides objectives more consistent with our overall approach .

Regarding the comparison to VPN algorithmically :
Following the best estimated action sequence removes a mismatch between the use of the model components at training and test - time : in our approach , the components are freely optimised for their use in estimating the optimal action - value or action - probability , rather than trained to match a greedy bootstrap with an on-policy internal path . We believe it is crucial to maintain an equivalent context at training and evaluation time .
This is also the motivation for optimising Q only after the full tree backup , rather than also after partial backups . We want to learn a model that is as good as possible within the specific architecture we use , rather than across a class of related architectures with varying tree-depths . It is possible that such transfer learning could help in some problems . However , it is important to note that intermediate value estimates are still used , as they are mixed into the final prediction during the backup .
Constructing the full tree at each time - step frees us to make value estimates for all ( root -node ) actions , which enables the extension to ATreeC -- something that ca n’t easily be done with VPN . This extension is more about using an *architecture * designed for value- prediction in the context of policy gradients , rather than using * algorithmic * components of Q-learning with policy gradients as in the work of [ O' Donoghue et al. , Schulman et al . ].
Our overall strategy also simplifies training , as the whole model can be used as a drop - in replacement for an action - value or policy network , without recombining the components in a different manner for on - policy training segments , target evaluation , and testing . In our view this is a valuable contribution over VPN .

Regarding the experimental comparison to VPN , it is clear that some details of hyperparameters or implementation constitute a large part of the difference ( as most clearly seen in the different baseline DQN results ) . This is precisely why we focus on comparing to our own , much stronger , DQN and A2C baselines . We included these data to facilitate other work using frameskip - 10 Atari as a domain for planning - inspired deep RL . We feel it is unreasonable to expect a reimplementation of VPN with tuning to approach the level of our baselines , and assume the authors of that work put a reasonable effort into optimising their algorithm .

The authors propose a new network architecture for RL that contains some relevant inductive biases about planning . This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction / planning task . The proposed architecture performs something analogous to a full - width tree search using an abstract model ( learned end - to - end ) . This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes . The final backup value is the Q-value prediction for a given state , or can represent a policy through a softmax .

I thought the paper was clear and well - motivated . The architecture ( and various associated tricks like state vector normalization ) are well - described for reproducibility .

Experimental results seem promising but I was n’t fully convinced of its conclusions . In both domains , TreeQN and AtreeC are compared to a DQN architecture , but it was n’t clear to me that this is the right baseline . Indeed TreeQN and AtreeC share the same conv stack in the encoder ( I think ? ) , but also have the extra capacity of the tree on top . Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity ? Same comment in Atari , but there it ’s not really obvious that the proposed architecture is helping . Baselines could include unsharing the weights in the tree , removing the max backup , having a regular MLP with similar capacity , etc .

Page 5 , the auxiliary loss on reward prediction seems appropriate , but it ’s not clear from the text and experiments whether it actually was necessary . Is it that makes interpretability of the model easier ( like we see in Fig 5c ) ? Or does it actually lead to better performance ?

Despite some shortcomings in the result section , I believe this is good work and worth communicating as is .

Thank you for your positive comments and useful feedback .

Concerning baselines , our preliminary experiments showed that simply adding more parameters via width or depth to a DQN architecture did not result in significant performance gains , which we will make clear in the paper . A large-scale investigation of such architectures and their combination with auxiliary losses on many Atari games may be infeasible for us , but we have added to the appendix a figure demonstrating the limitations of naively adding parameters to DQN on the box-pushing domain .

We did n’t do a systematic investigation of the reward prediction loss across all environments , but in preliminary experiments on Seaquest and the box -pushing environment it helped performance . Interpretable sequences for box - pushing tended to appear when rewards were immediately available , which leads us to believe the grounding from this loss played a part .


This was an interesting read .

I feel that there is a mismatch between intuition of what a model could do ( based on the structure of the architecture ) versus what a model does . Just because the transition function is shared and the model could learn to construct a tree , when trained end - to - end the system is not sufficiently constrained to learn this specific behaviour . More to a point . I think the search tree perspective is interesting , but is n’t this just a deeper model with shared weights ? And a max operation ? It seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state , right ? Is there any specific attempt to visualize or understand the embeddings inside the tree ? The same regarding the rewards . If there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards , why would the model use those free latent variables to encode rewards ? I think this is a pitfall that many deep network papers fall , where by laying out a particular structure it is directly inferred that the model discovers or follows a particular solution ( where the latent have prescribed semantics ) . I would argue that is rarely the case . When the system is learned end - to -end , the structure does not impose the behaviour of the model , and is up to the authors of the paper to prove that the trained model does anything similar to expanding a tree . And this is not by showing final performance on a game . If indeed the model does anything similar to search , than all intermediary representations should correspond to what semantically they should .
Ignoring my verbose comment , another view is that the baseline are disadvantaged to the treeQN , because they have less parameters ( and are less deep which has a huge impact on the learnability and expressivity of the deep network ) .


It ’s great to hear that you found this work interesting - and thank you for your feedback .

The goal of our research is to design methods that yield good performance in the considered tasks , and we hope the reviewers will evaluate our paper accordingly . Explicit grounding , or lack thereof , is merely a means to the end of maximising task performance . Therefore , while the empirical question of how explicitly to ground the model is fascinating , we believe the quality of the paper should not be measured in terms of how explicitly the model is grounded . See also the answer to the anonymous comment from 21st of November below .

We found that grounding the reward function ( which takes intermediate embeddings as input ) with an explicit loss did help performance , so we included that in the objective ( see Sec. 3.2 ) . However , we found that explicitly grounding the latent representations based on a reconstruction loss did not help . In the paper , we also discuss reasons why one should avoid such objectives when constructed in the observation space . Also note that intermediate value predictions are mixed into the final prediction . While this does n’t force a grounding , it encourages each of the intermediate embeddings to correspond to a valid state embedding .

The key idea behind our paper is that the architecture that makes sense for a grounded model ( e.g. , tree -planning ) should still provide a useful inductive bias for a learned model that is only weakly grounded or not grounded at all .

Concerning baselines , our preliminary experiments showed that simply adding more parameters via width or depth to a DQN architecture did not result in significant performance gains , which we will make clear in the paper . A large-scale investigation of such architectures and their combination with auxiliary losses on many Atari games may be infeasible for us , but we have added to the appendix a figure demonstrating the limitations of naively adding parameters to DQN on the box-pushing domain .

This paper introduces several ideas : scaling , warm - restarting learning rate , cutout augmentation .

I would like to see detailed ablation studies : how the performance is influenced by the warm - restarting learning rates , how the performance is influenced by cutout . Is the scaling scheme helpful for existing single - bit algorithms ?

Question for Table 3 : 1 - bit WRN 20 - 10 ( this paper ) outperforms WRN 22 - 10 with the same # parameters on C100 . I would like to see more explanations .


Thankyou for your comments and questions .

***

Reviewer Comment : “ I would like to see detailed ablation studies : how the performance is influenced by the warm - restarting learning rates , how the performance is influenced by cutout ”

Author Response :

We already separated out the influence of cutout in the original submission . We only used cutout for CIFAR 10/100 , and Table 1 showed separate columns for results without cutout ( indicated by superscript +) and those with cutout ( indicated by superscript ++ ) . Figure 5 ( right panel ) shows how the use of cutout influences convergence .

We did not originally provide comparisons with and without warm - restart , because the benefits of the warm - restart method ( both faster convergence and better accuracy ) for CIFAR 10/100 has already been established by Loshchilov and Hutter ( 2016 ) who compared the approach with a more typical schedule with a learning rate of 0.1/0.01/0.001 for 80/80/80 epochs . However , the reviewer has a point that the comparison has not previously been done for the case of single - bit-weights and hence we have now conducted some experiments .

Author actions :

1 . We have added a section in Results called “ Ablation Studies ” and included a new figure for CIFAR - 100 . The figure highlights that the warm - restart method does not provide a significant accuracy benefit for the full - precision case but does in the single- bit-weights case . The figure also shows a comparison of learning and not learning the batch - norm offsets and gains , in response to another question by this Reviewer , responded to below .

***

Reviewer question : “ Is the scaling scheme helpful for existing single - bit algorithms ?

Author Response : Our Section 2.1 describes how our approach builds on and enables the improvement of existing single - bit algorithms . Our new Section 4.1 shows how our use of warm - restart accelerates convergence , and provides best accuracy , especially for CIFAR - 10 . Our Section 5.2 discusses the specific case of how our method compares with Rastegari et al ( 2016 ) .

Author Action : we have added Section 4.1 and revised Section 5.2.

***

Reviewer question : “ Question for Table 3 : 1 - bit WRN 20 - 10 ( this paper ) outperforms WRN 22 - 10 with the same # parameters on C100 . I would like to see more explanations . ”

Author response : In this initial submission , we only explained this in general terms in Section 5.3 as being a result of our approach described in 3.2.1 . So we agree that the reviewer has a point . We did not highlight this aspect very much in the original submission , nor explain it in the specific cases tabulated , as we wanted the central emphasis to be on our single - bit- weight results . However , on reflection , improvements to the baseline approach are surely of interest to the community and worth emphasis .

For the specific case mentioned by the Reviewer , we remark that our 20 - 10 network is essentially the same as the 22 - 10 comparison network , where the extra 2 conv layers appear due to the use of learnt 1 x1 convolutional projections in downsampling residual paths , whereas we use average pooling instead .

To directly answer the question , there is one single factor that enabled us to significantly lower the error rate for the width - 10 wide ResNet architecture for CIFAR , which is that we turn off the learning of the batch norm parameters , as we found this reduces overfitting .

Author actions :

1 . We have now highlighted this contribution in the abstract .
2 . We have now highlighted the specific case mentioned in the Discussion in Section 5.3.
3 . We have added results in a new Section ( “ Ablation studies ” ) that show how the test error rate changes through training with and without learning of the batch - norm scale and offset .


The authors propose to train neural networks with 1 bit weights by storing and updating full precision weights in training , but using the reduced 1 bit version of the network to compute predictions and gradients in training . They add a few tricks to keep the optimization numerically efficient . Since right now more and more neural networks are deployed to end users , the authors make an interesting contribution to a very relevant question .

The approach is precisely described although the text sometimes could be a bit clearer ( for example , the text contains many important references to later sections ) .

The authors include a few other methods for comparision , but I think it would be very helpful to include also some methods that use a completely different approach to reduce the memory footprint . For example , weight pruning methods sometimes can give compression rates of around 100 while the 1bit methods by definition are limited to a compression rate of 32 . Additionally , for practical applications , methods like weight pruning might be more promising since they reduce both the memory load and the computational load .

Side mark : the manuscript has quite a few typos .


Thankyou for your comments .

***

Reviewer Comment : “ could be a bit clearer … the text contains many important references to later sections ”

Author Action : We have edited the text to improve this aspect .

***

Reviewer Comment : “ I think it would be very helpful to include also some methods that use a completely different approach to reduce the memory footprint … . for practical applications , methods like weight pruning might be more promising since they reduce both the memory load and the computational load ”

Author Response :

As well as reducing model size , our approach is strongly motivated by significantly reducing computational load by a different approach to reducing parameter number . The key point is that performing convolutions using 1 - bit weights can be implemented using adders rather than multipliers . Removing the need for multipliers offers enormous benefits in terms of chip size , speed and power consumption in custom digital hardware implementations of trained networks , and also offers substantial speedups even if implemented on GPUs . This has been demonstrated by Rastegari et al in “ XNOR - Net : ImageNet Classification Using Binary Convolutional Neural Networks ” ( arxiv : 1603.05279 , 2016 ) .

Existing pruning methods do not automatically offer the opportunity to avoid use of multiplications , since the un-pruned parameters are learned using full precision . It remains an open question beyond the scope of the current submission to determine whether pruning can be successfully applied to 1 - bit models like ours to in turn reduce the number of parameters .

Question to Reviewer : we have been unable to find methods that reduce the size of all - convolutional networks by a factor of 100 . This magnitude of reduction is , to our knowledge , only available in networks with very large fully - connected layers , such as AlexNet and VGGnet . For example , one submission to ICLR 2018 “ To Prune , or Not to Prune : Exploring the Efficacy of Pruning for Model Compression ” uses pruning applied to an Inception network that reduces the number of non- zero parameters from 27M to 3M , which is a factor of 9x . Can you please clarify if you know of papers that achieve this scale of pruning in all - convolutional networks such as ResNets ?

Author Action : we have added comments strengthening our motivation of reducing the use of multiplications , and added to our discussion of pruning in the prior work and discussion .

***

Reviewer Comment : “ the manuscript has quite a few typos . . ”

Author Action : we have carefully reviewed the entire manuscript and corrected typos .


The paper trains wide ResNets for 1 - bit per weight deployment .
The experiments are conducted on CIFAR - 10 , CIFAR - 100 , SVHN and ImageNet32 .

+ the paper reads well
+ the reported performance is compelling

Perhaps the authors should make it clear in the abstract by replacing :
" Here , we report methodological innovations that result in large reductions in error rates across multiple datasets for deep convolutional neural networks deployed using a single bit for each weight "
with
" Here , we report methodological innovations that result in large reductions in error rates across multiple datasets for wide ResNets deployed using a single bit for each weight "

I am curious how the proposed approach compares with SqueezeNet ( Iandola et al .,2016 ) in performance and memory savings .



Thankyou for your comments .

***

Reviewer comment : " + the reported performance is compelling " :

Author Response : To reinforce this aspect , since initial submission we have found the following ways to surpass the performance we initially reported :

1 . We now have conducted experiments on the full Imagenet dataset and have surpassed all previously published results for a single - bit per weight . Indeed , we provide the first report , to our knowledge , of a top - 5 error rate under 10 % for this case .
2 . For Imagenet32 , we realised that the weight decay we used was set to the larger CIFAR value of 0.0005 . We repeated our experiments with the usual Imagenet value of 0.0001 and achieved improved results .
3 . For experiments on CIFAR with cutout , we realised our previous experiments did not uniformly sample all pixels for cutout ; after fixing we achieved further reduced error rates .
4 . We have also completed experiments with CIFAR 10/100 for ResNets with depth 26 . We found the extra layers provided no benefit for the full - precision case , but a small advantage in the single - bit case .

Author Actions : We have updated the results tables in the revised manuscript , modified our descriptions of the use of CutOut , clarified our weight - decay values , and added comments in the Discussion section comparing aspects of the enhanced results .

***

Reviewer Comment : “ Perhaps the authors should make it clear in the abstract … ”

Author Response : You have a point that our experiments in the main text were all on wide ResNets . This followed from our strategy to commence with a near state - of - the- art baseline . However , our training approach is general and not specific to ResNets . For example , we provided some results for all - conv-nets in the Appendix B on the final page .

Author Actions : To improve clarity as suggested , we have added the phrase " Using depth - 20 wide residual networks as our main baseline " to our revised manuscript , but have retained the term " deep convolutional neural networks . "

***

Reviewer Comment : “ I am curious how the proposed approach compares with SqueezeNet ( Iandola et al .,2016 ) in performance and memory savings . “

Author Response : The Squeezenet paper focuses on memory savings relative to AlexNet . It uses two strategies to produce a memory - saving smaller model than an AlexNet : ( 1 ) replacing many 3 x3 kernels with 1 x1 kernels ; ( 2 ) deep compression .

Regarding Squeezenet memory -saving strategy ( 1 ) , we note that SqueezeNet is an all - convolutional network . We tried our single- bit-weights approach in many all - convolutional variants ( e.g. plain all - conv , Squeezenet , MobileNet , ResNeXt ) and found its effectiveness relative to full - precision baselines to be comparable for all variants . We also observed in many experiments that the total number of learnt parameters correlates very well with classification accuracy . When we applied a SqueezeNet variant to CIFAR - 100 , we found that to obtain the same accuracy as our ResNets , we had to increase the " width " until the SqueezeNet had approximately the same number of learnt parameters as the ResNet . We conclude that our method therefore reduces the model size of the baseline SqueezeNet architecture ( i.e. when no deep compression is used ) by a factor of 32 , albeit with an accuracy gap .

Regarding SqueezeNet memory -saving strategy ( 2 ) , the SqueezeNet paper reports that Deep Compression reduces the model size by approximately a factor of 10 with no accuracy loss . Our method reduces the same model size by a factor of 32 , but with a small accuracy loss that typically becomes larger as the full - precision accuracy gets smaller . It would certainly be interesting to explore whether Deep -Compression might be applied to our 1 - bit models , but our own focus is on methods that minimally alter training , and we leave investigation of more complex methods for future work .

Regarding Squeeze Net performance , the best accuracy reported in the SqueezenNet paper is 39.6 % top - 1 error , requiring 4.8 MB for the model ’s weights . Our single - bit-weight models achieve better than 33 % top - 1 error , and require 8.3 MB for the model ’s weights .

Author Actions : We added these comments to a new subsection in the Discussion section of our paper .

Overall , the paper is well - written and the proposed model is quite intuitive . Specifically , the idea is to represent entailment as a product of continuous functions over possible worlds . Specifically , the idea is to generate possible worlds , and compute the functions that encode entailment in those worlds . The functions themselves are designed as tree neural networks to take advantage of logical structure . Several different encoding benchmarks of the entailment task are designed to compare against the performance of the proposed model , using a newly created dataset . The results seem very impressive with > 99 % accuracy on tests sets .

One weakness with the paper was that it was only tested on 1 dataset . Also , should some form of cross-validation be applied to smooth out variance in the evaluation results . I am not sure if there are standard " shared " datasets for this task , which would make the results much stronger .
Also how about the tradeoff , i.e. , does training time significantly increase when we " imagine " more worlds . Also , in general , a discussion on the efficiency of training the proposed model as compared to TreeNN would be helpful .
The size of the world vectors , I would believe is quite important , so maybe a more detailed analysis on how this was chosen is important to replicate the results .
This problem , I think , is quite related to model counting . There has been a lot of work on model counting . a discussion on how this relates to those lines of work would be interesting .


After revision

I think the authors have improved the experiments substantially .

Thank you for your comments and fair criticisms . We have run substantial further evaluation of the previously trained models , which we hope will strengthen the case for this paper . We reply to some of the points you made in your review below , and hope you will find that the empirical evidence satisfactorily addressed the concerns you have raised .

“ One weakness with the paper was that it was only tested on 1 dataset . Also , should some form of cross-validation be applied to smooth out variance in the evaluation results . I am not sure if there are standard " shared " datasets for this task , which would make the results much stronger .

This is a good point . To address this question , we have generated two other test sets . The first one , Test ( big ) has 1 - 20 variables and 10 - 30 operators per formula . The second , Test ( massive ) has 20 - 26 variables with 20 - 30 operators per formula . Finally , we collected a " real world " test set , Test ( exam ) from formulas found in textbook and exam questions , pruning sequents from the training set that were alpha-equivalent to sequents found in exam data . See the updated Table 1 for the new test - sets , and Table 2 for the updated results . In particular , there is still a gap between what is achieved by our best models and what is theoretically possible ( > 25 % accuracy gap ) for the massive dataset , showing that further research on this topic is needed , and is hopefully enabled by this dataset .

“ Also how about the tradeoff , i.e. , does training time significantly increase when we " imagine " more worlds . “

Yes , the model takes longer to run ( in terms of time ) as we increase the number of worlds , since we need to evaluate the formulas in every world . But in terms of the number of training epochs , it does not take longer to run . One of the interesting things about the PossibleWorld Net is that the number of parameters ( trainable variables ) does not increase as we increase the number of worlds , nor does the model see more data . It just does more parallel computation per data point .

“ This problem , I think , is quite related to model counting . There has been a lot of work on model counting . a discussion on how this relates to those lines of work would be interesting . ”

Thanks , this is a good suggestion . We will certainly look into this for the final version .


SUMMARY

The paper is fairly broad in what it is trying to achieve , but the approach is well thought out . The purpose of the paper is to investigate the effectiveness of prior machine learning methods with predicting logical entailment and then provide a new model designed for the task . Explicitly , the paper asks the following questions : " Can neural networks understand logical formula well enough to detect entailment ? " , and " Which architectures are best at inferring , encoding , and relating features in a purely structural sequence - based problem ? " . The goals of the paper is to understand the learning bias of current architectures when they are tasked with learning logical entailment . The proposed network architecture , PossibleWorldNet , is then viewed as an improvement on an earlier architecture TreeNet .

POSITIVES

The structure of this paper was very well done . The paper attempts to do a lot , and succeeds on most fronts . The generated dataset used for testing logical entailment is given a constructive description which allows for future replication . The baseline benchmark networks are covered in depth and the reader is provided with a deep understanding on the limitations of some networks with regard to exploiting structure in data . The PossibleWorldNets is also given good coverage , and the equations provided show the means by which it operates .
• A clear methodological approach to the research . The paper covers how they created a dataset which can be used for logical entailment learning , and then explains clearly all the previous network models which will be used in testing as well as their proposed model .
• The background information regarding each model was exceptionally thorough . The paper went into great depth describing the pros and cons of earlier network models and why they may struggle with recognizing logical entailment .
• The section describing the creation of a dataset captures the basis for the research , learning logical entailment . They describe the creation of the data , as well as the means by which they increase the difficulty for learning .
• The paper provides an in depth description of their PossibleWorld Net model , and during experimentation we see clear evidence of the models capabilities .

NEGATIVES

One issue I had with the paper is regarding the creation of the logical entailment dataset . Not so much for how they explained the process of creating the dataset , that was very thorough , but the fact that this dataset was the only means to test the previous network models and their new proposed network model . I wonder if it would be better to find non- generated datasets which may contain data that have entailment relationships . It is questionable if their hand crafted network model is learned best on their hand crafted dataset .

The use of a singular dataset for learning logical entailment . The dataset was also created by the researchers for the express purpose of testing neural network capacity to learn logical entailment . I am hesitant to say their proposed network is an incredible achievement since PossibleWorld Net effectively beat out other methods on a dataset that they created expressly for it .

RELATED WORK

The paper has an extensive section dedicated to covering related work . I would say the research involved was very thorough and the researchers understood how their method was different as well as how it was improving on earlier approaches .

CONCLUSION

Given the thorough investigation into previous networks ’ capabilities in logical entailment learning , I would accept this paper as a valid scientific contribution . The paper performs a thorough analysis on the limitations that previous networks face with regard to exploiting structure from data . The paper also covers results of the experiments by not only pointing out their proposed network ’s success , but by analyzing why certain earlier network models were able to achieve competitive learning results . The structure of the PossibleWorld Net was also explained well , and during ex - perimentation demonstrated its ability to learn structure from data . The paper would have been improved through testing of multiple datasets , and not just on there self generated dataset , but the contribution of their research on their network and older networks is still justification enough for this paper .

Thank you for your supportive review and your kind comments . Based on questions you have raised with other reviewers , we have run further tests which we hope confirms your positive sentiment about the paper , and addressed any concerns you had about the testing regime used in the paper . We address here your specific , and very fair , criticism of our paper .

“ One issue I had with the paper is regarding the creation of the logical entailment dataset . Not so much for how they explained the process of creating the dataset , that was very thorough , but the fact that this dataset was the only means to test the previous network models and their new proposed network model . I wonder if it would be better to find non- generated datasets which may contain data that have entailment relationships . It is questionable if their hand crafted network model is learned best on their hand crafted dataset . ”

This is a good point . Since the initial submission , we have run a number of further experiments . In particular , we mined standard logic textbooks ( e.g. , Holbach ’s “ The Logic Manual ” , Mendelson ’s “ Introduction to Mathematical Logic ” ) to find a set of entailment questions that were not produced from our synthetic generative process . We then held - out these entailments ( and all entailments that were equivalent up to variable - renaming ) from the training sets . The new test - set is called “ Test ( Exam ) ” in the revised Table 1 on page 3 . We were gratified that our model achieved 96 % on this “ real - world ” test - set . See the updated Table 2 , reproduced below , including a variety of new larger test sets also described in this revision .

+--------------------+-------+--------+--------+-------+-----------+--------+
| | | test | test | test | test | test |
| model | valid|easy | hard | big | mass . | exam|
+--------------------+-------+--------+--------+-------+-----------+--------+
| Linear BoW | 52.6 | 51.4 | 50.0 | 49.7 | 50.0 | 52.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| MLP BoW | 57.8 | 57.1 | 51.0 | 55.8 | 49.9 | 56.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| ConvNet Enc. | 59.3 | 59.7 | 52.6 | 54.9 | 50.4 | 54.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| LSTM Enc. | 68.3 | 68.3 | 58.1 | 61.1 | 52.7 | 70.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| BiLSTM Enc. | 66.6 | 65.8 | 58.2 | 61.5 | 51.6 | 78.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| TreeNet Enc. | 72.7 | 72.2 | 69.7 | 67.9 | 56.6 | 85.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| TreeLSTM Enc| 79.1 | 77.8 | 74.2 | 74.2 | 59.3 | 75.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| LSTM Trav. | 62.5 | 61.8 | 56.2 | 57.3 | 50.6 | 61.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| TreeLSTM Tr. | 63.3 | 64.0 | 55.0 | 57.9 | 50.5 | 66.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+
| PWN | 98.7 | 98.6 | 96.7 | 93.9 | 73.4 | 96.0 |
+--------------------+-------+--------+--------+-------+-----------+--------+


This is a wonderful and a self - contained paper . In fact , it introduces a very important problem and it solves it .

The major point of the paper is demonstrating that it is possible to model logical entailment in neural networks . Hence , a corpus and a NN model are introduced . The corpus is used to demonstrate that the model , named PossibleWorld , is nearly perfect for the task . A comparative analysis is done with respect to state of the art recurrent NN . So far , so good .

Yet , what is the take home message ? In my opinion , the message is that generic NN should not be used for specific formal tasks whereas specific neural networks that model the task are desirable . This seems to be a trivial claim , but , since the PossibleWorld nearly completely solves the task , it is worth to be investigated .

The point that the paper leaves unexplained is : what is in the PossibleWorld Network that captures what we need ? The description of the network is in fact very criptic . No examples are given and a major effort is required to the reader . Can you provide examples and insights on why this is THE needed model ?

Finally , the paper does not discuss a large body of research that has been done in the past by Plate . Plate has investigated how symbolic predicates can be described in distributed representations . This is strictly related to the problem this paper investigates . As discussed in " Symbolic , Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning : a Survey " , 2017 , the link between symbolic and distributed representations has to be better investigated in order to propose innovative NN models . Your paper can be one of the first NN model that takes advantage of this strict link .

Thank you for your kind words at the beginning of the review , and for your excellent questions and comments . We find the topics addressed in your questions , and your critical points are–we think–fair ones . We confess we were a little surprised by the low score given , considering the generally positive tone of the first half of the review , but this ( along with the comments of other reviewers ) has prompted us to rethink the evaluation of the models in order to address your specific points and hopefully assuage your concerns . We have made revisions to the manuscript to include further tests of our already - trained models , and we respond to parts of your review below .

“ The point that the paper leaves unexplained is : what is in the PossibleWorld Network that captures what we need ? The description of the network is in fact very criptic . No examples are given and a major effort is required to the reader . “

Limitations of space prevented us providing examples in the body of the text . Here is a simple example from propositional logic . To check whether p ∨ q entails p , we consider all possible truth -value assignments to the variables p and q. We get four assignments :
p → ⊥ , q → ⊥
p → ⊥ , q → ⊤
p → ⊤ , q → ⊥
p → ⊤ , q → ⊤

Now p ∨ q entails p if , for each of these assignments , if the assignment satisfies p ∨ q , then it also satisfies p . In this example , the entailment is false , since the second assignment ( p → ⊥ , q → ⊤ ) satisfies p ∨ q but does not satisfy p.

We will endeavour to add such examples to the appendix if the paper is accepted .

“ Can you provide examples and insights on why this is THE needed model ? ”

Consider the standard model - theoretic definition of entailment : A entails B if , for every possible world w , if sat ( w , A ) then sat ( w , B ) :

A ⊧ B iff for every world w ∈ W , sat ( w , A ) implies sat ( w , B )

We replace possible worlds with random vectors , transform the universal quantification into a product , and provide a neural network that implements sat ( w , A ) . The reason we believe this is * the* needed model is that it is a continuous relaxation of the standard model - theoretic definition of entailment .

" In my opinion , the message is that generic NN should not be used for specific formal tasks whereas specific neural networks that model the task are desirable . This seems to be a trivial claim . "

At this high level of generality , the claim is , indeed , trivial . But our claim is more specific . We provide implementation details of a particular model that outperforms other models on this task . This model is also applicable outside the particular domain of logic .

The PossibleWorld Net was inspired by the model - theoretic definition of entailment , in terms of truth in all possible worlds . But it is not a specific model that is only useful for this particular task . It is a general model based on the following simple idea : first , evaluate the same model multiple times using different vectors of random noise as inputs ; second , combine the results from these multiple runs using a product . This general model is applicable outside the domain of logical entailment ; it could be useful for building robust image classifiers , for example .

Since the initial submission , we have run a number of experiments that are significantly more ambitious . See the updated Table 1 on page 3 . In the “ Big ” and “ Massive ” test sets , the expected number of truth - table rows needed to exhaustively verify entailment is 3000 and 800,000 . Our Possible World Net continues to out - perform the other models on these harder test - sets , but it does not completely solve the task . In particular , in the massive test set , it achieves 73 % . This score is significantly better than the other models , but it is not a complete solution .

We hope that these explanations , which have been integrated into the revised manuscript , alongside the inclusion of further tests , without need to change the training protocol or model definitions , on significantly more complex logic problems and " real - world " exam data , will help convince you that this work merits publication .

If you persist in your assessment , we will understand , but would be grateful if you could highlight what is lacking given this further empirical evidence provided in this revision , so that we may continue to improve the paper .

This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization . The proposed continuous relaxation scheme allows for gradient based - stochastic optimization for binary discrete variables under the reparameterization trick , and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones , with additional stretching and thresholding operations . Under a compound construction of sparsity , the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables , or be combined with other types of regularizations on the magnitude of non- zero components . The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods .

Pros :

- The paper is clearly written , self - contained and a pleasure to read .
- Based on the evidence provided , the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization

Cons :

- It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices
- It is unclear what properties does the proposed hard - concrete distribution have , e.g. , closed - form density , convexity , etc .
- If the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice , this paper would be more significant


We would first like to thank you for taking the time to review our submission ; we will now address your comments :

- We agree that comparing other L_p choices with the L_0 norm is beneficial ; we would like to point out that the GL ( Group Lasso ) baseline method for the LeNet5 - Caffe experiment employs L_1 regularization for pruning neurons and convolutional filters so we believe that it can serve as a way to measure differences between the L_0 norm and the most popular L_p alternative .

- Due to lack of space we provided a bit of more information about the hard concrete distribution at the appendix ; it has a closed - form density that involves the CDF and PDF of the concrete distribution .

- The stretching parameters were initially chosen heuristically and kept fixed for all of the experiments . The heuristic was to approximately aim for clipping to zero if the value of the random variable is less than 0.1 or rounding to 1 if the value of the random variable is larger than 0.9 . It should be noted that their choice is not particularly important due to their interplay with the temperature parameter of the concrete distribution ; they collectively determine the probabilities of the endpoints {0 , 1 } , i.e. p( z=0 ) and p ( z=1 ) . As a result we believe that the choice of the stretching parameters is not very important , given the fact that the temperature of the concrete distribution is tuned appropriately .


Learning sparse neural networks through L0 regularisation

Summary :

The authors introduce a gradient - based approach to minimise an objective function with an L0 sparse penalty . The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables ( representing whether a variable is present or not ) to an expectation over continuous variables , inspired by earlier work from Maddison et al ( ICLR 2017 ) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks . Here the application is to learn sparse feedforward networks in standard classification tasks , although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal . The method provides equivalent accuracy and sparsity to published state - of - the- art results on these datasets but it is argue that learning sparsity during the training process will lead to significant speed - ups - this is demonstrated by comparing to a theoretical benchmark ( standard training with dropout ) rather than through empirical testing against other implementations .

Pros :

The paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory .

Optimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters .

The work is put in context and related to some previous relaxation approaches to sparsity .

The method allows for sparsity to be learned during training rather than after training ( as in standard dropout approaches ) and this allows the algorithm to obtain significant per-iteration speed -ups , which improves through training .

Cons :

The method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state - of - the- art methods . Therefore the main advance is in terms of learning speed to obtain this similar performance . However , the learning speed - up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout . It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered , e.g. does the proposed sparse learning method converge at the same rate as the others ? I felt a more thorough experimental section would have greatly improved the work , focussing on this learning speed aspect .

It was unclear how much tuning of the lambda hyper - parameter , which tunes the sparsity , would be required in a practical application since tuning this parameter would increase computation time . It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper- parameter learning .

Minor point : it was n’t completely clear to me why the fact ( 3 ) is a variational approximation to a spike - and - slab is important ( Appendix ) . I do n’t see why the spike - and - slab is any more fundamental than the L0 norm prior in ( 2 ) , it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter . In the context here this did n’t seem a particularly relevant addition to the paper .



We would first like to thank you for the thorough and extensive review . Regarding whether the method converges in a similar way to standard networks ; indeed this is the case . On the CIFAR task with WRNs the L0 regularized networks had similar learning curves with the dropout equivalent networks . We have updated the paper with an example plot on CIFAR 10 .

Regarding the lambda hyperparameter ; this is true . Empirically , we did n’t have to tune this parameter a lot and considered a small set of values . Treating this parameter in a Bayesian way would indeed be a fruitful direction for future research .

As for the spike - and - slab connection ; we agree that it is a minor point ( hence it is in the appendix ) , but we still believe that it is a relevant addition to the paper . It provides an interpretation to the L0 objective that also allows for the incorporation of prior knowledge about the behaviour of the sparity in the form of a prior over the gates . This could then potentially allow for better regularization of the gating mechanism .

The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network . The basic problem is empirical risk minimization with a incremental penalty for each non zero weight . To tackle this problem , this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables . The authors note that this loss can also be seen as a specific variational bound of a Bayesian model over the weights . The key advantage of this method is that it gives a training time technique for sparsifying neural network computation , leading to potential wins in computation time during training .

The results presented in the paper are convincing . They achieve results competitive with previous methods , with the additional advantage that their sparse models are available during training time . They show order of magnitude reductions in computation time for small models , and more modest constant improvements for large models . The hard concrete distribution is a small but nice contribution on its own .

My only concern is the lack of discussion on the relationship between this method and Concrete Dropout ( https://arxiv.org/abs/1705.07832). Although the focus is apparently different , these methods are clearly closely related . A discussion of this relationship seems really important .

Specific comments / questions :
- The reduction of computation time is the key advantage , and it would have been nice to see a more thorough investigation of this . For example , it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units ( as opposed to single weights ) or other subsets of the computation . This would give a stronger sense of the kind of wins that are possible in this framework .
- Hard concrete is a nice contribution , but there are clearly many possibilities for these relaxations . Extra evaluations of different relaxations would be appreciated . At the very least a comparison to concrete would be nice .
- In equation 2 , the equality of the L0 norm with the sum of z assumes that tilde{theta} is not 0 .

We would first like to thank you for the constructive review ; we revised the paper and now it contains a discussion of concrete Dropout . The main difference is that concrete dropout does not allow for values of exact zero ( and one ) thus precluding the benefits of sparsity during training time . One potential way to employ concrete Dropout in this case would be to use it as a biased surrogate for the optimization of eq . 3 ; this could still allow for potential sparsity at test time by pruning according to thresholds , but nevertheless would require evaluating the full original model during training . As for your other comments :

- Perhaps it 's not very prominent but all of our results employ structured penalties , i.e. we are removing either entire convolutional feature maps or entire hidden units .

- For the reasons we previously mentioned , we believe that comparing with concrete dropout will not provide much extra information as the sparsity could only be achieved at test time and not during training ( which was one of the main objectives of this work ) . An alternative that maintains the sparsity during training time , and we experimented with in a pilot study , was a smoothing mechanism that involved the hard - sigmoid of a Gaussian r.v. . This turned out to be worse than the hard concrete procedure , and we attribute this to the unimodality of the underlying Gaussian distribution ( which cannot accurately capture the behaviour of a Bernoulli r.v. ) . We mentioned a couple of sentences about this in the related work section . We also included a comparison against “ Generalized Dropout ” ( GD ) that utilizes the straight - through estimator for the same LeNet - 5 task we considered in the experiments ; this can serve as a comparison against the proposed hard concrete smoothing procedure .

- This is indeed true and we have updated the text accordingly .

This paper introduces a representation learning step in the Intrinsically Motivated Exploration Process ( IMGEP ) framework .

Though this work is far from my expertise fields I find it quite easy to read and a good introduction to IMGEP .
Nevertheless I have some major concerns that prevent me from giving an acceptance decision .

1 ) The method uses mechanisms than can project back and forth a signal to the " outcome " space . Nevertheless only the encoder / projection part seems to be used in the algorithm presented p6 . For example the encoder part of an AE / VAE is used as a preprocesing stage of the phenomenon dynamic D . It should be obviously noticed that the decoder part could also be used for helping the inverse model I but apparently that is not the case in the proposed method .

2 ) The representation stage R seems to be learned at the beginning of the algorithm and then fixed . When using DNN as R ( when using AE / VAE ) why do n't you propagate a gradient through R when optimizing D and I ? In this way , learning R at the beginning is only an old good pre-training of DNN with AE .

3 ) Eventually , Why not directly considering R as lower layers of D and using up to date techniques to train it ? ( drop -out , weight clipping , batch normalization ... ) .
Why not using architecture adapted to images such as CNN ?



These comments suggest that the reviewer thinks that in the particular experiment we made , and thus the particular implementation of IMGEPs we used , we are training a single large neural network for learning forward and inversed models . We could have done this indeed , and in that case the reviewer ' suggestion would recommend very relevantly to use the lower - layers and / or decoding projection of the ( variational ) auto-encoders . However , we are not using neural networks for learning forward and inverse models , but rather non- parametric methods based on memorizing examplars associating the parameters of DMPs and their outcomes in the embedding space ( which itself comes from auto - encoders ) ,
in combination with local online regression models and optimization on these local models . This approach comes from the field of robotics , where is has shown extremely efficient for fast incremental learning of forward and inverse models . Comparing this approach with a full neural network approach ( which might generalize better but have difficulties for fast incremental learning ) would be a great topic for another paper . In the new version of the article , we have tried to improve the clarity of the description of the particular implementation of IMGEPs we have used .

[ Edit : After revisions , the authors have made a good - faith effort to improve the clarity and presentation of their paper : figures have been revised , key descriptions have been added , and ( perhaps most critically ) a couple of small sections outlining the contributions and significance of this work have been written . In light of these changes , I 've updated my score . ]

Summary :

The authors aim to overcome one of the central limitations of intrinsically motivated goal exploration algorithms by learning a representation without relying on a " designer " to manually specify the space of possible goals . This work is significant as it would allow one to learn a policy in complex environments even in the absence of a such a designer or even a clear notion of what would constitute a " good " distribution of goal states .

However , even after multiple reads , much of the remainder of the paper remains unclear . Many important details , including the metrics by which the authors evaluate performance of their work , can only be found in the appendix ; this makes the paper very difficult to follow .

There are too many metrics and too few conclusions for this paper . The authors introduce a handful of metrics for evaluating the performance of their approach ; I am unfamiliar with a couple of these metrics and there is not much exposition justifying their significance and inclusion in the paper . Furthermore , there are myriad plots showing the performance of the different algorithms , but very little explanation of the importance of the results . For instance , in the middle of page 9 , it is noted that some of the techniques " yield almost as low performance as " the randomized baseline , yet no attempt is made to explain why this might be the case or what implications it has for the authors ' approach . This problem pervades the paper : many metrics are introduced for how we might want to evaluate these techniques , yet there is no provided reason to prefer one over another ( or even why we might want to prefer them over the classical techniques ) .

Other comments :
- There remain open questions about the quality of the MSE numbers ; there are a number of instances in which the authors cite that the " Meta-Policy MSE is not a simple to interpret " ( The remainder of this sentence is incomplete in the paper ) , yet little is done to further justify why it was used here , or why many of the deep representation techniques do not perform very well .
- The authors do not list how many observations they are given before the deep representations are learned . Why is this ? Additionally , is it possible that not enough data was provided ?
- The authors assert that 10 dimensions was chosen arbitrarily for the size of the latent space , but this seems like a hugely important choice of parameter . What would happen if a dimension of 2 were chosen ? Would the performance of the deep representation models improve ? Would their performance rival that of RGE -FI ?
- The authors should motivate the algorithm on page 6 in words before simply inserting it into the body of the text . It would improve the clarity of the paper .
- The authors need to be clearer about their notation in a number of places . For instance , they use \gamma to represent the distribution of goals , yet it does not appear on page 7 , in the experimental setup .
- It is never explicitly mentioned exactly how the deep representation learning methods will be used . It is pretty clear to those who are familiar with the techniques that the latent space is what will be used , but a few equations would be instructive ( and would make the paper more self - contained ) .

In short , the paper has some interesting ideas , yet lacks a clear takeaway message . Instead , it contains a large number of metrics and computes them for a host of different possible variations of the proposed techniques , and does not include significant explanation for the results . Even given my lack of expertise in this subject , the paper has some clear flaws that need addressing .

Pros :
- A clear , well - written abstract and introduction
- While I am not experienced enough in the field to really comment on the originality , it does seem that the approach the authors have taken is original , and applies deep learning techniques to avoid having to custom - design a " feature space " for their particular family of problems .

Cons :
- The figure captions are all very " matter - of - fact " and , while they explain what each figure shows , provide no explanation of the results . The figure captions should be as self - contained as possible ( I should be able to understand the figures and the implications of the results from the captions alone ) .
- There is not much significance in the current form of the paper , owing to the lack of clear message . While the overarching problem is potentially interesting , the authors seem to make very little effort to draw conclusions from their results . I.e. it is difficult for me to easily visualize all of the " moving parts " of this work : a figure showing the relationship bet
- Too many individual ideas are presented in the paper , hurting clarity . As a result , the paper feels scattered . The authors do not have a clear message that neatly ties the results together .

> R3 " does not include significant explanation for the results " , " The figure captions are all very " matter - of - fact " and , while they explain what each figure shows , provide no explanation of the results . "
We agree . We have added several more detailed explanations of the results .

> R3 " why many of the deep representation techniques do not perform very well . "
We think this comment is due to our unclear explanation of our main target combined with the use of a misleading measure ( MSE ) . We hope the new explanation we provide , as well as the focus on exploration measures based on the KL divergence will enable to make it more clear that on the contrary several deep learning approaches are performing very well , some systematically outperforming the use of handcrafted goal space features ( see the common answer to all reviewers ) .

> R3 " The authors assert that 10 dimensions was chosen arbitrarily for the size of the latent space , but this seems like a hugely important choice of parameter . What would happen if a dimension of 2 were chosen ? Would the performance of the deep representation models improve ? Would their performance rival that of RGE - FI ? "

We agree that this is a very important point . We have in the new version included results when one gives algorithms the right number of dimensions ( 2 for arm -ball , 3 for arm -arrow ) , and showing that providing more dimensions to IMGEP - UGL algorithms than the " true " dimensionality of the phenomenon can actually be beneficial ( and we provide an explanation why this is the case ) .

> " The authors do not list how many observations they are given before the deep representations are learned . Why is this ? Additionally , is it possible that not enough data was provided ? "

For each environments , we trained the networks with a dataset of 10.000 elements uniformly sampled in the underlying state-space . This corresponds to 100 samples per dimension for the ' armball ' environment , and around 20 per dimension for the ' armarrow ' environment . This is not far from the number of samples considered in the dsprite dataset , in which around 30 samples per dimensions are considered . Moreover , our early experiments showed that for those two particular problems , adding more data did not change the exploration results .

> " - The authors should motivate the algorithm on page 6 in words before simply inserting it into the body of the text . It would improve the clarity of the paper . "

We have tried to better explain in words the general principles of this algorithm .

> " The authors need to be clearer about their notation in a number of places . For instance , they use gamma to represent the distribution of goals , yet it does not appear on page 7 , in the experimental setup . "

We have tried to correct these problems in notations .

> " It is never explicitly mentioned exactly how the deep representation learning methods will be used . It is pretty clear to those who are familiar with the techniques that the latent space is what will be used , but a few equations would be instructive ( and would make the paper more self - contained ) . "

yes indeed . We have added some new explanations .


The paper investigates different representation learning methods to create a latent space for intrinsic goal generation in guided exploration algorithms . The research is in principle very important and interesting .

The introduction discusses a great deal about intrinsic motivations and about goal generating algorithms . This is really great , just that the paper only focuses on a very small aspect of learning a state representation in an agent that has no intrinsic motivation other than trying to achieve random goals .
I think the paper ( not only the Intro ) could be a bit condensed to more concentrate on the actual contribution .

The contribution is that the quality of the representation and the sampling of goals is important for the exploration performance and that classical methods like ISOMap are better than Autoencoder - type methods .

Also , it is written in the Conclusions ( and in other places ) : " [ ..] we propose a new intrinsically Motivated goal exploration strategy .... " . This is not really true . There is nothing new with the intrinsically motivated selection of goals here , just that they are in another space . Also , there is no intrinsic motivation . I also think the title is misleading .

The paper is in principle interesting . However , I doubt that the experimental evaluations are substantial enough for profound conclusion .

Several points of critic :
- the input space was very simple in all experiments , not suitable for distinguishing between the algorithms , for instance , ISOMap typically suffers from noise and higher dimensional manifolds , etc .
- only the ball / arrow was in the input image , not the robotic arm . I understand this because in phase 1 the robot would not move , but this connects to the next point :
- The representation learning is only a preprocessing step requiring a magic first phase .
-> Representation is not updated during exploration
- The performance of any algorithm ( except FI ) in the Arm - Arrow task is really bad but without comment .
- I am skeptical about the VAE and RFVAE results . The difference between Gaussian sampling and the KDE is a bit alarming , as the KL in the VAE training is supposed to match the p ( z ) with N ( 0 ,1 ) . Given the power of the encoder / decoder it should be possible to properly represent the simple embedded 2D/3D manifold and not just a very small part of it as suggested by Fig 10 .
I have a hard time believing these results . I urge you to check for any potential errors made . If there are not mistakes then this is indeed alarming .

Questions :
- Is it true that the robot always starts from same initial condition ?! Context=Emptyset .
- For ISOMap etc , you also used a 10dim embedding ?

Suggestion :
- The main problem seems to be that some algorithms are not representing the whole input space .
- an additional measure that quantifies the difference between true input distribution and reproduced input distribution could tier the algorithms apart and would measure more what seems to be relevant here . One could for instance measure the KL - divergence between the true input and the sampled ( reconstructed ) input ( using samples and KDE or the like ) .
- This could be evaluated on many different inputs ( also those with a bit more complicated structure ) without actually performing the goal finding .
- BTW : I think Fig 10 is rather illustrative and should be somehow in the main part of the paper

On the positive side , the paper provides lots of details in the Appendix .
Also , it uses many different Representation Learning algorithms and uses measures from manifold learning to access their quality .

In the related literature , in particular concerning the intrinsic motivation , I think the following papers are relevant :
J. Schmidhuber , PowerPlay : training an increasingly general problem solver by continually searching for the simplest still unsolvable problem . Front. Psychol. , 2013 .

and

G. Martius , R. Der , and N. Ay. Information driven self -organization of complex robotic behaviors . PLoS ONE , 8 ( 5 ) :e63400 , 2013 .


Typos and small details :
p3 par2 : for PCA you cited Bishop . Not critical , but either cite one the original papers or maybe remove the cite altogether
p4 par - 2 : has multiple interests ... : interests -> purposes ?
p4 par - 1 : Outcome Space to the agent is is ...
Sec 2.2 par1 : are rapidly mentioned ... -> briefly
Sec 2.3 ... Outcome Space O , we can rewrite the architecture as :
and then comes the algorithm . This is a bit weird
Sec 3 : par1 : experimental campaign -> experiments ?
p7 : Context Space : the object was reset to a random position or always to the same position ?
Footnote 14 : superior to -> larger than
p8 par2 : Exploration Ratio Ratio_expl ... probably also want to add ( ER ) as it is later used
Sec 4 : slightly underneath -> slightly below
p9 par1 : unfinished sentence : It is worth noting that the ....
one sentence later : RP architecture ? RPE ?
Fig 3 : the error of the methods ( except FI ) are really bad . An MSE of 1 means hardly any performance !
p11 par2 : for e.g. with the SAGG ..... grammar ?

Plots in general : use bigger font sizes .



> R1 " an agent that has no intrinsic motivation other than trying to achieve random goals . "
" There is nothing new with the intrinsically motivated selection of goals here , just that they are in another space . Also , there is no intrinsic motivation . I also think the title is misleading . "

The concept of " intrinsically motivated learning and exploration " is not yet completely well - defined across ( even computionational ) communities , and we agree that the use of the term " intrinsically motivated exploration " in this article may seem unusual for some readers . However , we strongly think it makes sense to keep it for the following reasons .

There are several conceptual approaches to the idea of " intrinsically motivated learning and exploration " , and we believe our use of the term intrinsic-motivation is compatible with all of them :

- Focus on task - independance and self - generated goals : one approach of intrinsic motivation , rooted in its conceptual origins in psychology , is that it designates the set of mechanisms and behaviours of organized exploration which are not directed towards a single extrinsically imposed goal / problem ( or towards fullfilling physiological motivations like food search ) , but rather are self - organized towards intrinsically defined objectives and goals ( independant of physiological motivations like food search ) . From this perspective , mechanisms that self - generate goals , even randomly , are maybe the simplest and most prototypical form of intrinsically motivated exploration .

- Focus on information - gain or competence - gain driven exploration : Other approaches consider that intrinsically motivated exploration specifically refers to mechanisms where choices of actions or goals are based on explicit measures of expected information - gain about a predictive model , or novelty or surprise of visited states , or competence gain for self - generated goals . In the IMGEP framework , this corresponds specifically to IMGEP implementations where the goal sampling procedure is not random , but rather based on explicit estimations of expected competence gain , like in the SAGG -RIAC architecture or in modular IMGEPs of ( Forestier et al. , 2017 ) . In the experiments presented in this article , the choice of goals is made randomly as the focus is not on the efficiency of the goal sampling policy . However , it would be straightforward to use a selection of goals based on expected competence gain , and thus from this perspective the proposed algorithm adresses the general problem of how to learn goal representations in IMGEPs .

- Focus on noverly / diversity search mechanisms : Yet another approach to intrinsically motivated learning and exploration is one that refers to mechanisms that organize the learner 's exploration so that exploration of novel or diverse behaviours is fostered . A difference with the previous approach is that here one does not necessarily use internally a measure of novelty or diversity , but rather one uses it to characterize the dynamics of the behaviour . And an interesting property of random goal exploration implementations of IMGEPs is that while it does not measure explicitly novelty or diversity , it does in fact maximize it through the following mechanism : from the beginning and up to the point where the a large proportion of the space has been discovered , generating random goals will very often produce goals that are outside the convex hull of already discovered goals . This in turn mechanically leads to exploration of stochastic variants of motor programs that produce outcomes on the convex hull , which statistically pushes the convex hull further , and thus fosters exploration of motor programs that have a high probability to produce novel outcomes outside the already known convex hull .




> R1 " The representation learning is only a preprocessing step requiring a magic first phase .
> -> Representation is not updated during exploration "
> " - only the ball / arrow was in the input image , not the robotic arm . I understand this because in phase 1 the robot would not move , but this connects to the next point : "

Indeed , representation is not updated during exploration , and as mentioned in the conclusion we think doing this is a very important direction for future work . However , we have two strong justification for this decomposition , that we added in the paper .

First , we do not believe the preliminary pre-processing step is " magical " . Indeed , if one studies the work from the developmental learning perspective outlined in the introduction , where one takes inspiration from the processes of learning in infants , then this decomposition corresponds to a well - known developmental progression : in their first few weeks , motor exploration in infants is very limited ( due to multiple factors ) , while they spend a considerable amount of time observing what is happening in the outside world with their eyes ( e.g. observing images of others producing varieties of effects on objects ) . During this phase , a lot of perceptual learning happens , and this is reused later on for motor learning ( infant perceptual development often happens ahead of motor development in several important ways ) . In the article , the concept of " social guidance " presented in the introduction , and the availability of a database of observations of visual effects that can happen in the world , can be seen as a model of this first phase of infant learning by passively observing what is happening around them .

A second justification for this decomposition is more methodological . It is mainly an experimental tool for better understanding what is happening . Indeed , the underlying algorithmic mechanisms are already quite complex , and analyzing what is happening when one decomposes learning in these two phases ( representation learning , then exploration ) is an important scientific step . Presenting in the same article another study where representations would be updated continuously would result in too much material to be clearly presented in a conference paper .

> R1 " the input space was very simple in all experiments , not suitable for distinguishing between the algorithms , for instance , ISOMap typically suffers from noise and higher dimensional manifolds "

The use of the term " simple " depends on the perspective . From the perspective of a classical goal exploration process that would use the 4900 raw pixels as input , not knowing they are pixels and considering them similarly as when engineered representations are provided , then this is a complicated space and exploration is very difficult . At the same time , from the point of view of representation learning algorithms , this is indeed a moderately complex input space ( yet , we on purpose did not consider convolutionnal auto-encoders so that the task is not too simplified and results could apply to other modalities such as sound or proprioception ) . Third , if one considers the dimensionality of the real sensorimotor manifold in which action is happening ( 2 for arm -ball , 3 for arm -arrow ) , this does not seem to us to be too unrealistic as many of real world sensorimotor tasks are actually happening in low-dimensional task spaces ( e.g. rigid object manipulation happens in a 6D task space ) . So , overall we have chosen these experimental setups as we belive they are a good compromise between simplicity ( enabling us to understand well what is happening ) and complexity ( if one considers the learner does not already knows that the stimuli are pixels of an image ) .

> R1 " The performance of any algorithm ( except FI ) in the Arm - Arrow task is really bad but without comment . "
See general answer and new graphs in the paper : most algorithms actually perform very well from the main perspective of interest in the paper ( exploration efficiency ) .

> R1 " - I am skeptical about the VAE and RFVAE results . If there are not mistakes then this is indeed alarming . "
> R1 " - The main problem seems to be that some algorithms are not representing the whole input space .

Following your remark , we double checked the code and made an in depth verification of results . A small bug indeed existed , which made the projection of points in latent space wider than it should be . This was fixed in those new experiments , and we validated that the whole input space was represented in the latent representation . Despite this , it did n't changed the conclusion drawn in the original paper . Indeed , our new results show the same type of behavior as in the first version , in particular :
+ The exploration performances for VAE with KDE goal sampling distribution are still above Gaussian goal Sampling . Our experiments showed that convergence on the KL term of the loss can be more or less quick depending on the initialization . Since we used an number of iterations as stopping criterion for our trainings ( based on early experiments ) , we found that sometimes , at stop , despite achieving a low reconstruction error , the divergence was still pretty high . In those cases the representation was not perfectly matching an isotropic gaussian , which lead to biased sampling .
+ The performances of the RFVAE are still worse than any other algorithms . Our experiments showed that they introduce a lot of discontinuities in the representation , which along with physics boundaries of achievable states , can generate " pockets " in the representation from which a Random Goal Exploration ca n't escape . This would likely be different for a more advanced exploration strategy such as Active Goal exploration .

> R1 - Is it true that the robot always starts from same initial condition ?! Context=Emptyset .

yes . In ( Forestier et al. , ICDL - Epirob 2016 ) , a similar setup is used except that the starting conditions are randomized at each new episode ( and that goal representation are engineered ) : they show that the dynamics of exploration scales wells . Here we chose to start from the same initial condition to be able to display clearly in 2D the full space of discovered outcomes ( if one would include the starting ball position , this would be a 4D space ) .

> R1 - For ISOMap etc , you also used a 10dim embedding ?

yes .

> In the related literature , in particular concerning the intrinsic motivation , I think the following papers are relevant :
>J. Schmidhuber , PowerPlay : training an increasingly general problem solver by continually searching for the simplest still unsolvable problem . Front. Psychol. , 2013 .
>and
>G. Martius , R. Der , and N. Ay. Information driven self -organization of complex robotic behaviors . PLoS ONE , 8 ( 5 ) :e63400 , 2013 .

yes , these are relevant papers indeed , which are cited in reviews we cite , but we added them for more coverage .

This paper proposes a method for learning from noisy labels , particularly focusing on the case when data is n't redundantly labeled ( i.e. the same sample is n't labeled by multiple non - expert annotators ) . The authors provide both theoretical and experimental validation of their idea .

Pros :
+ The paper is generally very clearly written . The motivation , notation , and method are clear .
+ Plentiful experiments against relevant baselines are included , validating both the no-redundancy and plentiful redundancy cases .
+ The approach is a novel twist on an existing method for learning from noisy data .

Cons :
- All experiments use simulated workers ; this is probably common but still not very convincing .
- The authors missed an important related work which studies the same problem and comes up with a similar conclusion : Lin , Mausam , and Weld . " To re ( label ) , or not to re ( label ) . " HCOMP 2014 .
- The authors should have compared their approach to the " base " approach of Natarajan et al .
- It seems too simplistic too assume all workers are either hammers or spammers ; the interesting cases are when annotators are neither of these .
- The ResNet used for each experiment is different , and there is no explanation of the choice of architecture .

Questions :
- How would the model need to change to account for example difficulty ?
- Why are Joulin 2016 , Krause 2016 not relevant ?
- Best to clarify what the weights in the weighted sum of Natarajan are .
- " large training error on wrongly labeled examples " -- how do we know they are wrongly labeled , i.e. do we have a ground truth available apart from the crowdsourced labels ? Where does this ground truth come from ?
- Not clear what " Ensure " means in the algorithm description .
- In Sec. 4.4 , why is it important that the samples are fresh ?


Replies to each point follow :

1 . Re “ All experiments use simulated workers ; this is probably common but still not very convincing . ”

Please note that in experiments on MSCOCO , we procured the real noisy labels from the raw data . See in abstract : “ Experiments conducted on … and MSCOCO ( using the real crowdsourced labels ) ... ”

2 . Re “ The authors missed an important related work which studies the same problem and comes up with a similar conclusion : Lin , Mausam , and Weld . " To re ( label ) , or not to re ( label ) . " HCOMP 2014 . ”

We agree that this is one of the most relevant works . Note that we cited this work along with their 2016 paper along similar lines “ Re-active learning : Active learning with relabeling . ” . Also , note that unlike ours , their work does not use predictions of the supervised learning algorithm to estimate the true labels .

3 . Re “ The authors should have compared their approach to the " base " approach of Natarajan et al . ”

Their approach is designed for the binary classification setting when all the workers are identical . We study the multi-class classification setting where workers have varying qualities .

4 . Re “ It seems too simplistic to assume all workers are either hammers or spammers ; the interesting cases are when annotators are neither of these . ”

We agree and point out ( 1 ) that we considered two other worker models . In synthetic dataset , e.g. we consider class - wise hammer spammer , where each worker is hammer for some of the classes and spammer for the other classes . ( 2 ) We report experiments on MSCOCO with labels collected by real workers .

5 . Re “ The ResNet used for each experiment is different , and there is no explanation of the choice of architecture . ”

For simulated worker experiments on CIFAR10 and ImageNet , we used the fewest possible layers . These choices are dictated by the ResNet implementation that we used “ https://github.com/tornadomeet/ResNet/”. Smaller ResNet architectures save training time , enabling us to perform experiments on more baseline algorithms , worker noise models , and levels of redundancy .

For MSCOCO experiments , we used a 98 - layer ResNet because this is a relatively small dataset . Also , we did not have various experiments to run for different worker noise models here .

6 . Re “ How would the model need to change to account for example difficulty ? ”

When we include example difficulty in the model , there are three sets of latent parameters to be estimated : worker qualities , example difficulties and the true labels . A standard approach to learn these parameters is to use alternating maximum likelihood estimation where we initialize the two sets of parameters and estimate the third one and iterate over . In our algorithm , we would need to estimate example difficulties by maximizing the likelihood of the observed data given the intermediate estimate of worker qualities and the labeling function .

7 . Re “ Why are Joulin 2016 , Krause 2016 not relevant ? ”

Two important differences between these works & our setting : a ) they have only one label per example - no redundancy . b) they do not aim to estimate worker qualities .

8 . Re “ Best to clarify what the weights in the weighted sum of Natarajan are . ”

Update : We have provided the weights in Natarajan et al in the revised draft in the first paragraph of Section 4.1 - “ Learning with noisy labels ” .

9 . Re “ " large training error on wrongly labeled examples " -- how do we know they are wrongly labeled ...? ”
You are correct that we do not know which examples are wrongly labeled and we do not have ground truth available apart from the crowdsourced labels . We would humbly point out that the statement " large training error on wrongly labeled examples " is not a part of our algorithm . The purpose of the statement is to justify why comparing worker responses to the model prediction would give a good estimation of the worker qualities . It is further elaborated in the text below the line " large training error on wrongly labeled examples " .

10 . Re “ Not clear what " Ensure " means in the algorithm description . ”

In the algorithmic package , “ Input ” and “ Output ” are expressed with “ Require ” and “ Ensure ” respectively . So “ Ensure ” just means the output of the algorithm . We agree that “ Input ” and “ Output ” are clearer and modified the latest version to use these terms .

11 . Re “ In Sec. 4.4 , why is it important that the samples are fresh ? ”

As we mention in the paper , fresh samples are required for the analysis to hold . It allows the estimated worker qualities and the predictor function learned in each step to be independent of each other which is required for the Theorem 4.1 to hold . We point out that practically , fresh samples are not required for the algorithm to succeed , and in our implementation we do not use fresh samples in each round .


This paper focuses on the learning - from-crowds problem when there is only one ( or very few ) noisy label per item . The main framework is based on the Dawid- Skene model . By jointly update the classifier weights and the confusion matrices of workers , the predictions of the classifier can help on the estimation problem with rare crowdsourced labels . The paper discusses the influence of the label redundancy both theoretically and empirically . Results show that with a fixed budget , it ’s better to label many examples once rather than fewer examples multiple times .

The model and algorithm in this paper are simple and straightforward . However , I like the motivation of this paper and the discussion about the relationship between training efficiency and label redundancy . The problem of label aggregation with low redundancy is common in practice but hardly be formally analyzed and discussed . The conclusion that labeling more examples once is better can inspire other researchers to find more efficient ways to improve crowdsourcing .

About the technique details , this paper is clearly written , but some experimental comparisons and claims are not very convincing . Here I list some of my questions :
+ About the MBEM algorithm , it ’s better to make clear the difference between MBEM and a standard EM . Will it always converge ? What ’s its objective ?
+ The setting of Theorem 4.1 seems too simple . Can the results be extended to more general settings , such as when workers are not identical ?
+ When n = O ( m log m ) , the result that \epslon_1 is constant is counterintuitive , people usually think large redundancy r can bring benefits on estimation , can you explain more on this ?
+ During CIFAR - 10 experiments when r=1 , each example only have one label . For the baselines weighted - MV and weighted - EM , they can only be directly trained using the same noisy labels . So can you explain why their performance is slightly different in most settings ? Is it due to the randomly chosen procedure of the noisy labels ?
+ For ImageNet and MS-COCO experiments with a fixed budget , you reduced the training set when increasing the redundancy , which is unfair . The reduction of performance could mainly cause by seeing fewer raw images , but not the labels . It ’s better to train some semi-supervised model to make the settings more comparable .


Thanks for the clear review and actionable recommendations . We have modified the draft per your feedback and reply to each point below :
1 . Re : “ What ’s [ MBEM ’s ] objective ? ” : Thanks for spotting this oversight . The objective for MBEM is the maximum likelihood estimation of latent parameters under the Dawid- Skene model , where the true labels are replaced by the model predictions . We have added this to the revised draft in Section 4 , Algorithm . Yes , the MBEM will converge under mild conditions when the worker quality is above a threshold and number of training examples is sufficiently large .
2 . Re : “ Can the results be extended to more general settings , such as when workers are not identical ?
Please note that the Theorem 4.1 includes the scenario when the workers are not identical . The two critical quantities $ \alpha$ and $ \beta$ that capture the average worker quality in the Theorem are defined for a general setting when the workers are not identical in the appendix . For simplicity and to illustrate the main idea of the theorem in the main paper we have defined them for the particular setting when all the workers are identical .

3 . Re : “ When n = O ( m log m ) , the result that \epslon_1 is constant is counterintuitive , people usually think large redundancy r can bring benefits on estimation , can you explain more on this ? ”
The expression O ( m log m ) hides redundancy r as a constant . In the revised draft , we have modified the statement to “ when n = O ( ( m log m ) / r ) the epsilon_1 is sufficiently small . ” That is if the redundancy r is large the number of training examples n required for achieving epsilon_1 to be a small constant decreases .

4 . Re “ During CIFAR - 10 experiments when r=1 , each example only have one label . For the baselines weighted - MV and weighted - EM , they can only be directly trained using the same noisy labels . So can you explain why their performance is slightly different in most settings ? Is it due to the randomly chosen procedure of the noisy labels ? ”

Yes , you are correct . When r = 1 , the baselines weighted - MV and weighted - EM can only be trained using the same noisy labels . Please note that R=1 is only in the left - most figures for CIFAR10 experiments for the two settings of hammer - spammer and class - wise hammer - spammer , respectively . In these plots , the lines for weighted - MV and weighted - EM are nearly identical . The negligible differences owe only to random worker assignment and random initialization of parameters . In rest of the four figures of CIFAR10 experiments , the redundancy r varies along the x-axis .

5 . Re “ For ImageNet and MS-COCO experiments with a fixed budget , you reduced the training set when increasing the redundancy , which is unfair . The reduction of performance could mainly cause by seeing fewer raw images , but not the labels . It ’s better to train some semi-supervised model to make the settings more comparable . ”

We agree that in principle , the strongest baseline to prove our point that labeling once is optimal would allow the redundant labelers to make used of the unlabelled data in a semi-supervised fashion . We note that this does not directly fall out of our theory , which addresses the supervised case ( see Theorem 4.1 ) and thus may be beyond the scope of this paper . We also note that many current semi-supervised algorithms , such as Ladder Networks , show most significant improvements when the ratio of unlabeled to labeled data is quite large , and that it is not clear how advantageous current semi-supervised algorithms would be at a redundancy level of say 3 . While answering these questions conclusively is a non-trivial task and left for future work , we think that this is a great point and plan to investigate in the future how the utility of unlabeled data for semi-supervised learning may complicate the picture .


The authors proposed a supervised learning algorithm for modeling label and worker quality . Further utilize it to study one of the important problems in crowdsourcing - How much redundancy is required in crowdsourcing and whether low redundancy with abundant noise examples lead to better labels .

Overall the paper was well written . The motivation of the work is clearly explained and supported with relevant related work . The main contribution of the paper is in the bootstrapping algorithm which models the worker quality and labels in an iterative fashion . Though limited to binary classification , the paper proposed a theoretical framework extending the existing work on VC dimension to compute the upper bound on the risk . The authors also showed theoretically and empirically on synthetic data sets that the low redundancy and larger set of labels in crowdsourcing gives better results .

More detailed comments
1 . Instead of considering multi-class classification as one - vs- all binary classification , can you extend the theoretical guarantee on the risk to multi-class set up like Softmax which is widely used in research nowadays .
2 . Can you introduce the Risk - R in the paper before using it in Theorem 4.1
3 . Is there any limit on how many examples each worker has to label ? Can you comment more on how to pick that value in real - world settings ? Just saying sufficiently many ( Section 4.2 ) is not sufficient .
4 . Under the experiments , different variations of Majority Vote , EM and Oracle correction were used as baselines . Can you cite the references and also add some existing state - of - the- art techniques mentioned in the related work section .
5 . For the experiments on synthetic datasets , workers are randomly sampled with replacements . Were the scores reported based on average of multiple runs . If yes , can you please report the error bars .
6 . For the MS- COCO , examples can you provide more detailed results as shown for synthetic datasets ? Majority vote is a very weak baseline .

For the novel approach and the theoretical backing , I consider the paper to be a good one . The paper has scope for improvement .



Thanks for the thoughtful review and clearly enumerated critical points . We reply to each below :

1 . We agree that it would be desirable to extend the theoretical guarantees to the multiclass - classification setting with cross -entropy loss and we plan to explore this question in future work . However , this extension is non-trivial under the current framework . Equation 22 in Lemma A.2 does not apply for cross -entropy loss and it is not obvious how to complete the guarantees without this result .

2 . In the initial draft , we introduced the Risk - R in the problem formulation section . We ’re grateful for the feedback that this was not obvious when you arrived at theorem 4.1 and we have modified the draft to remind the reader at this point .

3 . Equation 7 in Theorem 4.1 states the condition on how many examples each worker has to label for the algorithm to succeed in estimating worker qualities . In particular , given m workers the algorithm needs to estimate O ( m ) latent parameters of their confusion matrices . From standard statistical analysis as reflected in Equation 7 , we need O ( m log m ) independent observations to estimate O ( m ) parameters . Therefore , if we have n training examples , and redundancy is r then the total number of observations nr should satisfy : nr > m log m. Hence , each worker has to label O( log m ) examples for the algorithm to succeed .

4 . We have compared our algorithm MBEM with four different algorithms and two oracle- based algorithms . Majority vote is a standard algorithm and Expectation Maximization ( EM ) is based on the classical Dawid Skene ( 1979 ) work , we have included reference to it in the revised draft . Weighted MV and weighted EM use a weighted loss function that is newly proposed in this work . The purpose of including these algorithms is to establish efficacy of weighted loss function over the standard loss function for noisy labels . Note that MBEM uses the weighted loss function in addition to the bootstrapping idea to estimate worker qualities .

We appreciate the request for a comparison against state - of - the- art techniques mentioned in the related work section . We are presently implementing the method from “ Lean Crowdsourcing ” ( Branson , Van Horn , Petrona 2017 ) as an additional baseline method and will add the results to the experiments section as soon as they are available ( http://openaccess.thecvf.com/content_cvpr_2017/html/Branson_Lean_Crowdsourcing_Combining_CVPR_2017_paper.html).

5 . Per your suggestions , in the new ( current ) draft we added the error bars for CIFAR10 . In the initial draft , we were reporting averages across multiple runs for CIFAR10 and MSCOCO . For ImageNet , the experiments are too expensive , so we only execute one run .

6 . We will add the results of the EM algorithm and weighted EM algorithm for MSCOCO experiment . We are also working presently on adding the method due to Branson et al . as a baseline to the MSCOCO experiment and will post results when available .


This paper proposes an adaptation of the SEARN algorithm to RNNs for generating text . In order to do so , they discuss various issues on how to scale the approach to large output vocabularies by sampling which actions the algorithm to explore .

Pros :
- Good literature review . But the future work on bandits is already happening :
Paper accepted at ACL 2017 : Bandit Structured Prediction for Neural Sequence-to-Sequence Learning . Julia Kreutzer , Artem Sokolov , Stefan Riezler .


Cons :
- The key argument of the paper is that SEARNN is a better IL - inspired algorithm than the previously proposed ones . However there is no direct comparison either theoretical or empirical against them . In the examples on spelling using the dataset of Bahdanau et al. 2017 , no comparison is made against their actor-critic method . Furthermore , given its simplicity , I would expect a comparison against scheduled sampling .

- A lot of important experimental details are in the appendices and they differ among experiments . For example , while mixed rollins are used in most experiments , reference rollins are used in MT , which is odd since it is a bad option theoretically . Also , no details are given on how the mixing in the rollouts was tuned . Finally , in the NMT comparison while it is stated that similar architecture is used in order to compare fairly against previous work , this is not the case eventually , as it is acknowledged at least in the case of MIXER . I would have expected the same encoder - decoder architecture to have been used for all the methods considered .

- the two losses introduced are not really new . The log-loss is just MLE , only assuming that instead of a fixed expert that always returns the same target , we have a dynamic one . Note that the notion of dynamic expert is present in the SEARN paper too . Goldberg and Nivre just adapted it to transition - based dependency parsing . Similarly , since the KL loss is the same as XENT , why give it a new name ?

- the top -k sampling method is essentially the same as the targeted exploration of Goodman et al. ( 2016 ) which the authors cite . Thus it is not a novel contribution .

- Not sure I see the difference between the stochastic nature of SEARNN and the online one of LOLS mentioned in section 7 . They both could be mini-batched similarly . Also , not sure I see why SEARNN can be used on any task , in comparison to other methods . They all seem to be equally capable .

Minor comments :
- Figure 1 : what is the difference between " cost-sensitive loss " and just " loss " ?
- local vs sequence - level losses : the point in Ranzato et al and Wiseman & Rush is that the loss they optimizise ( BLEU / ROUGE ) do not decompose over the the predictions of the RNNs .
- Ca n't see why SEARNN can help with the vanishing gradient problem . Seem to be rather orthogonal .


Reviewer2 provides an in - depth and thoughtful review . They express concerns about three potential issues : a lack of comparison to related methods , unclear experiments and erroneous novelty claims . We believe these criticisms stem for the most part from several key misunderstandings about the presented method and the claims made in the paper .
In the following , we make explicit these misunderstandings and we strive to clarify them .
We hope that reviewer 2 can help us improve the paper by pointing out the specific parts that they found confusing .

1 . How does SeaRNN relate to other IL - inspired algorithms ?

" The key argument of the paper is that SEARNN is a better IL - inspired algorithm than the previously proposed ones . However there is no direct comparison either theoretical or empirical against them . "

We disagree with this statement and show in the following that the paper does indeed contain both theoretical and empirical comparisons , including a section ( Discussion , Section 7 ) about theoretical comparison to related methods and a large - scale experiment where the performance of various methods is compared .

First off , the main aim of the paper is to introduce a novel IL - inspired method for training RNNs which alleviates the issues associated with traditional MLE training . We then contrast different methods and explore their pros and cons . These concrete elements of comparison , both theoretical and empirical , lead us to believe that SeaRNN is indeed well - positioned .

Theoretical comparisons :
As part of this exploration , we provide numerous theoretical points of comparison in the Discussion section ( Section 7 ) :

- we compare with schedule sampling ( Bengio et al , 2015 ) . They use a mixed roll - in , while we use either a reference or a learned roll - in . Furthermore , SeaRNN leverages roll - outs for estimation and custom losses , while schedule sampling simply uses the MLE loss .
- we underline an important difference between SeaRNN and most related methods ( be they RL -inspired e.g. MIXER ( Ranzato et al , 2016 ) and Actor-Critic ( Bahdanau et al , 2017 ) or IL - inspired e.g. BSO ( Wiseman et al , 2016 ) ) : the fact that since the training signal from their loss is quite sparse , they have to use warm starting , whereas SeaRNN does not .
- we remark that BSO requires being able to compute the evaluation metric on unfinished sequences ( see the definition of the associated loss in ( Wiseman and Rush , 2016 , Section 4.1 ) ) . While this is technically possible for BLEU , the scores obtained this way are arguably not meaningful . In contrast , SeaRNN always computes scores on full sequences .
- we explain that some IL - inspired methods ( see Ballesteros et al , 2016 and Sun et al , 2017 ) require a free cost - to - go oracle , whereas SeaRNN uses roll - outs for exploration and is thus more widely applicable , albeit at a higher computational cost .
- Incidentally , the last two points explain why we write that SeaRNN can be used on a wider amount of tasks , compared to some related methods .

Empirical comparisons :
" In the examples on spelling using the dataset of Bahdanau et al. 2017 , no comparison is made against their actor-critic method . Furthermore , given its simplicity , I would expect a comparison against scheduled sampling . »

As we explain in the caption of Table 1 , we cannot directly compare SeaRNN to Actor -Critic on the Spelling dataset , because the authors of this paper used a random test dataset and some key hyper parameters are missing from the open source implementation ( we obtained this information through private communication with them when first trying to compare our methods ) .
We do provide a point of comparison with Actor-Critic ( with the same architecture ) on a larger scale dataset , namely IWSLT '14 de -en MT .
Finally , we conducted thorough experiments with scheduled sampling on the NMT dataset . Unfortunately , we could not obtain any significant improvement over MLE , even with a careful schedule proposed by the authors of the scheduled sampling paper through private communication ( note that no positive results on NMT were reported in the original paper either ) . This is reported in the main text of the paper ( see Key takeaways in Section 6 , at the bottom of page 8 ) .
If the reviewer believes this would add to the paper , we will of course run this algorithm on the OCR and Spelling datasets and report the obtained results ( we have not conducted these experiments yet ) .

All told , we believe our paper does present theoretical and empirical comparisons to related methods . We have already conducted and reported on some of the experiments the reviewer asks for .

2 . Experimental details

" A lot of important experimental details are in the appendices and they differ among experiments . »
" For example , while mixed rollins are used in most experiments , reference rollins are used in MT , which is odd since it is a bad option theoretically . "
" Also , no details are given on how the mixing in the rollouts was tuned . "
" Finally , in the NMT comparison while it is stated that similar architecture is used in order to compare fairly against previous work , this is not the case eventually , as it is acknowledged at least in the case of MIXER . "

The reviewer points out that our experimental setup is unclear . We disagree with that statement and show in the following that all of the relevant information can be found in the main text of the paper and that differences are underlined and analyzed in details . We will strive to present this information more clearly .

First off , let us point out that there are no mixed roll - ins in any of the experiments . We compare reference and learned roll - ins for OCR and Spelling ( see Table 1 and the caption of Table 2 ) , and use reference roll - ins for NMT , as stated at the beginning of Section 6 ( in the middle of page 8 ) and at the end of this section ( see bottom of page 8 ) .

Second , while L2S theory indeed tells us that a learned roll - in should always be preferred to a reference one , on some datasets practitioners observe the reverse . We confirmed this with the authors of the SEARN paper ( Daumé et al , 2009 ) through private communication .
We provide potential explanations in the main text of the paper ( see Key takeaways in Section 6 , bottom of page 8 ) , namely :

- either our reference policy is too weak to provide good enough training signal
- or the problem obtained with a learned roll - in might be harder to optimize for than its equivalent obtained with a reference roll - in -- an issue which is overlooked by classical L2S theory .

We also explain what choice of hyper parameter we advocate , including resorting to a reference roll - in when a learned roll - in does not lead to good performance ( see ' Traditional L2S approches ' , Section 7 , top of page 9 ) .
We therefore argue that this choice in hyper parameter is made explicit and is motivated in the paper .

Third , the value of the mix - in probability for our roll - outs ( 0.5 ) is reported in the caption underneath Table 1 . It is the same for all datasets . We do not report any tuning of this value because we did not perform any . We followed Chang et al ( 2015 ) , where the authors indicate that their algorithm is not sensitive to this value , so we did not feel the need to optimize for it . We will add this reasoning to the paper to explain the value we took .

Finally , we do indeed use an architecture that is different from that of MIXER . This information is reported in the main text ( see Key takeaways in Section 6 ) , as we are explicit about the architectures of related methods . The reason for this difference is that we decided to reuse the architecture used both by BSO and by Actor-Critic . We have followed their setup as closely as possible , and are not aware of any meaningful difference with our own . If our presentation is not clear enough , we are happy to add this information at any place the reviewer sees fit .

Once again , we stress that all of this information is presented * in the main text * , and discussed at length . The only thing present in the appendix is an expanded version of the harder optimization problem hypothesis we make in ' Key takeaways ' in Section 6 .

3 . Novelty

" The two losses introduced are not really new . "
" The top -k sampling method is essentially the same as the targeted exploration of Goodman et al. ( 2016 ) which the authors cite . Thus it is not a novel contribution . "

We show that these assessments are the result of misunderstandings ( in some cases we simply do not make novelty claims , and in others what we propose is actually different from the referred techniques ) .

First , we want to reiterate the difference between a classical classification loss and a cost - sensitive loss , as these notions are fundamental to the whole field of L2S research . In a cost -sensitive classification problem , rather than having access to a single ground - truth output , one has access to a vector of costs , with one cost associated with each possible token . This unusual setup requires adapted losses . In particular , we are not aware of any other RNN training techniques which uses cost - sensitive losses , besides SeaRNN .

Second , concerning the log-loss ( LL ) , we explain that it indeed shares the structure of MLE , and replaces constant experts by dynamic ones ( see ‘ Log-loss ’ in Section 4 ) . We also point out that this technique is not new , even in the context of RNN training ( see our reference to Ballesteros et al , ( 2016 ) in ' L2S -inspired approaches ' in the Discussion section at the bottom of page 9 ) . We do not make novelty claims in that respect .
However , to our knowledge this is the first algorithm which uses the scores obtained by roll - outs to determine the value of the dynamic expert . This is the aspect of the loss which we consider to be novel .
If our claim is unclear we can definitely rephrase it in a way that the reviewer deems more satisfactory .

Third , we are not sure we understand the remark of the reviewer concerning the KL loss . In our setting , the KL divergence and the cross-entropy are indeed equivalent since the additional entropy term in XENT is constant with respect to the parameters of the model . We decided to call it KL as we saw this loss term as a divergence between two probability distributions ( and indeed we tried several other divergences , see Appendix C ) .
MLE can be thought of as a cross-entropy term between the model output and a Dirac distribution centered on the ground truth target .
However , the difference in our setup is that we have access to a richer , non - Dirac target distribution , which we derive from the cost vectors . The novelty in our loss resides in the application of the KL divergence ( or equivalently cross -entropy ) in a situation where one has access to a full probabilistic distribution over the tokens in the vocabulary instead of a single target output .

Finally , the top -k strategy is a simplified version of targeted sampling . Indeed , none of the strategies we test ( uniform , topk , policy sampling and biased policy sampling ) are novel . We acknowledge this in the main text of the paper and we make no claims about novelty with respect to these strategies .

Conclusion
We believe we have alleviated a number of concerns and clarified some misunderstandings which lead to unfavorable assessments about the paper . In light of these clarifications , we hope the reviewer will consider adjusting their evaluation accordingly , and helping us improve the paper through suggestions .


References :
Dzmitry Bahdanau , Philemon Brakel , Kelvin Xu , Anirudh Goyal , Ryan Lowe , Joelle Pineau , Aaron Courville , and Yoshua Bengio . An actor-critic algorithm for sequence prediction . In ICLR , 2017 .
Miguel Ballesteros , Yoav Goldberg , Chris Dyer , and Noah A Smith . Training with exploration improves a greedy stack - LSTM parser . In EMNLP , 2016 .
Samy Bengio , Oriol Vinyals , Navdeep Jaitly , and Noam Shazeer . Scheduled sampling for sequence prediction with recurrent neural networks . In NIPS , 2015 .
Kai-Wei Chang , Akshay Krishnamurthy , Alekh Agarwal , Hal Daumé , III , and John Langford . Learning to search better than your teacher . In ICML , 2015 .
Hal Daumé , III , John Langford , and Daniel Marcu . Search - based structured prediction . Machine Learning , 2009 .
Marc ’Aurelio Ranzato , Sumit Chopra , Michael Auli , and Wojciech Zaremba . Sequence level training with recurrent neural networks . In ICLR , 2016 .
Wen Sun , Arun Venkatraman , Geoffrey J. Gordon , Byron Boots , and J. Andrew Bagnell . Deeply aggrevated : Differentiable imitation learning for sequential prediction . In ICML , 2017 .
Sam Wiseman and Alexander M Rush . Sequence - to-sequence learning as beam -search optimization . In EMNLP , 2016 .

This paper has proposed a new method for classifying nodes of a graph . Their method can be used in both semi-supervised scenarios where the label of some of the nodes of the same graph as the graph in training is missing ( Transductive ) and in the scenario that the test is on a completely new graph ( Inductive ) .
Each layer of the network consists of feature representations for all of the nodes in the Graph . A linear transformation is applied to all the features in one layer and the output of the layer is the weighted sum of the transformed neighbours ( including the node ) . The attention logit between node i and its neighbour k is calculated by a one layer fully connected network on top of the concatenation of the transformed representation of node i and transformed representation of the neighbour k . They also can incorporate the multi-head attention mechanism and average / concatenate the output of each head .

Originality :
Authors improve upon GraphSAGE by replacing the aggregate and sampling function at each layer with an attention mechanism . However , the significance of the attention mechanism has not been studied in the experiments . For example by reporting the results when attention is turned off ( 1/|N_i| for every node ) and only a 0 - 1 mask for neighbours is used . They have compared with GraphSAGE only on PPI dataset . I would change my rating if they show that the 33 % gain is mainly due to the attention in compare to other hyper- parameters . [ The experiments are now more informative . Thanks ]
Also , in page 4 authors claim that GraphSAGE is limited because it samples a neighbourhood of each node and does n't aggregate over all the neighbours in order to keep its computational footprint consistent . However , the current implementation of the proposed method is computationally equal to using all the vertices in GraphSAGE .

Pros :
- Interesting combination of attention and local graph representation learning .
- Well written paper . It conveys the idea clearly .
- State-of - the - art results on three datasets .

Cons :
- When comparing with spectral methods it would be better to mention that the depth of embedding propagation in this method is upper-bounded by the depth of the network . Therefore , limiting its adaptability to broader class of graph datasets .
- Explaining how attention relates to previous body of work in embedding propagation and when it would be more powerful .

We would like to thank you for the comprehensive review ! Please refer to our global comment above for a list of all revisions we have applied to the paper --- we are hopeful that they have addressed your comments appropriately .

Primarily , thank you for suggesting the constant- attention experiment ( with 1 / | Ni| coefficients ) ! This not only directly evaluates the significance of the attention mechanism on the inductive task , but allows for a comparison with a GCN - like inductive structure . We have successfully shown a benefit of using attention :

The Const-GAT model achieved 0.934 +- 0.006 micro- F1 ;
The GAT model achieved 0.973 +- 0.002 micro-F1.

Which demonstrates a clear positive effect of using an attention mechanism ( given that all other architectural and training properties are kept fixed across the two models ) . These results are clearly communicated in our revised paper now ( Section 3.3 introduces the experiment in the “ Inductive learning ” paragraph , while the results are outlined in Table 3 and discussed in Section 3.4 , paragraph 4 ) .

Our intention was not to imply that our method is computationally more efficient than GraphSAGE --- only that GraphSAGE ’s design decisions ( sampling subsets of neighbourhoods ) have potentially limiting effects on its predictive power . We have rewrote bullet point 4 in Section 2.2 , to hopefully communicate this better .

Lastly , we make explicit that the depth of our propagation is upper-bounded by network depth in Section 2.2 , paragraph 2 . We remark that GCN - like models suffer from the same issue , and that skip connections ( or similar constructs ) may be readily used to effectively increase the depth to desirable levels . The primary benefit of leveraging attention , as opposed to prior approaches to graph -structured feature aggregation , is being able to ( implicitly ) assign different importances to different neighbours , while simultaneously generalising to a wide range of degree distributions --- these differences are stated in our paper in various locations ( e.g. Section 1 , paragraph 8 ; Section 2.2 , bullet point 2 ) .

We thank you once again for your review , which has definitely helped make our paper ’s contributions stronger !


The paper introduces a neural network architecture to operate on graph-structured
data named Graph Attention Networks .
Key components are an attention layer and the possibility to learn how to
weight different nodes in the neighborhood without requiring spectral decompositions
which are costly to be computed .

I found the paper clearly written and very well presented . I want to thank
the author for actively participating in the discussions and in clarifying already
many of the details that I was missing .

As also reported in the comments by T. Kipf I found the lack of comparison to previous
works on attention and on constructions of NN for graph data are missing .
In particular MoNet seems a more general framework , using features to compute node
similarity is another way to specify the " coordinate system " for convolution .
I would argue that in many cases the graph is given and that one would have
to exploit its structure rather than the simple first order neighbors structure .

I feel , in fact , that the paper deals mainly with " localized metric-learning " rather than
using the information in the graph itself . There is no
explicit usage of the graph beyond the selection of the local neighborhood .
In many ways when I first read it I though it would be a modified version of
memory networks ( which have not been cited ) . Sec. 2.1 is basically describing
a way to learn a matrix W so that the attention layer produces the weights to be
used for convolution , or the relative coordinate system , which is to me a
memory network like construction , where the memory is given by the neighborhood .

I find the idea to use the multi-head attention very interesting , but one should
consider the increase in number of parameters in the experimental section .

I agree that the proposed method is computationally efficient but the authors
should keep in mind that parallelizing across all edges involves lot of redundant
copies ( e.g. in a distributed system ) as the neighborhoods highly overlap , at
least for interesting graphs .

The advantage with respect to methods that try to use LSTM in this domain
in a naive manner is clear , however the similarity function ( attention ) in this
work could be interpreted as the variable dictating the visit ordering .

The authors seem to emphasize the use of GPU as the best way to scale their work
but I tend to think that when nodes have varying degrees they would be highly
unused . Main reason why they are widely used now is due to the structure in the
representation of convolutional operations .
Also in case of sparse data GPUs are not the best alternative .

Experiments are very well described and performed , however as explained earlier
some comparisons are needed .
An interesting experiment could be to use the attention weights as adjacency
matrix for GCN .

Overall I liked the paper and the presentation , I think it is a simple yet
effective way of dealing with graph structure data . However , I think that in
many interesting cases the graph structure is relevant and cannot be used
just to get the neighboring nodes ( e.g. in social network analysis ) .

First of all , thank you very much for your thorough review , and for the variety of useful pointers within it ! Please refer to our global comment above for a list of all revisions we have applied to the paper --- we are hopeful that they have addressed your comments appropriately .

We have now added all the references to attention - like constructions ( such as MoNet and neighbourhood attention ) to our related work , as well as memory networks ( see Section 1 , paragraphs 6 and 9 ; also Section 2.2 , bullet point 5 ) . We fully agree with your comments about the increase in parameter count with multi-head attention , computational redundancy , and comparative advantages of GPUs in this domain , and have explicitly added them as remarks to our model ’s analysis ( in Section 2.2 , bullet point 1 and paragraph 2 ) .

While we agree that the graph structure is given in many interesting cases , in our approach we specifically sought to produce an operator explicitly capable of solving inductive problems ( which appear often , e.g. , in the biomedical domain , where the method needs to be able to generalise to new structures ) . A potential way of reconciling this when a graph structure is provided is to combine GAT - like and spectral layers in the same architecture .

Further experiments ( as discussed by us in all previous comments ) have also been performed and are now explicitly listed in the paper ’s Results section ( please see Tables 2 and 3 for a summary ) . We have also attempted to use the GAT coefficients as the aggregation matrix for GCNs ( both in an averaged and multi-head manner ) - -- but found that there were no clear performance changes compared to using the Laplacian .

We thank you once again for your review , which has definitely helped make our paper ’s contributions stronger !

This is a paper about learning vector representations for the nodes of a graph . These embeddings can be used in downstream tasks the most common of which is node classification .

Several existing approaches have been proposed in recent years . The authors provide a fair and almost comprehensive discussion of state of the art approaches . There are a couple of exception that have already been mentioned in a comment from Thomas Kipf and Michael Bronstein . A more precise discussion of the differences between existing approaches ( especially MoNets ) should be a crucial addition to the paper . You provide such a comparison in your answer to Michael 's comment . To me , the comparison makes sense but it also shows that the ideas presented here are less novel than they might initially seem . The proposed method introduces two forms of ( simple ) attention . Nothing groundbreaking here but still interesting enough and well explained . It might also be a good idea to compare your method to something like LLE ( locally linear embedding ) . LLE also learns a weight for each of neighbors of a node and computes the embedding as a weighted average of the neighbor embeddings according to these weights . Your approach is different since it is learned end - to - end ( not in two separate steps ) and because it is applicable to arbitrary graphs ( not just graphs where every node has exactly k neighbors as in LLE ) . Still , something to relate to .

Please take a look at the comment by Fabian Jansen . I think he is on to something . It seems that the attention weight ( from i to j ) in the end is only a normalization operation that does n't take the embedding of node i into account .

There are two issues with the experiments .

First , you do n't report results on Pubmed because your method did n't scale . Considering that Pubmed has less than 20,000 nodes this shows a clear weakness of your approach . You write ( in an answer to a comment ) that it * should * be parallelizable but somehow you did n't make it work . We have to , however , evaluate the approach on what it is able to do at the moment . Having a complexity that is quadratic in the number of nodes is terrible and one of the major reasons learning with graphs has moved from kernels to neural approaches . While it is great that you acknowledge this openly as a weakness , it is currently not possible to claim that your method scales to even moderately sized graphs .

Second , the experimental set - up on the Cora and Citeseer data sets should be properly randomized . As Thomas pointed out , for graph data the variance can be quite high . For some split the method might perform really well and less well for others . In your answer titled " Requested clarifications " to a different comment you provide numbers randomized over 10 runs . Did you randomize the parameter initialization only or also the the train / val / test splits ? If you did the latter , this seems reasonable . In Kipf et al . 's GCN paper this is what was done ( not over 100 splits as some other commenter claimed . The average over 100 runs pertained to the ICA method only . )

Thank you very much for your detailed review ! Please refer to our global comment above for a list of all revisions we have applied to the paper --- we are hopeful that they have addressed your comments appropriately .

Fabian has indeed correctly identified that half of our attention weights were spurious . We have now rectified this by applying a simple nonlinearity ( the LeakyReLU ) prior to normalising , and anticipated that its application would provide better performance to the model on the PPI dataset ( which has a large number of training nodes ) . Indeed , we noticed no discernible change on Cora and Citeseer , but an increase in F1 -score on PPI ( now at 0.973 +- 0.002 after 10 runs ; previously , as given in our reply to one of the comments below , it was 0.952 +- 0.006 ) . The new results may be found in Tables 2 and 3.

In the meantime , we have been successful at leveraging TensorFlow ’s sparse_softmax operation , and produced a sparsified version of the GAT layer . We are happy to provide results on Pubmed , and they are now given in the revised version of the paper ( see Table 2 for a summary ) . We were able to match state - of - the- art level performance of MoNet and GCN ( at 79.0 +- 0.3 % after 100 runs ) . Similarly to the MoNet paper authors , we had to revise the GAT architecture slightly to accommodate Pubmed ’s extremely small training set size ( of 60 examples ) , and this is clearly remarked in our experimental setup ( Section 3.3 ) .

Finally , quoting directly from the work of Kipf and Welling :

“ We trained and tested our model on the same dataset splits as in Yang et al. ( 2016 ) and report mean accuracy of 100 runs with random weight initializations . ”

This implies that the splits were not randomised in the result reported by the GCN paper ( specifically , the one used to compare with other baseline approaches ) , but only the model initialisation --- and this is exactly what we do as well . We , in fact , use exactly the code provided by Thomas Kipf at https://github.com/tkipf/gcn/blob/master/gcn/utils.py#L24 to load the dataset splits .

We have added all the required references to MoNet and LLE ( and many other pieces of related work ) in the revised version ( Section 1 , paragraphs 6 and 9 ; also Section 2.2 , bullet point 5 ) - thank you for pointing out LLE to us , which is an interesting and relevant piece of related work !

We thank you once again for your review , which has definitely helped make our paper ’s contributions stronger !


ICLR I- Revnet


This paper build on top of ReVNets ( Gomez et al. , 2017 ) and introduce a variant that is fully
invertible . The model performs comparable to its variants without any loss of information .
They analyze the model and its learned representations from multiple perspectives in detail .

It is indeed very interesting an thought provoking to see that contrary to popular belief in the community no information loss is necessary to learn good generalizable features . What is missing , is more motivation for why such a property is desirable . As the authors mentions the model size has almost doubled compared to comparable ResNet . And the study of the property of the learned futures might probably limited to this i- RevNet only . It would be good to see more motivation , beside the valuable insight of knowing it ’s possible .

Generally the paper is well written and readable , but few minor comments :
1 - Better formatting such as putting results in model sizes , etc in tables will make them easier to find .
2 - Writing down more in detail 3.1 , ideally in algorithm or equation than all in text as makes it hard to read in current format .

We thank the reviewer very much for the valuable comments and for acknowledging that our main claims are very interesting and thought - provoking . In the following , we will elaborate on the increased model size and usefulness of such an architecture in detail .

To add another dimension to the model analysis and to shed light on the necessary model size , we have added a model which replaces the initial injective operator with a bijective operator as used in later layers . This model has almost the same number of parameters as the baselines and trains about a day faster , albeit performs worse by 1.5 % . This is to show , that model size can be reduced substantially while the invertibility property improves .

== > The authors mention model size has almost doubled

Thanks to this important remark , we have added another model that shows it is possible to avoid an excessive increase of model size in i- RevNets . The newly added model has 29M parameters as opposed to 28M in the RevNet baseline while having a top - 1 accuracy of 73.3 % , which is ~ 1.5 % worse than the RevNet baseline .

We thank the reviewer once again for raising this point and believe that the newly introduced model makes the paper even stronger , as it shows that the invertibility property can even be improved by decreasing model size .

== > Does the analysis apply to other models as well ?

We thank the reviewer for this question , section 5.1 shows that progressive properties that are known to hold for lossy Alex Net type models on limited datasets , are in fact also possible to obtain in an architecture that is not able to discard information about the input on a large - scale task like Imagenet .

To further strengthen the results , we have extended our analysis of the separation contraction to a ResNet baseline . Our results show , that the behaviour of the non-invertible ResNet is the same as the one observed in i- RevNets , substantiating the generality of our findings .

== > Why is such a model desirable ?

The core question we answer is if the success of deep convolutional networks is based on progressively discarding uninformative variability , which is a wide standing believe in the CV and ML community . We show this does not have to be the case , which has been acknowledged as " important " , " interesting " and " thought - provoking " by all reviewers . Thus , the invertibility property is desirable for understanding the success of deep learning better and shed light on some of the necessities for it to work well .
From a practical point of view , invertible models are useful for feature visualization [ 1 , 2 , 3 ] and possibly useful to overcome difficulties in upsampling / decoding pixel - wise tasks that are still quite challenging [ 4 ] . Further , lossless models might be a good candidate for transfer learning .

In summary , we do believe that besides the theoretical interest of our work , which has been acknowledged by all reviewers , there is also a potential impact in deep learning applications for invertible models .

We thank the reviewer once again for the important questions and remarks , we believe that the added discussion and results of the new bijective i- RevNet and ResNet baseline substantially improve the paper . We have also incorporated suggested formatting improvements into the manuscript .

[ 1 ] Mahendran , Aravindh , and Andrea Vedaldi . " Understanding deep image representations by inverting them . " Proceedings of the IEEE conference on computer vision and pattern recognition . 2015 .
[ 2 ] Dosovitskiy , Alexey , and Thomas Brox . " Inverting visual representations with convolutional networks . " Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2016 .
APA
[ 3 ] Selvaraju , Ramprasaath R. , et al . " Grad -cam : Why did you say that ? visual explanations from deep networks via gradient - based localization . " arXiv preprint arXiv:1610.02391 ( 2016 ) .
[ 4 ] Wojna , Zbigniew , et al . " The Devil is in the Decoder . " arXiv preprint arXiv:1707.05847 ( 2017 ) .


In this paper , the authors propose deep architecture that preserves mutual information between the input and the hidden representation and show that the loss of information can only occur at the final layer . They illustrate empirically that the loss of information can be avoided on large - scale classification such as ImageNet and propose to build an invertible deep network that is capable of retaining the information of the input signal through all the layers of the network until the last layer where the input could be reconstructed .

The authors demonstrate that progressive contraction and separation of the information can be obtained while at the same time allowing an exact reconstruction of the signal .

As it requires a special care to design an invertible architecture , the authors architecture is based on the recent reversible residual network ( RevNet ) introduced in ( Gomez et al. , 2017 ) and an invertible down - sampling operator introduced in ( Shi et al. , 2016 ) . The inverse ( classification ) path of the network uses the same convolutions as the forward ( reconstructing ) one . It also uses subtraction operations instead of additions in the output computation in order to reconstruct intermediate and input layers .

To show the effectiveness of their approach on large - scale classification problem , the authors report top - 1 error rates on the validation set of ILSVRC - 2012 . The obtained result is competitive with the original Resnet and the RevNet models . However , the proposed approach is expensive in terms of parameter budget as it requires almost 6.5 times more parameters than the RevNet and the Resnet architectures . Still , the classification and the reconstructing results are quite impressive as the work is the first empirical evidence that learning invertible representation that preserves information about the input is possible on large - scale classification tasks . Worth noting that recently , ( Shwartz - Ziv and Tishby ) demonstrated , not on large - scale datasets but on small ones , that an optimal representation for a classification task must reduce as much uninformative variability as possible while maximizing the mutual information between the desired output and its representation in order discriminate as much as possible between classes . This is called “ information bottleneck principle ” . The submitted paper shows that this principle is not a necessary condition large - scale classification .

The proposed approach is potentially of great benefit . It is also simple and easy to understand . The paper is well written and the authors position their work with respect to what has been done before . The spectral analysis of the differential operator in section 4.1 provide another motivation for the “ hard - constrained ” invertible architecture . Section 4.2 illustrates the ability of the network to reconstruct input signals . The visualization obtained suggests that network performs linear separation between complex learned factors . Section 5 shows that even when using either an SVM or a Nearest Neighbor classifier on n extracted features from a layer in the network , both classifiers progressively improve with deeper layers . When the d first principal components are used to summarize the n extracted features , the SVM and NN classifier performs better when d is bigger . This shows that the deeper the network gets , the more linearly separable and contracted the learned representations are .

In the conclusion , the authors state the following : “ The absence of loss of information is surprising , given the wide believe , that discarding information is essential for learning representations that generalize well to unseen data ” . Indeed , the authors have succeed in showing that this is not necessarily the case . However , the loss of information might be necessary to generalize well on unseen data and at the same time minimize the parameter budget for a given classification task .


We thank the reviewer very much for this encouraging review and the comments on our paper .
We would also like to thank the reviewer for acknowledging the presented results being impressive and potentially of great benefit .

Inspired by the reviewer 's remark on model size and the quest for an optimal parameter budget , we have added another model to the paper that has a similar number of parameters as the RevNet and ResNet baselines . This way we show that an increased number of parameters is not necessary to obtain the invertible architecture . This newly added i- RevNet replaces the initial injective mapping with a bijective mapping . In consequence , the new model is slightly different in architecture from the baselines , as it keeps the input dimensionality constant . We have replaced the analysis of the injective i- RevNet by an analysis of the bijective i- RevNet throughout the whole paper .

Furthermore , to show that the observed separation and contraction occur independently of invertibility , we have added a non-invertible ResNet baseline to the model analysis in section 5.1 . We have also added training plots of ResNets compared to i- RevNets . The results show a progressive separation and contraction in invertible and non-invertible models and very similar training behaviour .

Our main conclusions remain the same , while the new results substantiate their generality .

We thank the reviewer once again for the insightful comments thanks to which we were able to further improve the paper .



The paper is well written and easy to follow . The main contribution is to propose a variant of the RevNet architecture that has a built in pseudo-inverse , allowing for easy inversion . The results are very surprising in my view : the proposed architecture is nearly invertible and is able to achieve similar performance as highly competitive variants : ResNets and RevNets .

The main contribution is to use linear and invertible operators ( pixel shuffle ) for performing downsampling , instead of non- invertible variants like spatial pooling . While the change is small , conceptually is very important .

Could you please comment on the training time ? Although this is not the point of the paper , it would be very informative to include learning curves . Maybe discarding information is not essential for learning ( which is surprising ) , but the cost of not doing so is payed in learning time . Stating this trade - off would be informative . If I understand correctly , the training runs for about 150 epochs , which is maybe double of what the baseline ResNet would require ?

The authors evaluate in Section 4.2 the show samples obtained by the pseudo inverse and study the properties of the representations learned by the model . I find this section really interesting . Further analysis will make the paper stronger .

Are the images used for the interpolation train or test images ?

I assume that the network evaluated with the Basel Faces dataset , is the same one trained on Imagenet , is that the case ?

In particular , it would be interesting ( not required ) to evaluate if the learned representation is able to linearize a variety of geometric image transformations in a controlled setting as done in :

Hénaff , O , , and Simoncelli , E. " Geodesics of learned representations . " arXiv preprint arXiv:1511.06394 ( 2015 ) .

Could you please clarify , what do you mean with fine tuning the last layer with dropout ?

The authors should cite the work on learning invertible functions with tractable Jacobian determinant ( and exact and tractable log-likelihood evaluation ) for generative modeling . Clearly the goals are different , but nevertheless very related . Specifically ,

Dinh , L. et al " NICE : Non-linear independent components estimation . " arXiv preprint arXiv:1410.8516 ( 2014 ) .


Dinh , L. et al " Density estimation using Real NVP . " arXiv preprint arXiv:1605.08803 ( 2016 ) .

The authors mention that the forward pass of the network does not seem to suffer from significant instabilities . It would be very good to empirically evaluate this claim .


We thank the reviewer very much for raising many interesting and important points . Furthermore , we thank the reviewer for acknowledging that the presented results are surprising and our technical contributions conceptually important . We are also pleased that the reviewer finds the analysis of the learned representation very interesting .

To open up another dimension of the analysis , we have added a model which replaces the initial injective operator with a bijective operator as used in later layers . This model has almost the same number of parameters as the baseline and trains about a day faster , albeit performs worse by 1.5 % . This is to show , that model size can be reduced substantially while the invertibility property improves .

== > Maybe discarding information is not essential for learning ( which is surprising ) , but the cost of not doing so is paid in learning time .

Thank you for raising this interesting point .
We have added plots of the loss curves to the paper that show very similar training behaviour for an i- RevNet compared to a non-invertible ResNet baseline . Training hyperparameters ( e.g. learning rate schedule , training iterations , regularization ) are identical for all models we have analyzed in the paper .
Thus , there does not seem to be a cost to pay for not discarding information in terms of convergence behaviour .

== > Are the images used for the interpolation train or test images ?

The images used for interpolation are partially from datasets not seen during training ( describable textures , Basel faces ) and from the Imagenet training set . All interpolations have been obtained from the ILSVRC - 2012 trained model .

== > Could you please clarify , what do you mean with fine tuning the last layer with dropout ?

We thank the reviewer for raising this question . For the sake of brevity , we have removed this fine-tuning in the current revision entirely . This change only affected Figure 4 , we have updated the figure with interpolations from the newly added bijective model .

== > The authors mention that the forward pass of the network does not seem to suffer from significant instabilities . It would be very good to empirically evaluate this claim .

Thank you for this important remark , we have empirically evaluated our claims by measuring the normalized l2 error between original and reconstruction on the whole validation set of ILSVRC - 2012 and on randomly drawn uniform noise \in [ - 1, 1 ] , with the same number of draws as the size of the validation set . We report expectation of the error over all samples . The results show no significant instabilities and the error is visually imperceivable :

i- RevNet bijective :
ILSVRC -2012 validation set reconstruction error : 5.17e-06
50 k uniform noise draws reconstruction error : 2.63e-06

i- RevNet injective :
ILSVRC -2012 validation set reconstruction error : 8.26e-7
50 k uniform noise draws reconstruction error : 5.52e-07

We thank the reviewer for the additional references , we have added NICE and Real - NVP to the related work section and discussed their relationship to our work .

To add results on more controlled geometric transformations , we have added interpolations between small geometrical perturbations to the reconstruction experiment .

We thank the reviewer once again for the very interesting and important remarks . We believe they have substantially improved the manuscript and helped to clarify many important points .


This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1 . The proposed regularization penalize the critic function only when its gradient has a norm larger than one using some kind of squared hinge loss . The reasons for this choice are discussed and linked to theoretical properties of OT . Numerical experiments suggests that the proposed regularization leads to better posed optimization problem and even a slight advantage in terms of inception score on the CIFAR - 10 dataset .

The paper is interesting and well written , the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it 's a good idea . Still as discussed below the justification do not address a lots of interesting developments and implications of the method and should better discuss the relation with regularized optimal transport .

Discussion :

+ The paper spends a lot of time justifying the proposed method by discussing the limits of the " Improved training of Wasserstein GAN " from Gulrajani et al . ( 2017 ) . The two limits ( sampling from marginals instead of optimal coupling and differentiability of the critic ) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix . The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization ( see next ) .

+ The proposed approach is a relaxation of the constraints on the dual variable for the OT problem . As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss . This approach ( relaxing a strong constraint ) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture .

+ The paper is rather vague on the reason to go from Eq . ( 6 ) to Eq. ( 7 ) . ( gradient approximation between samples to gradient on samples ) . Does it lead to better stability to choose one or the other ?
How is it implemented in practice ? recent NN toolbox can easily compute the exact gradient and use it for the penalization but this is not clearly discussed even in appendix . Numerical experiments comparing the two implementation or at least a discussion is necessary .

+ The proposed approach has a very strong relations to the recently proposed regularized OT ( see [ 1 ] for a long list of regularizations ) and more precisely to the euclidean regularization . I understand that GANS ( and Wasserstein GAN ) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve . A discussion of the links is necessary and will clearly bring more theoretical ground to the method . Note that a square euclidean regularization leads to a regularization term in the dual of the form max ( 0 , f ( x ) + f ( y ) - |x - y | ) ^2 that is very similar to the proposed regularization . In other words the authors propose to do regularized OT ( possibly with a new regularization term ) and should discuss that .

+ The numerical experiments are encouraging but a bit short . The 2D example seem to work very well and the convergence curves are far better with the proposed regularization . But the real data CIFAR experiments are much less detailed with only a final inception score ( very similar to the competing method ) and no images even in appendix . The authors should also define ( maybe in appendix ) the conditional and unconditional inception scores and why they are important ( and why only some of them are computed in Table 1 ) .

+ This is more of a suggestion . The comparison of the dual critic to the true Wasserstein distance is very interesting . It would be nice to see the behavior for different values of lambda .


[ 1 ] Dessein , A. , Papadakis , N. , & Rouas , J. L. ( 2016 ) . Regularized Optimal Transport and the Rot Mover 's Distance . arXiv preprint arXiv:1610.06447 .


Review update after reply :

The authors have responded to most of my concerns and I think the paper is much stronger now and discuss the relation with regularized OT . I change the rating to Accept .


We thank the reviewer for his highly valuable comments and thoughtful suggestions ! Based on them , we applied the following main changes in the revised version of our paper :
We added a paragraph giving a short introduction to regularized OT in Section 2 and a paragraph about the connection to our proposed regularization in Section 5 ( special thanks for pointing us in this direction !!!) .
We extended the CIFAR experiments , by running more experiments with different values of the regularization parameter ( all show that WGAN - LP produces equivalent or better results and is less sensitive to the value of the regularization parameter ) and presenting a deeper investigation of the loss contributions of the regularization term . Interestingly we find , that the penalty of WGAN - GP is behaving similar to the one of WGAN - LP in settings with low regularization parameter . We have added theoretical considerations explaining this behaviour in Section 5 .

In the following we will reply directly to specific comments :

>> “ The paper spends a lot of time justifying the proposed method by discussing the limits of the " Improved training of Wasserstein GAN " from Gulrajani et al . ( 2017 ) . The two limits ( sampling from marginals instead of optimal coupling and differentiability of the critic ) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix . The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization ( see next ) . ”

We have n’t been able to find references , where computations of the examples can be found in the literature . Approaching WGANs from a deep learning viewpoint , we are also convinced that researchers interested in GANs without the necessary background in OT will find a quick discussion of the examples at least very helpful but possibly even necessary . ( See also opposing comments by Reviewer 2 . ) We have moved as much as we believe is adequate to the appendix .

“ The proposed approach is a relaxation of the constraints on the dual variable for the OT problem . As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss . This approach ( relaxing a strong constraint ) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture . ”

We added a sentence referring to relaxation of hard constraints in the objective of SVMs .

>> ” The paper is rather vague on the reason to go from Eq . ( 6 ) to Eq. ( 7 ) . ( gradient approximation between samples to gradient on samples ) . Does it lead to better stability to choose one or the other ? How is it implemented in practice ? recent NN toolbox can easily compute the exact gradient and use it for the penalization but this is not clearly discussed even in appendix . Numerical experiments comparing the two implementation or at least a discussion is necessary . ”

The main reason to go from Eq . ( 6 ) to Eq. ( 7 ) is that enforcing the constraint on the gradient norm implements a valid constraint into all directions from the given point , not just a condition on the difference between two points ( and just in only one direction ) . This should help for better generalization to unseen samples . We performed experiments to verify this ( the results are shown in Appendix D in the revised version of the paper ) : While regularization based on Eq. ( 6 ) worked well on toy data , it performed considerably weaker on CIFAR10 , supporting the advantage of a regularization as given in Eq . ( 7 ) .

For the computation we did indeed use standard implementations of the gradient in tensorflow ( see https://www.tensorflow.org/api_docs/python/tf/gradients and http://pytorch.org/docs/master/autograd.html#torch.autograd.grad).
Links to our code will be provided in case of acceptance .

.


>> “ + The proposed approach has a very strong relations to the recently proposed regularized OT ( see [ 1 ] for a long list of regularizations ) and more precisely to the euclidean regularization . I understand that GANS ( and Wasserstein GAN ) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve . A discussion of the links is necessary and will clearly bring more theoretical ground to the method . Note that a square euclidean regularization leads to a regularization term in the dual of the form max ( 0 , f ( x ) + f ( y ) - |x - y | ) ^2 that is very similar to the proposed regularization . In other words the authors propose to do regularized OT ( possibly with a new regularization term ) and should discuss that . ”

We are very thankful for pointing us to the link to regularized OT . The similarity to regularized OT with Euclidean regularization is highly interesting , and we discuss it now in Sections 2 and 5 .

But , at least from our understanding , the equivalence to regularized OT is not exactly given , since a regularization term in the primal does not seem to allow for the maximization in the dual over one function only . We believe the same problem to appear for any Bregman divergence and therefore doubt that any new regularization term gives the exact equivalence to any of the previously considered approaches in regularized OT that we are aware of .
We performed experiments with the variant of our regularization term , that is most similar to the Euclidean regularized OT ( see last paragraph in the experimental section ) , but it showed only good performance on toy dataset , but poor performance on larger datasets such as CIFAR - 10 .

>>+ The numerical experiments are encouraging but a bit short . The 2D example seem to work very well and the convergence curves are far better with the proposed regularization . But the real data CIFAR experiments are much less detailed with only a final inception score ( very similar to the competing method ) and no images even in appendix .

We run additional experiments on CIFAR - 10 for 3 more values of lambda ( 0.1,5,100 ) , all supporting the conclusion that WGAN - LP performs slightly better is much less dependent on the right choice of hyperparameter lambda than the WGAN - GP ( see Table 1 in the revised version ) . We also added a plot ( Fig 6 ) displaying the regularization term of WGAN - GP separated into contributions based on a gradient norm exceeding one and based on a gradient norm smaller one , which also supports the higher sensitivity of WGAN - GP to the right choice of hyperparameter , and additionally suggests that WGAN - GP in fact behaves similar to WGAN - LP when the hyperparameter lambda is chosen small enough to make it perform well .

>> “ The authors should also define ( maybe in appendix ) the conditional and unconditional inception scores and why they are important ( and why only some of them are computed in Table 1 ) ”

We added such a description into Appendix D.6.

>>” This is more of a suggestion . The comparison of the dual critic to the true Wasserstein distance is very interesting . It would be nice to see the behavior for different values of lambda . ”

Due to limitations in our access to computational resources , we were not yet able to conduct these experiments , but agree that this would be very interesting and plan to report such results in the camera ready version

This paper is proposing a new formulation for regularization of Wasserstein Generative Adversarial models ( WGAN ) . The original min / max formulation of the WGAN aim at minimizing over all measures , the maximal dispersion of expectation for 1 - Lipschitz with the one provided by the empirical measure . This problem is often regularized by adding a " gradient penalty " , \ie a penalty of the form " \lambda E_{z~\tau}} ( ||\grad f ( z ) | | - 1 ) ^2 " where \tau is the distribution of ( tx + ( 1 - x ) y ) where x is drawn according to the empirical measure and y is drawn according to the target measure . In this work the authors consider substituting the previous penalty by " \lambda E_{z~\tau}} ( max ( ||\grad f ( z ) | | - 1,0 ) ^2 " .

Overall the paper is too vague on the mathematical part , and the experiments provided are not particularly convincing in assessing the benefit of the new penalty .
The authors have tried to use mathematical formulations to motivate their choice , but they lack rigorous definitions / developments to make their point convincing .
They should also present early their model and their mathematical motivation : in what sense is their new penalty " preferable " ?



Presentation issues :
- in printed black and white versions most figures are meaningless .
- red and green should be avoided on the same plots , as colorblind people will not perceived any difference ...
- format for images should be vectorial ( eps or pdf ) , not jpg or png ...
- legend / sizes are not readable ( especially in printed version ) .

References issues :
- harmonize citations : if you add first name for some authors add them for all of them : why writing Harold W. Kuhn and C. Vilani for instance ?
- cramer - > Cramer
- wasserstein-> Wasserstein ( 2x )
- gans -> GANs
- Salimans et al. is provided twice , and the second is wrong anyway .



Specific comments :

page 1 :
- " different more recent contributions " -> more recent contributions
- avoid double brackets " ) ) "

page 2 :
- Please rewrite the first sentence below Definition 1 in a meaningful way .
- Section 3 : if \mu is an empirical distribution , it is customary to write it \mu_n or \hat \mu_n ( in a way that emphasizes the number of observations available ) .
- d is used as a discriminator and then as a distance . This is confusing ...

page 3 :
- " f that plays the role of an appraiser ( or critic ) ... " : this paragraph could be extended and possibly elements of the appendix could be added here .
- Section 4 : the way clipping is presented is totally unclear and vague . This should be improved .
- Eq ( 5 ) : as written the distribution of \tilde{x}=tx + ( 1 - t ) y is meaningless : What is x and y in this context ? please can you describe the distributions in a more precise way ?
- Proof of Proposition 5 ( cf. page 13 ) : this is a sketch of proof to me . Please state precise results using mathematical formulation .
- " Observation 1 " : real and generated data points are not introduced at this stage ... data points are not even introduced neither !

page 5 :
- the examples are hard to understand . It would be helpful to add the value of \pi^ * and f^ * for both models , and explaining in details how they fit the authors model .
- in Figure 2 the left example is useless to me . It could be removed to focus more extensively on the continuous case ( right example ) .
- the the -> the

page 6 :
- deterministic coupling could be discussed / motivated when introduced . Observation 3 states some property of non non- deterministic coupling but the concept itself seems somehow to appear out of the blue .

page 10 :
- Figure 6 : this example should be more carefully described in terms of distribution , f* , etc .

page 14 :
- Proposition 1 : the proof could be shorten by simply stating in the proposition that f and g are distribution ...

page 15 :
- " we wish to compute " -> we aim at showing ?
- f_1 is not defined sot the paragraph " the latter equation ... " showing that almost surely x \leq y is unclear to me , so is the result then .
It could be also interesting to ( geometrically ) interpret the coupling proposed . The would help understanding the proof , and possibly reuse the same idea in different context .

page 16 :
- proof of Proposition 2 : key idea here is using the positive and negative part of ( f - g ) . This could simplify the proof .

We thank the reviewer for the comments and suggestions and for checking the details of the arguments presented in the paper and thereby detecting room for substantial improvements .

This led to the following changes in the revised version of our paper :
We solved the issues in the reference section .
We improved the presentation according to your suggestions whenever possible ( as in proofs ) , improved the formulations , and removed typos .
We improved the images . In particular , we would like to thank the reviewer for noticing the red / green issue that we missed to take care of in some plots . Our new images should thereby be better to read and understand .

In the following we will reply directly to specific comments :


>> “ Overall the paper is too vague on the mathematical part , and the experiments provided are not particularly convincing in assessing the benefit of the new penalty .
The authors have tried to use mathematical formulations to motivate their choice , but they lack rigorous definitions / developments to make their point convincing . ”

Unfortunately , the complaint about the lack of rigour is too broad for us to understand what exactly the reviewer is missing . We do believe , however , that the mathematical formulations are complete and concise , only due to the limited space available , we were forced to move most of the mathematical proofs into the appendix . We would be happy to improve by adding missing definitions or arguments that we are unaware of , if we get pointed to specific suggestions .

>> “ They should also present early their model and their mathematical motivation : in what sense is their new penalty " preferable " ? ”

The new penalty is preferable over the previous ones , since
The new penalty does not exclude approximations of optimal critic functions as the weight clipping approach does ,
does not enforce a constraint that can not be justified ,
is therefore less dependent on the choice of the hyperparameter lambda ,
still builds on the great advantages of WGANs , which are one of the best performing GANs currently out there ( and even leads to slightly better and more stable performance in practice ) .

We made points 1 , 2 and 4 more clear in revised version of the paper and have added
more theoretical results in Section 5 and experimental results in Section 6 to verify point 3.


>> Section 3 : if \mu is an empirical distribution , it is customary to write it \mu_n or \hat \mu_n ( in a way that emphasizes the number of observations available ) .

The distributions do not necessarily need to be empirical here . Therefore , we decided to keep the distributions general with the according notation .

>> page 3:
- " f that plays the role of an appraiser ( or critic ) ... " : this paragraph could be extended and possibly elements of the appendix could be added here .

In a longer version , it would be nice to elaborate on this point in the main paragraph . Only due to the strong space constraints we were forced to move these considerations into the appendix , since ( despite being helpful for understanding ) they are not relevant for the rest of the paper .

>> “- Section 4 : the way clipping is presented is totally unclear and vague . This should be improved . ”

We state that weight clipping is “ to enforce the parameters of the network not to exceed a certain value c_max >0 in absolute value ” . Translated into formulas this is : there is some c_max > 0 such that |p|< c_max for all network parameters p , which is exactly the definition of weight clipping .

>> “- Proof of Proposition 5 ( cf. page 13 ) : this is a sketch of proof to me . Please state precise results using mathematical formulation . ”

Unfortunately we are unsure what the reviewer is referring to . ( There is no Prop 5 in the originally submitted version ) . We assume that Proposition 1 is meant .
In that case , we believe our version qualifies as more than a sketch of proof since it contains the complete set of arguments . If a reader feels more comfortable with mathematical notation , the reader may translate the written words into mathematical formulas to follow the arguments . In this case , we believe that formulas would even distract from the simplicity of arguments . In the end , our intention is to simplify the proof taken from the paper “ Improving training of Wasserstein GANs ” ( https://arxiv.org/abs/1704.00028).


>> “ page 5 : ”
- the examples are hard to understand . It would be helpful to add the value of \pi^ * and f^ * for both models , and explaining in details how they fit the authors model .

We have added a labeled y-axis for the values of f* . The optimal coupling is indicated in red as before . In terms of ( generalized ) probability distributions , this would correspond to delta functions defined by the coupled points X and Os .

>> “ - in Figure 2 the left example is useless to me . It could be removed to focus more extensively on the continuous case ( right example ) . ”

The discrete case is in our opinion much easier to understand than the continuous one , but motivates the reasoning for an optimal critic in the continuous case . We therefore suggest to leave it in . ( see also our comments on the suggestion of a geometrical interpretation of the coupling in the proof to Proposition 1 )

>> “ page 6 :
- deterministic coupling could be discussed / motivated when introduced . Observation 3 states some property of non non- deterministic coupling but the concept itself seems somehow to appear out of the blue . ”

We have added a short discussion on deterministic couplings .



>> “ page 10 :
- Figure 6 : this example should be more carefully described in terms of distribution , f* , etc . ”

The optimal coupling is described in the text . Drawing the corresponding connections of the coupling would make the image less clear . The continuous function is also sufficiently described in the text by defining the slope almost everywhere , recalling that any choice of y-intercept will produce an optimal critic function .

>> “ page 14 :
- Proposition 1 : the proof could be shorten by simply stating in the proposition that f and g are distribution … ”


We suspect that this refers to Proposition 2 instead .
In this case , we believe the proof is easier to phrase by starting with the density functions directly instead of starting with the distributions and then moving to the density functions ( which we feel is necessary for our proof )

page 15 :


>> “ - f_1 is not defined sot the paragraph " the latter equation ... " showing that almost surely x \leq y is unclear to me , so is the result then . ”

The latter equation was indeed unclear as written down . We have corrected it and removed f_1 from the notation .

>>” It could be also interesting to ( geometrically ) interpret the coupling proposed . This would help understanding the proof , and possibly reuse the same idea in different context . ”

The geometric intuition is given by the discrete example from Figure 2 . This is also exactly the reason why we suggest to keep the discrete case in the paper .

In words that would be to move the left / right half of one distribution to the left / right half of the other distribution respectively . ( We have added such a sentence to the beginning of the proof ) . We then use the freedom of non-uniqueness of the optimal coupling to simply find any coupling doing exactly that .

>> ” page 16 :
- proof of Proposition 2 : key idea here is using the positive and negative part of ( f - g ) . This could simplify the proof . ”

We do not quite understand this comment , as using the positive and negative part of f-g is exactly what we are doing . We have added the comment that the mathematical formulas describe exactly the positive and the negative part of ( f - g ) .


The article deals with regularization / penalization in the fitting of GANs , when based on a L_1 Wasserstein metric . Basics on mass transportation are briefly recalled in section 2 , while section 3 formulate the GANs approach in the Wasserstein context . Taking into account the Lipschitz constraint and ( non - ) differentiability of optimal critic functions f are discussed in section 4 and Section 5 proposes a way to penalize candidate functions f that do not satisfy the Lipschitz condition using a tuning parameter lambda , ruling a trade - off between marginal fitting and gradient control . The approach is illustrated by numerical experiments . Such results are hardly convincing , since the tuning of the parameter lambda plays a crucial role in the performance of the method . More importantly , The heuristic proposed in the paper is interesting and promising in some respects but there is a real lack of theoretical guarantees motivating the penalty form chosen , such a theoretical development could allow to understand what may rule the choice of an ideal value for lambda in particular .

We thank the reviewer for the valuable feedback .

We share the viewpoint that theoretical guarantees would be very much desirable and should further investigated , however we also think that rigorous convergence results , as for example in convex optimization , are hard to establish in a field of deep learning approaches , where there is still the lack of theoretical understanding in general .
On the other hand , we do believe that our research provides sufficient theoretical evidence for our method to be advantageous over existing approaches to WGANs .

In the following we will reply directly to specific comments :

>> “ The approach is illustrated by numerical experiments . Such results are hardly convincing , since the tuning of the parameter lambda plays a crucial role in the performance of the method . ”

It is a weakness of many models that they do depend on tuning hyperparameters in a very sensitive way . This has also been demonstrated for various GANs in a recent paper ( https://arxiv.org/abs/1711.10337). Our results , however , demonstrate that our version , WGAN - LP , is less sensitive to the tuning of lambda than WGAN - GP . That alone is a big advantage of our version to existing ones in our opinion . We tried to make this point more clear in the revised version and added theoretical considerations and more experimental results on CIFAR with different choices of the hyperparameter which consistently show a better performance of WGAN - LP and less sensitivity to the right choice of lambda .

>> “ More importantly , the heuristic proposed in the paper is interesting and promising in some respects but there is a real lack of theoretical guarantees motivating the penalty form chosen , such a theoretical development could allow to understand what may rule the choice of an ideal value for lambda in particular . ”

We believe that our approach is theoretically justified in the sense that it does point out theoretical issues of former approaches that were not noticed and corrects them . In this way it improves on one of best - working GANs in a theoretically justified way .
In the revised version , we are now also discussing the link to regularized optimal transport theory ( see new paragraphs in Sections 2 and 5 ) .
We agree , that a theoretical analysis that could guide the choice of the right value of the hyperparameter would be highly desirable , but those guarantees are hard to derive .
From a theoretical viewpoint ( that we could not see reflected in experimental results however ) we believe high hyperparameter choices would be ideal , since they “ strengthen ” the weak constraint . A choice of a high value for lambda would also be justified by the newly added connection to optimal transport theory ( see Section 5 ) . In addition , we added some theoretical observations on the dependence on lambda in Section 5 .


I have read the responses to the concerns raised by all reviewers . I find the clarifications and modifications satisfying , therefore I keep my rating of the paper to above acceptance threshold .

-----------------
ORIGINAL REVIEW :

The paper proposes a method for quantizing neural networks that allows weights to be quantized with different precision depending on their importance , taking into account the loss . If the weights are very relevant , it assigns more bits to them , and in the other extreme it does pruning of the weights .

This paper addresses a very relevant topic , because in limited resources there is a constrain in memory and computational power , which can be tackled by quantizing the weights of the network . The idea presented is an interesting extension to weight pruning with a close form approximate solution for computing the adaptive quantization of the weights .

The results presented in the experimental section are promising . The quantization is quite cheap to compute and the results are similar to other state - of - the- art quantization methods .
From the tables and figures , it is difficult to grasp the decrease in accuracy when using the quantized model , compared to the full precision model , and also the relative memory compression . It would be nice to have this reference in the plots of figure 3 . Also , it is difficult to see the benefits in terms of memory / accuracy compromise since not all competing quantization techniques are compared for all the datasets .
Another observation is that it seems from figure 2 that a lot of the weights are quantized with around 10 bits , and it is not clear how the compromise accuracy / memory can be turned to less memory , if possible . It would be interesting to know an analogy , for instance , saying that this adaptive compression in memory would be equivalent to quantizing all weights with n bits .

OTHER COMMENTS :

-missing references in several points of the paper . For instance , in the second paragraph of the introduction , 1st paragraph of section 2 .

- few typos :
*psi -> \psi in section 2.3
* simply -> simplify in proof of lemma 2.2
* Delta -> \ Delta in last paragraph of section 2.2
*l2 -> L_2 or l_2 in section 3.1 last paragraph .

Thank you for your valuable comments . We have modified the paper accordingly and highlighted the changes in blue . We have also resolved the missing references and typos . Below , we discuss the points mentioned by the reviewer in detail .

Question : From the tables and figures , it is difficult to grasp the decrease in accuracy when using the quantized model , compared to the full precision model , and also the relative memory compression . It would be nice to have this reference in the plots of figure 3.

Answer : Thanks for pointing this out . In the revised version , we highlight the optimal trade - off between accuracy and model size for each model in Figure 3 . We further report the accuracy and the reduction in the model size for these optimal models . We observe compression ratios of these optimal models equal to 64x , 35x , and 13 x ( corresponding to 98.4 % , 97 % , and 92 % reductions in model size ) for MNIST , CIFAR - 10 , and SVHN , with 0.12 % , - 0.02 % , and 0.7 % decrease in accuracy , respectively . We modified section 4 to clarify these results .



Question : Also , it is difficult to see the benefits in terms of memory / accuracy compromise since not all competing quantization techniques are compared for all the datasets .

Answer : In Figure 3 , we have added comparisons with the Binary Connect technique for all three datasets . This technique can often improve the accuracy of BNN with the same model size . Yet , these comparisons confirm our original results . That is , the proposed method almost always outperforms state - of - the- art of quantization ( BinaryConnect and BNN ) and consistently produces competitive results . We have modified section 4 with the discussion of these results .



Question : Another observation is that it seems from figure 2 that a lot of the weights are quantized with around 10 bits , and it is not clear how the compromise accuracy / memory can be turned to less memory , if possible . It would be interesting to know an analogy , for instance , saying that this adaptive compression in memory would be equivalent to quantizing all weights with n bits .

Answer : Figures 2 ( a , b , c ) , for clarity , only show non - pruned parameters , which comprise a small portion of the original parameters of the model . Taking these parameters into account , adaptive quantization compresses MNIST , CIFAR - 10 , and SVHN models to equivalent of 0.03 , 0.27 , and 1.3 bits per parameter , respectively ( results are for the optimal trade - off points highlighted in Figure 3 ) . These are all significantly smaller or comparable to state - of - the-art of quantization , that is , BNN and BinaryConnect ( 1 bit per parameter ) . We have modified section 4 with these clarifications and updated Figure 2 ( a , b , c ) to include the pruned parameters for comparison with non-pruned parameters .



Question : missing references in several points of the paper . For instance , in the second paragraph of the introduction , 1st paragraph of section 2 .

Answer : Thanks . We have included the references in the revised version of the paper .

Revised Review :

The authors have addressed most of my concerns with the revised manuscript . I now think the paper does just enough to warrant acceptance , although I remain a bit concerned that since the benefits are only achievable with customized hardware , the relevance / applicability of the work is somewhat limited .

Original Review :

The paper proposes a technique for quantizing the weights of a neural network , with bit-depth / precision varying on a per-parameter basis . The main idea is to minimize the number of bits used in the quantization while constraining the loss to remain below a specified upper bound . This is achieved by formulating an upper bound on the number of bits used via a set of " tolerances " ; this upper bound is then minimized while estimating any increase in loss using a first order Taylor approximation .

I have a number of questions and concerns about the proposed approach . First , at a high level , there are many details that are n't clear from the text . Quantization has some bookkeeping associated with it : In a per-parameter quantization setup it will be necessary to store not just the quantized parameter , but also the number of bits used in the quantization ( takes e.g. 4 - 5 extra bits ) , and there will be some metadata necessary to encode how the quantized value should be converted back to floating point ( e.g. , for 8 - bit quantization of a layer of weights , usually the min and max are stored ) . From Algorithm 1 it appears the quantization assumes parameters in the range [ 0 , 1 ] . Do n't negative values require another bit ? What happens to values larger than 1 ? How are even bit depths and associated asymmetries w.r.t. 0 handled ( e.g. , three bits can represent - 1 , 0 , and 1 , but 4 must choose to either not represent 0 or drop e.g . - 1 ) ? None of these details are clearly discussed in the paper , and it 's not at all clear that the estimates of compression are correct if these bookkeeping matters are n't taken into account properly .

Additionally the paper implies that this style of quantization has benefits for compute in addition to memory savings . This is highly dubious , since the method will require converting all parameters to a standard bit-depth on the fly ( probably back to floating point , since some parameters may have been quantized with bit depth up to 32 ) . Alternatively custom GEMM / conv routines would be required which are impossible to make efficient for weights with varying bit depths . So there are likely not runtime compute or memory savings from such an approach .

I have a few other specific questions : Are the gradients used to compute \mu computed on the whole dataset or minibatches ? How would this scale to larger datasets ? I am confused by the equality in Equation 8 : What happens for values shared by many different quantization bit depths ( e.g. , representing 0 presumably requires 1 bit , but may be associated with a much finer tolerance ) ? Should " minimization in equation 4 " refer to equation 3 ?

In the end , while do like the general idea of utilizing the gradient to identify how sensitive the model might be to quantization of various parameters , there are significant clarity issues in the paper , I am a bit uneasy about some of the compression results claimed without clearer description of the bookkeeping , and I do n't believe an approach of this kind has any significant practical relevance for saving runtime memory or compute resources .

Thank you for your insightful comments . We have modified the manuscript based on the questions from the reviewer . The changes and additions to the paper have been highlighted in blue . Below we discuss each of the questions in more detail one - by-one .

Question : I have a number of questions and concerns about the proposed approach . First , at a high level , there are many details that are n't clear from the text . Quantization has some bookkeeping associated with it : In a per-parameter quantization setup it will be necessary to store not just the quantized parameter , but also the number of bits used in the quantization ( takes e.g. 4 - 5 extra bits ) , and there will be some metadata necessary to encode how the quantized value should be converted back to floating point ( e.g. , for 8 - bit quantization of a layer of weights , usually the min and max are stored ) . From Algorithm 1 it appears the quantization assumes parameters in the range [ 0 , 1 ] . Do n't negative values require another bit ? What happens to values larger than 1 ? How are even bit depths and associated asymmetries w.r.t. 0 handled ( e.g. , three bits can represent - 1 , 0 , and 1 , but 4 must choose to either not represent 0 or drop e.g . - 1 ) ? None of these details are clearly discussed in the paper , and it 's not at all clear that the estimates of compression are correct if these bookkeeping matters are n't taken into account properly .

Answer : We agree with the reviewer that it is important to evaluate the potential overhead of bookkeeping . However , we should also have in mind that bookkeeping has an intricate relationship with the target hardware , which may lead to radically different results on different hardware platforms ( ranging from 0 to ~ 60 % ) . For example , our experiments show that on specialized hardware , such as the one designed by Albericio et al ( 2017 ) for processing variable bit width CNN , we can fully offset all bookkeeping overheads of storing quantization depths , while CPU / GPU may require up to 60 % additional storage . We will study this complex relationship separately , in our future work , and in the context of hardware implementation . In this paper , we limit the scope to algorithm analysis , independent of underlying hardware architectures . We note that in this analysis , we have evaluated the metadata as well as the additional sign bits . The metadata overhead is negligible ( about 4 bytes per layer ) due to the balanced quantization of algorithm 1 which divides the range [ 0 ,1 ] into equally sized partitions and assigns a single bit to each parameter . As we discuss in the answer to the next question , this scheme eliminates the need to convert parameters back to floating - point , and computations can be performed directly on the quantized values . For example , the 5 - bit signed value 01011 , for example , represents 2^ ( - 1 ) +2^(-3 ) +2^(-4) =0.6875 ( the initial 0 bit represents a positive value ) , which can be easily multiplied with other values using fixed - point shifts and additions . If it is necessary to have parameters in a larger range , say [ - S , S ] , a scale value like S ( 4 bytes of metadata ) could be allocated for each layer , that is applied to the output of that layer . We have clarified these points in the updated version of the paper , in section 2 and section 3 .

Albericio , Jorge , et al . " Bit -pragmatic deep neural network computing . " Proceedings of the 50th Annual IEEE / ACM International Symposium on Microarchitecture . ACM , 2017 .



Question : Additionally the paper implies that this style of quantization has benefits for compute in addition to memory savings . This is highly dubious , since the method will require converting all parameters to a standard bit-depth on the fly ( probably back to floating point , since some parameters may have been quantized with bit depth up to 32 ) . Alternatively custom GEMM / conv routines would be required which are impossible to make efficient for weights with varying bit depths . So there are likely not runtime compute or memory savings from such an approach .

Answer : We agree that on CPU / GPU interpreting variable - bit width parameters may incur computational costs . However , our quantization scheme significantly reduces the necessary computation on our target platforms , that is , specialized hardware like Alberricio et al ( 2017 ) or configurable hardware like FPGAs . These platforms can directly process the variable - bit width , fixed - point parameters without the need to convert them into floating point , and can implement custom computation units to efficiently perform matrix multiplication / convolutions by taking advantage of the small quantization depths of the parameters in the quantized model . We note that in our experiments , parameters are often quantized with far fewer bits than 32 , with little to no accuracy loss . Thus , our approach can significantly accelerate performance on this class of hardware by minimizing the required computations . We have clarified this in section 2 .



Question : I have a few other specific questions : Are the gradients used to compute \mu computed on the whole dataset or minibatches ? How would this scale to larger datasets ?

Answer : Gradients are calculated on minibatches . As we have specified in section 3 , we use the same batch size for training and quantization to keep the computation time short . Our experiments show that this decision does not have a negative effect on the accuracy of the quantized model . Thus , as long as we choose representative batch sizes , as we do for training , the algorithm scales to larger datasets with no need for modifications . We have modified Section 3 for clarification .



Question : I am confused by the equality in Equation 8 : What happens for values shared by many different quantization bit depths ( e.g. , representing 0 presumably requires 1 bit , but may be associated with a much finer tolerance ) ?

Answer : This equation explores the worst case for quantization error and shows that in this case the quantization depth is bounded by negative logarithm of the tolerance . In general , we can expect the quantization depth to be smaller than this value . That is because Algorithm 1 minimizes the bit width of a parameter with respect to its tolerance . If multiple bit widths satisfy this requirement , the smallest is always chosen . For example , a parameter with the signed value equal to 0.25 can be represented by both 001 and 0010 . Algorithm 1 however , will always return the former . We have modified Section 2 for clarification .



Question : Should " minimization in equation 4 " refer to equation 3 ?

Answer : Yes . Thank you for pointing this out . We have corrected the typo .

The authors present an interesting idea to reduce the size of neural networks via adaptive compression , allowing the network to use high precision where it is crucial and low precision in other parts . The problem and the proposed solution is well motivated . However , there are some elements of the manuscript that are hard to follow and need further clarification / information . These need to definitely be addressed before this paper can be accepted .

Specific comments / questions :
- Page 1 : Towards the bottom , in the 3rd to last line , reference is missing .
- Page 1 : It is a little hard to follow the motivation against existing methods .
- Page 2 : DenseNets and DeepCompression need citations
- Lemma 2.1 seems interesting - is this original work ? This needs to be clarified .
- Lemma 2.2 : Reference to Equation 17 ( which has not been presented in the manuscript at this point ) seems a little confusing and I am unable to following the reasoning and the subsequent proof which again refers to Equation 17 .
- Alg 2 : Should it be $ \ Delta$ or $ \ Delta_{k + 1 } $ ? Because in one if branch , we use $ \ Delta$ , in the other , we use the subscripted one .
- Derivation in section 2.3 has some typographical errors .
- What is $ d$ in Equation 20 ( with cases ) ? Without this information , it is unclear how the singular points are handled .
- Page 6 , first paragraph of Section 3 : The evaluation is a little confusing - when is the compression being applied during the training process , and how is the training continued post-compression ? What does each compression ' pass ' constitute of ?
- Figure 1b : what is the ' iteration ' on the horizontal axis , is it the number of iterations of Alg3 or Alg2 ? Hoping it is Alg3 but needs to be clarified in the text .
- Section 3 : What about compression results for CIFAR and SVNH ?

We appreciate your insightful comments . In the updated version of the paper , we have fixed the missing references and typos , and clarified the evaluation methodology as well as the other points mentioned by the reviewer . The changes have been highlighted in blue . Here , we address specific questions by the reviewer one - by-one .

1 . Page 1 : Towards the bottom , in the 3rd to last line , reference is missing .

- Added references Hubara ( 2016a ) and Han ( 2015 ) .


2 . Page 1 : It is a little hard to follow the motivation against existing methods .

- Modified the discussion in the introduction ( highlighted blue ) .


3 . Page 2 : DenseNets and DeepCompression need citations

- Added references Huang ( 2017 ) and Han ( 2015 ) in section 1 .


4 . Lemma 2.1 seems interesting - is this original work ? This needs to be clarified .

- Lemma 2.1 is an original contribution of the paper . We added clarification in Section 2 .


5 . Lemma 2.2 : Reference to Equation 17 ( which has not been presented in the manuscript at this point ) seems a little confusing and I am unable to following the reasoning and the subsequent proof which again refers to Equation 17 .

- We revised the cross references in the proof of lemma 2.2 . The constraint refers to the definitions in equations 11 and 12 .


6 . Alg 2 : Should it be $ \ Delta$ or $ \ Delta_{k + 1 } $ ? Because in one if branch , we use $ \ Delta$ , in the other , we use the subscripted one .

- Added the subscript in algorithm 2 .


7 . Derivation in section 2.3 has some typographical errors .

- Fixed the typographical errors .


8 . What is $ d$ in Equation 20 ( with cases ) ? Without this information , it is unclear how the singular points are handled .

- $ d$ in equation 20 refers to the difference between the loss bound $ \overline{l} $ and the loss in the current iteration of the algorithm $ l ( W_k ) $ : $ d = \overline{l} - l ( W_k ) $ . We have modified equation 20 accordingly .


9 . Page 6 , first paragraph of Section 3 : The evaluation is a little confusing

- Revised the first paragraph of section 3 to clarify the process of evaluation ( highlighted blue ) .


10 . when is the compression being applied during the training process , and how is the training continued post-compression ? What does each compression ' pass ' constitute of ?

- We added additional explanation in section 3 regarding when compression is performed and what a pass of compression constitutes . Specifically , adaptive quantization is applied to a model after the training is complete . The retraining steps after the compression are performed in full - precision , floating - point domain . Also , each pass of compression refers to a complete execution of algorithm 3 .


11 . Figure 1b : what is the ' iteration ' on the horizontal axis , is it the number of iterations of Alg3 or Alg2 ? Hoping it is Alg3 but needs to be clarified in the text .

- We clarified the definition of iteration in figure 1 . Each iteration , refers to one iteration of the loop in algorithm 2 .


12 . Section 3 : What about compression results for CIFAR and SVNH ?

- We have added the compression results for the optimal trade - off for all three datasets in the revised version ( Figure 3 ) . We have further added comparison with Binary Connect for all datasets and shown that the original conclusions hold . That is , the proposed algorithm almost always outperforms state - of - the- art of quantization ( BinaryConnect and BNN ) and consistently produces competitive results .



Quality
The theoretical results presented in the paper appear to be correct . However , the experimental evaluation is globally limited , hyperparameter tuning on test which is not fair .

Clarity
The paper is mostly clear , even though some parts deserve more discussion / clarification ( algorithm , experimental evaluation ) .

Originality
The theoretical results are original , and the SGD approach is a priori original as well .

Significance
The relaxed dual formulation and OT / Monge maps convergence results are interesting and can of of interest for researchers in the area , the other aspects of the paper are limited .

Pros :
- Theoretical results on the convergence of OT / Monge maps
- Regularized formulation compatible with SGD
Cons
- Experimental evaluation limited
- The large scale aspect lacks of thorough analysis
- The paper presents 2 contributions but at then end of the day , the development of each of them appears limited

Comments :

- The weak convergence results are interesting . However , the fact that no convergence rate is given makes the result weak .
In particular , it is possible that the number of examples needed for achieving a given approximation is at least exponential .
This can be coherent with the problem of Domain Adaptation that can be NP - hard even under the co-variate shift assumption ( Ben-David&Urner , ALT2012 ) .
Then , I think that the claim of page 6 saying that Domain Adaptation can be performed " nearly optimally " has then to be rephrased .
I think that results show that the approach is theoretically justified but optimality is not here yet .

Theorem 1 is only valid for entropy - based regularizations , what is the difficulty for having a similar result with L2 regularization ?

- The experimental evaluation on the running time is limited to one particular problem . If this subject is important , it would have been interesting to compare the approaches on other large scale problems and possibly with other implementations .
It is also surprising that the efficiency the L2 -regularized version is not evaluated .
For a paper interesting in large scale aspects , the experimental evaluation is rather weak .

The 2 methods compared in Fig 2 reach the same objective values at convergence , but is there any particular difference in the solutions found ?

- Algorithm 1 is presented without any discussion about complexity , rate of convergence . Could the authors discuss this aspect ?
The presentation of this algo is a bit short and could deserve more space ( in the supplementary )

- For the DA application , the considered datasets are classic but not really " large scale " , anyway this is a minor remark .
The setup is not completely clear , since the approach is interesting for out of sample data , so I would expect the map to be computed on a small sample of source data , and then all source instances to be projected on target with the learned map . This point is not very clear and we do not know how many source instances are used to compute the mapping - the mapping is incomplete on this point while this is an interesting aspect of the paper : this justifies even more the large scale aspect is the algo need less examples during learning to perform similar or even better classification .
Hyperparameter tuning is another aspect that is not sufficiently precise in the experimental setup : it seems that the parameters are tuned on test ( for all methods ) , which is not fair since target label information will not be available from a practical standpoint .

The authors claim that they did not want to compete with state of the art DA , but the approach of Perrot et al. , 2016 seems to a have a similar objective and could be used as a baseline .

Experiments on generative optimal transport are interesting and probably generate more discussion / perspectives .

--
After rebuttal
--
Authors have answered to many of my comments , I think this is an interesting paper , I increase my score .


Dear reviewer ,

We thank you for your positive review and have updated the paper accordingly .

“ - The weak convergence results are interesting . However , the fact that no convergence rate is given makes the result weak . [ … ] approximation is at least exponential . ”

We thank Reviewer4 for the clarification and reference . Indeed , we expect that the number of samples to achieve a given error on the OT plans grows exponentially with the dimension since it was proven in the case of the OT objective ( Boissard ( 2011 ) , Sriperumbudur et al. ( 2012 ) , Boissard & Le Gouic ( 2014 ) ) , and we expect the behavior is at least as ‘ bad ’ for the convergence of OT plans . An interesting line of research , mentioned in conclusion of Weed and Bach ( 2017 ) is to investigate whether regularization helps improve these rates .

Regarding the convergence rates of empirical OT plans , we believe this is an interesting but complex topic which deserves a study in its own right . To our knowledge , there are works proving convergence rates of the empirical OT objective ( see ref. above ) , but none about convergence rates of OT plans .

“ [ ...] DA can be performed " nearly optimally " has then to be rephrased . ”

We agree and have rephrased accordingly .

“ Theorem 1 [ ... ] , what is the difficulty for having a similar result with L2 regularization ? ”

Our proofs rely partly on asymptotic convergence rates of entropy - reg linear programs established by Cominetti & Saint Martin ( 1994 ) . To our knowledge , no extension has been obtained for L2 - reg linear programs , which prevents us from adapting our proofs . Extending these results to the L2 case would be indeed of great interest .

“ - The experimental evaluation on the running time is limited [ … ] . It is also surprising that the efficiency the L2 -regularized version is not evaluated . [ … ] , the experimental evaluation is rather weak . ”

No algorithm for computing the L2 -reg OT in large - scale or continuous settings have been proposed . Hence , we do not know other algorithms to compare with .

We mention that our experiments are large - scale considering the OT problem . For ex. , Genevay et al. ( 2016 ) considered measures supported on 20 k samples , while measures in our numerical - speed xp have 250 k samples . However , we agree that more numerical - speed xps would make our proposed Alg. 1 more convincing and will add experiments .

“ The 2 methods compared in Fig 2 [ ... ] , is there any particular difference in the solutions found ? ”

We performed speed - comparison experiments in the discrete setting , where the dual objective is strictly concave with a unique solution . The semi-dual objective is also strictly concave , and the dual variable solution of the semi-dual is the same as the first dual variable of the dual problem .

“ - Algorithm 1 is presented without any discussion about complexity , rate of convergence . ”

We agree and have added a paragraph “ Convergence rates and computational cost comparison ” .

“ The setup is not completely clear , since the approach is interesting for out of sample data , so I would expect the map to be computed on a small sample of source data , and then all source instances to be projected on target with the learned map . [ … ] needs less examples during learning to perform similar or even better classification . ”

One of our contribution is indeed to allow out - of - sample prediction which avoids learning again a full transport map if one dataset is augmented . But learning a Monge map is a very difficult problem and one should use all the available data , which is now possible thanks to our proposed stochastic algorithms . The fact that Perrot et al. ( 2016 ) used at most 1000 samples was due to the numerical complexity of the mapping estimation alg .

“ Hyperparameter tuning is another aspect that is not sufficiently precise in the experimental setup : it seems that the parameters are tuned on test [ … ] . ”

The parameter validation tuned on test is indeed unrealistic because we have indeed not access to target samples labels in practice . Still we believe it is reasonable and fair since it allows all methods to work at their best , without relying on approximate validation that might benefit one method over another . Note that unsupervised DA validation is still an open problem : some authors perform as we did ; or do validation using labels ; others do more realistic but less stable techniques such as circular validation .

“ The authors claim that they did not want to compete with state of the art DA , but the approach of Perrot et al. , 2016 seems to a have a similar objective and could be used as a baseline . ”

We can not compare fairly to Perrot et al. ( 2016 ) since they used a very small number of sample to estimate a map . But this would be a good baseline to show the importance of learning with a many samples . The method will be added to the xps very soon .

Reference not in the paper :
- Weed , Jonathan , and Francis Bach . " Sharp asymptotic and finite - sample rates of convergence of empirical measures in Wasserstein distance . " arXiv


The paper proves the weak convergence of the regularised OT problem to Kantorovich / Monge optimal transport problems .

I like the weak convergence results , but this is just weak convergence . It appears to be an overstatement to claim that the approach " nearly - optimally " transports one distribution to the other ( Cf e.g. Conclusion ) . There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2 . Also , near-optimality would refer to some parameters being chosen in the best possible way . I do not see that from the paper . However , the weak convergence results are good .

A better result , hinting on how " optimal " this can be , would have been to guarantee that the solution to regularised OT is within f( epsilon ) from the optimal one , or from within f( epsilon ) from the one with a smaller epsilon ( more possibilities exist ) . This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum .

I also like the choice of the two regularisers and wonder whether the authors have tried to make this more general , considering other regularisations ? After all , the L2 one is just an approximation of the entropic one .

Typoes :

1 - Kanthorovich -> Kantorovich ( Intro )
2 - Cal C <-> C ( eq. 4 )

Dear reviewer ,

We thank you for your positive review and relevant comments .

" I like the weak convergence results , but this is just weak convergence . It appears to be an overstatement to claim that the approach " nearly - optimally " transports one distribution to the other ( Cf e.g. Conclusion ) . There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2 . Also , near-optimality would refer to some parameters being chosen in the best possible way . I do not see that from the paper . However , the weak convergence results are good . "

Theorem 1 . proves weak convergence of regularized discrete plans . This is a natural convergence for random variables ( we emphasize that weak convergence is equivalent to the convergence w.r.t. , for instance , the Wasserstein distance ) . Regarding the convergence of Monge maps ( in Theorem 2 ) , other types of convergence , such as convergence in probability , would be of great interest indeed . We may consider this problem in some future work .

Using the term ' nearly - optimality ' was indeed vague as we have not defined what ‘ nearly ’ means . We have removed this expression from the the paper . Otherwise , ‘ optimal ’ or ‘optimality ’ refers to a solution of either the Monge problem ( 1 ) , the OT problem ( 2 ) , or the regularized OT problem ( 3 ) .

“ A better result , hinting on how " optimal " this can be , would have been to guarantee that the solution to regularised OT is within f( epsilon ) from the optimal one , or from within f( epsilon ) from the one with a smaller epsilon ( more possibilities exist ) . This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum . ”

We can indeed consider two cases when to measure how a solution to the regularized OT ( ROT ) problem is ‘ optimal ’ :
- How close in the solution of ROT to the solution of OT w.r.t. a given norm : in the discrete case , the paper of Cominetti & Saint Martin ( 1994 ) proves asymptotic exponential convergence rate for the entropic regularization case . We are not aware of similar result for the L2 regularization , which would be of great interest and deserves a study in its own right . In the continuous - continuous case , the recent paper from Carlier et al. ( 2017 ) only provides convergence results of entropy - regularized plans .
- How optimal is the OT objective computed with the solution of ROT : in that case various bounds about the ROT objective compared to the OT objective can be used . See for example Blondel et al . ( 2017 ) which provides bounds for both entropic and L2 regularizations .

“ I also like the choice of the two regularisers and wonder whether the authors have tried to make this more general , considering other regularisations ? After all , the L2 one is just an approximation of the entropic one . ”

This is indeed be possible . To extend our approach seamlessly , it would be sufficient that the regularizer R verifies : convexity , which ensures that the dual is well defined and unconstrained , and decomposability , which provides a dual of the form Eq . ( 6 ) . More details are given in Blondel et al . ( 2017 ) . We have added a small discussion about it in the main text of the updated paper , in the paragraph " Regularized OT dual " .

“ Typoes :
1 - Kanthorovich -> Kantorovich ( Intro )
2 - Cal C <-> C ( eq. 4 ) ”

This has been corrected , thank you .

References not in the paper :
Carlier , Guillaume , et al . " Convergence of entropic schemes for optimal transport and gradient flows . " SIAM Journal on Mathematical Analysis 49.2 ( 2017 ) : 1385-1418.

This paper proposes a new method for estimating optimal transport plans and maps among continuous distributions , or discrete distributions with large support size . First , the paper proposes a dual algorithm to estimate Kantorovich plans , i.e. a coupling between two input distributions minimizing a given cost function , using dual functions parameterized as neural networks . Then an algorithm is given to convert a generic plan into a Monge map , a deterministic function from one domain to the other , following the barycenter of the plan . The algorithms are shown to be consistent , and demonstrated to be more efficient than an existing semi-dual algorithm . Initial applications to domain adaptation and generative modeling are also shown .

These algorithms seem to be an improvement over the current state of the art for this problem setting , although more of a discussion of the relationship to the technique of Genevay et al . would be useful : how does your approach compare to the full - dual , continuous case of that paper if you simply replace their ball of RKHS functions with your class of deep networks ?

The consistency properties are nice , though they do n't provide much insight into the rate at which epsilon should be decreased with n or similar properties . The proofs are clear , and seem correct on a superficial readthrough ; I have not carefully verified them .

The proofs are mainly limited in that they do n't refer in any way to the class of approximating networks or the optimization algorithm , but rather only to the optimal solution . Although of course proving things about the actual outcomes of optimizing a deep network is extremely difficult , it would be helpful to have some kind of understanding of how the class of networks in use affects the solutions . In this way , your guarantees do n't say much more than those of Arjovsky et al. , who must assume that their " critic function " reaches the global optimum : essentially you add a regularization term , and show that as the regularization decreases it still works , but under seemingly the same kind of assumptions as Arjovsky et al . 's approach which does not add an explicit regularization term at all . Though it makes sense that your regularization might lead to a better estimator , you do n't seem to have shown so either in theory or empirically .

The performance comparison to the algorithm of Genevay et al . is somewhat limited : it is only on one particular problem , with three different hyperparameter settings . Also , since Genevay et al . propose using SAG for their algorithm , it seems strange to use plain SGD ; how would the results compare if you used SAG ( or SAGA / etc ) for both algorithms ?

In discussing the domain adaptation results , you mention that the L2 regularization " works very well in practice , " but do n't highlight that although it slightly outperforms entropy regularization in two of the problems , it does substantially worse in the other . Do you have any guesses as to why this might be ?

For generative modeling : you do have guarantees that , *if * your optimization and function parameterization can reach the global optimum , you will obtain the best map relative to the cost function . But it seems that the extent of these guarantees are comparable to those of several other generative models , including WGANs , the Sinkhorn - based models of Genevay et al. ( 2017 , https://arxiv.org/abs/1706.00292/), or e.g. with a different loss function the MMD - based models of Li , Swersky , and Zemel ( ICML 2015 ) / Dziugaite , Roy , and Ghahramani ( UAI 2015 ) . The different setting than the fundamental GAN - like setup of those models is intriguing , but specifying a cost function between the source and the target domains feels exceedingly unnatural compared to specifying a cost function just within one domain as in these other models .

Minor :

In ( 5 ) , what is the purpose of the - 1 term in R_e ? It seems to just subtract a constant 1 from the regularization term .

Dear reviewer ,

We thank you for your positive review and detailed comments .

" how does your approach compare to the full - dual , continuous case of that paper [.. ] ”

Conceptually there is no difference . The main advantage of using NNs lies in the implementation side : using kernel expansions has a O( ( iteration index ) ^ 2 ) cost per iterations , while using NNs keeps a constant O ( batch size ) cost .

We have added a paragraph “ Convergence rates and computational cost comparison ” .

" The consistency properties are nice , though they do n't provide much insight into the rate [.. ] . ”

- For a fixed number of samples and the reg . decreasing to 0 : Cominetti & Saint Martin ( 1994 ) proved an exponential rate for the convergence of the entropy - reg . OT plans to a non-regularized OT plan . This asymptotic result does not let infer a regularization value to achieve a given error . Building on top of these results would deserve a study in its own right .

- When reg. is fixed ( or 0 ) , and the number of samples grows to inf . : Several works study convergence rates of empirical Wasserstein distances ( i.e. the OT objective between empirical measures ) . Boissard ( 2011 ) , Sriperumbudur et al. ( 2012 ) ( thanks to reviewer 4 for this ref. ) , Boissard & Le Gouic ( 2014 ) , to name a few . However we are not aware of work addressing the same questions for the empirical OT plans ( and not just the OT objective ) . We believe this problem is more complicated .

Since our results relate to the convergence of OT plans , we believe they are new and of interest . Without them , our discussion in the introduction and experiments would not be theoretically well grounded : we could not justify that the image of a source measure through the learned Monge map approximates well the target measure , at least for some n big and eps small ( Corollary 1 ) . We understand that convergence rates are more useful and will investigate this in future work .

" The proofs are mainly limited in that they do n't refer in any way to the class of approximating networks [ ...] essentially you add a regularization term , and show that as the regularization decreases it still works , but under seemingly the same kind of assumptions as Arjovsky et al . 's approach which does not add an explicit regularization term at all . [ ... ] "

In a discrete setting , our Alg. 1 computes the exact regularized - OT since we are maximizing a concave objective without parameterizing the dual variables .

Whenever the problem involves continuous measure ( s ) , our NN parameterization only gives the exact solution when the latter belongs to this approximating class of NNs ( and the global maximum is obtained ) .

As you wrote , we do believe that this parameterization provides a “ smoother ” solution . But as we already have some entropic or L2 regularization in the OT problem , we find it complicated to analyze . Still , we agree that this is an interesting problem to investigate .

Arjovsky et al. used indeed the same idea of deep NN parameterization . However , their NN has to Lipschitz , which they enforce by weights clipping . This is unclear whether a NN with bounded weights can approximate any Lipschitz function . In our case , there is no restriction on the type of NNs .

" The performance comparison to the algorithm of Genevay et al. is somewhat limited [ ...] how would the results compare if you used SAG ( or SAGA / etc ) for both algorithms ? "

We plan to add numerical - speed experiments in the paper soon .

Genevay et al. used SAG in the discrete setting ( but used SGD in other settings ) . We prefer 1 ) providing a unified alg . regardless of the measures being discrete or continuous , 2 ) proposing an alg . which fits in automatic-differentiation softwares ( Tensorflow , Torch etc. ) , which often do not support SAG .

" In discussing the domain adaptation results , you mention that the L2 regularization " works very well in practice , " [ ... ] . "

We have removed this sentence . It is still unclear which regularization works better in practice depending on the problem . Our only claim is that the L2 reg. is numerically more stable than the entropic one .

" For generative modeling : you do have guarantees that , * if * [ ...] but specifying a cost function between the source and the target domains feels exceedingly unnatural [ … ] . ”

Indeed , most generative models focus on fitting a generator to a target distribution , without optimality criteria . Yet we believe that looking for generator which has properties can be useful for some applications . We see this experiment as a proof -of - concept that the learned Monge map can be good generator . We encourage and will consider future work where optimality of the generator ( w.r.t. to a cost ) is important ( such as image - to- image / text - to - text translations ) .

" In ( 5 ) , what is the purpose of the - 1 term in R_e ? It seems to just subtract a constant 1 from the regularization term . ”

You are right . It provides a simpler formulation in the primal - dual relationship Eq. ( 7 ) ( this is also used in Genevay et al ( 2016 ) , Peyré ( 2016 ) ) .

This paper explores a new approach to optimal transport . Contributions include a new dual - based algorithm for the fundamental task of computing an optimal transport coupling , the ability to deal with continuous distributions tractably by using a neural net to parameterize the functions which occur in the dual formulation , learning a Monge map parameterized by a neural net allowing extremely tractable mapping of samples from one distribution to another , and a plethora of supporting theoretical results . The paper presents significant , novel work in a straightforward , clear and engaging way . It represents an elegant combination of ideas , and a well - rounded combination of theory and experiments .

I should mention that I 'm not sufficiently familiar with the optimal transport literature to verify the detailed claims about where the proposed dual - based algorithm stands in relation to existing algorithms .

Major comments :

No major flaws . The introduction is particular well written , as an extremely clear and succinct introduction to optimal transport .

Minor comments :

In the introduction , for VAEs , it 's not the case that f ( X ) matches the target distribution . There are two levels of sampling : of the latent X and of the observed value given the latent . The second step of sampling is ignored in the description of VAEs in the first paragraph .

In the comparison to previous work , please explicitly mention the EMD algorithm , since it 's used in the experiments .

It would 've been nice to see an experimental comparison to the algorithm proposed by Arjovsky et al . ( 2017 ) , since this is mentioned favorably in the introduction .

In ( 3 ) , R is not defined . Suggest adding a forward reference to ( 5 ) .

In section 3.1 , it would be helpful to cite a reference to support the form of dual problem .

Perhaps the authors have just done a good job of laying the groundwork , but the dual - based approach proposed in section 3.1 seems quite natural . Is there any reason this sort of approach was n't used previously , even though this vein of thinking was being explored for example in the semi-dual algorithm ? If so , it would interesting to highlight the key obstacles that a naive dual - based approach would encounter and how these are overcome .

In algorithm 1 , it is confusing to use u to mean both the parameters of the neural net and the function represented by the neural net .

There are many terms in R_e in ( 5 ) which appear to have no effect on optimization , such as a ( x ) and b( y ) in the denominator and " - 1 " . It seems like R_e boils down to just the entropy .

The definition of F_\epsilon is made unnecessarily confusing by the omission of x and y as arguments .

It would be great to mention very briefly any helpful intuition as to why F_\epsilon and H_\epsilon have the forms they do .

In the discussion of Table 1 , it would be helpful to spell out the differences between the different Bary proj algorithms , since I would 've expected EMD , Sinkhorn and Alg. 1 with R_e to all perform similarly .

In Figure 4 some of the samples are quite non-physical . Is their any helpful intuition about what goes wrong ?

What cost is used for generative modeling on MNIST ?

For generative modeling on MNIST , " 784d vector " is less clear than " 784 - dimensional vector " . The fact that the variable d is equal to 768 is not explicitly stated .

It seems a bit strange to say " The property we gain compared to other generative models is that our generator is a nearly optimal map w.r.t. this cost " as if this was an advantage of the proposed method , since arguably there is n't a really natural cost in the generative modeling case ( unlike in the domain adaptation case ) ; the latent variable seems kind of conceptually distinct from observation space .

Appendix A is n't referred to from the main text as far as I could tell . Just merge it into the main text ?





Dear reviewer ,

Thank you very much for your positive review and detailed comments . Please find below our replies to your comments .

" In the introduction , for VAEs , it 's not the case that f ( X ) matches the target distribution . [ ...] The second step of sampling is ignored in the description of VAEs in the first paragraph . "

At training time , there are indeed two neural networks involved in the VAE model , one encoder and one decoder . Here , we refer to X as the latent variable , i.e. the distribution obtained by the image of the input data by the encoder . We hence refer to f as the decoder network . With these notations , we believe that f is learned so that f ( X ) matches the distribution of the input data .

" In the comparison to previous work , please explicitly mention the EMD algorithm , since it 's used in the experiments . "

We used a c+ + implementation of the network simplex algorithm ( http://liris.cnrs.fr/~nbonneel/FastTransport/). We have added this link as a footnote .

" It would 've been nice to see an experimental comparison to the algorithm proposed by Arjovsky et al . ( 2017 ) , since this is mentioned favorably in the introduction . "

Our algorithm shares indeed similarities with the one proposed by Arjovsky et al . ( 2017 ) : they both use NN parameterizations of the OT dual variables . Both our algorithms have the same complexity . However , in our case , we compute regularized OT , while Arjovsky et al. ( 2017 ) unregularized OT . Hence , we found it more relevant to compare to Genevay et al . ( 2016 ) who computed exactly the same objective as us ( in the entropy reg. case ) .

" Is there any reason this sort of approach was n't used previously , even though this vein of thinking was being explored for example in the semi-dual algorithm ? "

Let us emphasize that the simplex algorithm is an efficient OT solver for measures supported up to a few thousands samples , and may be suitable in many applications .

It seems that the need to compute OT in large - scale settings is largely driven by the machine - learning community , with the recent idea that the OT objective can be a powerful loss function ( Rolet et al . ( 2016 ) , Arjovsky et al. ( 2017 ) ) , as well as the OT plans can be used to perform domain adaptation ( Courty et al . ( 2016 ) .

Moreover , our dual approach is simple thanks to the convex regularization of the primal OT problem , which was also introduced relatively recently ( Cuturi ( 2013 ) ) .

Finally , our approach is not flawless : the use of deep NN makes the problem non-convex ( in the semi-discrete and continuous - continuous cases ) .

" In algorithm 1 , it is confusing to use u to mean both the parameters of the neural net and the function represented by the neural net . "

We wanted to emphasize that our algorithm is conceptually the same in all settings ( discrete-discrete , semi-discrete and continuous - continuous ) . We are thinking of better notations to make it less confusing .

" There are many terms in R_e in ( 5 ) which appear to have no effect on optimization , such as a ( x ) and b( y ) in the denominator and " - 1 " . It seems like R_e boils down to just the entropy . "

We have removed a and b from the text . We can remove the - 1 in the entropy regularizer , but 1 ) it would make the primal - dual relationship less ‘ simple ’ and 2 ) this would not be in line with the work of Genevay et al . ( 2016 ) or Peyré ( 2016 ) .

" I would 've expected EMD , Sinkhorn and Alg. 1 with R_e to all perform similarly . "

We had a typo in the result of the “ Bar. proj . Alg. 1 R_e ” case . We have rerun the experiment and it indeed performs similarly as Sinkhorn , as expected . We apologize for that .
EMD ( i.e. non -regularized OT ) is not expected to perform as Sinkhorn since the regularization has an effect of the OT plan and hence on the barycentric projection .

" In Figure 4 some of the samples are quite non-physical . Is their any helpful intuition about what goes wrong ? "

The barycentric projection performs an averaging ( w.r.t. the squared Euclidean metric which was chosen in that xp ) between target samples ( weighted according to the optimal plan ) . In some case , this averaging might lead to so these non- physical shapes .

" What cost is used for generative modeling on MNIST ? "

We used the squared Euclidean distance .

" It seems a bit strange to say " The property we gain compared to other generative models is that our generator is a nearly optimal map w.r.t. this cost " [ ...] the latent variable seems kind of conceptually distinct from observation space . "

Indeed , most generative models do not need to have an ' optimal ' the generator . Yet we believe that looking for map which has certain ( regularity ) properties can be useful for some applications . We may consider further work in generative modeling where optimality of the mapping ( w.r.t. to a given cost ) can be important ( such as image - to-image translation ) .

References :
Peyré , Gabriel . " Entropic approximation of Wasserstein gradient flows . " SIAM Journal on Imaging Sciences 8.4 ( 2015 ) : 2323-2351.

This paper proposed a domain generalization approach by domain - dependent data augmentation . The augmentation is guided by a network that is trained to classify a data point to different domains . Experiments on four datasets verify the effectiveness of the proposed approach .

Strengths :
+ The proposed classification model is domain - dependent , as opposed to being domain-invariant . This is new and differs from most existing works on domain adaptation / generalization , to the best of my knowledge .
+ The experiments show that the proposed method outperforms two baselines . However , more related approaches could be included to strengthen the experiments ( see below for details ) .


Weaknesses :
- The paper studies domain generalization and yet fails to position it in the right literature . By a simple search of " domain generalization " using Google Scholar , I found several existing works on this problem and have listed some below . The authors may consider to include them in both the related works and the experiments .

Questions :
1 . It is intuitive to directly define the data augmentation by x_i + Grad_x J_d . Why is it necessary to instead define it as the inverse transformation G^{ - 1 } ( g ' ) and then go through the approximations to derive the final augmentation ?
2 . Is the CrossGrad training necessary ? What if one trains the network in two steps ? Step 1 : learn G using J_d and a regularization to avoid misclassification over the labels using the original data . Step 2 : Learn the classification network ( possibly different from G ) by the domain - dependent augmentation .


Saeid Motiian , Marco Piccirilli , Donald A. Adjeroh , and Gianfranco Doretto . Unified deep supervised
domain adaptation and generalization . In IEEE International Conference on Computer
Vision ( ICCV ) , 2017 .

Muandet , K. , Balduzzi , D. and Schölkopf , B. , 2013 . Domain generalization via invariant feature representation . In Proceedings of the 30th International Conference on Machine Learning ( ICML - 13 ) ( pp. 10 - 18 ) .

Xu , Z. , Li , W. , Niu , L. and Xu , D. , 2014 , September . Exploiting low-rank structure from latent domains for domain generalization . In European Conference on Computer Vision ( pp. 628-643 ) . Springer , Cham .

Ghifary , M. , Bastiaan Kleijn , W. , Zhang , M. and Balduzzi , D. , 2015 . Domain generalization for object recognition with multi-task autoencoders . In Proceedings of the IEEE international conference on computer vision ( pp. 2551-2559 ) .

Gan , C. , Yang , T. and Gong , B. , 2016 . Learning attributes equals multi-source domain generalization . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( pp. 87-97 ) .

The rebuttal addresses my questions . The authors are recommended to explicitly use " domain generalization " in the paper and / or the title to make the language consistent with the literature .

We thank the reviewer for their time and effort .

> It is intuitive to directly define the data augmentation by x_i + Grad_x J_d . Why is it necessary to instead define it as the inverse transformation G^{ - 1 } ( g ' ) and then go through the approximations to derive the final augmentation ?

Yes , computationally they turn out to be the same but our exposition provides some insight on why perturbing x by Grad_x J_d should provide generalization along domain . Also , we hope this more flexible framework will inspire future work on alternative ways of sampling g-s and the corresponding inverse . We will add a discussion of both ways of motivating the perturbation ( g from x , and x from g ) in our revised draft .

> Is the CrossGrad training necessary ? What if one trains the network in two steps ? Step 1 : learn G using J_d and a regularization to avoid misclassification over the labels using the original data . Step 2 : Learn the classification network ( possibly different from G ) by the domain - dependent augmentation .

We implemented the suggested method and found that it performs worse than the baseline . The accuracy for label classification on the Google Fonts dataset , on the test set , is around .63 while the baseline is around .68 . If we learn G as a separate first step using training domains , there is no guarantee that G will generalize in a “ meaningful way ” across unseen domains . Then using this G for data augmentation will not be helpful . Cross Grad tries to force G to learn a meaningful continuous domain representation using perturbation in the label space .

> The paper studies domain generalization and yet fails to position it in the right literature . By a simple search of " domain generalization " using Google Scholar , I found several existing works on this problem and have listed some below . The authors may consider to include them in both the related works and the experiments .

We thank the reviewer for pointing us to these references . Among these , we ’ve already cited Motiian et al . ’s work on domain generalization and used their model for comparison on the MNIST task . The other references will be discussed in the related work section in our revised draft . Many previous approaches try to erase domain information . For instance , Muandet et al . ' 13 and Ghifary et al . ' 15 try to extract generalizable features across domains . Our model is different in that we try to leverage the information that domain features provide about labels .


The method is posed in the Bayesian setting , the main idea being to achieve the data augmentation through domain - guided perturbations of input instances . Different from traditional adaptation methods , where the adaptation step is applied explicitly , in this paper the authors exploit labeled instances from several domains to collectively train a system that can handle new domains without the adaptation step . While this is another way of looking at domain adaptation , it may be misleading to say ' without ' adaptation step . By the gradient perturbations on multi-domain training data , the learning of the adaptation step is effectively done . This should be clarified in the paper . The notion of using ' scarce ' training domains to cover possible choices for the target domain is interesting and novel . The experimental validation should also include a deeper analysis of this factor : how the proposed adaptation performance is affected by the scarcity of the training multi-domain data . While this is partially shown in Table 8 , it seems that by adding more domains the performance is compromised ( compared to the baseline ) ( ? ) . It would be useful to see how the model ranks the multiple domains in terms of their relatedness to the target domain . Figs 6 - 7 are unclear and difficult to read . The captions should provide more information about the main point of these figures .

We thank the reviewer for their time and effort .

> While this is another way of looking at domain adaptation , it may be misleading to say ' without ' adaptation step . By the gradient perturbations on multi-domain training data , the learning of the adaptation step is effectively done . This should be clarified in the paper .

The word “ adaptation ” suggests modifying the model for a given target domain . In contrast , we focus on domain - generalization , where we are not given any specific target domain . But the reviewer is right in that there is an implicit adaptation component , which we shall clarify in the revised draft .

> The notion of using ' scarce ' training domains to cover possible choices for the target domain is interesting and novel . The experimental validation should also include a deeper analysis of this factor : how the proposed adaptation performance is affected by the scarcity of the training multi-domain data . While this is partially shown in Table 8 , it seems that by adding more domains the performance is compromised ( compared to the baseline ) ( ?) .

We shall add a deeper analysis / discussion in the revision . With training data covering a larger number of domains , the baseline automatically tends to become domain -aware , and domain generalization techniques ( ours and the others ) have less room for improvement . However , it is possible that we can improve on our current performance by tuning the network parameters for different levels of scarcity . We shall explore this .

> It would be useful to see how the model ranks the multiple domains in terms of their relatedness to the target domain .

The analysis in Figure 6 is motivated by exactly this question . Instead of giving a single number ( or rank ) , we have visualized the relation between the training and test domains in terms of ( projections of ) the “ g ” embedding . We could repeat these plots for other domains like Font , though these are probably best presented in supplementary material .

> Figs 6 - 7 are unclear and difficult to read . The captions should provide more information about the main point of these figures

We will address this in our revised draft .


Quality , clarity : Very well written , well motivated , convincing experiments and analysis
Originality : I think they framed the problem of domain- robustness very well : how to obtain a " domain level embedding " which generalizes to unseen domains . To do this the authors introduce the CrossGrad method , which trains both a label classification task and a domain classification task ( from which the domain - embedding is obtained )
Significance : Robustness in new domains is a very important practical and theoretical issue .

Pros :
- It 's novel , interesting , well written , and appears to work very well in the experiments provided .

Cons :
- Formally , for the embedding to generalize one needs to make the " domain continuity assumption " , which is not guaranteed to hold in any realistic settings ( e.g. when there are no underlying continuous factors )
- The training set needs to be in the form ( x , y , d ) where 'd ' is a domain , this information might not exist or be only partially present .
- A single step required 2 forward and 2 backward passes - thus is twice as expensive .

Constructive comments :
- Algorithm 1 uses both X_l and X_d , yet the text only discusses X_d , there is some symmetry , but more discussion will help .
- LabelGrad is mentioned in section 4 but defined in section 4.1 , it should be briefly defined in the first mention .

We thank the reviewer for their time and effort .
> ' Formally , for the embedding to generalize one needs to make the " domain continuity assumption " , which is not guaranteed to hold in any realistic settings ( e.g. when there are no underlying continuous factors ) '

Yes , real - life domains are mostly discrete ( e.g. fonts , speakers , etc ) but their variation can often be captured via latent continuous features ( e.g. slant , ligature size , etc. for fonts ; and speaking rate , pitch , intensity , etc. for speech ) . Cross Grad strives to characterize these continuous features for capturing domain variation .

> ' The training set needs to be in the form ( x , y , d ) where 'd ' is a domain , this information might not exist or be only partially present . '

We do not need domain information for all the data that will be used in our eventual training : we can bootstrap from a relatively small amount of domain - labeled data , by training a classifier for the domains present in the training data , which can then be used to label the rest of the data . We also note that the domain adaptation / generalization literature typically does rely on training data with source - domain labels .

We are fixing the revision with other helpful comments by the reviewer on notation and writing .


The authors proposed to perturbed the estimated domain features for data augmentation , which is done by using the gradients of label and domain classification losses . The idea is interesting and new . And the paper is well written .

My major conerns are as follows :
1 . Section 3 seems a bit too lengthy or redundant to derive the data augmentation by introducing the latent domain features g . In fact , without g , it also makes sense to perturb x as done in lines 6 and 7 in Alg . 1.
2 . The assumption in ( A1 ) can only be guaranteed under certain theoretical conditions . The authors should provide more explanations to better convey the assumption to readers .

Minors :
1 . LabelGrad was not defined when firstly being used in Section 4 .
2 . Fig. 1 looks redundant .

We thank the reviewer for their time and effort .

> Section 3 seems a bit too lengthy or redundant to derive the data augmentation by introducing the latent domain features g . In fact , without g , it also makes sense to perturb x as done in lines 6 and 7 in Alg . 1 . 2.

A similar concern was raised by Reviewer 1 as well . We wanted to provide some insight on why perturbing x by Grad_x J_d should provide generalization along domain . Also , we hope this more flexible framework will inspire future work on alternative ways of sampling g-s and the corresponding inverse .

> The assumption in ( A1 ) can only be guaranteed under certain theoretical conditions . The authors should provide more explanations to better convey the assumption to readers .

Yes , but in many cases , domain variation can be captured via latent continuous features ( e.g. slant , ligature size , etc. for fonts ; and speaking rate , pitch , intensity , etc. for speech ) . Cross Grad strives to characterize these continuous features for capturing domain variation . We shall elaborate on this further in our revised draft .

> Minors : 1 . LabelGrad was not defined when firstly being used in Section 4 . 2 . Fig. 1 looks redundant .

We will fix the issues pointed out .


In this work , the authors suggest the use of control variate schemes for estimating gradient values , within a reinforcement learning framework . The authors also introduce a specific control variate technique based on the so-called Stein ’s identity . The paper is interesting and well - written .

I have some question and some consideration that can be useful for improving the appealing of the paper .

- I believe that different Monte Carlo ( or Quasi-Monte Carlo ) strategies can be applied in order to estimate the integral ( expected value ) in Eq. ( 1 ) , as also suggested in this work . Are there other alternatives in the literature ? Please , please discuss and cite some papers if required .

- I suggest to divide Section 3.1 in two subsections . The first one introducing Stein ’s identity and the related comments that you need , and a second one , starting after Theorem 3.1 , with title “ Stein Control Variate ” .

- Please also discuss the relationships , connections , and possible applications of your technique to other algorithms used in Bayesian optimization , active learning and / or sequential learning , for instance as

M. U. Gutmann and J. Corander , “ Bayesian optimization for likelihood - free inference of simulator - based statistical mod - els , ” Journal of Machine Learning Research , vol. 16 , pp. 4256 – 4302 , 2015 .

G. da Silva Ferreira and D. Gamerman , “ Optimal design in geostatistics under preferential sampling , ” Bayesian Analysis , vol. 10 , no. 3 , pp. 711–735 , 2015 .

L. Martino , J. Vicent , G. Camps - Valls , " Automatic Emulator and Optimized Look -up Table Generation for Radiative Transfer Models " , IEEE International Geoscience and Remote Sensing Symposium ( IGARSS ) , 2017 .

- Please also discuss the dependence of your algorithm with respect to the starting baseline function \phi_0 .

Thank you very much for the thoughtful feedbacks , with which we could further improve our paper .

* Different Mote Carlo strategies for estimating the integral ? Because the setting here is model - free , that is , we only have a black - box to simulate from the environment , without knowing the underlying distribution , there more limited MC strategies can be used than typical integration problems . Nevertheless , some advanced techniques such as Bayesian quadrature can be used ( Ghavamzadeh et al . Bayesian Policy Gradient and Actor-Critic Algorithms ) .

* We will modify Section 3.1 according to your suggestion .

* It would be very interesting to consider the application of this technique to Bayesian optimization . We will certainly discuss the possibility in the future work section .

This paper proposed a class of control variate methods based on Stein 's identity . Stein 's identity has been widely used in classical statistics and recently in statistical machine learning literature . Nevertheless , applying Stein 's identity to estimating policy gradient is a novel approach in reinforcement learning community . To me , this approach is the right way of constructing control variates for estimating policy gradient . The authors also did a good job in connecting with existing works and gave concrete examples for Gaussian policies . The experimental results also look promising .

It would be nice to include some theoretical analyses like under what conditions , the proposed method can achieve smaller sample complexity than existing works .

Overall this is a strong paper and I recommend to accept .





Thank you very much for the review . We are interested in studying theoretical properties of the estimators as well , but because of the non-convex nature of RL problems , it may be better to start theoretical analysis in simpler cases such as convex problems , in which some interesting results on convergence rate can be potentially be obtained ( perhaps in connection to stochastic variance reduced gradient in some way ) .

The paper proposes action - dependent baselines for reducing variance in policy gradient , through the derivation based on Stein ’s identity and control functionals . The method relates closely to prior work on action - dependent baselines , but explores in particular on - policy fitting and a few other design choices that empirically improve the performance .

A criticism of the paper is that it does not require Stein ’s identity / control functionals literature to derive Eq. 8 , since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [ Gu et. al. , 2017 ] as reparameterizable control variate . The derivation through Stein ’s identity does not seem to provide additional insights / algorithm designs beyond direct derivation through reparameterization trick .

The empirical results appear promising , and in particular in comparison with Q- Prop , which fits Q-function using off - policy TD learning . However , the discussion on the causes of the difference should be elaborated much more , as it appears there are substantial differences besides on - policy / off - policy fitting of the Q , such as :

- FitLinear fits linear Q ( through parameterization based on linearization of Q ) using on - policy learning , rather than fitting nonlinear Q and then at application time linearize around the mean action . A closer comparison would be to use same locally linear Q function for off - policy learning in Q-Prop .

- The use of on - policy fitted value baseline within Q-function parameterization during on - policy fitting is nice . Similar comparison should be done with off - policy fitting in Q-Prop .

I wonder if on - policy fitting of Q can be elaborated more . Specifically , on-policy fitting of V seems to require a few design details to have best performance [ GAE , Schulman et. al. , 2016 ] : fitting on previous batch instead of current batch to avoid overfitting ( this is expected for your method as well , since by fitting to current batch the control variate then depends nontrivially on samples that are being applied ) , and possible use of trust - region regularization to prevent V from changing too much across iterations .

The paper presents promising results with direct on - policy fitting of action - dependent baseline , which is promising since it does not require long training iterations as in off - policy fitting in Q- Prop . As discussed above , it is encouraged to elaborate other potential causes that led to performance differences . The experimental results are presented well for a range of Mujoco tasks .

Pros :

- Simple , effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time

- Good empirical evaluation

Cons :

- The name Stein control variate seems misleading since the algorithm / method does not rely on derivation through Stein ’s identity etc . and does not inherit novel insights due to this derivation .


Thank you very much for the review and pointing out potential improvements . The followings are the response to your comments :

* Thanks for pointing out IPG and on-policy vs . off - policy fitting ; we will provide a thorough discussion on this . We have been mainly focussing on fitting \phi with on -policy , because the optimal control variates should theoretically depend on the current policy and hence " on - policy " in its nature . However , we did experiment ways to use additional off - policy data to our update and find that using additional off - policy data can in fact further improve our method . We find it is hard to have a fair comparison between on policy vs . off - policy fitting because it largely depends on how we implement each of them . Instead , an interesting future direction for us is to investigate principled ways to combine them to improve beyond what we can achieve now .

We should point out the difference between IPG and our method is not only the way we fit \phi , another perhaps more significant difference is that IPG ( depending which particular version ) also averages over off - policy data when estimating the gradient , while our method always only averages over the on-policy data .

* In our comparison , Q-prop also uses an on-policy fitted value function inside the Q-function .

* Thank you very much for suggesting better ways of on - policy fitting of V . We are interested in testing them for future works . Currently , V is fitted by all the current data which theoretically introduces a ( possibly small ) bias because the current data is used twice in the gradient estimator , so using the data from the previous iteration may yield improvement .


* Regarding the name , although it turned out our result can be derived using reparameterization trick , Stein 's identity is what motivated this work originally , and we lean towards keeping it as the motivation since Stein 's identity generally provides a principled way to think about control variates ( which essentially requires zero-expectation identities mathematically ) .

Stein 's identity and reparameterization trick are two orthogonal ways to think about this work , and it is useful to keep both of them to give a more comprehensive view . It is not true that Stein 's identity is not directly useful in our work : By using ( the original ) Stein 's identity on the top of the basic formula , we can derive a different control variate for Gaussian policy that has lower variance ( and it is what we used in experiments ) . It is possible that we can further generalize the result by using Stein 's identity in more creative ways . On the other hand , we will emphasize more the role of reparameterization trick in the revision .

Pros :
The paper proposes a “ bi-directional block self - attention network ( Bi-BloSAN ) ” for sequence encoding , which inherits the advantages of multi-head ( Vaswani et al. , 2017 ) and DiSAN ( Shen et al. , 2017 ) network but is claimed to be more memory - efficient . The paper is written clearly and is easy to follow . The source code is released for duplicability . The main originality is using block ( or hierarchical ) structures ; i.e. , the proposed models split the an entire sequence into blocks , apply an intra-block SAN to each block for modeling local context , and then apply an inter- block SAN to the output for all blocks to capture long - range dependency . The proposed model was tested on nine benchmarks and achieve good efficiency - memory trade - off .

Cons :
- Methodology of the paper is very incremental compared with previous models .
- Many of the baselines listed in the paper are not competitive ; e.g. , for SNLI , state - of - the - art results are not included in the paper .
- The paper argues advantages of the proposed models over CNN by assuming the latter only captures local dependency , which , however , is not supported by discussion on or comparison with hierarchical CNN .
- The block splitting ( as detailed in appendix ) is rather arbitrary in terms of that it potentially divides coherent language segments apart . This is unnatural , e.g. , compared with alternatives such as using linguistic segments as blocks .
- The main originality of paper is the block style . However , the paper does n’t analyze how and why the block brings improvement .
- If we remove intra-block self - attention ( but only keep token - level self - attention ) , whether the performance will be significantly worse ?


== Following above==
- Q4 . The block splitting ( as detailed in appendix ) is rather arbitrary in terms of that it potentially divides coherent language segments apart . This is unnatural , e.g. , compared with alternatives such as using linguistic segments as blocks .

Here are two reasons for not using linguistic segments as blocks in our model . Firstly , the property of significantly reducing memory cannot be guaranteed if using linguistic segments , because either too long or too short segments will lead to expensive memory consumption , and we cannot easily control the length of linguistic segments provided by other tools . For example , in Eq.( 19 ) , either large and small block length r is likely to result in large memory . Secondly , the process of achieving linguistic segments potentially increases computation / memory cost , introduces overhead and requires more complex implementation . In addition , although we do not use linguistic segments for block splitting , our model can still capture the dependencies between tokens from different blocks by using the block - level context fusion and feature fusion gate developed in this paper .


- Q5 . The main originality of paper is the block style . However , the paper does n’t analyze how and why the block brings improvement .

The block or two - layer self - attention substantially reduces the memory and computational costs required by previous self - attention mechanisms , which is proportional to the square of sequence length . Meanwhile , it achieves competitive or better accuracy than RNNs / CNNs . We give a formally explanation of how this block idea can reduce the memory in Appendix A.


- Q6 . If we remove intra-block self - attention ( but only keep token - level self - attention ) , whether the performance will be significantly worse ?

Compared to test accuracy 85.7 % of Bi-BloSAN on SNLI , the accuracy will be decreased to 85.2 % if we remove the intra-block attention ( keep block - level attention ) , whereas the accuracy will be decreased to 85.3 % if we remove inter- block self - attention ( keep token - level self - attention in blocks ) . Moreover , if we only use token - level self - attention , the model will be identical to the directional self - attention [ 2 ] . You can refer to the ablation study at the end of Section 4.1 for more details .



References
[ 1 ] Vaswani , Ashish , et al . " Attention is all you need . CoRR abs /1706.03762 . " ( 2017 ) .
[ 2 ] Shen , Tao , et al . " Disan : Directional self - attention network for rnn / cnn-free language understanding . " arXiv preprint arXiv:1709.04696 ( 2017 ) .
[ 3 ] Srivastava , Rupesh Kumar , Klaus Greff , and Jürgen Schmidhuber . " Highway networks . " arXiv preprint arXiv:1505.00387 ( 2015 ) .
[ 4 ] Nie , Yixin , and Mohit Bansal . " Shortcut -stacked sentence encoders for multi-domain inference . " arXiv preprint arXiv :1708.02312 ( 2017 ) .
[ 5 ] Jihun Choi , Kang Min Yoo and Sang-goo Lee . " Learning to compose task -specific tree structures . " arXiv preprint arXiv:1707.02786 ( 2017 ) .
[ 6 ] Kim , Yoon . " Convolutional neural networks for sentence classification . " arXiv preprint arXiv:1408.5882 ( 2014 ) .
[ 7 ] Kaiser , Łukasz , and Samy Bengio . " Can Active Memory Replace Attention ? . " Advances in Neural Information Processing Systems . 2016 .
[ 8 ] Kalchbrenner , Nal , et al . " Neural machine translation in linear time . " arXiv preprint arXiv:1610.10099 ( 2016 ) .
[ 9 ] Gehring , Jonas , et al . " Convolutional Sequence to Sequence Learning . " arXiv preprint arXiv :1705.03122 ( 2017 ) .

Thank you for your elaborative comments ! We discuss the Cons you pointed out one by one as follows .

- Q1 . Methodology of the paper is very incremental compared with previous models .

Yes , the idea of using block or two - level attention is simple . In fact , it is similar to the idea behind almost all the hierarchical models . However , it has never been studied on self - attentions based models , especially on attention - only models ( as much as we know , Transformer [ 1 ] and DiSAN [ 2 ] are the merely two published attention - only models ) , for context fusion . Moreover , it solves a critical problem of previous self - attention mechanisms , i.e. , expensive memory consumption , which was a burden of applying attention to long sequences and an inevitable weakness compared to popular RNN models . Hence , it is a simple idea , which leads to a simple model , but effectively solves an important problem .

In addition , given this idea , it is non-trivial to design a neural net architecture for context fusion , we still need to figure out : 1 ) How to split the sequence so the memory can be effectively reduced ? 2 ) How to capture the dependency between two elements from different blocks ? 3 ) How to produce contextual - aware representation for each element on each level ? 4 ) How to combine the output of different levels so the information from lower level does not fade out ? For example , on top of Figure 3 , we duplicate the block features e_i to each element as its high - level representation , use skip ( highway [ 3 ] ) connections to achieve its lower level representations x_i and h_i , and then design a fusion gate to combine the three representations . This design assigns each element with both high - level and low - level representations and combine them on top of the model to produce a contextual - aware representation per input element . Without it , the two -level attention can only give us e_i , which cannot explicitly model the dependency between elements from different blocks , and cannot be used for context fusion . This method has not been used in construction of attention - based models because multi-level self - attention had not been studied before .


- Q2 . Many of the baselines listed in the paper are not competitive ; e.g. , for SNLI , state - of - the - art results are not included in the paper .

In the experiment on SNLI , Bi-BloSAN is only used to produce sentence encoding . For a fair comparison , we only compare it with the sentence - encoding based models listed separately on the leaderboard of SNLI . Up to ICLR submission deadline , Bi-BloSAN achieves the best test accuracy among all of them .

After ICLR submission deadline , the leaderboard has been updated with several new methods . We copy the results of the new methods in the following .
The Proposed Model ) 480D Bi-BloSAN 2.8M 85.7 %
1 ) 300D Residual stacked encoders [ 4 ] 9.7M 85.7 %
2 ) 600D Gumbel TreeLSTM encoders [ 5 ] 10.0M 86.0 %
3 ) 600D Residual stacked encoders [ 4 ] 29.0M 86.0 %
These results show that compared to the newly updated methods , Bi-BloSAN uses significantly less parameters but achieves competitive test accuracy .


- Q3 . The paper argues advantages of the proposed models over CNN by assuming the latter only captures local dependency , which , however , is not supported by discussion on or comparison with hierarchical CNN .

The discussion about CNN in the current version mainly focuses on single layer CNN with multi-window [ 6 ] , which is widely used in NLP community , and does not mention too much about recent studies on hierarchical CNNs . The hierarchical CNNs in NLP , such as Extended Neural GPU [ 7 ] , ByteNet [ 8 ] , and ConvS2S [ 9 ] , are able to model relatively long - range dependency by using stacking CNNs , which can increase the number of input elements represented in a state . Nonetheless , as mentioned in [ 1 ] , the number of operations ( i.e. CNNs ) required to relate signals from two arbitrary input grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet . This makes it more difficult to learn dependencies between distant positions . However , self - attention based method only requires constant number of operations , no matter how far it is between two elements . We will add the discussion on hierarchical CNNs in the revision .

To test the performance of hierarchical CNN for context fusion , we implemented it on SNLI dataset . In particular , we used 3 - layer 300D CNNs with kernel length 5 ( i.e. , using n- gram of n=5 ) . By following [ 1 ] , we also applied " Gated Linear Units ( GLU ) " [ 2 ] and residual connection [ 3 ] to the hierarchical CNN . We tuned the keep probability of dropout between 0.65 and 0.85 with step-size 0.05 . The code of this hierarchical CNNs can be found at https://github.com/code4review/BiBloSA/blob/master/context_fusion/hierarchical_cnn.py

This model has 3.4M parameters . It spends 343s per training epoch and 2.9s for inference on dev set . Its test accuracy is 83.92 % ( with dev accuracy 84.15 % and train accuracy 91.28 % ) , which slightly outperforms the CNNs with multi-window [ 4 ] shown in our paper , but is still worse than other baselines and Bi-BloSAN . We will add these results to the revision .

[ 1 ] Gehring , Jonas , et al . " Convolutional Sequence to Sequence Learning . " arXiv preprint arXiv :1705.03122 ( 2017 ) .
[ 2 ] Dauphin , Yann N. , et al . " Language modeling with gated convolutional networks . " arXiv preprint arXiv:1612.08083 ( 2016 ) .
[ 3 ] He , Kaiming , et al . " Deep residual learning for image recognition . " Proceedings of the IEEE conference on computer vision and pattern recognition . 2016 .
[ 4 ] Kim , Yoon . " Convolutional neural networks for sentence classification . " arXiv preprint arXiv:1408.5882 ( 2014 ) .

This high -quality paper tackles the quadratic dependency of memory on sequence length in attention - based models , and presents strong empirical results across multiple evaluation tasks . The approach is basically to apply self - attention at two levels , such that each level only has a small , fixed number of items , thereby limiting the memory requirement while having negligible impact on speed . It captures local information into so-called blocks using self - attention , and then applies a second level of self - attention over the blocks themselves .

The paper is well organized and clearly written , modulo minor language mistakes that should be easy to fix with further proof-reading . The contextualization of the method relative to CNNs / RNNs / Transformers is good , and the beneficial trade - offs between memory , runtime and accuracy are thoroughly investigated , and they 're compelling .

I am curious how the story would look if one tried to push beyond two levels ...? For example , how effective might a further inter-sentence attention level be for obtaining representations for long documents ?

Minor points :
- Text between Eq 4 & 5 : W^{ ( 1 ) } appears twice ; one instance should probably be W^{ ( 2 ) } .
- Multiple locations , e.g. S4.1 : for NLI , the word is * premise* , not * promise * .
- Missing word in first sentence of S4.1 : ... reason __ the ...

Thank you for your strong support to our work ! We will carefully fix the typos you pointed out .

- Q1 . I am curious how the story would look if one tried to push beyond two levels ...? For example , how effective might a further inter-sentence attention level be for obtaining representations for long documents ?

We have different answers to this question for sequences with different lengths .

For context fusion or embedding of single sentences ( which is the main focus of this paper ) , a two - level self - attention is usually sufficient to reduce the memory consumption and meanwhile to inherit most power of original SAN in modeling contextual dependencies . Compared to multi-level attention , it preserves the local dependencies in longer subsequence and directly controls the memory utility rate , by using less parameters and computations than multi-level one .

For the context fusion of a document or a passage , which already has a multi-level structure ( document-passages - sentences - phrases ) , it is worth considering to use multi-level self - attention to model the contextual relationship when the memory consumption needs to be small . Recently , self - attention has been applied to long text as a popular context fusion strategy in machine comprehension task [ 1 , 2 ] . In this task , the original self - attention requires lots of memory , and cannot be solely applied due to the difficulty of context fusion for a long passage / document . It is more practical to use LSTM or GRU as context fusion layers and use self - attention as a complementary module capturing the distance - irrelevant dependency . But the recurrent structure of LSTM / GRU leads to inefficiency in computation . Therefore , multi-level self - attention could provide a both memory and time efficient solution . For example , we can design a three - level self - attention structure , which consists of intra-block intra sentence , inter-block intra sentence and inter-sentence self - attentions , to produce context - aware representations of tokens from a passage . Such model can overcome the weaknesses of both RNN / CNN - based SANs ( only used as a complimentary module to context fusion layers ) and the RNN / CNN - free SANs ( with explosion of memory consumption when text length grows ) .



References
[ 1 ] Hu , Minghao , Yuxing Peng , and Xipeng Qiu . " Reinforced mnemonic reader for machine comprehension . " CoRR , abs /1705.02798 ( 2017 ) .
[ 2 ] Huang , Hsin - Yuan , et al . " FusionNet : Fusing via Fully -Aware Attention with Application to Machine Comprehension . " arXiv preprint arXiv :1711.07341 ( 2017 ) .

This paper introduces bi-directional block self -attention model ( Bi-BioSAN ) as a general - purpose encoder for sequence modeling tasks in NLP . The experiments include tasks like natural language inference , reading comprehension ( SquAD ) , semantic relatedness and sentence classifications . The new model shows decent performance when comparing with Bi-LSTM , CNN and other baselines while running at a reasonably fast speed .

The advantage of this model is that we can use little memory ( as in RNNs ) and enjoy the parallelizable computation as in ( SANs ) , and achieve similar ( or better ) performance .

While I do appreciate the solid experiment section , I do n't think the model itself is sufficient contribution for a publication at ICLR . First , there is not much innovation in the model architecture . The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self - attention for each of them , and then using the same mechanisms as a pooling operation followed by a fusion level . I think this more counts as careful engineering of the SAN model rather than a main innovation . Second , the model introduces much more parameters . In the experiments , it can easily use 2 times parameters than the commonly used encoders . What if we use the same amount of parameters for Bi-LSTM encoders ? Will the gap between the new model and the commonly used ones be smaller ?

====

I appreciate the answers the authors added and I change the score to 6 .

Thanks for your comments !

- Q1 . First , there is not much innovation in the model architecture . The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self - attention for each of them , and then using the same mechanisms as a pooling operation followed by a fusion level . I think this more counts as careful engineering of the SAN model rather than a main innovation .

Yes , the idea of using block or two - level attention is simple . In fact , it is similar to the idea behind almost all the hierarchical models . However , it has never been studied on self - attentions based models , especially on attention - only models ( as much as we know , Transformer [ 1 ] and DiSAN [ 2 ] are the merely two published attention - only models ) , for context fusion . Moreover , it solves a critical problem of previous self - attention mechanisms , i.e. , expensive memory consumption , which was a burden of applying attention to long sequences and an inevitable weakness compared to popular RNN models . Hence , it is a simple idea , which leads to a simple model , but effectively solves an important problem .

In addition , given this idea , it is non-trivial to design a neural net architecture for context fusion , we still need to figure out : 1 ) How to split the sequence so the memory can be effectively reduced ? 2 ) How to capture the dependency between two elements from different blocks ? 3 ) How to produce contextual - aware representation for each element on each level ? 4 ) How to combine the output of different levels so the information from lower level does not fade out ? For example , on top of Figure 3 , we duplicate the block features e_i to each element as its high - level representation , use skip ( highway [ 3 ] ) connections to achieve its lower level representations x_i and h_i , and then design a fusion gate to combine the three representations . This design assigns each element with both high - level and low - level representations and combine them on top of the model to produce a contextual - aware representation per input element . Without it , the two -level attention can only give us e_i , which cannot explicitly model the dependency between elements from different blocks , and cannot be used for context fusion . This method has not been used in construction of attention - based models because multi-level self - attention had not been studied before .


- Q2 . Second , the model introduces much more parameters . In the experiments , it can easily use 2 times parameters than the commonly used encoders . What if we use the same amount of parameters for Bi-LSTM encoders ? Will the gap between the new model and the commonly used ones be smaller ?

As suggested by you , we studied two cases in which Bi-LSTM and Bi-BloSAN have similar number of parameters . The gap does not change in both cases . We will add these new results to our revision .

1 ) We increase the number of hidden units in Bi-LSTM encoders from 600 to 800 . This increases the number of parameters from 2.9M to 4.8M , which is more than 4.1M of Bi-BloSAN . We implement this 800D Bi-LSTM encoder on the SNLI dataset which is the largest benchmark dataset used in this paper . After tuning of the hyperparameters ( e.g. , dropout keep probability is increased from 0.65 to 0.80 with step 0.05 in case of overfitting ) , the best test accuracy is 84.95 % ( with dev accuracy of 85.67 % ) .

2 ) We decrease the number of hidden units in Bi-BloSAN from 600 to 480 . This reduces the number of parameters from 4.1M to 2.8M , which is similar to that of the commonly used encoders . Interestingly , without tuning the keep probability of dropout , the test accuracy of this 480D Bi-BloSAN is 85.66 % ( with dev accuracy 86.08 % and train accuracy 91.68 % ) .

Additionally , a recent NLP paper [ 4 ] shows that increasing the dimension of an RNN encoder from 128D to 2048D does not result in substantially improvement of the performance ( from 21.50 to 21.86 of BLEU score on newstest2013 for machine translation ) . This is consistent with the results above .



References
[ 1 ] Vaswani , Ashish , et al . " Attention is all you need . CoRR abs /1706.03762 . " ( 2017 ) .
[ 2 ] Shen , Tao , et al . " Disan : Directional self - attention network for rnn / cnn-free language understanding . " arXiv preprint arXiv:1709.04696 ( 2017 ) .
[ 3 ] Srivastava , Rupesh Kumar , Klaus Greff , and Jürgen Schmidhuber . " Highway networks . " arXiv preprint arXiv:1505.00387 ( 2015 ) .
[ 4 ] Britz , Denny , et al . " Massive exploration of neural machine translation architectures . " arXiv preprint arXiv:1703.03906 ( 2017 ) .

In this work , the authors propose a procedure for tuning the parameters of an HMC algorithm ( I guess , if I have understood correctly ) .

I think this paper has a good and strong point : this work points out the difficulties in choosing properly the parameters in a HMC method ( such as the step and the number of iterations in the leapfrog integrator , for instance ) . In the literature , specially in machine learning , there is ``fever ’’ about HMC , in my opinion , partially unjustified .

If I have understood , your method is an adaptive HMC algorithm where the parameters are updated online ; or is the training done in advance ? Please , remark and clarify this point .

However , I have other additional comments :

- Eqs . ( 4 ) and ( 5 ) are quite complicated ; I think a running toy example can help the interested reader .

- I suggest to compare the proposed method to other efficient methods that do not use the gradient information ( in some cases as multimodal posteriors , the use of the gradient information can be counter - productive for sampling purposes ) , such as Multiple Try Metropolis ( MTM ) schemes

L. Martino , J. Read , On the flexibility of the design of Multiple Try Metropolis schemes , Computational Statistics , Volume 28 , Issue 6 , Pages : 2797-2823 , 2013 ,

adaptive techniques ,

H. Haario , E. Saksman , and J. Tamminen . An adaptive Metropolis algorithm . Bernoulli , 7 ( 2 ):223–242 , April 2001 ,

and component - wise strategies as Gibbs Sampling ,

W. R. Gilks and P. Wild , Adaptive rejection sampling for Gibbs sampling , Appl. Statist. , vol. 41 , no. 2 , pp. 337–348 , 199 .

At least , add a brief paragraph in the introduction citing and discussing this possible alternatives .

Thank you for your review and the pointer to references .

We wish to emphasize that our method is able , but not limited to , automatically tuning HMC parameters ( which systems like Stan already have well - tested heuristics for ) . Our approach generalizes HMC , and is capable of learning proposal distributions that do not correspond to any tuned HMC proposal ( but which can still be plugged into the Metropolis-Hastings algorithm to generate a valid MCMC algorithm ) . Indeed , in our experiments , we find that our approach significantly outperforms well - tuned HMC kernels .

The training is done during the burn - in phase , and the trained sampler is then frozen . This is a common approach to adapting transition - kernel hyperparameters in the MCMC literature .

Regarding the references , we added those in the text . We also want to emphasize that all of these are complementary to and could be combined with our method . For example , we could incorporate the intuition behind MTM by having several parametric operators and training each one when used .

Additionally , in the process of revisiting our experiments to compare against LAHMC , we empirically found that weighting the second term of our loss ( the ‘ burn - in ’ term ) could lead to even more improved auto-correlation and ESS on the diagnostic distributions . We therefore updated the paper and report the results obtained with slightly tuning that parameter ( setting it to 0 or 1 ) .

The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly . Mixing is one of the most challenge problems for a MCMC sampler , particularly when there are many modes in a distribution . The derivations look correct to me . In the experiments , the proposed algorithm was compared to other methods , e.g. , A- NICE -MC and HMC . It showed that the proposed method could mix between the modes in the posterior . Although the method could mix well when applied to those particular experiments , it lacks theoretical justifications why the method could mix well .

Thank you very much for your review and comments . Guaranteeing mixing between modes is a fundamental ( # P - Hard ) problem . As such , we do not hope to solve it in the general case . Rather , we propose a method to greatly increase the flexibility and adaptability of a class of samplers which is already state of the art in many contexts . The relation between mixing time and expected square jump distance is thoroughly treated in [ Pasarica & Gelman , 2010 ] , and is the theoretical inspiration for our choice of training loss .

We further emphasize that , barring optimization issues , our method should always fare at least as well as HMC in terms of mixing .

Thank you once again , we have updated the text to more clearly discuss why our approach might be expected to lead to better mixing .

Additionally , in the process of revisiting our experiments to compare against LAHMC , we empirically found that weighting the second term of our loss ( the ‘ burn - in ’ term ) could lead to even more improved auto-correlation and ESS on the diagnostic distributions . We therefore updated the paper and report the results obtained with slightly tuning that parameter ( setting it to 0 or 1 ) .

The paper introduces a non-volume - preserving generalization of HMC whose transitions are determined by a set of neural network functions . These functions are trained to maximize expected squared jump distance .
This works because each variable ( of the state space ) is modified in turn , so that the resulting update is invertible , with a tractable transformation inspired by Dinh et al 2016 .

Overall , I believe this paper is of good quality , clearly and carefully written , and potentially accelerates mixing in a state - of - the-art MCMC method , HMC , in many practical cases . A few downsides are commented on below .

The experimental section proves the usefulness of the method on a range of relevant test cases ; in addition , an application to a latent variable model is provided sec5 . 2.
Fig 1a presents results in terms of numbers of gradient evaluations , but I could n't find much in the way of computational cost of L2HMC in the paper . I ca n't see where the number " 124 x " in sec 5.1 stems from . As a user , I would be interested in the typical computational cost of both " MCMC sampler training " and MCMC sampler usage ( inference ? ) , compared to competing methods . This is admittedly hard to quantify objectively , but just an order of magnitude would be helpful for orientation .
Would it be relevant , in sec5.1 , to compare to other methods than just HMC , eg LAHMC ?

I am missing an intuition for several things : eq7 , the time encoding defined in Appendix C

Appendix Fig5 , I can not quite see how the caption claim is supported by the figure ( just hardly for VAE , but not for HMC ) .

The number " 124 x ESS " in sec5 . 1 seems at odds with the number in the abstract , " 50x " .

# Minor errors
- sec1 : " The sampler is trained to minimize a variation " : should be maximize
" as well as on a the real - world "
- sec3.2 " and 1 / 2 v^ T v the kinetic " : " energy " missing
- sec4 : the acronym L2HMC is not expanded anywhere in the paper
The sentence " We will denote the complete augmented ... p( d ) " might be moved to after " from a uniform distribution " in the same paragraph .
In paragraph starting " We now update x " :
- specify for clarity : " the first update , which yields x ' " / " the second update , which yields x ' ' "
- " only affects $ x_{\bar{m}^t}$ " : should be $ x'_{\bar{m}^t} $ ( prime missing )
- the syntax using subscript m^t is confusing to read ; would n't it be clearer to write this as a function , eg " mask ( x ' , m^t ) " ?
- inside zeta_2 and zeta_3 , do you not mean $ m^t " and $ \bar{m}^t $ ?
- sec5 : add reference for first mention of " A NICE MC "
- Appendix A :
- " Let 's " -> " Let "
- eq12 should be x ''= ...
- Appendix C : space missing after " Section 5.1 "
- Appendix D1 : " In this section is presented " : sounds odd
- Appendix D3 : presumably this should consist of the figure 5 ? Maybe specify .

We first and foremost want to thank you for your time and extremely valuable comments . We have uploaded a new version of the paper based on the feedback , and have addressed specific points below .

Clarification about 50 x vs 124x :
We decided against advertising the 124x number as it is misleading considering that HMC completely failed on this task ; the correct ratio was too large for us to experimentally measure . As such , we reported the one for the Strongly - Correlated Gaussian . We clarified this in the text and detail that L2HMC can succeed when HMC fails .

Intuition on Eq 7.:
We define this reciprocal loss to encourage mixing across the entire state space . The second term corresponds exactly to Expected Square Jump Distance , which we want to maximize as a proxy for mixing . The first term discourages a particle from not- moving at all in a region of state space -- if d( x , x ’ ) = 0 , the first term would be infinite . We clarified that part in the text .

Time encoding :
Our operator L_\theta consists of the composition of M augmented leapfrog steps . For each of those leapfrog , the timestep t is provided as input to the networks Q , S and T. Instead of providing it as a single scalar value , we provide it as a 2 - d vector [ cos ( 2 * pi * t / M ) , sin ( 2 * pi * t / M ) ] .

Regarding samples in Fig5 :
Sample quality and sharpness are inherently hard things to evaluate . Our observation was that many digits generated by L2HMC -DGLM look very sharp ( Line 1 Column 2 , Line 2 Column 8 , Line 5 Column 2 , Line 7 Columns 3 and 7 … ) . However , we will weaken the claim in the caption .

Comparison with LAHMC :
We compared our method to LAHMC on the evaluated energy functions . L2HMC significantly outperforms LAHMC on all tasks , for the same number of gradient evaluations . LAHMC is also unable to mix between modes in the MoG case . Results are reported in Appendix C.1.

We also note that L2HMC could be easily combined with LAHMC , by replacing the leapfrog integrator of LAHMC with the learned one of L2HMC .

In the process of revisiting our experiments to compare against LAHMC , we empirically found that weighting the second term of our loss ( the ‘ burn - in ’ term ) could lead to even more improved auto-correlation and ESS on the diagnostic distributions . We therefore updated the paper and report the results obtained with slightly tuning that parameter ( setting it to 0 or 1 ) .

Question about computation :
For the 2d-SCG case , on CPU , the training of the sampler took 160 seconds . The L2HMC overhead for sampling , with a batch -size of 200 , was about 36 % . This is negligible compared to an 106 x improved ESS . We also should note that for the latent generative model case , we train the sampler online with the same computations used to train everything else ; in that case L2HMC and HMC perform the exact same number of gradient evaluation of the energy and thus requires no training budget .

Thank you once again for your valuable feedback , we hope this helps answer your questions !

This article aims at understanding the role played by the different words in a sentence , taking into account their order in the sentence . In sentiment analysis for instance , this capacity is critical to model properly negation .
As state - of - the-art approaches rely on LSTM , the authors want to understand which information comes from which gate . After a short remainder regarding LSTM , the authors propose a framework to disambiguate interactions between gates . In order to obtain an analytic formulation of the decomposition , the authors propose to linearize activation functions in the network .
In the experiment section , authors compare themselves to a standard logistic regression ( based on a bag of words representation ) . They also check the unigram sentiment scores ( without context ) .
The main issue consists in modeling the dynamics inside a sentence ( when a negation or a ' used to be ' reverses the sentiment ) . The proposed approach works fine on selected samples .


The related work section is entirely focused on deep learning while the experiment section is dedicated to sentiment analysis . This section should be rebalanced . Even if the authors claim that their approach is general , they also show that it fits well the sentiment analysis task in particular .

On top of that , a lot of fine - grained sentiment analysis tools has been developed outside deep - learning : the authors should refer to those works .

Finally , authors should provide some quantitative analysis on sentiment classification : a lot of standard benchmarks are widely use in the literature and we need to see how the proposed method performs with respect to the state - of - the-art .


Given the chosen tasks , this work should be compared to the beermind system :
http://deepx.ucsd.edu/#/home/beermind
and the associated publication
http://arxiv.org/pdf/1511.03683.pdf

Thank you for your helpful comments . As you 'll see below , they lead to a meaningful improvement in the framing of our paper .

The problem we are solving is not extracting interactions , in a general sense , from text data , nor is it predicting sentiment . The problem we are solving is , for a given , trained , LSTM , explaining individual predictions being made , without modifying the architecture . This is an important distinction , which informs what denotes related work , and what methods we compare against . It is also one that was not entirely clear in the original version , and we so we updated our abstract ( lines 4 - 6 ) , introduction ( paragraph 2 , lines 1 - 3 ) , conclusion ( lines 1 - 2 ) and added equation 10 to better express this . When framed in this way , we believe that our baselines are the correct ones to demonstrate the novelty of our results .

" In the experiment section , authors compare themselves to a standard logistic regression ( based on a bag of words representation ) . They also check the unigram sentiment scores ( without context ) .
The main issue consists in modeling the dynamics inside a sentence ( when a negation or a ' used to be ' reverses the sentiment ) . The proposed approach works fine on selected samples . "

To clarify , only section 4.2 compares against a logistic regression , and deals with solely unigram sentiment scores . Sections 4.3- 4.6 do not involve logistic regression , and deal with general n-grams and interactions .

It is worth noting that we were very careful to not rely on " selected samples " , i.e. cherry - picking , as our primary means of validation . Rather , we provide anecdotes to motivate searches across the full dataset for different types of compositionality , with each of 4.3 , 4.4 and 4.5 involving different criteria , such as negation . For each of these different instances , we ultimately base our conclusions on the distributions of importance scores extracted from our LSTM across all phrases / reviews containing each kind of compositionality . These distributions can be found in figures 1 - 4 and provide , in our opinion , a more compelling case .

" The related work section is entirely focused on deep learning while the experiment section is dedicated to sentiment analysis . This section should be rebalanced . Even if the authors claim that their approach is general , they also show that it fits well the sentiment analysis task in particular . "

The primary contribution of this paper is an algorithm for interpreting predictions made by LSTMs , not improved prediction performance on sentiment analysis . Consequently , the related work section focuses on prior work in interpreting deep learning algorithms , particularly LSTMs . In our experiment section , we fit a single LSTM per dataset and analyse the behaviour of the LSTM interpretations produced by CD , along with four interpretation baselines , in different settings . The LSTMs are fit using standard procedures , and we make no claims of state of the art predictive performance from our model .

" Finally , authors should provide some quantitative analysis on sentiment classification : a lot of standard benchmarks are widely use in the literature and we need to see how the proposed method performs with respect to the state - of - the-art . "

To be clear , we assume that when you refer to benchmarks , and ask for performance with respect to state - of - the -art , you are referring to predictive accuracy . We do not claim to be state of the art in terms of predictive accuracy . In fact , as we note in 4.1.1 and 4.1.2 , our models follow implementations of baselines used for predictive accuracy in prior papers .

Rather , what we do claim is state of the art for interpreting predictions made by an LSTM . To justify this claim , we compare against four LSTM interpretation benchmarks across four different evaluation settings . Given the focus of our paper , we thought these were the most relevant comparisons .

As we mentioned above , we 've updated our abstract ( lines 4 - 6 ) , introduction ( paragraph 2 , lines 1 - 3 ) , conclusion ( lines 1 - 2 ) to clarify this distinction .

" Given the chosen tasks , this work should be compared to the beermind system : http://deepx.ucsd.edu/#/home/beermind and the associated publication http://arxiv.org/pdf/1511.03683.pdf "

Thanks for the pointer , this paper was a very interesting read . It seems that the focus is primarily on generating reviews for a given user / item pair , and secondarily on predicting sentiment . Given that our paper is focused on interpreting LSTMs , not on generating reviews or predictive performance for sentiment analysis , we are unsure what a meaningful , relevant comparison would look like .

In this paper , the authors propose a new LSTM variant that allows to produce interpretations by capturing the contributions of the words to the final prediction and the way their learned representations are combined in order to yield that prediction . They propose a new approach that they call Contextual Decomposition ( CD ) . Their approach consists of disambiguating interaction between LSTM ’s gates where gates are linearized so the products between them is over linear sums of contributions from different factors . The hidden and cell states are also decomposed in terms of contributions to the “ phrase ” in question and contributions from elements outside of the phrase . The motivation of the proposed decomposition using LSTMs is that the latter are powerful at capturing complex non- linear interactions , so , it would be useful to observe how these interactions are handled and to interpret the LSTM ’s predictions . As the authors intention is to build a way of interpreting LSTMs output and not to increase the model ’s accuracy , the empirical results illustrate the ability of their decomposition of giving a plausible interpretation to the elements of a sentence . They compare their method with different existing method by illustrating samples from the Yelp Polarity and SST datasets . They also show the ability of separating the distribution of CD scores related to positive and negative phrases on respectively Yelp Polarity and SST .

The proposed approach is potentially of great benefit as it is simple and elegant and could lead to new methods in the same direction of research . The sample illustrations , the scatter plots and the CD score distributions are helpful to asses the benefit of the proposed approach .

The writing could be improved as it contains parts where it leads to confusion . The details related to the linearization ( section 3.2.2 ) , the training ( 4.1 ) could be improved . In equation 25 , it is not clear what π_{i}^ ( - 1 ) and x_{π_{i}} represent but the example in equation 26 makes it clearer . The section would be clearer if each index and notation used is explained explicitly .

( CD in known for Contrastive Divergence in the deep learning community . It would be better if Contextual Decomposition is not referred by CD . )

Training details are given in section 4.1 where the authors mention the integrated gradient baseline without mentioning the reference paper to it ( however they do mention the reference paper at each of table 1 and 2 ) . it would be clearer for the reader if the references are also mentioned in section 4.1 where integrated gradient is introduced . Along with the reference , a brief description of that baseline could be given .

The “ Leave one out ” baseline is never mentioned in text before section 4.4 ( and tables 1 and 2 ) . Neither the reference nor the description of the baseline are given . It would have been clearer to the reader if this had been the case .

Overall , the paper contribution is of great benefit . The quality of the paper could be improved if the above explanations and details are given explicitly in the text .

Thanks for the detailed and thoughtful review . We 've responded to some of your comments below .

" In this paper , the authors propose a new LSTM variant that allows to produce interpretations by capturing the contributions of the words to the final prediction and the way their learned representations are combined in order to yield that prediction . "
To clarify , we are not proposing a new neural architecture . Our new method , contextual decomposition , is an interpretation method for a standard LSTM . Given a trained LSTM , it can be applied without altering the underlying model , re-training , or any additional work . We think that this is more impactful than proposing a new architecture , as it does n't force users to alter their model to get interpretations , nor to sacrifice the LSTM 's predictive accuracy .

This was a common misconception across reviewers , so we updated our abstract ( lines 4 - 6 ) , introduction ( paragraph 2 , lines 1 - 3 ) and conclusion ( lines 1 - 2 ) to clarify this distinction .

" The writing could be improved as it contains parts where it leads to confusion . The details related to the linearization ( section 3.2.2 ) , the training ( 4.1 ) could be improved . In equation 25 , it is not clear what π_{i}^ ( - 1 ) and x_{π_{i}} represent but the example in equation 26 makes it clearer . The section would be clearer if each index and notation used is explained explicitly . "
Thanks for pointing out these areas for improvement . We 've expanded 3.2.2 to make it clearer and better motivate our notation , adding equation 26 and re-writing the subsequent paragraph . The x_{\pi_{i}} you note was actually a typo - it has been corrected to y_{\pi_{i}} .

For 4.1 , we split off the baseline portion of 4.1 into 4.1.3 , which should make it clearer , while also addressing some of your concerns below around introducing , citing and describing baselines .

" Training details are given in section 4.1 where the authors mention the integrated gradient baseline without mentioning the reference paper to it ( however they do mention the reference paper at each of table 1 and 2 ) . it would be clearer for the reader if the references are also mentioned in section 4.1 where integrated gradient is introduced . Along with the reference , a brief description of that baseline could be given . "
Integrated gradients is now properly referenced . As discussed above , we added 4.1.3 , which includes proper references , and refers the reader to the related work section for a description ( We felt that reproducing descriptions of baselines in 4.1.3 would basically require replicating the related work paragraph ) .

" The “ Leave one out ” baseline is never mentioned in text before section 4.4 ( and tables 1 and 2 ) . Neither the reference nor the description of the baseline are given . It would have been clearer to the reader if this had been the case . "
Thanks for pointing this out . We added a mention and citation of leave one out in the new baselines section , 4.1.3 . Although the paper and method were discussed in related work ( paragraph 1 , lines 5 - 7 ) , we added a reference to the name “ leave one out ” for clarity .

The authors address the problem of making LSTMs more interpretable via the contextual decomposition of the state vectors . By linearizing the updates in the recurrent network , the proposed scheme allows one to extract word importance information directly from the gating dynamics and infer word - to- word interactions .

The problem of making neural networks more understandable is important in general . For NLP , this relates to the ability of capturing phrase features that go beyond single - word importance scores . A nice contribution of the paper is to show that this can highly improve classification performance on the task of sentiment analysis . However , the author could have spent some more time in explaining the technical consequences of the proposed linear approximation . For example , why is the linear approximation always good ? And what is the performance loss compared to a fully nonlinear network ?

The experimental results suggest that the algorithm can outperform state - of - the- art methods on various tasks .

Some questions :
- is any of the existing algorithms used for the comparison supposed to capture interactions between words and phrases ? If not , why is the proposed algorithm compared to them on interaction related tasks ?
- why the output of the algorithms is compared with the logistic regression score ? May the fact that logistic regression is a linear model be linked to the good performance of the proposed method ? Would it be possible to compare the output of the algorithms with human given scores on a small subset of words ?
- the recent success of LSTMs is often associated with their ability to learn complex and non-linear relationships but the proposed method is based on the linearization of the network . How can the algorithm be able to capture non-linear interactions ? What is the difference between the proposed model and a simple linear model ?

Thank you for your helpful comments .
" A nice contribution of the paper is to show that this can highly improve classification performance on the task of sentiment analysis . "
We actually do something different than what this implies . CD is an algorithm for producing interpretations of LSTM predictions . In particular , given a trained LSTM , it produces interpretations for individual predictions without modifying the LSTM 's architecture in any way , leaving the predictions , and accuracy , unchanged . The purpose of our evaluation is not to improve predictive performance but rather to demonstrate that these importance scores accurately reflect the LSTMs dynamics ( e.g. negation , compositionality ) and , in particular , do so better than prior methods . We have updated our abstract ( lines 4 - 6 ) , introduction ( paragraph 2 , lines 1 - 3 ) , conclusion ( lines 1 - 2 ) , and added equation 10 in order to avoid similar misunderstandings by future readers .
" However , the author could have spent some more time in explaining the technical consequences of the proposed linear approximation . For example , why is the linear approximation always good ? And what is the performance loss compared to a fully nonlinear network ? "
Our linearization is not an approximation , it is exact . CD produces an exact decomposition of the values fed into the LSTM ’s final softmax into a sum of two terms : contributions resulting solely from the specified phrase , and others . Mathematically , this is shown in the newly added equation 10 . Moreover , CD is used for interpretation of the original LSTM , not as a separate prediction algorithm , so that there is no performance loss .
" - is any of the existing algorithms used for the comparison supposed to capture interactions between words and phrases ? If not , why is the proposed algorithm compared to them on interaction related tasks ? "
This is a great question , we assume you ’re referencing the finding in 4.5 that CD can extract negations . To the best of our knowledge , no prior algorithm has made the claim of being able to extract interactions from LSTMs . Our ability to do so is a significant contribution . Although not previously discussed , the leave one out method can be adapted to produce an interaction value , which we report in figure 1 . However , the produced interactions do n't seem to contain much information , perhaps explaining why they were not included in the original paper . Nonetheless , leave one out is the only baseline we are aware of , so we thought it important to report them for comparison .
" why the output of the algorithms is compared with the logistic regression score ? May the fact that logistic regression is a linear model be linked to the good performance of the proposed method ? "
I assume you 're referencing 4.2 here . When the underlying model is sufficiently accurate ( which it is in our case ) , logistic regression coefficients are generally viewed to provide qualitatively sensible importance scores . In other words , the ordering provided by the coefficients generally lines up very well with what humans qualitatively view as important . Thus , a sensible check for the behaviour of an interpretation algorithm is whether or not it can recover qualitatively similar coefficients , as measured by correlation .

To elaborate , if a logistic regression coefficient is very positive , such as for “ terrific ” , we would expect the word importance score extracted from an LSTM to also be very positive . Similarly , if the logistic regression coefficient is zero , such as for “ the ” , we would expect the LSTM 's word importance to be quite small . We do not expect these relationships to be perfect , but the fact that they are reasonably strong supports our claim that our method produces comparable or superior word importance scores .

“ Would it be possible to compare the output of the algorithms with human given scores on a small subset of words ? ”
The problem of running human experiments to validate interpretations is an interesting and active research area . However , running such experiments is a substantive endeavour , which unfortunately puts it outside the scope of this paper . We do agree that it would be an exciting prospect for future work , though . Nonetheless , as discussed above , we do believe that our approach provides valuable information , if not as much as a full human experiment .

" - the recent success of LSTMs is often associated with their ability to learn complex and non-linear relationships but the proposed method is based on the linearization of the network . How can the algorithm be able to capture non-linear interactions ? What is the difference between the proposed model and a simple linear model ? "
We hope that our earlier comments resolve this question . In particular , our proposed method is not a separate prediction method , but rather an interpretation method for a standard LSTM . In response to this confusion , we have updated our abstract ( lines 4 - 6 ) , introduction ( paragraph 2 , lines 1 - 3 ) , conclusion ( lines 1 - 2 ) , and added equation 10 .


This paper attacks an important problems with an interesting and promising methodology . The authors deal with inference in models of collective behavior , specifically at how to infer the parameters of a mean field game representation of collective behavior . The technique the authors innovate is to specify a mean field game as a model , and then use inverse reinforcement learning to learn the reward functions of agents in the mean field game .

This work has many virtues , and could be an impactful piece . There is still minimal work at the intersection of machine learning and collective behavior , and this paper could help to stimulate the growth of that intersection . The application to collective behavior could be an interesting novel application to many in machine learning , and conversely the inference techniques that are innovated should be novel to many researchers in collective behavior .

At the same time , the scientific content of the work has critical conceptual flaws . Most fundamentally , the authors appear to implicitly center their work around highly controversial claims about the ontological status of group optimization , without the careful justification necessary to make this kind of argument . In addition to that , the authors appear to implicitly assume that utility function inference can be used for causal inference .

That is , there are two distinct mistakes the authors make in their scientific claims :
1 ) The authors write as if mean field games represent population optimization ( Mean field games are not about what a _group_ optimizes ; they are about what _individuals_ optimize , and this individual optimization leads to certain patterns in collective behaviors )
2 ) The authors write as if utility / reward function inference alone can provide causal understanding of collective or individual behavior

1 -

I should say that I am highly sympathetic to the claim that many types of collective behavior can be viewed as optimizing some kind of objective function . However , this claim is far from mainstream , and is in fact highly contested . For instance , many prominent pieces of work in the study of collective behavior have highlighted its irrational aspects , from the madness of crowds to herding in financial markets .

Since it is so fringe to attribute causal agency to groups , let alone optimal agency , in the remainder of my review I will give the authors the benefit of the doubt and assume when they say things like " population behavior may be optimal " , they mean " the behavior of individuals within a population may be optimal " . If the authors do mean to say this , they should be more careful about their language use in this regard ( individuals are the actors , not populations ) . If the authors do indeed mean to attribute causal agency to groups ( as suggested in their MDP representation ) , they will run into all the criticisms I would have about an individual - level analysis and more . Suffice it to say , mean field games themselves do n't make claims about aggregate- level optimization . A Nash equilibrium achieves a balance between individual - level reward functions . These reward functions are only interpretable at the individual level . There is no objective function the group itself in aggregate is optimizing in mean field games . For instance , even though the mean field game model of the Mexican wave produces wave solutions , the model is premised on people having individual utility functions that lead to emergent wave behavior . The model does not have the representational capacity to explain that people actually intend to create the emergent behavior of a wave ( even though in this case they do ) . Furthermore , the fact that mean field games aggregate to a single - agent MDP does not imply that that the group can rightfully be thought of as an agent optimizing the reward function , because there is an exact correspondence between the rewards of the individual agents in the MFG and of the aggregate agent in the MDP by construction .

2 -

The authors also claim that their inference methods can help explain why people choose to talk about certain topics . As far as the extent to which utility / reward function inference can provide causal explanations of individual ( or collective ) behavior , the argument that is invariably brought against a claim of optimization is that almost any behavior can be explained as optimal post-hoc with enough degrees of freedom in the utiliy function of the behavioral model . Since optimization frameworks are so flexible , they have little explanatory power and are hard to falsify . In fact , there is literally no way that the modeling framework of the authors even affords the possibility that individual / collective behavior is not optimal . Optimality is taken as an assumption that allows the authors to infer what reward function is being optimized .

The authors state that the reward function they infer helps to interpret collective behavior because it reveals what people are optimizing . However , the reward function actually discovered is not interpretable at all . It is simply a summary of the statistical properties of changes in popularity of the topics of conversation in the Twitter data the authors ' study . To quote the authors ' insights : " The learned reward function reveals that a real social media population favors states characterized by a highly non-uniform distribution with negative mass gradient in decreasing order of topic popularity , as well as transitions that increase this distribution imbalance . " The authors might as well have simply visualized the topic popularities and changes in popularities to arrive at such an insight . To take the authors claims literally , we would say that people have an intrinsic preference for everyone to arbitrarily be talking about the same thing , regardless of the content or relevance of that topic . To draw an analogy , this is like observing that on some days everybody on the street is carrying open umbrellas and on other days not , and inferring that the people on the street have a preference for everyone having their umbrellas open together ( and the model would then predict that if one person opens an umbrella on a sunny day , everybody else will too ) .

To the authors credit , they do make a brief attempt to present empirical evidence for their optimization view , stating succinctly : " The high prediction accuracy of the learned policy provides evidence that real population behavior can be understood and modeled as the result of an emergent population - level optimization with respect to a reward function . " Needless to say , this one - sentence argument for a highly controversial scientific claims falls flat on closer inspection . Setting aside the issues of correlation versus causation , predictive accuracy does not in and of itself provide scientific plausibility . When an n-gram model produces text that is in the style of a particular writer , we do not conclude that the writer must have been composing based on the n-gram 's generative mechanism . Predictive accuracy only provides evidence when combined in the first place with scientific plausibility through other avenues of evidence .

The authors could attempt to address these issues by making what is called an " as - if " argument , but it 's not even clear such an argument could work here in general .

With all this in mind , it would be more instructive to show that the inference method the authors introduce could infer the correct utility functions used in standard mean field games , such as modeling traffic congestion and the Mexican wave .

--

All that said , the general approach taken in the authors ' work is highly promising , and there are many fruitful directions I would be exicted to see this work taken --- e.g. , combining endogenous and exogenous rewards or looking at more complex applications . As a technical contribution , the paper is wonderful , and I would enthusiastically support acceptance . The authors simply either need to be much more careful with the scientific claims about collective behavior they make , or limit the scope of the contribution of the paper to be modeling / inference in the area of collective behavior . Mean field games are an important class of models in collective behavior , and being able to infer their parameters is a nice step forward purely due to the importance of that class of games . Identifying where the authors ' inference method could be applied to draw valid scientific conclusions about collective behavior could then be an avenue for future work . Examples of plausible scientific applications might include parameter inference in settings where mean field games are already typically applied in order to improve the fit of those models or to learn about trade - offs people make in their utility functions in those settings .

--

Other minor comments :
- ( Introduction ) It is not clear at all how the Arab Spring , Black Lives Matter , and fake news are similar --- i.e. , whether a single model could provide insight into these highly heterogeneous events --- nor is it clear what end the authors hope to achieve by modeling them --- the ethics of modeling protests in a field crowded with powerful institutional actors is worth carefully considering .
- If I understand correctly , the fact that the authors assume a factored reward function seems limiting . Is n't the major benefit of game theory it 's ability to accommodate utility functions that depend on the actions of others ?
- The authors state that one of their essential insights is that " solving the optimization problem of a single - agent MDP is equivalent to solving the inference problem of an MFG . " This statement feels a bit too cute at the expense of clarity . The authors perform inference via inverse - RL , so it 's more clear to say the authors are attempting to use statistical inference to figure out what is being optimized .
- The relationship between MFGs and a single - agent MDP is nice and a fine observation , but not as surprising as the authors frame it as . Any multiagent MDP can be naively represented as a single - agent MDP where the agent has control over the entire population , and we already know that stochastic games are closely related to MDPs . It 's therefore hard to imagine that there woud n't be some sort of correspondence .

We greatly appreciate your insightful and high quality feedback . We agree that the intersection of machine learning and modeling collective processes deserves more exploration . Overall , we will improve our language when describing population behavior , and interpretation the inference results more carefully . Below , we address the two critical concerns in detail .

1 . Interpretation of MFG as population - level or individual - level optimization

In all instances where we mention actions , decisions and optimality , we meant individuals . Some examples are ``aggregate effect of individual actions '' ( abstract ) and ``aggregate decisions of all individuals '' ( intro ) . We did not intend to claim that MFG has a reward that the population itself as an agent tries to optimize . We absolutely agree that MFG models a Nash equilibrium arrived from individual choice , which is shown by equation 6 on page 4 . To say that a single - agent policy is optimal for the constructed MDP reward is not the same as saying that any individual optimizes for this constructed reward . Our writing is in accord with the former , not the latter . To improve clarity , we will clearly say that individuals only optimize for the MFG reward , and that the optimal policy for the constructed MDP is only a tool for generating population trajectories , without making any claim about the ontological status of group optimization .

On a related note , could you refer us to particular works that highlight irrational aspects of collective behavior ?

2 . Use of reward function to understand behavior

If we understood correctly , the concern about falsifiability is the following : given some optimization framework and demonstration data , there always exists a reward function for which the demonstration is optimal , which means that the hypothesis of optimality is vacuous . To take an extreme case , it is well known that inverse problems suffer from degeneracy , e.g. any behavior is optimal with respect to an all - zero reward ( Ng & Russell 2000 ) . But among all possible rewards that can be learned from data , many may not allow the forward dynamics to reproduce data similar to the observations . This is partly the reason that we evaluated predictive accuracy of the model , similar to the evaluation of IRL on task completion in robotics ( Finn et al. 2016 ) .

Regarding interpretation of the reward , it is true that we could visualize the statistical distribution of population distributions and transition matrices , to see which types are favored . It is uncertain whether this is easier or harder for extracting insight from data . However , we do not fully understand why summarizing statistical properties of data has equal utility as learning a reward : e.g. in a finite-horizon gridworld with positive reward only at one terminal state , merely looking at statistics of expert trajectories reveals nothing about which state-action pair is good . We accept the advice to restrict our interpretation of reward to be within our model ’s representational capacity , due to the lack of semantics in the model .

We acknowledge the warning about using predictive accuracy to justify claims about the physical world . Perhaps saying ``modeled ’’ , rather than ``understood ’’ , better conveys our intended message that the MFG only a descriptive model so far . We aimed to tackle the question `` What is a good description of population behavior ’’ , rather than the question `` Why does the population behave this way '' .

We agree that recovering a pre-specified reward in a synthetic MFG is useful to show . We chose to focus exclusively on a real experiment because one of our main motivations was to ground MFG research on real observations .

Response to additional comments :
1 . We chose to motivate the population modeling problem using these events because : social media enabled a much larger population to participate virtually than would have been possible otherwise ; each event involves a large population concentrated on the same general topic but differentiated into discrete subtopics ; our experiment data comes from a social media population . We can add this clarification to our introduction .
2 . The use of r_{ij} ( pi , P_i ) rather than r_{ij} ( pi , P ) still couples V_i^n to the actions by individuals in other topics , because the choice of P_i partially determines the next state pi^{n + 1 } , which determines the actions by individuals at other topics j , which in turn affects V_i^n via the summation over j of V_j^{n + 1} . This can be seen by unrolling equation ( 7 ) and using ( 3 ) . The dependence is on actions taken by others at the next time step .
3 . The MaxEnt IRL procedure maximizes a log likelihood , so viewing it as either statistical inference or optimization should both be valid . Since MDP and RL are optimal control frameworks , we labeled it as optimization .
4 . We wrote with more emphasis in some places of the text because MFG may be less well - known in the community , hoping that a stronger tone may help with clarity .

The paper considers the problem of representing and learning the behavior of a large population of agents , in an attempt to construct an effective predictive model of the behavior . The main concern is with large populations where it is not possible to represent each agent individually , hence the need to use a population level description . The main contribution of the paper is in relating the theories of Mean Field Games ( MFG ) and Reinforcement Learning ( RL ) within the classic context of Markov Decision Processes ( MDPs ) . The method suggested uses inverse RL to learn both the reward function and the forward dynamics of the MFG from data , and its effectiveness is demonstrated on social media data .
The paper contributes along three lines , covering theory , algorithm and experiment . The theoretical contribution begins by transforming a continuous time MFG formulation to a discrete time formulation ( proposition 1 ) , and then relates the MFG to an associated MDP problem . The first contribution seems rather straightforward and appears to have been done previously , while the second is interesting , yet simple to prove . However , Theorem 2 sets the stage for an algorithm developed in section 4 of the paper that suggests an RL solution to the MFG problem . The key insight here is that solving an optimization problem on an MDP of a single agent is equivalent to solving the inference problem of the ( population - level ) MFG . Practically , this leads to learning a reward function from demonstrations using a maximum likelihood approach , where the reward is represented using a deep neural network , and the policy is learned through an actor-critic algorithm , based on gradient descent with respect to the policy parameters . The algorithm provides an improvement over previous approaches limited to toy problems with artificially created reward functions . Finally , the approach is demonstrated on real - world social data with the aim of recovering the reward function and predicting the future trajectory . The results compare favorably with two baselines , vector auto-regression and recurrent neural networks .
I have found the paper to be interesting , and , although I am not an expert in MFGs , novel and well - articulated . Moreover , it appears to hold promise for modeling social media in general . I would appreciate clarification on several issues which would improve the presentability of the results .
1 ) The authors discuss on p. 6 variance reduction techniques . I would appreciate a more complete description or , at least , a more precise reference than to a complete paper .
2 ) The experimental results use state that “ Although the set of topics differ semantically each day , indexing topics in order of decreasing initial popularity suffices for identifying the topic sets across all days . ” This statement is unclear to me and I would appreciate a more detailed explanation .
3 ) The authors make the following statement : “ … learning the MFG model required only the initial population distribution of each day in the training set , while VAR and RNN used the distributions over all hours of each day . ” Please clarify the distinction and between the algorithms here . In general , details are missing about how the VAR and RNN were run .
4 ) The approach uses expert demonstration ( line 7 in Algorithm 1 ) . It was not clear to me how this is done in the experiment .


Thank you for highlighting the main points of the paper in detail , and identifying that our contribution lies at the relatively underexplored intersection of RL and models of population behavior . To address the questions raised :

1 . There is a substantial amount of prior work on variance reduction in gradient - based methods for MDPs ( Sutton & Barto 1998 , Weaver & Tao 2001 , Greensmith et al. 2004 , Lawrence et al. 2003 ) . Using the policy gradient theorem ( e.g. section 13.2 in Sutton & Barto ) , one can see that subtracting any arbitrary state - dependent function from the action - value estimate does not introduce bias . As can be seen in eq 1 of Greensmith et al. , subtracting a baseline reduces variance as long as the covariance is large . Therefore , an optimal baseline can be found to minimize variance ( Weaver & Tao 2001 ) . Sutton & Barto 1998 give an empirical demonstration in Figure 2.5 that an appropriately chosen baseline can speed up convergence . We can include a citation to section 3 of Sutton et al. 1999 , as it gives an equivalent statement of using the value function as a baseline .

2 . Here is an example : suppose there are three topics ( t1 , t2 , t3 ) , and the initial count of participants at 9 am of day 1 is ( 10 , 5 , 15 ) . So we reorder the topics to be ( t3 , t1 , t2 ) and relabel them as ( s1 , s2 , s3 ) . This is what we meant by ``indexing topics in order of decreasing initial popularity . '' On day 2 , the topics may be semantically different , e.g. ( t4 , t5 , t6 ) , with initial participation counts ( 5 , 15 , 10 ) , so we reorder them to be ( t5 , t6 , t4 ) , and again assign labels ( s1 , s2 , s3 ) . So now both t3 of day 1 and t5 of day 2 are relabeled ( i.e. ``identified ' ' ) as s 1 . This is what we meant by ``identifying topic sets across all days .'' This is how we abstract away semantic content and only work with the distribution ( i.e. ranking ) . This reordering lets us interpret our collected demonstration trajectories in a consistent manner , e.g. each trajectory is like running one episode of the constructed MDP , with different starting states ( i.e. different initial pi^0 ) but with the same fixed topic set . Since real populations are influenced by both ranking and semantics , we acknowledge that this method limited the scope of the current work . It suggests a possible extension , e.g. augmenting our basic MFG model to account for topic semantics .

3 . This can be understood from line 4 of Algorithm 2 in Appendix B. For each training episode of the forward RL , we randomly pick a starting pi^0 from the collection of all measured initial distributions . During the single episode , the forward RL never uses the measured pi^1 , ... , pi^{N - 1 } because our constructed MDP provides the transition equation , which produces next states pi^{n + 1 } from pi^n and the action P^n produced by the policy being learned ( lines 6 and 7 of Algorithm 2 ) . In contrast , both VAR and RNN are classic examples of supervised learning , where each pi^n , ... , pi^{n - m} ( for some m ) in the training set is used to predict pi^{n + 1} . We will supplement Appendix D to describe VAR and RNN in more detail .

4 . Expert demonstration trajectories , sampled from the full set of measured trajectories in line 7 of Alg 1 , are used to compute the loss in Equation 13 . We take all the state-action pairs of the demonstration trajectories , pass them as a batch into the reward neural network , and add the resulting scalars to get the first term of Equation 13 . Learning of the reward is done via gradient descent on this loss , with respect to the neural net parameters W. The process is the same for the second term , which uses trajectories generated from the policy at that iteration .

The paper proposes a novel approach on estimating the parameters
of Mean field games ( MFG ) . The key of the method is a reduction of the unknown parameter MFG to an unknown parameter Markov Decision Process ( MDP ) .

This is an important class of models and I recommend the acceptance of the paper .

I think that the general discussion about the collective behavior application should be more carefully presented and some better examples of applications should be easy to provide . In addition the authors may want to enrich their literature review and give references to alternative work on unknown MDP estimation methods cf . [ 1 ] , [ 2 ] below .

[ 1 ] Burnetas , A. N. , & Katehakis , M. N. ( 1997 ) . Optimal adaptive policies for Markov decision processes . Mathematics of Operations Research , 22 ( 1 ) , 222-255 .

[ 2 ] Budhiraja , A. , Liu , X. , & Shwartz , A. ( 2012 ) . Action time sharing policies for ergodic control of Markov chains . SIAM Journal on Control and Optimization , 50 ( 1 ) , 171-195 .

We highly appreciate your support for the merits of MFG models , especially in synthesis with the well - studied framework of MDP . We agree that our discussion of the collective behavior and interpretation of results should be presented more carefully , and we will update our wording to be more precise . For applications , we will further highlight the synthetic experiments in previous MFG research , and suggest the analogous real - world applications .

Thank you for directing us to alternative work in MDPs with unknown parameters .
1 . Looking at Burnetas & Katehakis ( 1997 ) , we see a thematic similarity : they consider the case of an unknown transition law in finite state-action spaces , and also extend the analysis to a model where reward has distribution with unknown parameters dependent on states and actions . Likewise , we consider a reward function with unknown parameters to be learned . Although our constructed MDP has a known deterministic transition , we simulate the MDP and learn via RL to handle continuous states and action spaces . We agree that we should reference Burnetas & Katehakis ' contribution to this research theme .
2 . If we understood Budhiraja , Liu & Shwartz ( 2012 ) correctly , they construct a class of action time sharing ( ATS ) policies that give the same long - term costs as a stationary Markov control , and which enable estimation of unknown model parameters ( via deviation from optimal control ) while maintaining the same cost per unit time . We agree that the problem of fulfilling a secondary objective while optimizing for a given cost ( which does n't necessarily depend on those secondary parameters ) is an interesting one , and seems to be novel for RL research . We can see that the framework of simultaneously estimating unknown parameters while optimizing a known cost is related to the inverse RL framework used in our work , i.e. simultaneously learning an unknown cost and finding an optimal policy . Can we confirm that this is a correct understanding of your comment ?


This paper presents Defense - GAN : a GAN that used at test time to map the input generate an image ( G ( z ) ) close ( in MSE ( G ( z ) , x ) ) to the input image ( x ) , by applying several steps of gradient descent of this MSE . The GAN is a WGAN trained on the train set ( only to keep the generator ) . The goal of the whole approach is to be robust to adversarial examples , without having to change the ( downstream task ) classifier , only swapping in the G ( z ) for the x .

+ The paper is easy to follow .
+ It seems ( but I am not an expert in adversarial examples ) to cite the relevant litterature ( that I know of ) and compare to reasonably established attacks and defenses .
+ Simple / directly applicable approach that seems to work experimentally , but
- A missing baseline is to take the nearest neighbour of the ( perturbed ) x from the training set .
- Only MNIST -sized images , and MNIST - like ( 60 k train set , 10 labels ) datasets : MNIST and F-MNIST .
- Between 0.043sec and 0.825 sec to reconstruct an MNIST - sized image .
? Mag Net results were very often worse than no defense in Table 4 , could you comment on that ?
- In white - box attacks , it seems to me like L steps of gradient descent on MSE ( G ( z ) , x ) should be directly extended to L steps of ( at least ) FGSM - based attacks , at least as a control .

We appreciate the constructive criticism and detailed analysis of our paper .

A ) Nearest -neighbor baseline :
Taking the nearest neighbor of the potentially perturbed x from the training set can be seen as a simple way of removing adversarial noise , and is tantamount to a 1 - nearest - neighbor ( 1 - NN ) classifier . On MNIST , a 1 - NN classifier achieves an 88.6 % accuracy on FGSM adversarial examples with epsilon = 0.3 , found using the B substitute network . Defense - GAN - Rec and Defense-GAN - Orig average about 92.5 % across the four different classifier networks when the substitute model is fixed to B. Similar trends are found for other substitute models . There is an improvement of about 4 % by using Defense - GAN . It is also worth noting that in the case of MNIST , a 1 - NN classifier works reasonably well ( achieving around 95 % on clean images ) . This is not the case for more complex datasets : for example , if the problem at hand is face attributes classification , nearest neighbors may not necessarily belong to the same class , and therefore NN classifiers will perform poorly .

B ) Only MNIST -sized images :
Based on the reviewer ’s suggestion , we have added additional white - box results on the Large-scale CelebFaces Attributes ( CelebA ) dataset in the appendix of the paper . The results show that Defense - GAN can still be used with more complex datasets including larger and RGB images . For further details , please refer to Appendix F in the revised version .

C ) Time to reconstruct images :
We agree with the reviewer that Defense - GAN introduces additional inference time by reconstructing images using GD on the MSE loss . However , we show its effectiveness against various attacks , especially in comparison to other simpler defenses . Furthermore , we have not optimized the running time of our algorithm , as it was not the focus of this work . This is a worthwhile effort to pursue in the future by trying to better utilize computational resources .
Per the reviewer ’s comment , we have timed some reconstruction steps for CelebA images ( which are 15.6 times larger than MNIST / F-MNIST ) . For R = 2 , we have :
L = 10 , 0.132 sec
L = 25 , 0.106 sec
L = 50 , 0.210 sec
L = 100 , 0.413 sec
L = 200 , 0.824 sec
The reconstruction time for CelebA did not scale with the size of the image .

D ) Mag Net results are sometimes worse than no defense in Table 4 :
Even though it seems counter- intuitive that a defense mechanism can sometimes cause a decrease in performance , this stems from the fact that white - box attackers also know the exact defense mechanism used . In the case of MagNet , the defense mechanism is another feedforward network which , in conjunction with the original classifier , can be viewed as a new deeper feedforward network . Attacks on this bigger network can sometimes be more successful than attacks on the original network . Furthermore , MagNet was not designed to be robust against white - box attacks .

E ) Using L steps of white - box FGSM :
Per our understanding , the reviewer is suggesting using iterative FGSM . We do agree that for a fair comparison , L steps of iterative FGSM could be used . However , we note that CW is an iterative optimization - based attack , and is more powerful than iterative FGSM . Since we have shown robustness against CW attacks in Table 4 , we believe iterative FGSM results will be similar .


This paper presents a method to cope with adversarial examples in classification tasks , leveraging a generative model of the inputs . Given an accurate generative model of the input , this approach first projects the input onto the manifold learned by the generative model ( the idea being that inputs on this manifold reflect the non-adversarial input distribution ) . This projected input is then used to produce the classification probabilities . The authors test their method on various adversarially constructed inputs ( with varying degrees of noise ) .

Questions / Comments :

- I am interested in unpacking the improvement of Defense - GAN over the MagNet auto-encoder based method . Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data ? If the decoder from the MagNet approach were treated purely as a generative model , and the same optimization - based projection approach ( proposed in this work ) was followed , would the results be comparable ?

- Is there anything special about the GAN approach , versus other generative approaches ?

- In the black - box vs . white - box scenarios , can the attacker know the GAN parameters ? Is that what is meant by the " defense network " ( in experiments bullet 2 ) ?

- How computationally expensive is this approach take compared to MagNet or other adversarial approaches ?

Quality : The method appears to be technically correct .

Clarity : This paper clearly written ; both method and experiments are presented well .

Originality : I am not familiar enough with adversarial learning to assess the novelty of this approach .

Significance : I believe the main contribution of this method is the optimization - based approach to project onto a generative model 's manifold . I think this kernel has the potential to be explored further ( e.g. computational speed - up , projection metrics ) .

We thank the reviewer for the insightful comments and discussions .

A ) Defense - GAN vs . Mag Net vs. other generative approaches :
We believe that the MagNet auto-encoder suffers lower accuracy compared to Defense - GAN due to the fact that the “ reconstruction ” step in MagNet is a feed - forward network as opposed to an optimization - based projection as in Defense - GAN . Overall , the combination of MagNet and the classifier can be seen as one deeper classification network , and has a wide attack surface compared to Defense-GAN .
As suggested by the reviewer , if the MagNet decoder ( or another generative approach ) was treated as a generative model , and the same optimization - based projection approach was followed , the model with more representative power would perform better . From our experience , GANs tend to have more representative power , but this is still an active area of research and discussion . We believe that , since GANs are specifically designed to optimize for generative tasks , using a GAN in conjunction with our proposed optimization - based projection would outperform an encoder with the same projection method . However , this would be an interesting future research direction . In addition , we were able to show some theoretical guarantees regarding the use and representative power of GANs in equation ( 7 ) .

B ) Black - and white - box attacks :
In our work and previous literature , it is assumed that in black - box scenarios the attacker does not know the classifier network nor the defense mechanism ( and any parameters thereof ) . The only information the attacker can use is the classifier output .
In white - box scenarios , the attacker knows the entire system including the classifier network , defense mechanisms , and all parameters ( which in our case , include GAN parameters ) . By “ defense network ” in Experiments bullet 2 , we mean the generator network .

C ) Computational complexity :
Defense-GAN adds inference - time complexity to the classifier . As discussed in Appendix G ( Appendix F in the original version of the paper ) , this complexity depends on L , the number of GD steps used to reconstruct images , and ( to a lesser extent ) R , the number of random restarts . At training time , Defense - GAN requires training a GAN , but no retraining of the classifier is necessary .
In comparison , MagNet also adds inference - time complexity . However , the time overhead is much smaller than Defense - GAN as MagNet is simply a feedforward network . At training time , the overhead is similar to Defense-GAN ( training the encoder , no retraining of the classifier ) .
Adversarial training adds no inference - time complexity . However , training time can be significantly larger than for other methods since re-training the classifier is required ( preceded by generating the adversarial examples to augment the training dataset ) .


The authors describe a new defense mechanism against adversarial attacks on classifiers ( e.g. , FGSM ) . They propose utilizing Generative Adversarial Networks ( GAN ) , which are usually used for training generative models for an unknown distribution , but have a natural adversarial interpretation . In particular , a GAN consists of a generator NN G which maps a random vector z to an example x , and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution . The GAN is trained to minimize the max min loss of D on this discrimination task , thereby producing a G ( in the limit ) whose outputs are indistinguishable from the true distribution by the best discriminator .

Utilizing a trained GAN , the authors propose the following defense at inference time . Given a sample x ( which has been adversarially perturbed ) , first project x onto the range of G by solving the minimization problem z* = argmin_z || G ( z ) - x||_2 . This is done by SGD . Then apply any classifier trained on the true distribution on the resulting x* = G ( z* ) .

In the case of existing black - box attacks , the authors argue ( convincingly ) that the method is both flexible and empirically effective . In particular , the defense can be applied in conjunction with any classifier ( including already hardened classifiers ) , and does not assume any specific attack model . Nevertheless , it appears to be effective against FGSM attacks , and competitive with adversarial training specifically to defend against FGSM .

The authors provide less - convincing evidence that the defense is effective against white - box attacks . In particular , the method is shown to be robust against FGSM , RAND + FGSM , and CW white - box attacks . However , it is not clear to me that the method is invulnerable to novel white - box attacks . In particular , it seems that the attacker can design an x which projects onto some desired x* ( using some other method entirely ) , which then fools the classifier downstream .

Nevertheless , the method is shown to be an effective tool for hardening any classifier against existing black - box attacks
( which is arguably of great practical value ) . It is novel and should generate further research with respect to understanding its vulnerabilities more completely .

Minor Comments :
The sentence starting “ Unless otherwise specified … ” at the top of page 7 is confusing given the actual contents of Tables 1 and 2 , which are clarified only by looking at Table 5 in the appendix . This should be fixed .


We thank the reviewer for the constructive review and comments .

A ) Regarding the effectiveness against white - box attacks :
As the reviewer has pointed out , we have shown the robustness of our method to existing white - box attacks such as FGSM , RAND + FGSM , and CW . Indeed , a good attack strategy could be to design an x which projects onto a desired x* = G ( z* ) . However , this requires solving for :

Find x s.t. the output of the gradient - descent block is z*.

Per our understanding , the reviewer ’s suggestion is the following :
Find a desired x* in the range of the generator which fools the classifier .
Find an x which projects onto x* , i.e. , such that the output of the GD block is z* , where G ( z* ) = x*.
Step 1 is a more challenging version of existing attacks , due to the constraint that the adversarial example should lie in the range of the generator . While step 1 could potentially be solvable , the real difficulty lies in step 2 . In fact , it is not obvious how to find such an x given x* . What comes to mind is attempting to solve step 2 using an optimization framework , e.g. :
Minimize ( over x , z* ) 1
Subject to G ( z* ) = x *
z* is the output of the GD block after L steps .

We have shown in Appendix B that solving this problem using GD gets more and more prohibitive as L increases .
Furthermore , since we use random initializations of z , if the random seed is not accessible by the attacker , there is no guarantee that a fixed x will result in the same fixed z every time after L steps of GD on the MSE .
Due to these factors , we believe that our method is robust to a wide range of gradient - based white - box attacks . However , we are very much interested in further research of novel attack methods .

B ) We have fixed the minor comments by specifically mentioning the classifier and substitute models for every Table and Figure throughout the paper .


Overall : Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes . Previous approaches to training data mixing are ( 1 ) from random classes , or ( 2 ) from the same class . The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network . Results look like significant improvements over standard learning setups .

Detailed Evaluation : The approach presented is simple , clearly presented , and looks effective on benchmarks . In terms of originality , it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power . The authors have also provided extensive analysis from the point of views ( 1 ) network architecture , ( 2 ) mixing method , ( 3 ) number of labels / classes in mix , ( 4 ) mixing layers -- really well done due-diligence across different model and task parameters .

Minor Asks :
( 1 ) Clarification on how the error rates are defined . Especially since the standard learning task could be 0 - 1 loss and this new BC learning task could be based on distribution divergence ( if we 're not using argmax as class label ) .
( 2 ) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes . Since pairs of classes could be huge if the total number of classes is large , it 'll be nice to see how this scales . I.e. are we talking about a space of 10 total classes or 10000 total classes ? How does num required epochs get impacted as we increase this class space ?
( 3 ) Clarify how G_1/20 and G_2/20 is important / derived - I assume it 's unit conversion from decibels .
( 4 ) Please explain why it is important to use the smoothed average of 10 softmax predictions in evaluation ... what happens if you just randomly pick one of the 10 crops for prediction ?

Thanks for your positive review . Our method is novel in that we train the model to output the mixing ratio between two different classes .

Answers for minor asks :
( 1 ) We do not define error rate of BC learning in training phase . In testing phase , the error rate definition of BC learning is the same as that of standard learning because we do not mix any sounds in testing phase .

( 2 ) Thanks for helpful advice . Although we could not try more than 50 classes , we have investigated the relationship between performance and the number of training epochs not only on ESC - 50 ( 50 classes , Fig. 6 ) but also ESC - 10 ( 10 classes ) . As a result , the sufficient number of training epochs for BC learning on ESC - 10 was 900 , which is smaller than that for BC learning on ESC - 50 ( 1,200 epochs ) , whereas that for standard learning was 600 epochs on both ESC - 50 and ESC - 10 . We assume that the number of training epochs needed would become large when there are many classes , as you have suggested . We will add this discussion to the final version .

( 3 ) Yes , G_1 and G_2 are derived from unit conversion from decibels to amplitudes . We will clarify it . Please see also the reply to AnonReviewer2 .

( 4 ) We have tried random 1 - crop testing and center 1 - crop testing on EnvNet on ESC - 50 ( standard learning ) . The error rates of random 1 - crop testing and center 1 - crop testing were 41.3 % and 39.2 % , respectively , whereas that of 10 - crop testing was 29.2 % as in the paper . Averaging the predictions of multiple windows leads to a stable performance . We assume this is because we can not know where the target sound exists in a testing sound , and the target sound sometimes has a long duration .

This manuscript proposes a method to improve the performance of a generic learning method by generating " in between class " ( BC ) training samples . The manuscript motivates the necessity of such technique and presents the basic intuition . The authors show how the so-called BC learning helps training different deep architectures for the sound recognition task .

My first remark regards the presentation of the technique . The authors argue that it is not a data augmentation technique , but rather a learning method . I strongly disagree with this statement , not only because the technique deals exactly with augmenting data , but also because it can be used in combination to any learning method ( including non - deep learning methodologies ) . Naturally , the literature review deals with data augmentation technique , which supports my point of view .

In this regard , I would have expected comparison with other state - of - the- art data augmentation techniques . The usefulness of the BC technique is proven to a certain extent ( see paragraph below ) but there is not comparison with state - of - the-art . In other words , the authors do not compare the proposed method with other methods doing data augmentation . This is crucial to understand the advantages of the BC technique .

There is a more fundamental question for which I was not able to find an explicit answer in the manuscript . Intuitively , the diagram shown in Figure 4 works well for 3 classes in dimension 2 . If we add another class , no matter how do we define the borders , there will be one pair of classes for which the transition from one to another will pass through the region of a third class . The situation worsens with more classes . However , this can be solved by adding one dimension , 4 classes and 3 dimensions seems something feasible . One can easily understand that if there is one more class than the number of dimensions , the assumption should be feasible , but beyond it starts to get problematic . This discussion does not appear at all in the manuscript and it would be an important limitation of the method , specially when dealing with large - scale data sets .

Overall I believe the paper is not mature enough for publication .

Some minor comments :
- 2.1 : We introduce --> We discussion
- Pieczak 2015 a did not propose the extraction of MFCC .
- the x_i and t_i of section 3.2.2 should not be denoted with the same letters as in 3.2.1.
- The correspondence with a semantic feature space is too pretentious , specially since no experiment in this direction is shown .
- I understand that there is no mixing in the test phase , perhaps it would be useful to recall it .

Thanks for your helpful review .

- Regarding the presentation of BC learning :
It is true that BC learning is a data augmentation method as you have suggested , from a view point of using augmented data . However , our method is novel in that we change the objective of training by training the model to output the mixing ratio , which is fundamentally a different idea from previous data augmentation methods . The novelty or key point of our method is not mixing multiple sounds , but rather learning method of training the model to output the mixing ratio . That is why we represent our method as " learning method . " We intuitively describe why such a learning method is effective in Section 3.3 and demonstrate the effectiveness of BC learning through wide - ranging experiments .

- Regarding comparison with other data augmentation methods :
First , we compared BC learning with other data augmentation methods that mix multiple sounds in ablation analysis ( see Table 2 ) , and showed that our method of mixing just two classes with equation 2 and training the model to output the mixing ratio performs the best .

Second , our BC learning can be combined with any data augmentation methods that do not mix multiple sounds by mixing two augmented data . In Section 4.1.3 , we demonstrated that BC learning is even " compatible " with a strong data augmentation , which we believe is more important than being " stronger " than that . This data augmentation method uses scale and amplitude augmentation similar to Salamon & Bello ( 2017 ) in addition to padding and cropping , and thus , it is close to the state - of - the - art level . As shown in Table 1 , the error rates of DSRNet when using only BC learning ( 18.2 % , 10.6 % , and 23.4 % on ESC - 50 , ESC - 10 , and UrbanSound8K , respectively ) were lower than those when using the strong data augmentation ( 21.2 % , 10.9 % , and 24.9 % ) . Furthermore , as a result of combination of BC learning and the strong data augmentation , we achieved a further higher performance ( 15.1 % , 8.6 % , and 21.7 % ) . In this way , we demonstrated the strongness and compatibility of BC learning with other data augmentation techniques through various experiments .

Here , we assume that the effect of BC learning is even strengthened when using a stronger data augmentation scheme . Because the potential within-class variance becomes large when using a strong data augmentation , the overlap between the feature distribution of each class and that of mixed sounds tends to become large and it becomes more difficult for model to output the mixing ratio ( see also Fig. 2 ) . Therefore , the effect of enlargement of Fisher ’s criterion would become stronger .

- Regarding the limitation of BC learning :
Thanks for your advice . What you have pointed out is correct . However , the dimension of the feature space d is generally designed to be larger than the number of classes c ( e.g. , EnvNet / DSRNet : 4096 ; SoundNet : 256 ; M18 : 512 ; and Logmel-CNN : 5000 ) . If d < c - 1 , the features cannot sufficiently represent categorical information , and the model would not be able to achieve a good performance . We have tried to train an EnvNet whose dimension of fully connected layer was made less than 49 on ESC - 50 with standard learning , but the loss did not begin to decrease . It is not a matter of BC learning . Furthermore , even if there is a network whose d is smaller than c-1 , BC learning would enlarge Fisher 's criterion and regularize the positional relationship as much as possible . Therefore , we do not think it is an important limitation of BC learning .


Thanks for other helpful comments . We will reflect them to the final version . Note than we showed the correspondence with a semantic feature space by visualizing the features of mixed sounds in Fig. 3.

The propose data augmentation and BC learning is relevant , much robust than frequency jitter or simple data augmentation .

In equation 2 , please check the measure of the mixture . Why not simply use a dB criteria ?

The comments about applying a CNN to local features or novel approach to increase sound recognition could be completed with some ICLR 2017 work towards injected priors using Chirplet Transform .

The authors might discuss more how to extend their model to image recognition , or at least of other modalities as suggested .

Section 3.2.2 shall be placed later on , and clarified .

Discussion on mixing more than two sounds leads could be completed by associative properties , we think ... ?


Thanks for your positive review .

- Regarding equation 2 :
We use 10^ ( G_1/20 ) and 10^ ( G_2/20 ) instead of simple G_1 and G_2 to convert decibels to amplitudes . We hypothesize that the ratio of auditory perception for the network is the same as the ratio of amplitude , and define p so that the auditory perception of the mixed sound becomes r : ( 1 - r ) . This is because the main component functions of CNNs , such as conv / fc , relu , max pooling , and average pooling , satisfy homogeneity ( i.e. , f ( ax ) = af ( x ) ) if we ignore the bias . We will clarify the derivation and meaning of equation 2 .

- Regarding how to extend BC learning to other modalities :
We assume that BC learning can also be applied to image classification . Image data can be treated as 2 - D waveforms along x- and y- axis that contain various areas of frequency information in quite a similar manner to sound data . In addition , recent studies on speech / sound recognition have demonstrated that each filter of CNNs learns to respond to a particular frequency area ( e.g. , Sainath et al. , 2015 b ) . Considering them , we assume that CNNs have aspect of recognizing images by treating them as waveforms in a similar manner to how they recognize sounds , and what works on sounds must also work on images . A simple mixing method ( r x_1 + ( 1 - r ) x _2 ) would work well , but we assume that a mixing method that treats the images as waveforms ( similar to equation 2 ) leads to a further performance improvement .

- Regarding mixing more than two classes :
Mixing more than two classes would have a similar effect to mixing just two classes . However , the number of class combinations dramatically increases , and it would become difficult to train . Mixing just two classes can directory impose a constraint on the feature distribution ( as we describe in Section 3.3 ) . Therefore , we assume that mixing just two classes is the most efficient . Experimental results also show that mixing two classes performs better than mixing three classes ( see Table 2 ) .

Thanks for other helpful comments . We will reflect them to the final version .

Please see my detailed comments in the " official comment "

The extensive revisions addressed most of my concerns

Quality
======
The idea is interesting , the theory is hand - wavy at best ( ADDRESSED but still a bit vague ) , the experiments show that it works but do n't evaluate many interesting / relevant aspects ( ADDRESSED ) . It is also unclear how much tuning is involved ( ADDRESSED ) .

Clarity
=====
The paper reads OK . The general idea is clear but the algorithm is only provided in vague text form ( and actually changing from sequential to asynchronous without any justification why this should work ) ( ADDRESSED ) leaving many details up the the reader 's best guess ( ADDRESSED ) .

Originality
=========
The idea looks original .

Significance
==========
If it works as advertised this approach would mean a drastic speedup on previously unseen task from the same distribution .

Pros and Cons
============
+ interesting idea
- we do everything asynchronously and in parallel and it magically works ( ADDRESSED )
- many open questions / missing details ( ADDRESSED )

Hey , thanks for the feedback . We ’ve addressed some clarifications in the response to your official comment , titled " Proposed Changes " . We hope that these ideas clear up misunderstandings , and fill in details that may have been explained unclearly .

This paper considers the reinforcement learning problem setup in which an agent must solve not one , but a set of tasks in some domain , in which the state space and action space are fixed . The authors consider the problem of learning a useful set of ‘ sub policies ’ that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution .

I found the paper to be generally well written and the key ideas easy to understand on first pass . The authors should be commended for this . Aside from a few minor grammatical issues ( e.g. missing articles here and there ) , the writing can not be too strongly faulted .

The problem setup is of general interest to the community . Metalearning in the multitask setup seems to be gaining attention and is certainly a necessary step towards building rapidly adaptable agents .

While the concepts were clearly introduced , I think the authors need to make , much more strongly , the case that the method is actually valuable . In that vein , I would have liked to see more work done on elucidating how this method works ‘ under the hood ’ . For example , it is not at all clear how the number of sub policies affects performance ( one would imagine that there is a clear trade off ) , nor how this number should be chosen . It seems obvious that this choice would also affect the subtle dynamics between holding the master policy constant while updating the sub policies and vice versa . While the authors briefly touch on some of these issues in the rationale section , I found these arguments largely unsubstantiated . Moreover , this leads to a number of unjustified hyper - parameters in the method which I suspect would affect the training catastrophically without significant fine-tuning .

There are also obvious avenues to be followed to check / bolster the intuitions behind the method . By way of example , my sense is that the procedure described in the paper uncovers a set of sub policies that form a `good ’ cover for the task space - if so simply plotting out what they policies look like ( or better yet how they adapt in time ) would be very insightful ( the rooms domain is perhaps a good candidate for this ) .

While the key ideas are clearly articulated the practical value of the procedure is insufficiently motivated . The paper would benefit hugely from additional analysis .

Hey , appreciate the feedback .

To address your concern about how performance depends on hyperparameters , we ran additional experiments comparing the effects of various parameter adjustments . See the graph ( https://imgur.com/a/TLyQv), which we have added in Fig.9 on the current revision . ( Default parameters are a sub-policy count of 2 , and a warmup duration of 20 ) . As long as a few minimums are met ( at least 2 sub-policies ) , performance is not overly dependent on fine- tuned parameters . The parameters we describe in the paper can be seen as a “ baseline minimum ” of parameters to reach a strong solution on the various tasks .

Regarding displaying the behavior of sub-policies , we show a decomposition of the three sub-policies discovered in the Maze task in Figure 6 : moving up , right , and down . We display how the policies adapt over time in our supplemental videos , linked on the first page ( https://sites.google.com/site/mlshsupplementals/, specifically https://www.youtube.com/watch?v=9nvjy9aJi50).

This paper proposes a novel hierarchical reinforcement learning method for a fairly particular setting . The setting is one where the agent must solve some task for many episodes in a sequence , after which the task will change and the process repeats . The proposed solution method splits the agent into two components , a master policy which is reset to random initial weights for each new task , and several sub-policies ( motor primitives ) that are selected between by the master policy every N steps and whose weights are not reset on task switches . The core idea is that the master policy is given a relatively easy learning task of selecting between useful motor primitives and this can be efficiently learned from scratch on each new task , whereas learning the motor primitives occurs slowly over many different tasks . To push this motivation into the learning process , the master policy is updated always but the sub-policies are only updated after an extended warmup period ( called the joint- update or training period ) . This experiments include both small domains ( moving to 2D goals and four-rooms ) and more complex physics simulations ( 4 - legged ants and humanoids ) . In both the simple and complex domains , the proposed method ( MLSH ) is able to robustly achieve good performance .

This approach to obtaining complex structured behavior appears impressive despite the amount of temporal structure that must be provided to the method ( the choice of N , the warmup period , and the joint- update period ) . Relying on the temporal structure for the hierarchy , and forcing the master policy to be relearned from scratch for each new task may be problematic in general , but this work shows that in some complex settings , a simple temporal decomposition may be sufficient to encourage the development of reusable motor primitives and to also enable quick learning of meta-policies over these motor-primitives . Moreover , the results show that these temporal hierarchies are helpful in these domains , as the corresponding non-hierarchical methods failed on the more challenging tasks .

The paper could be improved in some places ( e.g. unclear aliases of joint - update or training periods , describing how the parameters were chosen , and describing what kinds of sub-policies are learned in these domains when different parameter choices are made ) .


Thanks for the response . We ’ll fix the typo of “ training period ” -> “ joint -update period ” in the next version . We ’ll also clean up the intuition behind parameter choice ( see our response “ Analysis on Parameter Choice ” ) .

1 ) Summary
This paper proposed a new method for predicting multiple future frames in videos . A new formulation is proposed where the frames ’ inherent noise is modeled separate from the uncertainty of the future . This separation allows for directly modeling the stochasticity in the sequence through a random variable z ~ p ( z ) where the posterior q( z | past and future frames ) is approximated by a neural network , and as a result , sampling of a random future is possible through sampling from the prior p( z ) during testing . The random variable z can be modeled in a time - variant and time - invariant way . Additionally , this paper proposes a training procedure to prevent their method from ignoring the stochastic phenomena modeled by z . In the experimental section , the authors highlight the advantages of their method in 1 ) a synthetic dataset of shapes meant to clearly show the stochasticity in the prediction , 2 ) two robotic arm datasets for video prediction given and not given actions , and 3 ) A challenging human action dataset in which they perform future prediction only given previous frames .



2 ) Pros :
+ Novel / Sound future frame prediction formulation and training for modeling the stochasticity of future prediction .
+ Experiments on the synthetic shapes and robotic arm datasets highlight the proposed method ’s power of multiple future frame prediction possible .
+ Good analysis on the number of samples improving the chance of outputting the correct future , the modeling power of the posterior for reconstructing the future , and a wide variety of qualitative examples .
+ Work is significant for the problem of modeling the stochastic nature of future frame prediction in videos .




3 ) Cons :
Approximate posterior in non-synthetic datasets :
The variable z seems to not be modeling the future very well . In the robot arm qualitative experiments , the robot motion is well modeled , however , the background is not . Given that for the approximate posterior computation the entire sequence is given ( e.g. reconstruction is performed ) , I would expect the background motion to also be modeled well . This issue is more evident in the Human 3.6 M experiments , as it seems to output blurriness regardless of the true future being observed . This problem may mean the method is failing to model a large variety of objects and clearly works for the robotic arm because a very similar large shape ( e.g. robot arm ) is seen in the training data . Do you have any comments on this ?



Finn et al 2016 PNSR performance on Human 3.6M :
Is the same exact data , pre-processing , training , and architecture being utilized ? In her paper , the PSNR for the first timestep on Human 3.6M is about 41 ( maybe 42 ? ) while in this paper it is 38 .



Additional evaluation on Human 3.6M :
PSNR is not a good evaluation metric for frame prediction as it is biased towards blurriness , and also SSIM does not give us an objective evaluation in the sense of semantic quality of predicted frames . It would be good if the authors present additional quantitative evaluation to show that the predicted frames contain useful semantic information [ 1 , 2 , 3 , 4 ] . For example , evaluating the predicted frames for the Human 3.6 M dataset to see if the human is still detectable in the image or if the expected action is being predicted could be useful to verify that the predicted frames contain the expected meaningful information compared to the baselines .



Additional comments :
Are all 15 actions being used for the Human 3.6 M experiments ? If so , the fact of the time - invariant model performs better than the time - variant one may not be the consistent action being performed ( last sentence of 5.2 ) . The motion performed by the actors in each action highly overlaps ( talking on the phone action may go from sitting to walking a little to sitting again , and so on ) . Unless actions such as walking and discussion were only used , it is unlikely the time - invariant z is performing better because of consistent action . Do you have any comments on this ?



4 ) Conclusion
This paper proposes an interesting novel approach for predicting multiple futures in videos , however , the results are not fully convincing in all datasets . If the authors can provide additional quantitative evaluation besides PSNR and SSIM ( e.g. evaluation on semantic quality ) , and also address the comments above , the current score will improve .



References :
[ 1 ] Emily Denton and Vighnesh Birodkar . Unsupervised Learning of Disentangled Representations from Video . In NIPS , 2017 .
[ 2 ] Ruben Villegas , Jimei Yang , Yuliang Zou , Sungryull Sohn , Xunyu Lin , and Honglak Lee. Learning to generate long - term future via hierarchical prediction . In ICML , 2017 .
[ 3 ] Tero Karras , Timo Aila , Samuli Laine , and Jaakko Lehtinen . Progressive Growing of GANs for Improved Quality , Stability , and Variation . arXiv preprint arXiv:1710.10196 , 2017 .
[ 4 ] Tim Salimans , Ian Goodfellow , Wojciech Zaremba , Vicki Cheung , Alec Radford , and Xi Chen . Improved Techniques for Training GANs . In NIPS , 2017 .


Revised review :
Given the authors ' thorough answers to my concerns , I have decided to change my score . I would like to thank the authors for a very nice paper that will definitely help the community towards developing better video prediction algorithms that can now predict multiple futures .

Thank you for your insightful comments and suggestions . We have addressed most of your concerns . Please see our responses below and let us know if you have any further comments on the paper . Thanks !

- " Additional evaluation on Human 3.6M : PSNR is not a good evaluation metric for frame prediction "

Thank you for this suggestion . We ’ve updated the paper ( please look at Figure 7 and 6th paragraph of 5.3 ) to address your comment . In order to investigate the quality difference between SV2P predicted frames and “ Finn et al ( 2016 ) ” , we performed a new experiment in which we used the open sourced version of the object detector from “ Huang et al . ( 2016 ) ” :
https://github.com/tensorflow/models/blob/master/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
to detect the humans inside the predicted frames . We used the confidence of this detection as an additional metric to evaluate the difference between different methods . The results of this comparison which shows higher quality for SV2P can be found in ( newly added ) Figure 7 .


- " Are all 15 actions being used for the Human 3.6 M experiments ? "

We ’ve updated the 2nd bullet point in 5.1 to clear this up in the paper . Yes , we are using all the actions . In regard to changing actions : since the videos are relatively short ( about 20 frames ) , there are n't any videos where the actor changes the behavior in the middle . That said , the identity of the behavior is not the only source of stochasticity , since even within a single action ( e.g. , walking ) , the actor might choose to walk at different speeds and in different directions .


- " I would expect the background motion to also be modeled well . ”

We 've added a discussion of this in Section 5.3 ( paragraph 4 ) . Note that the approximate posterior over z is still trained with the ELBO , which means that it must compress the information in future events . Perfect reconstruction of high -quality images from posterior distributions over latent states is an open problem , and the results in our experiments compare favorably to those typically observed even in single - image VAEs ( e.g. see Xue et al . ( 2016 ) )


- " Finn et al 2016 PNSR performance on Human 3.6M : In her paper , the PSNR for the first timestep on Human 3.6M is about 41 ( maybe 42 ? ) while in this paper it is 38 "

For “ Finn et al. ( 2016 ) ” , we used the open-source version of the code here :
https://github.com/tensorflow/models/tree/master/research/video_prediction
which is a reimplementation of the models used in the Finn et al . ‘ 16 paper . We are not exactly sure where the discrepancy is coming from . However , we would like to point out that whatever issue resulted in slightly slower PSNR for the deterministic model would have affected our model as well , since we used the same code for the base model . Hence , the comparison is still valid .


Quality : above threshold
Clarity : above threshold , but experiment details are missing .
Originality : slightly above threshold .
Significance : above threshold

Pros :

This paper proposes a stochastic variational video prediction model . It can be used for prediction in optionally available external action cases . The inference network is a convolution net and the generative network is using a previously structure with minor modification . The result shows its ability to sample future frames and outperforms with methods in qualitative and quantitive metrics .

Cons :

1 . It is a nice idea and it seems to perform well in practice , but are there careful experiments justifying the 3 - stage training scheme ? For example , compared with other schemes like alternating between 3 stages , dynamically soft weighting terms .

2 . It is briefly mentioned in the context , but has there any attempt towards incorporating previous frames context for z , instead of sampling from prior ? This piece seems much important in the scenarios which this paper covers .

3 . No details about training ( training data size , batches , optimization ) are provided in the relevant section , which greatly reduces the reproducibility and understanding of the proposed method . For example , it is not clear whether the model can generative samples that are not previously seen in the training set . It is strongly suggested training details be provided .

4 . Minor , If I understand correctly , in equation in the last paragraph above 3.1 , z instead of z_t


Thank you for your great comments . comments and constructive criticism . We updated the paper to address all of your comments . Please let us know if you have any more suggestions or comments . Thanks !


- “ No details about training ( training data size , batches , optimization ) are provided in the relevant section ”

Thank you for your great comment . To further investigate the effect of our proposed training method , we conducted more experiments by alternating between different steps of suggested method . The updated Figure 4c reflects the results of this experiments . As it can be seen in this graph , the suggested steps help with both stability and convergence of the model .

We also provided details of the training method in Appendix A to address your comment regarding using soft - terms as well as reproducibility . We will also release the code after acceptance .


- “ It is briefly mentioned in the context , but has there any attempt towards incorporating previous frames context for z , instead of sampling from prior ? This piece seems much important in the scenarios which this paper covers . “

This is one of the future work directions mentioned in the conclusion section . We ’ve expanded this discussion in the conclusion a bit to address this better .


- “ Minor , If I understand correctly , in equation in the last paragraph above 3.1 , z instead of z_t ”

Thank you for the detailed comment . We fixed the typo .


The submission presents a method or video prediction from single ( or multiple ) frames , which is capable of producing stochastic predictions by means of training a variational encoder - decoder model . Stochastic video prediction is a ( still ) somewhat under- researched direction , due to its inherent difficulty .

The method can take on several variants : time-invariant [ latent variable ] vs. time -variant , or action - conditioned vs unconditioned . The generative part of the method is mostly borrowed from Finn et al . ( 2016 ) . Figure 1 clearly motivates the problem . The method itself is fairly clearly described in Section 3 ; in particular , it is clear why conditioning on all frames during training is helpful . As a small remark , however , it remains unclear what the action vector a_t is comprised of , also in the experiments .

The experimental results are good - looking , especially when looking at the provided web site images .
The main goal of the quantitative comparison results ( Section 5.2 ) is to determine whether the true future is among the generated futures . While this is important , a question that remains un - discussed is whether all generated stochastic samples are from realistic futures . The employed metrics ( best PSNR / SSIM among multiple samples ) can only capture the former , and are also pixel - based , not perceptual .

The quantitative comparisons are mostly convincing , but Figure 6 needs some further clarification . It is mentioned in the text that " time - varying latent sampling is more stable beyond the time horizon used during training " . While true for Figure 6 b ) , this statement is contradicted by both Figure 6a ) and 6 c ) , and Figure 6d ) seems to be missing the time - invariant version completely ( or it overlaps exactly , which would also need explanation ) . As such , I 'm not completely clear on whether the time variant or invariant version is the stronger performer .

The qualitative comparisons ( Section 5.3 ) are difficult to assess in the printed material , or even on-screen . The animated images on the web site provide a much better impression of the true capabilities , and I find them convincing .

The experiments only compare to Reed et al . ( 2017 ) / Kalchbrenner et al. ( 2017 ) , with Finn at al. ( 2016 ) as a non-stochastic baseline , but no comparisons to , e.g. , Vondrick et al. ( 2016 ) are given . Stochastic prediction with generative adversarial networks is a bit dismissed in Section 2 with a mention of the mode - collapse problem .

Overall the submission makes a significant enough contribution by demonstrating a ( mostly ) working stochastic prediction method on real data .

Thank you for insightful comments and constructive criticism . We updated the paper to address all of your comments . Please let us know if you have any more suggestions or comments . Thanks !

- " As a small remark , however , it remains unclear what the action vector a_t is comprised of , also in the experiments . "

Thank you for the good point . We ’ve updated the paper to clarify what the actions are for each dataset . Please check 5.1 for more clarification .


- " a question that remains un - discussed is whether all generated stochastic samples are from realistic futures "

We ’ve updated the paper to clarify this issue . Please look at the 2nd paragraph of Section 4 for updates . In short , as we observed empirically from the predicted videos , the output videos are within the realistic possibilities . However , in some cases , the predicted frames are not realistic and are averaging more than one future ( e.g. first random sample in Figure 1- C ) .


- " The employed metrics ( best PSNR / SSIM among multiple samples ) can only capture the former , and are also pixel - based , not perceptual . "

Thank you for the great comment . We ’ve updated the paper ( please look at Figure 7 and 6th paragraph of 5.3 ) to address your comment . In order to investigate the quality difference between SV2P predicted frames and “ Finn et al ( 2016 ) ” , we performed a new experiment in which we used the open sourced version of the object detector from “ Huang et al . ( 2016 ) ” :
https://github.com/tensorflow/models/blob/master/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
to detect the humans inside the predicted frames . We used the confidence of this detection as an additional metric to evaluate the difference between different methods . The results of this comparison which shows higher quality for SV2P can be found in ( newly added ) Figure 7 .


- " time -varying latent sampling is more stable beyond the time horizon used during training " . While true for Figure 6 b ) , this statement is contradicted by both Figure 6a ) and 6 c ) . "

Thank you for the great question . We ’ve updated the paper ( last two paragraphs of 5.2 ) to include your observation . Please note that our original claim was that the time - variant latent seems to be more “ stable ” beyond the time horizon used during training ( which is highly evident in Figure 6 b ) . And we are NOT claiming that time - variant latent generates “ higher quality ” results . However , we agree that this stability is not always the case as it is more evident in late frames of Figure 6a .




The work claims a measure of robustness of networks that is attack -agnostic . Robustness measure is turned into the problem of finding a local Lipschitz constant which is given by the maximum of the norm of the gradient of the associated function . That quantity is then estimated by sampling from the domain of maximization and observing the maximum value of the norm out of those samples . Such a maximum process is then described by the reverse Weibull distribution which is used in the estimation .

The paper closely follows Hein and Andriushchenko ( 2017 ) . There is a slight modification that enlarges the class of functions for which the theory is applicable ( Lemma 3.3 ) . As far as I know , the contribution of the work starts in Section 4 where the authors show how to practically estimate the maximum process through back - prop where mini-batching helps increase the number of samples . This is a rather simple idea that is shown to be effective in Figure 3 . The following section ( the part starting from 5.3 ) presents the key to the success of the proposed measure .

This is an important problem and the paper attempts to tackle it in a computationally efficient way . The fact that the norms of attacks are slightly above the proposed score is promising , however , there is always the risk of finding a lower bound that is too small ( zeros and large gaps in Figure 3 ) . It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score .

Finally , a minor point : Definition 3.1 has a confusing notation , f is a K -valued vector throughout the paper but it also denotes the number that represents the prediction in Definition 3.1 . I believe this is just a typo .

Edit : Thanks for the fixes and clarification of essential parts in the paper .



1 . Regarding the comment : “ The fact that the norms of attacks are slightly above the proposed score is promising , however , there is always the risk of finding a lower bound that is too small ( zeros and large gaps in Figure 3 ) . It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score ” :

We thank the reviewer for bringing this issue to our attention . Indeed , zero and small lower bounds were caused by the unstable MLE solver in scipy . We have fixed this issue by renormalizing samples before MLE and updated the results in Table 4 and Figure 6 in p.11 of the revised paper . In Figure 5 , we show the empirical CDF of the gaps for 100 ImageNet images , and find that most gaps are indeed small . We also report the percentage of images where p-value in K-S test is greater than 0.05 in Figure 3 ( p.8 ) and Table 5 ( p.16 ) . The numbers are all close to 100 % , justifying the hypothesis that the sampled maximum gradient norms follow the reverse Weibull distribution .

2 . Regarding the comment : “ Finally , a minor point : Definition 3.1 has a confusing notation , f is a K -valued vector throughout the paper but it also denotes the number that represents the prediction in Definition 3.1 . I believe this is just a typo ” :

We thank the reviewer for pointing out this typo . We have fixed the typos in Definition 3.1 accordingly .


Summary
========

The authors present CLEVER , an algorithm which consists in evaluating the ( local ) Lipschitz constant of a trained network around a data point . This is used to compute a lower - bound on the minimal perturbation of the data point needed to fool the network .

The method proposed in the paper already exists for classical function , they only transpose it to neural networks . Moreover , the lower bound comes from basic results in the analysis of Lipschitz continuous functions .


Clarity
=====

The paper is clear and well - written .


Originality
=========

This idea is not new : if we search for " Lipschitz constant estimation " in google scholar , we get for example
Wood , G. R. , and B. P. Zhang . " Estimation of the Lipschitz constant of a function . " ( 1996 )
which presents a similar algorithm ( i.e. , estimation of the maximum slope with reverse Weibull ) .


Technical quality
==============

The main theoretical result in the paper is the analysis of the lower- bound on \delta , the smallest perturbation to apply on
a data point to fool the network . This result is obtained almost directly by writing the bound on Lipschitz- continuous function
| f( y ) - f ( x ) | < L || y-x ||
where x = x_0 and y = x_0 + \delta .

Comments :
- Lemma 3.1 : why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity ? Moreover , a Lipschitz - continuous function does not need to be differentiable at all ( e.g. |x | is Lipschitz with constant 1 but sharp at x=0 ) . Indeed , this constant can be easier obtained if the gradient exists , but this is not a requirement .

- ( Flaw ? ) Theorem 3.2 : This theorem works for fixed target - class since g = f_c - f_j for fixed g . However , once g = min_j f_c - f_j , this theorem is not clear with the constant Lq. Indeed , the function g should be
g ( x ) = min_{k \neq c} f_c ( x ) - f_k ( x ) .
Thus its Lipschitz constant is different , potentially equal to
L_q = max_{k} \| L_q^k \| ,
where L_q^k is the Lipschitz constant of f_c-f_k . If the theorem remains unchanged after this modification , you should clarify the proof . Otherwise , the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened .

- Theorem 4.1 : I do not see the purpose of this result in this paper . This should be better motivated .


Numerical experiments
====================

Globally , the numerical experiments are in favor of the presented method . The authors should also add information about the time it takes to compute the bound , the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower - bound and the best adversarial example .

Moreover , the numerical experiments look to be realized in the context of targeted attack . To show the real effectiveness of the approach , the authors should also show the effectiveness of the lower - bound in the context of non- targeted attack .


#######################################################

Post-rebuttal review
---------------------------

Given the details the authors provided to my review , I decided to adjust my score . The method is simple and shows to be extremely effective / accurate in practice .

Detailed answers :

1 ) Indeed , I was not aware that the paper only focuses on one dimensional functions . However , they still work with less assumption , i.e. , with no differential functions . I was pointing out the similarities between their approach and your : the two algorithms ( CLEVER and Slope ) are basically the same , and using a limit you can go from " slope " to " gradient norm " .
In any case , I have read the revision and the additional numerical experiment to compare Clever with their method is a good point .

2 ) " Overall , our analysis is simple and more intuitive , and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work . "
This is right . I am just surprised is has not been done before , since it requires only few lines of derivation . I searched a bit but it is not possible to find any kind of similar results . Moreover , this leads to good performances , so there is no needs to have something more complex .

3 ) " The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward "
Indeed , people usually use the Lipschitz continuity using the L2norm , but the original definition is wider .
Quickly , if you have a differential , scalar function from a space E -> R , then the gradient is a function from space E to E* , the dual of the space E.
Let || . || the norm of space E. Then , || . ||* is the dual norm of ||.|| , and also the norm of E *.
In that case , Lipschitz continuity writes
f ( x ) - f ( y ) <= L || x - y || , with L >= max_{x in E *} || f' ( x ) ||*
In the case where || . || is an \ell - p norm , then || . ||* is an \ell - q norm ; with 1 / p +1 / q = 1.

If you are interested , there is a clear and concise explanation in the introduction of this paper : Accelerating the cubic regularization of Newton ’s method on convex problems , by Yurii Nesterov .

I have no additional remarks for 4 ) -> 9 ) , since everything is fixed in the new version of the paper .




7 . Regarding the comment : “ Globally , the numerical experiments are in favor of the presented method . The authors should also add information about the time it takes to compute the bound , the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower - bound and the best adversarial example . ” :

We thank the reviewer for this suggestion . Following your suggestion , we have included additional experimental results in Section 5.4 - Time v.s. Estimation Accuracy . In Figure 7 , we vary the number of samples ( N_b=50,100,250,500 ) and compute the L2 CLEVER scores for three large ImageNet models , Inception - v3 , ResNet - 50 and MobileNet . We observe that 50 or 100 samples are usually sufficient to get a reasonably accurate robustness estimation despite using a smaller number of samples . On a single GTX 1080 Ti GPU , the cost of 1 sample ( with N_s = 1024 in Algorithm 1 ) is measured as 1.2 s for MobileNet , 5.5 s for ResNet - 50 and 7.3 s for Inception - v3 , thus the computational cost of CLEVER is feasible for state - of - the- art large - scale deep neural networks . Additional figures for MNIST and CIFAR datasets are given in Figure 9 in Appendix E2 . We also added Figure 5 to show the empirical CDF of the gap between CLEVER score and the L2 distortion founded by CW attacks ( the best attack ) for 3 imagenet networks with random targets . It shows that at least 80 % of the images have small gaps , demonstrating the effectiveness of our approach .

8 . Regarding the comment : “ Moreover , the numerical experiments look to be realized in the context of targeted attack . To show the real effectiveness of the approach , the authors should also show the effectiveness of the lower- bound in the context of non-targeted attack . ” :

We thank the reviewer for this important suggestion . Following your suggestion , we have added the experiments of un - targeted attack in Section 5.3 . The results comparing average untargeted clever score and distortion found by CW and I - FGSM attacks are summarized in Table 2 . We show that CW and I - FGSM attack results agree with the predicted robustness by CLEVER score , demonstrating the effectiveness of our approach .

9 . Finally , we thank again the reviewer for the positive comments on the clarity of our paper and we hope our answers above were able to address all the comments regarding originality and technical contributions of our paper . As suggested by the reviewer , in the current version of paper , we have included three sets of new experimental results regarding
( 1 ) untargeted attacks ( Section 5.3 , Table 2 )
( 2 ) comparison to the slope sampling method of Wood & Zheng ( 1996 ) paper ( Section 5.3 , Table 3 , Figure 4 )
( 3 ) more numerical results of previous experiments ( Section 5.3 , Figure 5 , Figure 7 and Figure 9 )
to show the advantage of our proposed method .

As we highly value all reviewers ’ inputs , we would like to use this opportunity to ask for your comments on the updated version during the author rebuttal stage . We believe we have carefully addressed all of your concerns , and we sincerely hope you could reconsider your decision .



4 . Regarding the comment : “ Moreover , a Lipschitz - continuous function does not need to be differentiable at all ( e.g. |x | is Lipschitz with constant 1 but sharp at x=0 ) . Indeed , this constant can be easier obtained if the gradient exists , but this is not a requirement ” :

We thank the reviewer for this comment . Indeed , as we show in Lemma 3.3 , we can easily extend our analysis using the Lipschitz assumption to obtain the robustness guarantee for non-differentiable functions with a finite number of non-differentiable points ( like networks with ReLU activations ) .

5 . Regarding the comment : “ ( Flaw ? ) Theorem 3.2 : This theorem works for fixed target - class since g = f_c - f_j for fixed g . However , once g = min_j f_c - f_j , this theorem is not clear with the constant Lq. Indeed , the function g should be
g ( x ) = min_{k \neq c} f_c ( x ) - f_k ( x ) .
Thus its Lipschitz constant is different , potentially equal to
L_q = max_{k} \| L_q^k \| ,
where L_q^k is the Lipschitz constant of f_c-f_k . If the theorem remains unchanged after this modification , you should clarify the proof . Otherwise , the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened . ” :

We thank the reviewer for pointing out this potential ambiguity . There was an abuse of notation in Theorem 3.2 where the Lipschitz constant L_q is the lipschitz constant for function f_c-f_j , which is dependent on the index j . We have revised the notation accordingly in the revised paper and we use L_q^j to denote it is a Lipschitz constant of function ( f_c - f_j ) and is dependent on index j . For the untargeted attack that the reviewer is referring to , we note that Theorem 3.2 is indeed for un - targeted attacks , as it takes the min over all the targeted attack bound . We have made it clearer in the revised paper by adding a note of “ Formal guarantee on lower bound for untargeted attack ” in Theorem 3.2 . In comparison , we also added Corollary 3.2.2 to give the formal guarantee for * targeted * attack . The algorithms for computing CLEVER for targeted and untargeted attacks are summarized in Algorithm 1 and 2 in Section 4.2 . We note that we also included additional experiments for untargeted attacks in Table 2 in Section 5.3.

6 . Regarding the comment : “ Theorem 4.1 : I do not see the purpose of this result in this paper . This should be better motivated . ” :

We thank the reviewer for pointing this important observation . In the revised paper , we give a clearer explanation in the beginning of Section 4.1 of why we derive the CDF of $ ||\nabla g( x ) ||_q$ . The reason is that in this work , we propose to use a new sampling method and extreme value theory to estimate the local Lipschitz constant ; extreme value theory requires samples in a distribution of $ ||\nabla g( x ) ||_q$ . A reader may wonder how the this distribution looks like . As an example , we show that we can derive the CDF of $ ||\nabla g( x ) ||_q $ for a 2 - layer neural network with ReLU activation in Theorem 6.1 in Appendix D.



We thank the reviewer for the positive comments on the clarity of our paper . However , we believe there might be some misunderstanding on the originality and technical quality of our work . Please allow us to clarify below .

1 . Regarding the comment : “ This idea is not new : if we search for " Lipschitz constant estimation " in google scholar , we get for example Wood , G. R. , and B. P. Zhang . " Estimation of the Lipschitz constant of a function . " ( 1996 ) which presents a similar algorithm ( i.e. , estimation of the maximum slope with reverse Weibull ) ” :

We thank the reviewer for pointing out this very early work of local Lipschitz constant estimation . We note that their sampling methodology is entirely different from our approach , as they estimate the Lipschitz constant by calculating the “ slope ” between pairs of sample points whereas in this paper we take the samples on the norm of the gradient directly . As “ slope ” is an approximation of gradient norm , it is conceivably ( and also verified by our experiments in section 5.3 , Table 3 and Figure 4 ) that the estimation will be less accurate than our method of directly computing the max norm of gradient . In addition , they only justified Lipschitz constant estimation for an *one-dimensional * function whereas our classifier function is very high -dimensional ( d = 784 for MNIST , 3072 for CIFAR , 150,528 for ImageNet ) . In fact , how to accurately estimate Lipschitz constant for a high -dimensional function is still an open question . In this paper , we proposed to estimate Lipschitz constant by directly computing max norm of the gradient for the samples and using extreme value theory . As we show in Table 3 and Figure 4 in p.10 of the revised paper , Wood and Zhang ’s ( 1996 ) approach ( denoted as SLOPE ) performs poorly on estimating Lipschitz constant for high - dimensional functions ( i.e. , neural net classifiers ) and hence it is not suitable to use their method to evaluate adversarial perturbations in neural networks .

2 . Regarding the comment : “ The main theoretical result in the paper is the analysis of the lower - bound on \delta , the smallest perturbation to apply on a data point to fool the network . This result is obtained almost directly by writing the bound on Lipschitz-continuous function ” :

We thank the reviewer for this comment . Although our analysis is intuitive and straightforward , to the best of our knowledge , this is the * first * work that directly uses Lipschitz continuity to prove such a perturbation analysis . In comparison , Hein & Andriushchenko ( 2017 ) implicitly assumed Lipschitz continuity but used mean value theorem and Holder ’s inequality in their analysis , which is not straightforward to achieve the same result , as also suggested by the reviewer . In addition to the difference in derivation of the bound , we would like to emphasize that our analysis can be easily extended to non-differentiable functions with a finite number of non-differentiable points , whereas Hein & Andriushchenko ’s analysis is restricted to continuously differentiable functions . Overall , our analysis is simple and more intuitive , and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work .

3 . Regarding the comment : “ Lemma 3.1 : why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity ? ” :

Lemma 3.1 is not just the definition of Lipschitz continuity ; it also gives the relationship between ( local ) Lipschitz constant in general Lp ( p>=1 ) norm and the dual norm of gradient . The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward , thus we refer readers to Paulavicius and Zilinskas paper .


Dear AnonReviewer 2 ,

Following your valuable comments and suggestions , we have addressed the confusions in our theory , made comparisons with the additional reference you mentioned and added new numerical results . We have listed all changes we made in the general response to help you quickly find out the added materials . Thanks to your insightful comments , we believe our paper has been greatly improved after addressing all the concerns raised . For responses to any particular questions , please kindly read the corresponding section of our rebuttal .

We will greatly appreciate it if you can provide new comments on the revised version of our paper . Thank you !

Sincerely ,
Authors of Paper 767

In this work , the objective is to analyze the robustness of a neural network to any sort of attack .

This is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function . This approach is quite standard in learning theory , I am not aware of how original this point of view is within the deep learning community .

This is estimated by obtaining values of the norm of the gradient ( also naturally linked to the Lipschitz properties of the function ) by backpropagation . This is again a natural idea .


1 . Regarding the comment of using local Lipschitz properties of the network function :

We thank the reviewer for pointing this out . We note that this paper is the * first * work to derive the lower bound of minimum distortion using ( local ) cross -Lipschitz continuity assumption . For continuously differentiable classification functions , we show that with the Lipschitz continuity assumption , our result is consistent with Hein & Andriushchenko ( 2017 ) , who used Mean Value Theorem and Holder ’s inequality to obtain the same lower bound . In addition , we show in Lemma 3.3 that our approach can easily extend to non-differentiable functions ( e.g. ReLU activations ) , whereas the analysis in Hein & Andriushchenko ( 2017 ) is restricted to continuously differentiable functions .

2 . Regarding the comment of using the norm of the gradient by backpropagation to estimate Lipschitz constant :

We note that there exist other estimation methods , e.g. Wood & Zhang ( 1996 ) as mentioned by AnonReviewer 2 , where they calculate the slope between pairs of sample points instead of taking the samples on the norm of the gradient in this paper . However , as shown in Table 3 and Figure 4 in p.10 of the revised paper , their approach ( denoted as SLOPE ) perform poorly on estimating Lipschitz constant for high - dimensional functions like neural networks , thus are not suitable to estimate minimum adversarial distortions .


This paper studies input discretization and white - box attacks on it to make deep networks robust to adversarial examples . They propose one - hot and thermometer encodings as input discretization and
also propose DGA and LS- PGA as white - box attacks on it .
Robustness to adversarial examples for thermometer encoding is demonstrated through experiments .

The empirical fact that thermometer encoding is more robust to adversarial examples than one - hot encoding ,
is interesting . The reason why thermometer performs better than one - hot should be pursued more .

[ Strong points ]
* Propose a new type of input discretization called thermometer encodings .
* Propose new white - box attacks on discretized inputs .
* Deep networks with thermometer encoded inputs empirically have higher accuracy on adversarial examples .

[ Weak points ]
* No theoretical guarantee for thermometer encoding inputs .
* The reason why thermometer performs better than one - hot has not unveiled yet .

[ Detailed comments ]
Thermometer encodings do not preserve pairwise distance information .
Consider the case with b_1=0.1 , b_2=0.2 , b_3=0.3 , b_4=0.4 and x_i=0.09 , x_j=0.21 and x_k=0.39 .
Then , 0.12=|x_j-x_i|<|x_k-x_j|=0.18 but ||tau ( b( x_i ) ) - tau ( n ( x_j ) ) ||_2=sqrt ( 2 )>1=||tau ( b( x_k ) ) - tau ( n( x_j ) ) ||_2.

Thank you for your feedback ! The finding that thermometer encodings perform better than one - hot encodings was a primarily empirical finding , and further exploration is certainly needed . The goal of this work was primarily to establish the strength of discretized encodings , and thermometer encodings in particular . We currently believe that the primary reason that thermometer encodings perform better and exhibit smoother convergence than one - hot encodings in the adversarial defense regime is the superior inductive bias ; nearby pixel values typically have similar semantic content .

You are correct that thermometer encodings do not preserve pairwise distance information ; our statement was true only in the case where the number of discretization levels is equal to the number of possible pixel values , as in the PixelRNN paper . Since this is not realistic in most settings , we have weakened our claim to instead state that thermometer encodings maintain ordering information , which is true in all cases .

This is a beautiful work that introduces both ( 1 ) a novel way of defending against adversarial examples generated in a black - box or white - box setting , and ( 2 ) a principled attack to test the robustness of defenses based on discretized input domains . Using a binary encoding of the input to reduce the attack surface is a brilliant idea . Even though the dimensionality of the input space is increased , the intrinsic dimensionality of the data is drastically reduced . The direct relationship between robustness to adversarial examples and intrinsic dimensionality is well known ( paper by Fawzi . ) . This article exploits this property nicely by designing an encoding that preserves pairwise distances by construction . It is well written overall , and the experiments support the claims of the authors .

This work has a crucial limitation : scalability .
The proposed method scales the input space dimension linearly with the number of discretization steps . Consequently , it has a significant impact on the number of parameters of the model when the dimensionality of the inputs is large . All the experiments in the paper report use relatively small dimensional datasets . For larger input spaces such as Imagenet , the picture could be entirely different :

- How would thermometer encoding impact the performance on clean examples for larger dimensionality data ( e.g. , Imagenet ) ?
- Would the proposed method be significantly different from bit depth reduction in such setting ?
- What would be the impact of the hyper- parameter k in such configuration ?
- Would the proposed method still be robust to white box attack ?
- The DGA and LS- PGA attacks look at all buckets that are
within ε of the actual value , at every step . Would this be feasible in a large dimensional setting ? More generally , would the resulting adversarial training technique be practically possible ?

While positive results on Imagenet would make this work a home run , negative results would not affect the beauty of the proposal and would shed critical light on the settings in which thermometer encoding is applicable . I lean on the accept side , and I am willing to increase the score greatly if the above questions are answered .

Thank you for your feedback ! We agree that learning more about the scaling properties would be enormously useful , but unfortunately , running adversarial training on full - size Image Net simply proved too challenging . In our first attempt , we were forced to reduce our batch size in order to fit everything into memory , but this lead to poor convergence . We have now scaled up our resources in order to run it properly , and we hope to have results within the next few weeks ; however , these experiments are still ongoing , and we have no results to report at this time . Unfortunately , since most of your concerns are empirical , this means that we cannot properly address them .

One note is that when setting up these experiments , the primary bottleneck was the memory consumption of the model itself , especially under multiple steps of attack . It ’s true that increasing the number of discretization levels causes a linear increase in memory consumption proportional to the size of the input , but this is negligible compared to the memory usage of the actual model : in our Wide ResNet implementation , we estimate the memory used by the first layer ( and thus multiplied by the input discretization step ) to be only 1 - 2 % of the overall memory used . Both vanilla and 16 - level - thermometer - encoded inputs were subject to approximately the same constraints on batch sizes , steps of attack , etc. , when using a wide ResNet with width of 4 and depth of 16 .

In addition to having a relatively small proportional memory increase , input discretization requires relatively few additional model parameters : 0.03 % extra parameters for MNIST , 0.08 % for CIFAR - 10 and CIFAR - 100 , and 2.3 % for SVHN , as described in section 5 . This indicates that the model size will not be a bottleneck in scaling discretized models .

The authors present an in - depth study of discretizing / quantizing the input as a defense against adversarial examples . The idea is that the threshold effects of discretization make it harder to find adversarial examples that only make small alterations of the image , but also that it introduces more non- linearities , which might increase robustness . In addition , discretization has little negative impact on the performance on clean data . The authors also propose a version of single - step or multi-step attacks against models that use discretized inputs , and present extensive experiments on MNIST , CIFAR - 10 , CIFAR - 100 and SVHN , against standard baselines and , on MNIST and CIFAR - 10 , against a version of quantization in which the values are represented by a small number of bits .

The merits of the paper is that the study is rather comprehensive : a large number of datasets were used , two types of discretization were tried , and the authors propose an attack mechanism better that seems reasonable considering the defense they consider . The two main claims of the paper , namely that discretization does n't hurt performance on natural test examples and that better robustness ( in the author 's experimental setup ) is achieved through the discretized encoding , are properly backed up by the experiments .

Yet , the applicability of the method in practice is still to be demonstrated . The threshold effects might imply that small perturbations of the input ( in the l_infty sense ) will not have a large effect on their discritized version , but it may also go the other way : an opponent might be able to greatly change the discretized input without drastically changing the input . Figure 8 in the appendix is a bit worrysome on that point , as the performance of the discretized version drops rapidly to 0 when the opponents gets a bit stronger . Did the authors observe the same kind of bahavior on other datasets ? What would the authors propose to mitigate this issue ? To what extend the good results that are exhibited in the paper are valid over the wide range of opponent 's strengths ?

minor comment :
- the experiments on CIFAR - 100 in Appendix E are carried out by mixing adversarial / clean examples while training , whereas those on SVHN in Appendix F use adversarial examples only .


Thank you for your feedback !

> ... the performance of the discretized version drops rapidly to 0 when the opponents gets a bit stronger .

We observe this behavior on the datasets we tested on , which were MNIST and CIFAR . ( CIFAR does not drop all the way to 0 , but does drop sharply . ) We believe that this is an expected result , stemming from the intuition that the relationship between the input and the loss is highly nonlinear . When presented with an input which it has never been exposed to ( i.e. a pixel has been moved into a bucket that is beyond the adversarial training threshold ) , the effect on the loss is highly random . Many of these perturbed inputs will increase the loss , and it is therefore easy to find an adversarial example .

Controlling for the wide range of opponent ’s strengths is an important issue , one which is endemic to adversarial defenses in general . The “ standard setting ” for the adversarial example problem ( in which we constrain the L-infinity norm of the perturbed image to an epsilon ball around the original image ) was designed to ensure that any adversarially - perturbed image is still recognizable as its original image by a human . However , this artificial constraint excludes many other potential attacks that also result in human-recognizable images . State -of - the art defenses in the standard setting can still be easily defeated by non-standard attacks ; for recent examples of this , see ICLR submission “ Adversarial Spheres ” ( appendix A ) , as well as “ Adversarial Patch ” by Brown et. al ( https://arxiv.org/abs/1712.09665).

With this in mind , we believe that the fact that the performance of thermometer - encoded models degrades more quickly than that of vanilla models beyond the training epsilon is a weakness , but no worse in practice than other defenses . A “ larger epsilon ” attack is just one special case of a “ non-standard ” attack ; there are an enormous number of other non-standard attacks , some of which are more effective against vanilla models , some of which are more effective against thermometer encodings , and some of which are devastating to both . If we permit non-standard attacks , a fair comparison would show that all current approaches are easily breakable . There is nothing special about the “ larger epsilon ” attack that makes a vulnerability to this non-standard attack in particular more problematic than vulnerabilities to other non-standard attacks , in practice .

Additionally , on the CIFAR dataset , we found that even though discretized inputs are impacted much more severely by examples perturbed by more than the training threshold , the discretized models are sufficiently strong to begin with that they still outperform real - valued models even after this vulnerability has been exploited . ( See updated Figure 8b. ) CIFAR is more reflective of real - world datasets , so even with this weakness , thermometer - encoded models may outperform real - valued models in practice .

Based on your feedback , we updated our submission to include this discussion in Appendix G , and added Figure 8 b showing the CIFAR results . Also , we discovered a bug which caused the unquantized attack in Figure 8 to be too weak ; essentially , we were using a fixed step size of 0.01 for 40 steps which caused the perturbation to never hit the boundary for epsilon > 0.4 . We have updated the figure to reflect the correct values . ( The fixed results are qualitatively equivalent , so this change does not affect the conclusions . )


* sec.2.2 is about label - preserving translation and many notations are introduced . However , it is not clear what label here refers to , and it does not shown in the notation so far at all . Only until the end of sec.2.2 , the function F ( . ) is introduced and its revelation - Google Search as label function is discussed only at Fig.4 and sec .2.3.
* pp.5 first paragraph : when assuming D_X and D_Y being perfect , why L_GAN_forward = L_GAN_backward = 0 ? To trace back , in fact it is helpful to have at least a simple intro / def. to the functions D ( . ) and G ( . ) of Eq. ( 1 ) .
* Somehow there is a feeling that the notations in sec.2.1 and sec.2.2 are not well aligned . It is helpful to start providing the math notations as early as sec.2.1 , so labels , pseudo labels , the algorithm illustrated in Fig .2 etc. can be consistently integrated with the rest notations .
* F ( ) is firstly shown in Fig .2 the beginning of pp.3 , and is mentioned in the main text as late as of pp .5.
* Table 2 : The CNN baseline gives an error rate of 7.80 while the proposed variants are 7.73 and 7.60 respectively . The difference of 0.07/ 0.20 are not so significant . Any explanation for that ?
Minor issues :
* The uppercase X in the sentence before Eq. ( 2 ) should be calligraphic X

Dear Reviewer1 ,

First of all , we really appreciate your constructive review and detailed comments on our work . We would like to share our responses to the concerns raised by the reviewer .

1 . pp.5 first paragraph : when assuming D_X and D_Y being perfect , why L_GAN_forward = L_GAN_backward = 0 ? To trace back , in fact it is helpful to have at least a simple intro / def. to the functions D ( . ) and G ( . ) of Eq. ( 1 ) .
= > Assume a perfect discriminator that always outputs 1 if the image is from the target ( real ) domain and 0 if it is from the source ( fake ) domain , regardless of how precise the generators are . Then , the standard GAN loss becomes 0 since L_{GAN } ( X ,Y ) = E_{Y} [ log ( 1 ) ] + E_{X} [ log ( 1 - 0 ) ] = 0 . To make the descriptions clearer , we added the definition of G ( ) to Sec 2.1 along with other notations , and the definition of D ( ) before equation ( 1 ) .

2 . Somehow there is a feeling that the notations in sec.2.1 and sec.2.2 are not well aligned . It is helpful to start providing the math notations as early as sec.2.1 , so labels , pseudo labels , the algorithm illustrated in Fig .2 etc. can be consistently integrated with the rest notations .
= > We appreciate your great suggestion . As mentioned above , we included a separate subsection on notations and definitions . Further , we modified Sec. 2 to align the notations used in the two different sections .

3 . F ( ) is firstly shown in Fig .2 the beginning of pp.3 , and is mentioned in the main text as late as of pp .5.
= > We fixed this .

4 . Minor issues : * The uppercase X in the sentence before Eq. ( 2 ) should be calligraphic X
= > We fixed this .

5 . Table 2 : The CNN baseline gives an error rate of 7.80 while the proposed variants are 7.73 and 7.60 respectively . The difference of 0.07/ 0.20 are not so significant . Any explanation for that ?
= > We would like to first clarify that the CNN baseline alone gives error rate of 11.2 . The scheme that achieves the error rate of 7.80 is not a simple CNN baseline but the SimGAN ( CVPR ’ 17 ) approach ( forward mapping + training a predictor on the forward - translated images ) , which is the previous state - of - the-art . With regards to the significance of the improvements , we believe that the error rate achieved by our algorithm is very close to the best error rate that one can hope for . Hence , in this regime , improving the state - of - the - art performance even by a small margin requires much efforts .

Thanks ,
Authors

Review , ICLR 2018 , Simulated + Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings

Summary :

The paper presents several extensions to the method presented in SimGAN ( Shirvastava et al. 2017 ) .
First , it adds a procedure to make the distribution of parameters of the simulation closer to the one in real world images . A predictor is trained on simulated images created with a manually initialized distribution . This predictor is used to estimate pseudo labels for the unlabeled real - world data . The distribution of the estimated pseudo labels is used produce a new set of simulated images . This process is iterated .
Second , it adds the idea of cycle consistency ( e.g. , from CycleGAN ) in order to counter mode collapse and label shift .
Third , since cycle consistency does not fully solve the label shift problem , a feature consistency loss is added .
Finally , in contrast to ll related methods , the final system used for inference is not a predictor trained on a mix of real and “ fake ” images from the real - world target domain . Instead the predictor is trained purely on synthetic data and it is fed real world examples by using the back / real - to-sim -generator ( trained in the conjunction with the forward mapping cycle ) to map the real inputs to “ fake ” synthetic ones .

The paper is well written . The novelty is incremental in most parts , but the overall system can be seen as novel .

In particular , I am not aware of any published work that uses of the ( backwards ) real - to- sim generator plus sim- only trained predictor for inference ( although I personally know several people who had the same idea and have been working on it ) . I like this part because it perfectly makes sense not to let the generator hallucinate real - world effects on rather clean simulated data , but the other way around , remove all kinds of variations to produce a clean image from which the prediction should be easier .

The paper should include Bousmalis et al. , “ Unsupervised Pixel - Level Domain Adaptation With Generative Adversarial Networks ” , CVPR 2017 in its discussion , since it is very closely related to Shirvastava et al. 2017 .

With respect to the feature consistency loss the paper should also discuss related work defining losses over feature activations for very similar reasons , such as in image stylization ( e.g. L. A. Gatys et al . “ Image Style Transfer Using Convolutional Neural Networks ” CVPR 2016 , L. A. Gatys et al . “ Controlling Perceptual Factors in Neural Style Transfer ” CVPR 2017 ) , or the recently presented “ Photographic Image Synthesis with Cascaded Refinement Networks ” , ICCV 2017 .
Bousmalis et al. , “ Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping ” , arXiv :1709.07857 , even uses the same technique in the context of training GANs .

Adaptive Data Generation :
I do not fully see the point in matching the distribution of parameters of the real world samples with the simulated data . For the few , easily interpretable parameters in the given task it should be relatively easy to specify reasonable ranges . If the simulation in some edge cases produces samples that are beyond the range of what occurs in the real world , that is maybe not very efficient , but I would be surprised if it ultimately hurt the performance of the predictor .
I do see the advantage when training the GAN though , since a good discriminator would learn to pick out those samples as generated . Again , though , I am not very sure whether that would hurt the performance of the overall system in practice .

Limiting the parameters to those values of the real world data also seems rather restricting . If the real world data does not cover certain ranges , not because those values are infeasible or infrequent , but just because it so happens that this range was not covered in the data acquisition , the simulation could be used to fill in those ranges .
Additionally , the whole procedure of training on sim data and then pseudo- labeling the real data with it is based on the assumption that a predictor trained on simulated data only already works quite well on real data . It might be possible in the case of the task at hand , but for more challenging domain adaptation problem it might not be feasible .
There is also no guarantee for the convergence of the cycle , which is also evident from the experiments ( Table 1 . After three iterations the angle error increases again . ( The use of the Hellinger distance is unclear to me since it , as explained in the text , does not correspond with what is being optimized ) . In the experiments the cycle was stopped after two iterations . However , how would you know when to stop if you did n’t have ground truth labels for the real world data ?

Comparisons :
The experiments should include a comparison to using the forward generator trained in this framework to train a predictor on “ fake ” real data and test it on real data ( ie. a line “ ours | RS | R | ? ” in Table 2 , and a more direct comparison to Shrivastava ) . This would be necessary to prove the benefit of using the back - generator + sim trained predictor .


Detailed comments :
* Figure 1 seems not to be referenced in the text .
* I do n’t understand the choice for reduction of the sim parameters . Why was , e.g. , the yaw and pitch parameters of the eyeball set equal to those of the camera ? Also , I guess there is a typo in the last equality ( pitch and yaw of the camera ? ) .
* The Bibliography needs to be checked . Names of journals and conferences are inconsistent , long and short forms mixed , year several times , “ Proceedings ” multiple times , ...


Dear Reviewer3 ,

First of all , we really appreciate your constructive review and detailed comments on our work . We would like to share our responses to the concerns raised by the reviewer .

1 . In particular , I am not aware of any published work that uses of the ( backwards ) real - to- sim generator plus sim - only trained predictor for inference …
= > We thank the reviewer for constructive feedback . We agree that our backward approach has some advantages over the forward approach exactly due to the reasons the reviewer mentioned .

2 - 1 . The paper should include Bousmalis et al …
2 - 2 . With respect to the feature consistency loss the paper should also discuss related work …
= > We thank the reviewer for sharing with us the key reference on the S+U learning and the works that proposed the feature consistency . We added them in our revision .

3 . Adaptive Data Generation : I do not fully see the point in matching the distribution of parameters of the real world samples with the simulated data ..
= > We agree that having samples that are beyond the range of real sample should not hurt the prediction performance . As the reviewer pointed out , the adaptive data generation part is for generating synthetic samples in a “ sample - efficient ” way . To see this , consider the following genie-aided experiments . The first label distribution of the simulator is set such that the mean and variance match those of the true label distribution , measured from the test set . The second one is set such that the mean is the same but the variance is twice larger than that of the ture one . ( Note that this also generates all possible labels . ) We ran our algorithms with these label distributions , and observed that the first one achieves the error rate of “ 7.74 ” and the second one achieves “ 8.88 ” . As the reviewer expected , we believe that the gap will diminish as the dataset size grows .

4 . Limiting the parameters to those values of the real world data also seems rather restricting …
= > If the real data provided in the training set is not representative enough , our approach may not be able to generalize well on unseen data . One may address this limitation by incorporating prior knowledge about the label distributions or by manually tuning the parameters . A thorough study is needed to understand how one could obtain diverse synthetic images via such methods in a systematic way . We added a remark on this in the revised draft .

5 . Additionally … the assumption that a predictor trained on simulated data only already works quite well on real data …
= > Indeed , our adaptive data generation method assumes that the predictor trained on simulated data works quite well on real data . Hence , if the predictor trained solely on simulated data provide completely wrong pseudo-labels , matching the synthetic label distribution with the pseudo- label distribution may not be helpful at all . For instance , when we pseudo- labeled the images in the SVHN dataset using a digit classifier that is trained on the MNIST dataset , our first stage failed to refine the synthetic label distribution . It is an interesting open question whether or not one can devise a similar adaptive data generation method for such cases . We added a remark on this limitation in the revised draft .

6 . There is also no guarantee for the convergence of the cycle … However , how would you know when to stop …?
= > Yes , the convergence is not guaranteed , and hence one needs to choose the right result to proceed with . Actually , we mentioned in the first manuscript that we made use of a small validation set ( 1 % of the test data set ) consisting of real data with labels to do so . For clarity , we also added the validation errors to the table .

7 . The use of the Hellinger distance is unclear to me since it , as explained in the text , does not correspond with what is being optimized .
= > We used the Hellinger distance for quantifying the progress of the adaptive data generation algorithm . To clarify this , we added the sequence of means and variances to Table 1 instead of the Hellinger distance .

8 . The experiments should include a comparison to using the forward generator trained in this framework to train a predictor on “ fake ” real data and test it on real data ( ie. a line “ ours | RS | R | ? ” in Table 2 , and a more direct comparison to Shrivastava ) … .
= > We added the performance corresponding to “ ours | RS | R ” to Table 2 : It achieves a performance that is slightly better than the SimGAN ’s one but worse than that of our back - generator .

9 . I do n’t understand the choice for reduction of the sim parameters …
= > Our internal experiments ( not reported in the earlier draft ) revealed that the choice of parameter reduction barely affects the performance of the algorithm . For clarification , in the revision ( see appendix ) , we added experimental results comparing two different reduction methods .

10 . Figure 1 not referenced & A typo in the last equality & The Bibliography needs to be checked
= > We fixed them .

Thanks ,
Authors


General comment :

This paper proposes a GAN - based method which learns bidirectional mappings between the real - data and the simulated data . The proposed methods builds upon the CycleGAN and the Simulated + Unsupervised ( S+U ) learning frameworks . The authors show that the proposed method is able to fully leverage the flexibility of simulators by presenting an improved performance on the gaze estimation task .

Detailed comments :

1 . The proposed method seems to be a direct combination of the CycleGAN and the S+U learning . Firstly , the CycleGAN propose a to learn a bidirectional GAN model between for image translation . Here the author apply it by " translating " the the simulated data to real - data . Moreover , the mapping from simulated data to the real - data is learned , the S+U learning framework propose to train a model on the simulated data .

Hence , this paper seems to directly apply S+U learning to CycleGAN . The properties of the proposed method comes immediately from CycleGAN and S+U learning . Without deeper insights of the proposed method , the novelty of this paper is not sufficient .

2 . When discussing CycleGAN , the authors claim that CycleGAN is not good at preserving the labels . However , it is not clear what the meaning of preserving labels is . It would be nice if the authors clearly define this notion and rigorously discuss why CycleGAN is insufficient to reach such a goal and why combining with S+U learning would help .

3 . This work seems closely related to S+U learning . It would be nice if the authors also summarize S+U learning in Section 2 , in the similar way they summarize CycleGAN in Section 2.2.

4 . In Section 2.2 , the authors claim that the Cycle-consistency loss in CycleGAN is not sufficient for label preservation . To improve , they propose to use the feature consistency loss . However , the final loss function also contains this cycle-consistency loss . Moreover , in the experiments , the authors indeed use the cycle-consistency loss by setting \lambda_{cyc} = 10 . But the feature consistency loss may not be used by setting \lambda_{feature} = 0 or 0.5 . From table Two , it appears that whether using the feature - consistency loss does not have significant effect on the performance .

It would be nice to conduct more experiments to show the effect of adding the feature - consistent loss . Say , setting \lambda_{cyc} = 0 and try different values of \lambda_{feature} . Otherwise it is unclear whether the feature - consistent loss is necessary .


Dear Reviewer2 ,

First of all , we really appreciate your constructive review and detailed comments on our work . We would like to share our responses to the concerns raised by the reviewer .

1 . The proposed method seems to be a direct combination of the CycleGAN and the S+U learning … . Without deeper insights of the proposed method , the novelty of this paper is not sufficient .

= > While we fully agree that some components of our framework are largely inspired by the CycleGAN and the S+U learning , our framework has its own novelty as follows .
1 ) It starts with a novel adaptive data generation process , which we observed necessary to achieve the state - of - the- art performances in our own experiments . To see this , we added the test errors to Table 1 in the revision . One can observe that one cannot achieve the state - of - the- art performances without having a “ good ” synthetic label distribution , which can be obtained by our adaptive data generation process .
2 ) Our approach is different from the traditional S+U learning framework : In the original S+U learning framework , the synthetic data is mapped to the real data , and then a predictor is trained on the translated data . In our work , we do not train our predictors after we learn the bidirectional mapping : Instead , we simply map test images to the synthetic domain , and directly apply predictors , which are trained solely with the synthetic data set .
Our approach has a two - fold advantages over the traditional framework . First , we observe that our backward approach can achieve an improved prediction performance . To see this , we also added the test error measure with the traditional forward mapping approach , which is worse than our backward mapping approach . Further , our approach also has a significant saving in terms of computation over the traditional S+U learning since one does not have to retrain predictors for each target domain . ( Having one good predictor trained on the synthetic domain suffices ! )

2 . When discussing CycleGAN , the authors claim that CycleGAN is not good at preserving the labels . However , it is not clear what the meaning of preserving labels is . It would be nice if the authors clearly define this notion and rigorously discuss why CycleGAN is insufficient to reach such a goal and why combining with S+U learning would help .
= > According to the reviewer ’s comment , we made the following changes in how we describe the label - loss problem and our approach to mitigate the problem . We added a subsection ( Sec 2.1 ) where we define notations and introduce new terms . We also added references to several prior works , which proposed a similar concept called ‘ content representation ’ or ‘ feature matching ’ for other related tasks . Further , we would like to clarify that we are proposing the use of “ feature consistency loss ” to address this challenge .

3 . This work seems closely related to S+U learning . It would be nice if the authors also summarize S+U learning in Section 2 , in the similar way they summarize CycleGAN in Section 2.2.
= > We now added more detailed description of the original S+U learning papers to Sec 2.4 in the revision .

4 . In Section 2.2 , the authors claim that the Cycle-consistency loss in CycleGAN is not sufficient for label preservation . To improve , they propose to use the feature consistency loss . However , the final loss function also contains this cycle-consistency loss .
= > First of all , we would like to clarify the following . In this work , our claim was that the cycle-consistency loss “ alone ” may not preserve labels well , and hence we propose to use it together with “ feature - consistency loss ” . That is , we are proposing to use both of them . For clarification , we revised the relevant descriptions .

5 . Moreover , in the experiments , the authors indeed use the cycle-consistency loss by setting \lambda_{cyc} = 10 . But the feature consistency loss may not be used by setting \lambda_{feature} = 0 or 0.5 . From table Two , it appears that whether using the feature - consistency loss does not have significant effect on the performance . It would be nice to conduct more experiments to show the effect of adding the feature - consistent loss . Say , setting \lambda_{cyc} = 0 and try different values of \lambda_{feature} . Otherwise it is unclear whether the feature - consistent loss is necessary .
= > According to the reviewer ’s comment , we included additional experimental results to see the roles of \lambda_{feature} and \lambda_{cycle} . More specifically , we run our algorithm with different combinations of \lambda_{feature} \in {0 , 0.1 , 0.5 , 1.0 } and \lambda_{cycle} \in { 0,1,5,10,50 } . As a result , we observe that setting “ \lambda_{feature} = 0.5 , \lambda_{cycle} = 10 ” achieved the best performance among the tested cases , proving the necessity of using both consistency terms . For details , see the experimental results in the appendix of the revision .

Thanks ,
Authors

Summary :
The paper presents a novel method for answering “ How many … ? ” questions in the VQA datasets . Unlike previously proposed approaches , the proposed method uses an iterative sequential decision process for counting the relevant entity . The proposed model makes discrete choices about what to count at each time step . Another qualitative difference compared to existing approaches is that the proposed method returns bounding boxes for the counted object . The training and evaluation of the proposed model and baselines is done on a subset of the existing VQA dataset that consists of “ How many … ? ” questions . The experimental results show that the proposed model outperforms the baselines discussed in the paper .

Strengths :
1 . The idea of sequential counting is novel and interesting .
2 . The analysis of model performance by grouping the questions as per frequency with which the counting object appeared in the training data is insightful .

Weaknesses :
1 . The proposed dataset consists of 17,714 QA pairs in the dev set , whereas only 5,000 QA pairs in the test set . Such a 3.5:1 split of dev and test seems unconventional . Also , the size of the test set seems pretty small given the diversity of the questions in the VQA dataset .
2 . The paper lacks quantitative comparison with existing models for counting such as with Chattopadhyay et al . This would require the authors to report the accuracies of existing models by training and evaluating on the same subset as that used for the proposed model . Absence of such a comparison makes it difficult to judge how well the proposed model is performing compared to existing models .
3 . The paper lacks analysis on how much of performance improvement is due to visual genome data augmentation and pre-training ? When comparing with existing models ( as suggested in above ) , this analysis should be done , so as to identify the improvements coming from the proposed model alone .
4 . The paper does not report the variation in model performance when changing the weights of the various terms involved in the loss function ( equations 15 and 16 ) .
5 . Regarding Chattopadhyay et al . the paper says that “ However , their analysis was limited to the specific subset of examples where their approach was applicable . ” It would be good it authors could elaborate on this a bit more .
6 . The relation prediction part of the vision module in the proposed model seems quite similar to the Relation Networks , but the paper does not mention Relation Networks . It would be good to cite the Relation Networks paper and state clearly if the motivation is drawn from Relation Networks .
7 . It is not clear what are the 6 common relationships that are being considered in equation 1 . Could authors please specify these ?
8 . In equation 1 , if only 6 relationships are being considered , then why does f^R map to R^7 instead of R^6 ?
9 . In equations 4 and 5 , it is not clarified what each symbol represents , making it difficult to understand .
10 . What is R in equation 15 ? Is it reward ?

Overall :
The paper proposes a novel and interesting idea for solving counting questions in the Visual Question Answering tasks . However , the writing of the paper needs to be improved to make is easier to follow . The experimental set - up – the size of the test dataset seems too small . And lastly , the paper needs to add comparisons with existing models on the same datasets as used for the proposed model . So , the paper seems to be not ready for the publication yet .

We would like to thank the reviewer for their thoughtful and constructive feedback . Before addressing the reviewer ’s concerns , we should mention that since the original submission , we have observed superior performance ( for all the models we consider ) when using the visual features released with the original UpDown paper ( those used by Anderson et al . ) . We believe this choice simplifies our work by focusing our contribution on the counting module and , by using publicly available visual features , facilitates future comparison .


Regarding comment 1 : To examine the robustness of the test metrics , we re-computed the accuracy for the development and test splits after diverting 6500 randomly chosen QA pairs from dev to test ( giving the adjusted dev / test splits 11 k QA pairs each ) . We did this for the 8 IRLC models from the hyperparameter sweep whose penalty weights surrounded the optimum . On the original dev / test splits , those models have average accuracies of 56.21 & 57.06 . In the adjusted split , the average accuracies are 56.18 & 56.64 . This analysis suggests that the smaller test size does introduce some noise into the accuracy measurement , but the effect of that noise is small compared to the scale of the performance differences between SoftCount , UpDown and IRLC .

Regarding comments 2 , 3 and 5 : The reviewer points out that we did not sufficiently place our work in the context of Chattopadhyay et al . We agree and have attempted to correct that mistake in our revised submission ( changes appear in Related Works and Models [ page 4 ] sections ) .
To further clarify our reasoning here , there are three main reasons we do not compare to their work .
1 ) Our work and theirs both examine counting , but we use counting as a lens for exploring interpretability in visual question answering . This led to considerably different architectures as considered between our works .
2 ) Our work examines generalization for unseen or few - shot classes . Since the question is a sentence , not a single word , there are a number of cases where the effective class is a combination of noun and adjective or position ( eg. black birds , people sitting on the bench ) . Chattopadhyay et al. only handles counting a fixed set of object classes and lacks the flexibility required for question answering .
3 ) The reviewer has suggested that we train and evaluate a model based on their proposals ( i.e. “ seq - sub ” ) ; however , to do so would make it very difficult to control for the quality / structure of visual features and the mechanism of visual - linguistic fusion . Additionally , the seq-sub architecture is not amenable to question answering or supervision from VQA data alone .
All in all , we believe that the issues described above makes quantitatively comparing our work to that of Chattopadhyay et al . overly complicated and we hope the reviewer will agree with our assessment .

As part of incorporating the new visual features , we have revised the model section describing the vision module . This revision has resulted in the removal of the confusing text that the reviewer mentioned in comments 6 - 8.

Following this change , we cannot readily assess how details of pre-training affect ultimate performance , as recommended in comment 3 . However , we have included an experiment to demonstrate the effect of data augmentation with Visual Genome ( Appendix Section C in revised submission ) . We observe that removing the Visual Genome data reduces accuracy by 2.7 % on average and increases RMSE by 0.12 on average and that IRLC is most robust to the decrease in training data .

In addition , we have incorporated the experiment suggested in comment 4 ( Appendix Section C in revised submission ) . The results demonstrate the range of weight settings in which the penalties improve performance .

We have also clarified the model descriptions referred to in comments 9 and 10 .


Anderson et al . Bottom - Up and Top - Down Attention for Image Captioning and VQA . In CVPR , 2017 .

Chattopadhyay et al . Counting Everyday Objects in Everyday Scenes . In CVPR , 2017 .



------------------
Summary :
------------------
This work introduces a discrete and interpretable model for answering visually grounded counting questions . The proposed model executes a sequential decision process in which it 1 ) selects an image region to " add to the count " and then 2 ) updates the likelihood of selecting other regions based on their relationships ( defined broadly ) to the selected region . After substantial module pre-trianing , the model is trained end - to - end with the REINFORCE policy gradient method ( with the recently proposed self - critical sequence training baseline ) . Compared to existing approaches for counting ( or VQA in general ) , this approach not only produces lower error but also provides a more human-intuitive discrete , instance - pointing representation of counting .

-----------------------
Preliminary Evaluation :
-----------------------
The paper presents an interesting approach that seems to outperform existing methods . More importantly in my view , the model treats counting as a discrete human-intuitive process . The presentation and experiments are okay overall but I have a few questions and requests below that I feel would strengthen the submission .

------------------
Strengths :
------------------
- I generally agree with the authors that approaching counting as a region - set selection problem provides an interpretable and human-intuitive methodology that seems more appropriate than attentional or monolithic approaches .

- To the best of my knowledge , the writing does a good job of placing the work in the context of existing literature .

- The dataset construction is given appropriate attention to restrict its instances to counting questions and will be made available to the public .

- The model outperforms existing approaches given the same visual and linguistic inputs / encodings . While I find improvements in RMSE a bit underwhelming , I 'm still generally positive about the results given the improved accuracy and human-intuitiveness of the grounded outputs .

- I appreciated the analysis of the effect of " commonness " and think it provides interesting insight into the generalization of the proposed model .

- Qualitative examples are interesting .

------------------
Weaknesses :
------------------
- There is a lot going on in this paper as far as model construction and training procedures go . In its current state , many of the details are pushed to the supplement such that the main paper would be insufficient for replication . The authors also do not promise code release .

- Maybe it is just my unfamiliarity with it , but the caption grounding auxiliary - task feels insufficiently introduced in the main paper . I also find it a bit discouraging that the details of joint training is regulated to the supplementary material , especially given that the UpDown is not using it ! I would like to see an ablation of the proposed model without joint training .

- Both the IRLC and SoftCount models are trained with objectives that are aware of the ordinal nature of the output space ( such that predicting 2 when the answer is 20 is worse than predicting 19 ) . Unfortunately , the UpDown model is trained with cross -entropy and lacks access to this notion . I believe that this difference results in the large gap in RMSE between IRLC / SoftCount and UpDown . Ideally an updated version of UpDown trained under an order - aware loss would be presented during the rebuttal period . Barring that due to time constraints , I would otherwise like to see some analysis to explore this difference , maybe checking to see if UpDown is putting mass in smooth blobs around the predicted answer ( though there may be better ways to see if UpDown has captured similar notions of output order as the other models ) .

- I would like to see a couple of simple baselines evaluated on HowMany - QA . Specifically , I think the paper would be stronger if results were put in context with a question only model and a model which just outputs the mean training count . Inter-human agreement would also be interesting to discuss ( especially for high counts ) .

- The IRLC model has a significantly large ( 4x ) capacity scoring function than the baseline methods . If this is restricted , do we see significant changes to the results ?

- This is a relatively mild complaint . This model is more human-intuitive than existing approaches , but when it does make an error by selecting incorrect objects or terminating early , it is no more transparent about the cause of these errors than any other approach . As such , claims about interpretability should be made cautiously .

------------------
Curiosities :
------------------
- In my experience , Visual Genome annotations are often noisy , with many different labels being applied to the same object in different images . For per-image counts , I do n't imagine this will be too troubling but was curious if you ran into any challenges .

- It looks like both IRLC and UpDown consistently either get the correct count ( for small counts ) or underestimate . This is not the Gaussian sort of regression error that we might expect from a counting problem .

- Could you speak to the sensitivity of the proposed model with respect to different loss weightings ? I saw the values used in Section B of the supplement and they seem somewhat specific .

------------------
Minor errors :
------------------
[ 5.1 end of paragraph 2 ] ' that accuracy and RSME and not ' -> ' that accuracy and RSME are not '
[ Fig 9 caption ] ' The initial scores are lack ' -> ' The initial scores lack '

We would like to thank the reviewer for their thoughtful and constructive feedback . Before addressing the reviewer ’s concerns , we should mention that since the original submission , we have observed superior performance ( for all the models we consider ) when using the visual features released with the original UpDown paper ( those used by Anderson et al . ) . We believe this choice simplifies our work by focusing our contribution on the counting module and , by using publicly available visual features , facilitates future comparison .

We have revised our paper to provide more detail about the influence of joint- training and have incorporated more detail into the main text . We also now compare each model with and without joint training and make it the default for each model to include joint training ( since , with the new visual features , each model benefits from the extra supervision ) . These changes appear in Model section 4.2 , Table 2 of the Results section , and Appendix Section B.1 in the revised submission .

Unfortunately , we did not have sufficient time to update the training procedure of the UpDown model . Instead , we follow the reviewer ’s suggestion and include an analysis of the smoothness of the model ’s predictions . These changes appear in Appendix Section C of the revised submission .

We have included the two simple baselines recommended by the reviewer . These changes appear in Results section 5.1 and Table 2 in the revised submission .

Addressing the reviewer ’s curiosities …
The only issue noisy annotations in Visual Genome might cause would be with pre-training the vision module . Since we have opted to use publicly available pre-trained features , we ultimately do n’t deal with that noise ourselves . However , the new features were pre-trained with Visual Genome and produce better results than our original attempt . We could speculate that our original features were worse because we did not account for the noisy labels , but that ’s hard to say for certain .
We have revised the paper to include the results of a small sweep over the loss weightings ( Appendix Section C in revised submission ) .

Anderson et al . Bottom - Up and Top - Down Attention for Image Captioning and VQA . In CVPR , 2017 .


This paper proposed a new approach for counting in VQA called Interpretable Counting in Visual Question Answering . The authors create a new dataset ( HowMany - QA ) by processing the VQA 2.0 and Visual Genome dataset . In the paper , the authors use object detection framework ( R - FCN ) to extract bounding boxes information as well as visual features and propose three different strategies for counting . 1 : SoftCount ; 2 : UpDown ; 3 : IRLC . The authors show results on HowMany - QA dataset for the proposed methods , and the proposed IRLC method achieves the best performance among all the baselines .

[ Strenghts ]

This paper first introduced a cleaned visual counting dataset by processing existing VQA 2.0 and Visual Genome dataset , which can filter out partial non-counting questions . The proposed split is a good testbed for counting in VQA .

The authors proposed 3 different methods for counting , which both use object detection feature trained on visual genome dataset . The object detector is trained with multiple objectives including object detection , relation detection , attribute classification and caption grounding to produce rich object representation . The author first proposed 2 baselines : SoftCount uses a Huber loss , UpDown uses a cross entropy loss . And further proposed interpretable RL counter which enumerates the object as a sequential decision process . The proposed IRLC more intuitive and outperform the previous VQA method ( UpDown ) on both accuracy and RMSE .

[ Weaknesses ]

This paper proposed an interesting and intuitive counting model for VQA . However , there are several weaknesses existed :

1 : The object detector is pre-trained with multiple objectives . However , there is no ablation study to show the differences . Since the model only uses the object and relationship feature as input , the authors could show results on counting with different objects detector . For example , object detector trained using object + relation v.s. object + relation + attribute v.s. object + relation + attribute + caption .

2 : Figure 9 shows an impressive result of the proposed method . Given the detection result , there are a lot of repetitive candidates detection bounding boxes . Without any strong supervision , IRLC could select the correct bounding boxes associated with the different objects . This is interesting , however , the authors did n't show any quantitative results on this . One experiment could verify the performance on IRLC is to compute the IOU between the GT COCO bounding box annotation on a small validation set . The validation set could be obtained by comparing the number of the bounding box and VQA answer with respect to similar COCO categories and VQA entities .

3 : The proposed IRLC is not significantly outperform baseline method ( SoftCount ) with respect to RMSE ( 0.1 ) . However , it would be interesting to see how the counting performance can change the result of object detection . As Chattopadhyay 's CVPR2017 paper Sec 5.3 on the same subset as in point 2 .

[ Summary ]

This paper proposed an interesting and interpretable model for counting in VQA . It formulated the counting as a sequential decision process that enumerated the subset of target objects . The authors introduce several new techniques in the IRLC counter . However , there is a lack of ablation study on the proposed model . Taking all these into account , I suggest accepting this paper if the authors could provide more ablation study on the proposed methods .


We would like to thank the reviewer for their thoughtful and constructive feedback . Before addressing the reviewer ’s concerns , we should mention that since the original submission , we have observed superior performance ( for all the models we consider ) when using the visual features released with the original UpDown paper ( those used by Anderson et al . ) . We believe this choice simplifies our work by focusing our contribution on the counting module and , by using publicly available visual features , facilitates future comparison .

One consequence of this decision is that we forgo the option to do ablation studies on the vision module , as suggestion by the reviewer in comment 1 . We very much agree that it will be interesting to see how these details of pre-training improve the model ’s ability to map questions onto object representations . Therefore , in our revisions , we include a new ablation experiment on joint- training counting and caption grounding ( changes appear in Table 2 of the Results section in the revised submission ) .

In comment 2 , the reviewer mentions the possibility of comparing the counting objects during question answering to the ground truth objects from the COCO labels . We thank the reviewer for this insightful suggestion and have included a new analysis to perform this comparison ( changes appear in the Results section 5.2 and Appendix Section D in the revised submission ) . The results demonstrate that , despite the similarity between these two models with respect to RMSE , the objects counted by IRLC are more relevant to the question than the objects counted by SoftCount .

The reviewer has also pointed out the possibility that our model could improve object detection . Because our model counts from questions , we believe our approach might be best suited to improve generalization of object detection to more diverse classes -- for example , to detect objects based not only on their class but also attributes and / or context in the scene . Time constraints prevent us from exploring this notion in the revised submission , but it is our goal to do so before a final submission .

Anderson et al . Bottom - Up and Top - Down Attention for Image Captioning and VQA . In CVPR , 2017 .

Chattopadhyay et al . Counting Everyday Objects in Everyday Scenes . In CVPR , 2017 .


This work proposes a variation of the DenseNet architecture that can cope with computational resource limits at test time . The paper is very well written , experiments are clearly presented and convincing and , most importantly , the research question is exciting ( and often overlooked ) .

My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al . ( 2017 ) . The authors add a hierarchical , multi-scale structure and show that DenseNet can better cope with it than ResNet ( e.g. , Fig. 3 ) . They investigate pros and cons in detail adding more valuable analysis in the appendix . However , this work is basically an extension of the DenseNet approach with a new problem statement and additional , in - depth analysis .

Some more minor comments :

- Please enlarge Fig. 4.
- I did not fully grasp the details in the first " Solution " paragraph on P5 . Please extend and describe in more detail .

In conclusion , this is a very well written paper that designs the network architecture ( of DenseNet ) such that it is optimized to include CPU budgets at test time . I recommend acceptance to ICLR18 .





Thanks for positive comments .

# difference to DenseNet
Although dense connectivity is one of the two key components in our MSDNet , this paper is quite different from the original DenseNet paper : ( 1 ) in this paper we tackle a very different problem , the inference of deep models with computational resource limits at test time ; ( 2 ) we show the multi-scale features are crucial for learning accurate early classifiers . Finally , MSDNet yields 2 x to 5 x faster inference speed than DenseNet under the batch budgeted setting .

# minors
Thanks for these suggestions . We have incorporated them in the updated version .

This paper presents a method for image classification given test - time computational budgeting constraints . Two problems are considered : " any - time " classification , in which there is a time constraint to evaluate a single example , and batched budgets , in which there is a fixed budget available to classify a large batch of images . A convolutional neural network structure with a diagonal propagation layout over depth and scale is used , so that each activation map is constructed using dense connections from both same and finer scale features . In this way , coarse - scale maps are constructed quickly , then continuously updated with feed - forward propagation from lower layers and finer scales , so they can be used for image classification at any intermediate stage . Evaluations are performed on ImageNet and CIFAR - 100 .

I would have liked to see the MC baselines also evaluated on ImageNet --- I 'm not sure why they are n't there as well ? Also on p.6 I 'm not entirely clear on how the " network reduction " is performed --- it looks like finer scales are progressively dropped in successive blocks , but I do n't think they exactly correspond to those that would be needed to evaluate the full model ( this is " lazy evaluation " ) . A picture would help here , showing where the depth-layers are divided between blocks .

I was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result : It seems this relies on having a batch that is both large and varied , so that its evaluation time will converge towards the expectation . So this is n't really a hard constraint ( just an expected result for batches that are large and varied enough ) . This is fine , but could perhaps be pointed out if that is indeed the case .

Overall , this seems like a natural and effective approach , and achieves good results .


Thanks for the positive comments .

# MC baselines on ImageNet
We exclude these results in our current version as we observed that they are far from competitive on both CIFAR - 10 and CIFAR - 100 . We are testing the MC baselines on ImageNet , and will include it in a later version , but wo n’t expect them to be strong baselines .

# network reduction
The ‘ network reduction ’ is a design choice to reduce redundancy in the network , while ‘ lazy evaluation ’ is a strategy to avoid redundant computations . We have added a figure ( Figure 9 ) in the appendix to illustrate the reduced network as suggested .

# batched budgeted evaluation
Thanks for pointing out . We have emphasize that the notion of budget in this context is a “ soft constraint ” given a large batch of testing samples .

This paper introduces a new model to perform image classification with limited computational resources at test time . The model is based on a multi-scale convolutional neural network similar to the neural fabric ( Saxena and Verbeek 2016 ) , but with dense connections ( Huang et al. , 2017 ) and with a classifier at each layer . The multiple classifiers allow for a finer selection of the amount of computation needed for a given input image . The multi-scale representation allows for better performance at early stages of the network . Finally the dense connectivity allows to reduce the negative effect that early classifiers have on the feature representation for the following layers .
A thorough evaluation on ImageNet and Cifar100 shows that the network can perform better than previous models and ensembles of previous models with a reduced amount of computation .

Pros :
- The presentation is clear and easy to follow .
- The structure of the network is clearly justified in section 4 .
- The use of dense connectivity to avoid the loss of performance of using early - exit classifier is very interesting .
- The evaluation in terms of anytime prediction and budgeted batch classification can represent real case scenarios .
- Results are very promising , with 5 x speed - ups and same or better accuracy that previous models .
- The extensive experimentation shows that the proposed network is better than previous approaches under different regimes .

Cons :
- Results about the more efficient densenet * could be shown in the main paper

Additional Comments :
- Why in training you used logistic loss instead of the more common cross-entropy loss ? Has this any connection with the final performance of the network ?
- In fig . 5 left for completeness I would like to see also results for DenseNet^MT and ResNet^MT
- In fig . 5 left I can not find the 4 % and 8 % higher accuracy with 0.5x10^10 to 1.0x10^10 FLOPs , as mentioned in section 5.1 anytime prediction results
- How the budget in terms of Mul - Adds is actually estimated ?

I think that this paper present a very powerful approach to speed - up the computational cost of a CNN at test time and clearly explains some of the common trade - offs between speed and accuracy and how to improve them . The experimental evaluation is complete and accurate .



Thank you for the encouraging comments !

# DenseNet *
We have included the DenseNet * results in the main paper as suggested . We placed this network originally in the appendix to keep the focus of the main manuscript on the MSDNet architecture , and it was introduced for the first time in this paper ( although as a competitive baseline ) .

# logistic loss
We actually used the cross entropy loss in our experiments . We have fixed this sentence . Thanks for pointing out .

# DenseNet^MC and ResNet^MC on ImageNet ( left panel of Fig .5 )
We observed that DenseNet^MC and ResNet^MC are two of the weakest baselines on both CIFAR - 10 and CIFAR - 100 datasets . Therefore , we thought their results on Image Net probably wo n’t add much to the paper . We can add these results in a later version .

# improvements in the anytime setting
It should be 4 % and 8 % higher accuracy when the budget ranges from 0.1x10^10 * to 0.3x10^10 * FLOPs . We have corrected it in the updated version .

# actually budget
For many devices , e.g. , ARM processor , the actual inference time is basically a linear function of the number of Mul - Add operations . Thus in practice , given a specific device , we can estimate the budget in terms of Mul - Add according to the real time budget .

The paper deals with the increasingly popular GAN approach to constructing generative models . Following the first formulation of GANs in 2014 , it was soon realized that the training dynamics was highly unstable , leading to significant difficulties in achieving stable results . The paper by Arjovsky et al ( 2017 ) provided a framework based on the Wasserstein distance , a distance measure between probability distributions belonging to the class of so-called Integral Probability Metrics ( IPMs ) . This approach solved the stability issues of GANs and demonstrated improved empirical results . Several other works were then developed to deal with these stability issues , specifically the Fisher IPM . Both these methods relied on discriminating between distributions P and Q based on computing a function f , belonging to an appropriate function class {\cal F} , that maximizes the deviation E_{x ~ P}f ( x ) - E_{x ~ Q}f ( x ) . The main issue relates to the choice of the class {\cal F} . For the Wasserstein distance this was the class of L_1 Lipschitz functions , while for the Fisher distance it was the class of square integrable functions . The present paper introduces a new notion of distance , where {\cal F} is the defined through the Sobolev norm , based on the L_2 norm of the gradient of f( x ) , with respect to a measure \mu ( x ) , where the latter can be freely chosen under certain assumptions .

The authors prove a theorem related to the properties of the Sobolev norm , and express it in terms of the component - wise conditional distributions . Moreover , they show that the optimal critic f is obtained by solving a PDE subject to zero boundary conditions . They then use their suggested metric in order to develop a GAN algorithm , and present experimental results demonstrating its utility . The Sobolev IPM has two nice features . First , it is based on the component - wise conditional distribution of the CDFs , and , second , its relation to the Laplacian regularizer from manifold learning . Its 1D version also relates to the well - known von Mises Cramer statistics used in hypothesis testing .

The paper belongs to a class of recent papers attempting to suggest improvements to the original GAN algorithm , relying on the KL divergence . It is well conceived and articulated , and provides an interesting and potentially powerful new direction to improve GANs in practice . However , it is somewhat difficult to follow the paper , and would urge the authors to improve and augment their presentation of the following issues .
1 ) One often poses regularization schemes based on optimality criteria . Is there any optimality principle under which the Sobolev IPM is a desired choice ?
2 ) The authors argue that their approach is especially well suited for discrete sequential data . This issue was not clear to me , and it would be good if the authors could expand on this issue and provide a clearer explanation .
3 ) How would the Sobolev norm behave under a change of coordinates or a homeomorphism of the space ? Would it make sense to require some invariance in this respect ?
4 ) The Lagrangian in eq . ( 9 ) contains both a Lagrange constraint on the Sobolev norm and a penalty term . Why are both needed ? Why do the updates of \lambda and p in Algorithm 1 used different schemes ( SGD and ADAM , respectively ) .
5 ) Table 2 , p. 13 – it would be nice to see a comparison to the recently introduced gradient penalty approach , Gulrajani et al. , Improved training of wasserstein gans . arXiv preprint arXiv:1704.00028 , 2017 .
6 ) The integral defining F_p ( x ) on p. 3 has x as an argument on the LHS and as an integrand of the RHS . Please correct this . Also specify that x= ( x_1 , \ldots , x_d ) .


We thank the reviewer for his encouraging and thoughtful comments and his suggestions that we already incorporated in the revision . We address below his main questions :

1 - It would be interesting to find a primal formulation that has sobolev IPM as a dual . Given the PDE that the critic of Sobolev satisfies , one way to find in which optimal way the Sobolev discrepancy is defined , is to attempt to write its
dynamic form . While we do n’t have yet a formal proof ( we are working on it ) we conjecture that sobolev IPM may relate to the following dynamic problem :

inf_{f_t in W_2,0 ( q_t ) } \int _{0 } ^T \nor{\nabla_x f^t ( x ) } q_t ( x ) dx
d ( q_t ) / dt = - div ( q_t \ nabla_x f^t ( x ) )
q_0=Q , q_T=P
Note that Wasserstein 2 distance has a similar form due to Benamou et al ( " A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem " by Jean - David Benamou et al. ) and has instead of \nabla_x f , a function g \in \mathbb{ R}^d in L_2 ( q_t ) . We think Sobolev IPM looks for “ smooth flows ” to move density masses . The effort for moving the mass is measured with the sobolev norm of critics and we wish to minimize this effort .

2 - When we wish to compare discrete sequences we face two problems :

a- The generator is continuous and the real data is discrete . Wasserstein distance is well known to address this issue of discrete / continuous matching . Divergences like KL or JS compare density functions and hence are not suited for this task . For sobolev IPM : by noising the discrete data ( smoothing ) we match two continuous distributions based on the coordinatewise conditional CDFs . Since the noising is annealed over training we will end up matching the discrete and continuous distributions . CDFs are better suited for the discrete nature of the problem as shown in Figure 1 in the revision of the paper .

b - another problem is that we need for sequences a discrepancy between processes , rather than just distributions . For time processes , how the joint distributions factors in terms of conditioning is very important . Sobolev IPM gives a little advantage by comparing for each coordinate a unique quantity CDF ( x_i| x^{ - i} ) PDF ( x^{-i} ) , since the Sobolev discrepancy captures some conditioning it has the ability of measuring coordinate dependencies , crucial for sequence / time process modeling .

See also our answer to Reviewer 1 , part 3 . We included Figure 1 in the paper revision to illustrate this .

3 - Behavior under coordinate change :

Discrepancy is unchanged but critic rotated : Assume we rotate P and Q and use mu=P + Q /2 , the discrepancy wo n’t change but the critic gradients will be rotated .

Incorporating invariance in mu : Since mu is free , we think a coordinate change might be a good place for incorporating desired invariances . for instance if we set d\mu ( gx ) = \frac{ P ( gx ) + Q ( g x ) }{2 } dg dx , g is an element in a lie group , our critic will be smooth and well defined on the support of an “ augmented ” distribution , which will probably benefit the semi-supervised learning with sobolev GAN . This is in the spirit of the recent work on invariances with GAN from Kumar et al NIPS 2017 .

Kumar et al NIPS 2017 . Semi supervised learning with GANs : Manifold invariance with improved inference .

4 - Augmented Lagrangian : We are using an augmented Lagrangian to enforce the equality constraint . Hence we have a lagrangian and a penalty term . Note that in Benamou paper an augmented lagrangian was also used . We used adam for optimizing the critic and the generator which is default practice , while the simple SGD for the lagrangian is following common practice in ADMM ( convex optimization ) where also our penalty coefficient is the learning rate for the lagrangian ( see eg slide 9 of https://web.stanford.edu/~boyd/papers/pdf/admm_slides.pdf)

5 - The WGAN - GP paper did not provide SSL results ; in our experiments we did not get WGAN - GP to work for SSL ( same experimental setup ) . We now report those results in Table 2 page 15 .

6 - This was fixed in the revision thanks for pointing it out .


Summary : The authors provide another type of GAN -- the Sobolev GAN -- which is the typical setup of a GAN but using a function class F for which f belongs to F iff \grad f belongs to L^2 ( mu ) . They relate this MMD to the Cramer and Fisher distance and then produce a recipe for training GANs with this sort of function class . In their empirical examples , they show it has similar performance to the WGAN -GP .

Overall , the paper has some interesting mathematical relationships to other MMDs . However , I finished reading the paper wondering why one would want to trust this GAN over any of the other GANs . I may have missed it , but I did n't see any compelling theoretical reason the gradients from this method would prove superior to many of the other GANs in existence today . The authors argue " from [ equation 5 ] we see that we are comparing CDFs , which are better behaved on discrete distributions , " but I was n't sure what exactly to make of this comment .

Nits :
* The " Stein metric " is actually called the Stein discrepancy [ see Gorham & Mackey ( 2015 ) Measuring Sample Quality using Stein 's Method ] .

We thank the reviewer for his comments and address their main concerns : ( 1 ) empirical performance and ( 2 ) compelling theoretical reasons for superior gradients . We uploaded a revision of the paper to incorporate the reviewer comments .

- Empirical performance similar to WGAN - GP : agreed for text modeling , but not for SSL . ( 1 ) In text generation , our goal was to understand the success of WGAN - GP . The sobolev point of view gives us the insight that the gradient penalty is not only introducing stability to the training , but also providing implicit conditioning needed for sequence modeling . ( 2 ) In SSL , while WGAN - GP fails , we show how to use sobolev IPM in SSL and link it to laplacian regularization in manifold learning ( Please see updated Table 2 page 15 where we added a comparison to WGAN - GP )

- Gradients of sobolev critic :

1 - Convergence of critics in Sobolev Sense : One interesting property we show ( Lemma 1 originally in appendix , we moved it now to the main paper ) is about the approximation of the optimal critic f^* , when f is parameterized in a hypothesis class ( i.e. neural network , f_H ) .
Most GANs do n’t have any guarantee .
- Fisher GAN provides an approximation guarantee in Lebesgue sense , meaning norm ( f^ * - f_H ) ..
- Sobolev GAN provides an approximation guarantee in Sobolev sense , meaning norm ( grad f^ * - grad f_H ) .
Having in mind that the gradient of the critic is the information that is passed on to the generator , we see that this convergence in Sobolev sense to the optimal critic is an important property for GAN training . We highlighted this in the revision of the paper .

2 - Meaningful Gradient Directions by fokker planck diffusion : We show that the optimal critic satisfies a PDE that relates to a deterministic fokker planck diffusion ( https://en.wikipedia.org/wiki/Fokker–Planck_equation). Assume our goal is to move a density Q to a density P. we start by computing the sobolev critic between Q and P . If we have particles X_t whose initial distribution distributions Q_0 = Q and we move those particles with the gradient of the critic ( X_t=X_{t - 1 } + epsilon \nabla_x f^t_{q_t , P} ( x ) ) . we do this process for T steps by recomputing the critic at each time between the particle distributions q_t and P . The relation of Sobolev IPM to fokker planck diffusion gives us the evolution of the density of the moving particles . At the end of the process we are guaranteed by fokker planck diffusion to converge to the distribution P. Note that this diffusion has two steps : compute the critic between q_t and P , and update the particles X_t . This is similar to the gradient descent applied to learning GANS : compute critic , and update particles , with the main difference that we are working here with densities , in GAN the generator distribution is degenerate and does not have a density .

For an illustration of how we transport Q to P using critic gradients , we provide these examples :
one is using sobolev IPM critic that has the diffusion property ( https://www.dropbox.com/s/ymwmddvsdpj29lq/sobolev_near_mu_q_full_65.mp4?dl=0 ) and one with the MMD distance critic ( https://www.dropbox.com/s/ehpmhfghh5wbo5n/mmd_near_65.mp4?dl=0) that does not have this diffusion property . In those videos we see how the pink particles ( Q ) move to have same density as the black particles ( P ) . In each frame the particles move with the gradient of the critic . The critic is recomputed between 2 frames , between the new density q_t and P in black . The level sets of the new critic are shown in each frame .
After 65 iterations with the same learning rate , we see that sobolev IPM succeeds at this task , while the MMD fails . One can see that the descent on sobolev discrepancy converges almost completely in just 65 iterations but the same is not true for MMD based descent method . Although , MMD based descent method takes almost 300 iterations to converge which can be seen in the video link given below : https://www.dropbox.com/s/shffwn3tffvb52r/mmd_near_full.mp4?dl=0



3 - CDF for continuous / discrete matching : Note that when we want to generate text , we are facing a problem of a continuous generator and discrete real data . If we were to compare a continuous and a discrete distribution based on pdfs , this will fail ( e.g. KL , JSD , etc ) . Wasserstein distance for instance in one dimension is known to be the comparison of inverse CDFs and this makes it possible to compare continuous and discrete distributions . For sobolev IPM : by noising the discrete data we match two continuous distributions based on the coordinatewise conditional CDFs . Since the noising is annealed over training we will end up matching the discrete and continuous distributions . We hypothesise that the implicit conditioning ( coordinatewise conditional CDF matching ) implied by the gradient regularizer allows end to end training of GAN text generation , since we are implicitly conditioning on the context for each coordinate . We added plots to illustrate this ( Figure 1 in the revision ) .

The paper proposes a different gradient penalty for GAN critics .
The proposed penalty is forcing the expected squared norm of the gradient to be equal to 1 .
The corresponding integral probability metric is well analysed .

Pros :
- The paper provides a nice overview of WGAN - GP , Fisher GAN and Sobolev GAN .
The differences and similarities and mentioned .
- The paper shows that Sobolev IPM is comparing coordinate - wise conditional CDFs .
- The 1D example in Section 4.2 shows a limitation of Fisher GAN .

Cons :
- The introduced gradient penalty is harder to optimize .
Algorithm 1 is using a biased estimate of the penalty .
If having independent samples in the minibatch ,
it would be possible to construct an unbiased estimate of the penalty .
- An unbiased estimate of the gradient penalty
will be hard to construct when not having two independent real samples .
E.g. , when doing conditional modeling with a RNN .
- The algorithm requires to train the critic well
before using the critic .
The paper does not provide an improvement over WGAN - GP in this direction .
MMD GAN and Cramer GAN may require less critic training steps .
- The experimental results do not demonstrate an improvement over WGAN -GP .
- Too much credit is given to implicit conditioning .
The Jensen Shanon divergence can be also written as a chain
of coordinate - wise JS divergences . That does not guarantee non- zero gradients
from the critic . A critic with non- zero gradients seems to be more important .


Minor typos :
s / pernalty / penalty /
s/ccordinate / coordinate /

We thank the reviewer for his comments . We have uploaded a revision of the paper to incorporate the reviewer comments .

First we stress that the goal of this paper is to better * understand * the gradient penalty and to know what it adds to the learning problem , rather than an emphasis on performance , although we show better performance than WGAN - GP in semi-supervised learning ( we will expand on this later ) .
WGAN - GP is the first paper to show text GAN working end to end , our goal was to explain what is behind this success .

We hope to answer the main concerns of the reviewer :

1 ) Biased estimate of the penalty . Reviewer 4 thinks reusing the same samples between objective and constraint introduces bias , however the use of the same samples in the loss and the constraint is a well - studied problem and is not an issue theoretically and practically . See for example ( Shivaswamy and Jebara JMLR 2010 http://www.jmlr.org/papers/volume11/shivaswamy10a/shivaswamy10a.pdf).
The Lagrangian formulation does not need another minibatch for an unbiased estimate , while the quadratic penalty theoretically needs one . But using some concentration inequalities for data dependent constraints as done in ( Shivaswamy and Jebara , Section 4.6 ) , one can show that this bias is very small and does not add any difficulty to the optimization . In Shivaswamy and Jebara , the term “ landmarks ” is used for the samples in the constraint . It is shown in this work that using the same samples for the loss and the constraints , introduces a small bias that vanishes with the number of samples . We will add a discussion in the paper .
Furthermore we did not find empirically any difficulty in optimizing Sobolev GAN . ( we added remark 1 under Algorithm 1 to refer to the small bias and to Shivaswamy and Jebara )


2 ) From a theoretical perspective , all GAN formulations ( including MMD GAN and Cramer GAN ) require full maximization of the discriminator ( critic ) in the inner loop ( Arjovsky and Bottou ICLR 2017 ) . In practice , usually a small number of iterations n_c is used instead in the inner loop ( disc maximization ) .
As stated in Appendix D , empirically we find that we can use n_c= 1 or 2 in Sobolev GAN .

3 ) Performance : WGAN - GP has not shown any semi-supervised learning performance . In our experiments , WGAN - GP for semi supervised learning gives bad performance ( see updated Table 2 page 15 ) . We show in this paper how to make use of the sobolev norm to regularize SSL in IPM based SSL , and we show the connection to laplacian regularization in manifold learning .

4A ) Example in one D between 2 Diracs is not only a limitation of Fisher IPM , it is a limitation of any distance comparing PDFs as shown in Arjovsky et al . This is in line with the intuition of the reviewer on the importance of non zero gradients .

4B ) Implicit conditioning : the Reviewer may have missed the point of the implicit conditioning introduced by Sobolev IPM ( coordinate - wise conditional CDF form , equation 5 ) . Indeed the Bayes rule can be used to rewrite JS coordinate wise , nevertheless this conditioning acts on PDF ( probability density functions ) : it compares PDF{x_i|x^{-i}} PDF ( x^{ -i} ) , which is equal for all coordinate to PDF ( x_1 , dots x_d ) . Note that the Sobolev IPM compares instead * unique quantities for each coordinate * CDF{x_i|x^{ - i}} PDF ( x^{ - i} ) ( see Equation 5 ) , making the model able to learn coordinate conditional distributions , in other words the ability to model context . The empirical evidence for the benefit of implicit conditioning is in Section 6.1 where we show when training Fisher GAN for text generation we need curriculum conditioning , while Sobolev GAN does n’t . We highlight there the crucial role of conditioning in sequence learning , it is not only a vanishing gradient problem . The gradient penalty in Sobolev GAN and WGAN - GP imply this implicit conditioning that is crucial in end to end sequence / text generation using GAN .


This paper designs a new IPM ( Integral Probability Metric ) that uses the gradient properties of the test function . The advantage of Sobolev IPM over the Fisher IPM is illustrated by the insight given in Section 4.2 . This is convincing . For comparing the true distribution and the generated distribution , it is much better to provide a quantitative measurement , rather than a 0 - 1 dichotomy . This target is implicitly achieved by the reproducing kernel methods and the original phi-divergence .

On the other side , the paper is hard to follow , and it contains many long sentences , some 4 - 5 lines . The formulation of the Sobolev norm could be improved at the top of page 6 .

We thank the reviewer for his encouraging and supportive comments . We have revised the paper and we improved the presentation of the sobolev IPM on top of page 6 and added additional theoretical results in this section , as well as more illustrative examples ( Figure 1 ) .

This paper proposes a recurrent neural network for visual question answering . The recurrent neural network is equipped with a carefully designed recurrent unit called MAC ( Memory , Attention and Control ) cell , which encourages sequential reasoning by restraining interaction between inputs and its hidden states . The proposed model shows the state - of - the- art performance on CLEVR and CLEVR - Humans dataset , which are standard benchmarks for visual reasoning problem . Additional experiments with limited training data shows the data efficiency of the model , which supports its strong generalization ability .

The proposed model in this paper is designed with reasonable motivations and shows strong experimental results in terms of overall accuracy and the data efficiency . However , an issue in the writing , usage of external component and lack of experimental justification of the design choices hinder the clear understanding of the proposed model .

An issue in the writing
Overall , the paper is well written and easy to understand , but Section 3.2.3 ( The Write Unit ) has contradictory statements about their implementation . Specifically , they proposed three different ways to update the memory ( simple update , self attention and memory gate ) , but it is not clear which method is used in the end .

Usage of external component
The proposed model uses pretrained word vectors called GloVE , which has boosted the performance on visual question answering . This experimental setting makes fair comparison with the previous works difficult as the pre-trained word vectors are not used for the previous works . To isolate the strength of the proposed reasoning module , I ask to provide experiments without pretrained word vectors .

Lack of experimental justification of the design choices
The proposed recurrent unit contains various design choices such as separation of three different units ( control unit , read unit and memory unit ) , attention based input processing and different memory updates stem from different motivations . However , these design choices are not justified well because there is neither ablation study nor visualization of internal states . Any analysis or empirical study on these design choices is necessary to understand the characteristics of the model . Here , I suggest to provide few visualizations of attention weights and ablation study that could support indispensability of the design choices .



Thank you very much for your review - we truly appreciate it !
We have uploaded a revision ( by the rebuttal deadline , jan 5 ) that addresses all your comments :

1 . We have revised the description of the writing unit to make it more clear - we have experimented with several variants for this unit - the " standard " one ( for which all the results are about ) , and 3 variants : a. with self - attention , b. with gating , and c. with both self - attention and gating . In the ablations study section we have included results for each of these for the whole dataset , 10 % of the dataset and also showed training curves for each variant .

2 . We have trained the models without GloVE and added these results along with clarification to the experiments section .

3 . We have included ablation studies in order to justify the architecture design choices and elucidate their impact . We have also added visualizations of attention weights for several examples and discussed them .

Thanks a lot again for your review !
- Paper858 Authors

Summary :
This paper proposed an extension of the dynamic coattention network ( DCN ) with deeper residual layers and self-attention . It also introduced a mixed objective with self - critical policy learning to encourage predictions with high word overlap with the gold answer span . The resulting DCN + model achieved significant improvement over DCN .

Strengths :
The model and the mixed objective is well - motivated and clearly explained .
Near state -of- the-art performance on SQuAD dataset ( according to the SQuAD leaderboard ) .

Other questions and comments :
The ablation shows 0.7 improvement on EM with mixed objective . It is interesting that the mixed objective ( which targets F1 ) also brings improvement on EM .


Thanks for your comments . In practice the F1 and EM metrics are closely correlated . We chose to use the F1 score as a metric because it offers fine grain signals as to how well the span predicted matches the ground truth span , whereas the EM score only rewards exact gloss matches .

This paper proposed an improved version of dynamic coattention networks , which is used for question answering tasks . Specifically , there are 2 aspects to improve DCN : one is to use a mixed objective that combines cross entropy with self - critical policy learning , the other one is to imporve DCN with deep residual coattention encoder . The proposed model achieved STOA performance on Stanford Question Asnwering Dataset and several ablation experiments show the effectiveness of these two improvements . Although DCN + is an improvement of DCN , I think the improvement is not incremental .

One question is that since the model is compicated , will the authors release the source code to repeat all the experimental results ?

Thanks for your comments . We can try to release the source code after the decision process .

The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR . First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions . The other main modification is to train their DCN + model using both cross entropy loss and F1 score ( using RL supervision ) in order to reward the system for making partial matching predictions . Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3 % , both on F1 and exact match accuracy , over other comparable systems . An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance .

The paper is well written , ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and / or answers . It seems to me that this paper is a significant contribution to the field of question answering systems .


Thank you for your comments !


This paper proposes a small modification to the monotonic attention in [ 1 ] by adding a soft attention to the segment predicted by the monotonic attention . The paper is very well written and easy to follow . The experiments are also convincing . Here are a few suggestions and questions to make the paper stronger .

The first set of questions is about the monotonic attention . Training the monotonic attention with expected context vectors is intuitive , but can this be justified further ? For example , how far does using the expected context vector deviate from marginalizing the monotonic attention ? The greedy step , described in the first paragraph of page 4 , also has an effect on the produced attention . How does the greedy step affect training and decoding ? It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding . These questions should really be answered in [ 1 ] . Since the authors are extending their work and since these issues might cause training difficulties , it might be useful to look into these design choices .

The second question is about the window size $ w$ . Instead of imposing a fixed window size , which might not make sense for tasks with varying length segments such as the two in the paper , why not attend to the entire segment , i.e. , from the current boundary to the previous boundary ?

It is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2 . ( The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot . ) How does the MoCha attention look like for words whose orthography is very nonphonemic , for example , AAA and WWW ?

For the experiments , it is intriguing to see that $ w=2 $ works best for speech recognition . If that 's the case , would it be easier to double the hidden layer size and use the vanilla monotonic attention ? The latter should be a special case of the former , and in general you can always increase the size of the hidden layer to incorporate the windowed information . Would the special cases lead to worse performance and if so why is there a difference ?

[ 1 ] C Raffel , M Luong , P Liu , R Weiss , D Eck , Online and linear-time attention by enforcing monotonic alignments , 2017

Thank you for your thorough review and thoughtful questions ! We 're glad you found the paper easy to follow and the experiments convincing . We 've updated the paper to address your questions with additional experiments , and we provide some additional context below .

> how far does using the expected context vector deviate from marginalizing the monotonic attention ? The greedy step , described in the first paragraph of page 4 , also has an effect on the produced attention . How does the greedy step affect training and decoding ?
Because we encourage the monotonic selection probabilities to be binary over the course of training by adding pre-sigmoid noise , these probabilities indeed tend to be 0 or 1 at convergence . As a result , the greedy process is effectively equivalent to completely marginalizing out the alignment . Note that we do n't use the greedy step during training because we explicitly compute the probability distribution induced by the possible alignment paths . We have added some wording to the paper to clarify these points .

> It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding . These questions should really be answered in [ 1 ] . Since the authors are extending their work and since these issues might cause training difficulties , it might be useful to look into these design choices .
Indeed , [ 1 ] includes a " Practitioner 's Guide " in Appendix G , which has discussion of how the sigmoid noise , weight norm , etc. can affect results . We will add a reference to this practitioner 's guide in the main text . If you think it would be helpful , we can provide similar recommendations based on our own experiences in an appendix .

> The second question is about the window size $ w$ . Instead of imposing a fixed window size , which might not make sense for tasks with varying length segments such as the two in the paper , why not attend to the entire segment , i.e. , from the current boundary to the previous boundary ?
In fact , we also tried exactly what you suggested in early experiments ! We had planned to call this approach " MAtChA " for Monotonic Adaptive Chunkwise Attention . As it turns out , it is possible to train this type of attention mechanism with an efficient dynamic program ( analogous to the one used for monotonic attention and MoChA ) . However , ultimately MAtChA did not outperform MoChA in any of our experiments . In addition , the dynamic program used for training takes O( T^2U ) memory instead of O ( TU ) memory ( because you must marginalize out both the chunk start and end point , instead of just the end point ) , so we decided not to include it in the paper . Prompted by your question , we 've decided to put a discussion of MAtChA with a derivation of the dynamic program into the appendix .

> The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot .
Great idea , we changed the figure to remove the delta and delta-delta features .

> How does the MoCha attention look like for words whose orthography is very nonphonemic , for example , AAA and WWW ?
That 's a very interesting point to discuss , so we added a note about this in the paper . However , we were unable to find any such examples in the development set of the Wall Street Journal corpus , so we were n't able to study this directly . Note that even for nonphonemic utterances , the attention alignment still tends to be monotonic - see for example Appendix A of " Listen , Attend and Spell " where a softmax attention model gives a monotonic alignment for your " AAA " example .

> If that 's the case , would it be easier to double the hidden layer size and use the vanilla monotonic attention ?
Thanks for this suggestion - indeed , using MoChA incurs a modest parameter increase ( about 1 % in our speech recognition experiments ) because of the second independent energy function . To address this difference , we ran an experiment where we doubled the attention energy function 's hidden dimension in a monotonic attention model ( similar in terms of parameter count to adding a second attention energy function ) and halved this hidden dimension in a MoChA model . In both cases , the change in performance was not significant over eight trials , implying that large gains achieved by MoChA were not caused by this change . We added information about this experiment to the main text .

The paper proposes an extension to a previous monotonic attention model ( Raffel et al 2017 ) to attend to a fixed - sized window up to the alignment position . Both the soft attention approximation used for training the monotonic attention model , and the online decoding algorithm is extended to the chunkwise model . In terms of the model this is a relatively small extention of Raffel et al 2017 .

Results show that for online speech recognition the model matches the performance of an offline soft attention baseline , doing significantly better than the monotonic attention model . Is the offline attention baseline unidirectional or bidirectional ? In case it is unidirectional it cannot really be claimed that the proposed model 's performance is competitive with an offline model .

My concern with the statement that all hyper - parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model . Especially given that w=2 works best for speech recognition , it not clear that the model extension is actually helping . My other concern is that in speech recognition the time - scale of the encoding is somewhat arbitrary , so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer . While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement .

For document summarization the paper presents a strong result for an online model , but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this . If the contribution is in terms of speed ( as shown with the synthetic benchmark in appendix B ) more emphesis should be placed on this in the paper .
Sentence summarization tasks do exhibit mostly monotonic alignment , and most previous models with monotonic structure were evaluated on that , so why not test that here ?

I like the fact that the model is truely online , but that contribution was made by Raffel et al 2017 , and this paper at best proposes a slightly better way to train and apply that model .

---
The additional experiments in the new version gives stronger support in favour of the proposed model architecture ( vs the effect of hyperparameter choices ) . While I 'm still on the fence on whether this paper is strong enough to be accepted for ICLR , this version is certainly improves the quality of the paper .


Thanks for your thorough review ! We updated the paper to address your comments , and provide some additional discussion below .

> Is the offline attention baseline unidirectional or bidirectional ? In case it is unidirectional it cannot really be claimed that the proposed model 's performance is competitive with an offline model .
Thank you for pointing out this important distinction . The encoder in the softmax attention baseline is indeed unidirectional . We made this choice because using a unidirectional encoder is a prerequisite for an online model . We are interested in answering the question " how much performance is lost when using MoChA compared to using an offline attention mechanism ? " so changing the encoder could conflate the difference in performance between the two models . The question " how much performance is lost when switching from a bidirectional encoder to a unidirectional encoder ? " is interesting and important , but is orthogonal to what we are studying and has also been thoroughly considered in the past ( e.g. in Graves et al. 2013 ) . We have updated our wording to reflect exactly what we are studying and claiming .

> My concern with the statement that all hyper - parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model .
This is also an important concern ; however , the additional parameters required by MoChA compared to monotonic attention is tiny compared to the total number of parameters in the model because switching to MoChA amounts solely to adding a second attention energy function . For example , in our speech experiments , using MoChA increases the number of parameters by only 1.1 % . To fully address this question , we ran experiments where we doubled the attention energy function 's hidden dimension in a monotonic attention model and halved this hidden dimension in a MoChA model . This reconciles the difference in parameters in a natural way . In both cases , the change in performance was not significant over eight trials , implying that large gains achieved by MoChA were not caused by the change in parameter count . We added this information to the main text so that the comparison is clearer .

> My other concern is that in speech recognition the time - scale of the encoding is somewhat arbitrary , so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer .
While we agree that increasing the receptive field of the convolutional layers could be helpful , we note that the recurrent layers in the encoder can in principle provide an arbitrarily long temporal context on their own . In addition , Bahdanau et al . 2014 implied that attention provides a more efficient way give the decoder greater long - term context . To test this empirically , we ran the suggested experiment where we doubled the convolutional filter size along the time axis in a monotonic attention - based model and found that it did not significantly change performance over eight trials . We added this experiment to the main text .

> For document summarization the paper presents a strong result for an online model , but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this .
Our main rationale for including the document summarization experiment was to test MoChA in a setting where the input-output alignment was not monotonic . In terms of practicality , using MoChA would result in both a more efficient model ( as you suggested ) but could also allow for new applications such as online summarization . We added some additional clarification to the text as to our intentions behind this experiment .

> Sentence summarization tasks do exhibit mostly monotonic alignment , and most previous models with monotonic structure were evaluated on that , so why not test that here ?
We avoided sentence summarization for the simple reason that it is an easy enough task that monotonic attention already matches the performance of softmax attention ( see results in Raffel et al. 2017 ) . We expect that MoChA would also match softmax attention 's performance . Instead , we chose to try it on the more difficult ( and more realistic ) setting of CNN / daily mail . We included this discussion in the text of our paper to further motivate our experiment .

> this paper at best proposes a slightly better way to train and apply that model .
We consider MoChA to be a conceptually simple but remarkably effective improvement to monotonic attention . This is backed up by our experimental results , showing that we are able to significantly beat monotonic attention in settings where the alignment is monotonic ( speech ) and nonmonotonic ( summarization ) . We see the simplicity of implementing MoChA on top of monotonic attention as a strength of our approach , in that it allows researchers and practitioners to easily leverage it .

This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk ( certain window ) .

Pros .
- the paper is clearly written .
- the proposed method is applied to several sequence - to- sequence benchmarks , and the paper show the effectiveness of the proposed method ( comparable to full attention and better than previous hard monotonic assignments ) .
Cons .
- in terms of the originality , the methodology of this method is rather incremental from the prior study ( Raffel et al ) , but it shows significant gains from it .
- in terms of considering a monotonic alignment , Hori et al , " Advances in Joint CTC - Attention based End-to - End Speech Recognition with a Deep CNN Encoder and RNN - LM , " in Interspeech ' 17 , also tries to solve this issue by combining CTC and attention - based methods . The paper should also discuss this method in Section 4 .

Comments :
- Eq. ( 16 ) : $ j$ in the denominator should be $ t_j$ .


Thank you for your review ! We are glad you found the paper clearly written , and that you were convinced by our experimental evaluation . Addressing your specific comments :

> in terms of the originality , this method is rather incremental from the prior study ( Raffel et al )
We would argue that the strength of our model demonstrates that this is not an incremental result ; specifically , we saw a roughly 20 % relative improvement compared to monotonic attention in terms of both the word error rate on speech recognition and ROUGE - 2 on document summarization . Further , on speech recognition , we showed for the first time that an online attention mechanism could match the performance of an ( offline ) softmax attention mechanism , which opens up the possibilities of using this framework in online settings . While MoChA can be seen as a conceptually straightforward extension of Monotonic Attention , we actually see that as a benefit of the approach - it would potentially be less impactful if achieving these benefits required a complicated modification to the seq2seq framework . We have added some language to emphasize this at the end of Section 1 .

> - in terms of considering a monotonic alignment , Hori et al , " Advances in Joint CTC - Attention based End-to - End Speech Recognition with a Deep CNN Encoder and RNN - LM , " in Interspeech ' 17 , also tries to solve this issue by combining CTC and attention - based methods . The paper should also discuss this method in Section 4 .
Thank you for bringing this paper to our attention . The primary difference between that paper and ours it that it still uses an offline softmax attention mechanism , so could not be used in online settings . However , it provides promising evidence that our approach could be combined with CTC to achieve further gains in online settings . We 've added this reference and some discussion of it to our related work section .

> Eq. ( 16 ) : $ j$ in the denominator should be $ t_j$ .
Excellent catch , thank you ! We have fixed this .

****
I acknowledge the author 's comments and improve my score to 7 .
****

Summary :
The authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations .
The basic idea is to use datasets with known factors of variation , z , and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c.
The authors propose measures disentanglement , informativeness and completeness to evaluate the latent code c , mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables .
The paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could .

Significance :
The proposed methodology is relevant , because disentangling representations are an active field of research and currently are not evaluated in a standardized way .

Clarity :
The paper is lucidly written and very understandable .

Quality :
The authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant .
A few comments :
1 . How do the authors propose to deal with multimodal true latent factors ? What if multiple sets of z can generate the same observations and how does the evaluation of disentanglement fairly work if the underlying model cannot be uniquely recovered from the data ?
2 . Scoring disentanglement against known sources of variation is sensible and studied well here , but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation ?
3 . the actual sources of variation are interpretable and explicit measurable quantities here . However , oftentimes a source of variation can be a variable that is hard or impossible to express in a simple vector z ( for instance the sentiment of a scene ) even when these factors are known . How do the authors propose to move past narrow definitions of factors of variation and handle more complex variables ? Arguably , disentangling is a step towards concept learning and concepts might be harder to formalize than the approach taken here where in the experiment the variables are well - behaved and relatively easy to quantify since they relate to image formation physics .
4 . For a paper introducing a formal experimental framework and metrics or evaluation I find that the paper is light on experiments and evaluation . I would hope that at the very least a broad range of generative models and some recognition models are used to evaluate here , especially a variational autoencoder , beta - VAE and so on . Furthermore the authors could consider applying their framework to other datasets and offering a benchmark experiment and code for the community to establish this as a means of evaluation to maximize the impact of a paper aimed at reproducibility and good science .

Novelty :
Previous papers like " beta - VAE " ( Higgins et al. 2017 ) and " Bayesian Representation Learning With Oracle Constraints " by Karaletsos et al ( ICLR 16 ) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors , but have fallen short of proposing a formal framework like this paper does . It would be good to add a section gathering such attempts at evaluation previously made and trying to unify them under the proposed framework .


Thank you for your feedback . Please see our response to reviewer 2 , which addresses the points made by all three reviewers .

The paper addresses the problem of devising a quantitative benchmark to evaluate the capability of algorithms to disentangle factors of variation in the data .

* Quality *
The problem addressed is surely relevant in general terms . However , the contributed framework did not account for previously proposed metrics ( such as equivariance , invariance and equivalence ) . Within the experimental results , only two methods are considered : although Info-GAN is a reliable competitor , PCA seems a little too basic to compete against . The choice of using noise - free data only is a limiting constraint ( in [ Chen et al. 2016 ] , Info-GAN is applied to real - world data ) .
Finally , in order to corroborate the quantitative results , authors should have reported some visual experiments in order to assess whether a change in c_j really correspond to a change in the corresponding factor of variation z_i according to the learnt monomial matrix .

* Clarity *
The explanation of the theoretical framework is not clear . In fact , Figure 1 is straight in identifying disentanglement and completeness as a deviation from an ideal bijective mapping . But , then , the authors missed to clarify how the definitions of D_i and C_j translate this requirement into math .
Also , the criterion of informativeness of Section 2 is split into two sub-criteria in Section 3.3 , namely test set NRMSE and Zero - Shot NRMSE : such shift needs to be smoothed and better explained , possibly introducing it in Section 2 .

* Originality *
The paper does not allow to judge whether the three proposed criteria are original or not with respect to the previously proposed ones of [ Goodfellow et al. 2009 , Lenc & Vedaldi 2015 , Cohen & Welling 2014 , Jayaraman & Grauman 2015 ] .

* Significance *
The significance of the proposed evaluation framework is not fully clear . The initial assumption of considering factors of variations related to graphics - generated data undermines the relevance of the work . Actually , authors only consider synthetic ( noise - free ) data belonging to one class only , thus not including the factors of variations related to noise and / or different classes .

PROS :
The problem faced by the authors is interesting

CONS :
The criteria of disentanglement , informativeness & completeness are not fully clear as they are presented .
The proposed criteria are not compared with previously proposed ones - equivariance , invariance and equivalence [ Goodfellow et al. 2009 , Lenc & Vedaldi 2015 , Cohen & Welling 2014 , Jayaraman & Grauman 2015 ] . Thus , it is not possible to elicit from the paper to which extent they are novel or how they are related ..
The dataset considered is noise - free and considers one class only . Thus , several factors of variation are excluded a priori and this undermines the significance of the analysis .
The experimental evaluation only considers two methods , comparing Info-GAN , a state - of - the - art method , with a very basic PCA .


** FINAL EVALUATION **
The reviewer rates this paper with a weak reject due to the following points .
1 ) The novel criteria are not compared with existing ones [ Goodfellow et al. 2009 , Lenc & Vedaldi 2015 , Cohen & Welling 2014 , Jayaraman & Grauman 2015 ] .
2 ) There are two flaws in the experimental validation :
2.1 ) The number of methods in comparison ( InfoGAN and PCA ) is limited .
2.2 ) A synthetic dataset is only considered .

The reviewer is favorable in rising the rating towards acceptance if points 1 and 2 will be fixed .

** EVALUATION AFTER AUTHORS ' REBUTTAL **
The reviewer has read the responses provided by the authors during the rebuttal period . In particular , with respect to the highlighted points 1 and 2 , point 1 has been thoroughly answered and the novelty with respect previous work is now clearly stated in the paper . Despite the same level of clarification has not been reached for what concerns point 2 , the proposed framework ( although still limited in relevance due to the lack of more realistic settings ) can be useful for the community as a benchmark to verify the level of disentanglement than newly proposed deep architectures can achieve . Finally , by also taking into account the positive evaluation provided by the fellow reviewers , the rating of the paper has been risen towards acceptance .


Thank you for your feedback . Please see our response to reviewer 2 , which addresses the points made by all three reviewers .

The authors consider the metrics for evaluating disentangled representations . They define three criteria : Disentanglement , Informativeness , and Completeness . They learning a linear mapping from the latent code to an idealized set of disentangled generative factors , and then define information - theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights . Experimental evaluation considers a dataset of 200 k images of a teapot with varying pose and color .

I think that defining metrics for evaluating the degree of disentanglement in representations is great problem to look at . Overall , the metrics approached by the authors are reasonable , though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me .

A second limitation of the work is the reliance on a " true " set of disentangled factors . We generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner , which means that we will in general not have access supervision data for the disentangled factors . Could the authors perhaps comment on how well these metrics would work in the semi-supervised case ?

Overall , I would say this is somewhat borderline , but I could be convinced to argue for acceptance based on the other reviews and the author response .

Minor Commments :

- Tables 1 and 2 would be easier to unpack if the authors were to list the names of the variables ( i.e. azimuth instead of z_0 ) or at least list what each variable is in the caption .

- It is not entirely clear to me how the proposed metrics , whose definitions all reference magnitudes of weights , generalize to the case of random forests .

We thank the reviewers for their helpful comments , and appreciate the
view that " defining metrics for evaluating the degree of
disentanglement in representations is great problem to look at " .

Two reviewers raise the issue that our work requires a " true " set of
generative factors in order to carry out the evaluation . Our response
is that if it is not possible to quantify disentanglement in this
situation , it will certainly be much more difficult to quantify it
when the ground truth is not known , and this must be the first step .
We have now emphasized in the abstract , introduction and conclusion
that the method applies when the ground truth generative factors are
known .

R1 :
> However , the contributed framework did not account for previously proposed
> metrics ( such a equivariance , invariance and equivalence ) .
> ...
> The paper does not allow to judge whether the three proposed criteria
> are original or not with respect to the previously proposed ones of
> [ Goodfellow et al. 2009 , Lenc & Vedaldi 2015 , Cohen & Welling 2014 ,
> Jayaraman & Grauman 2015 ] .
> …
> The novel criteria are not compared with existing ones [ Goodfellow et al .
> 2009 , Lenc & Vedaldi 2015 , Cohen & Welling 2014 , Jayaraman & Grauman
> 2015 ] .
and
R3 :
> Previous papers like " beta - VAE " ( Higgins et al. 2017 ) and " Bayesian
> Representation Learning With Oracle Constraints " by Karaletsos et al
> ( ICLR 16 ) have followed similar experimental protocols inspired by the
> same underlying idea of recovering known latent factors , but have
> fallen short of proposing a formal framework like this paper does . It
> would be good to add a section gathering such attempts at evaluation
> previously made and trying to unify them under the proposed
> framework .

We have added sec 3 to expand the coverage of related work . The
relationship to equivariance and invariance is covered in the last
paragraph of sec 3 ; note that such properties arise naturally
from a properly disentangled and informative representation .

We have expanded the comparison to Higgins et al. ( 2017 ) and
Karaletsos et al ( 2016 ) in sec 3 . We have also added here a paragraph
on similarities / differences to the work of Yang and Amari ( 1997 ) wrt
the evaluation of ICA , following comments we received on the paper .

R2 :
> Within the experimental results , only two methods are considered :
> although Info-GAN is a reliable competitor , PCA seems a little too
> basic to compete against .
> ...
> The experimental evaluation only considers two methods , comparing
> Info-GAN , a state - of - the - art method , with a very basic PCA .
and
R3 :
> The paper ultimately is light on comprehensive evaluation of popular models
> on a variety of datasets and as such does not quite yield the insights it
> could .
> ...
> For a paper introducing a formal experimental framework and metrics or
> evaluation I find that the paper is light on experiments and evaluation . I
> would hope that at the very least a broad range of generative models and
> some recognition models are used to evaluate here , especially a variational
> autoencoder , beta-VAE and so on .

Our experiments highlight the differences between a baseline ( PCA ) and a
state-of-the-art method ( InfoGAN ) . This contrastive
comparison demonstrates the appropriateness of the framework , with the
three criteria clearly explaining why InfoGAN 's learnt code is superior to PCA 's
and the metric scores quantifying this level of superiority . We will make the
code and dataset publicly available on acceptance of the paper and hope this
facilitates further comparisons and eventually the establishment of quantitative
benchmarks for disentangled factor learning . We note e.g. that the authors of
the beta- VAE have not published their code , which has made conducting the
requested experiments more difficult .

R3 :
> Furthermore the authors could consider ... offering a benchmark
> experiment and code for the community to establish this as a means
> of evaluation to maximize the impact of a paper aimed at
> reproducibility and good science .

We will be happy to make the dataset and code publicly available
on acceptance of the paper , as now mentioned in the conclusion .

R2 :
> though the way pseudo-distribution are define in terms of normalized weight
> magnitudes is seems a little ad hoc to me .
> ...
> It is not entirely clear to me how the proposed metrics , whose
> definitions all reference magnitudes of weights , generalize to the
> case of random forests .

We thank the reviewer for this feedback . We have now clarified these points
by defining the relative importances R_{ij} on p2 , and discussing the definition
of importances for random forests as per Breiman et al ( 1984 ) on p3.

R1 :
> The significance of the proposed evaluation framework is not fully clear . The
> initial assumption of considering factors of variations related to
> graphics - generated data undermines the relevance of the work .

We have further clarified the significance of the framework in Section
1 & 5 . The framework is not restricted to graphics - generated data ,
it could also be used e.g. with speech synthesis .

R1 :
> But , then , the authors missed to clarify how the definitions of D_i
> and C_j translate this requirement into math .

The descriptions of disentanglement and completeness on p 2&3
make it clear how D_i and C_j quantify the deviation from an
ideal bijective mapping .

R1 :
> Also , the criterion of informativeness of Section 2 is split into two
> sub-criteria in Section 3.3 , namely test set NRMSE and Zero - Shot
> NRMSE : such shift needs to be smoothed and better explained , possibly
> introducing it in Section 2 .

We thank the reviewer for pointing this out . We have now clarified ( sec 4.1 final
sentence ) that the zero- shot inference task is a " bonus " , and not a core
component of the framework .

R1 :
> The dataset considered is noise - free and considers one class
> only . Thus , several factors of variation are excluded a priori and
> this undermines the significance of the analysis .

It would be easy to add noise ( e.g. Gaussian ) to the output of the
renderer , but we do not believe that this would have a substantial
effect on the results . It would be interesting to expand the
experiments to cover more object classes , but we believe that the
framework and experiments presented already constitute a substantial
advance .

R3 :
> 1 . How do the authors propose to deal with multimodal true latent
> factors ? What if multiple sets of z can generate the same observations
> and how does the evaluation of disentanglement fairly work if the
> underlying model cannot be uniquely recovered from the data ?

If multiple sets of z can generate the same observations , then this
should be reflected in a ( multimodal ) distribution within the codes
c . If this were present then it would be propagated via the mapping f
from c to z into a distribution over z's . Current methods like InfoGAN
tend to make a unimodal assumption about Q( c|x ) , but if this were
multimodal then the above mechanism would work , and one could use the
obvious log-likelihood criterion log p( z|c ) to train the
regression network ( e.g. like a mixture of experts , Jacobs et al
1991 ) . Of course the ordinary least squares criterion is just a
special case of this with a Gaussian noise model for p( z|c ) .
We have also added a paragraph at the bottom of p3 concerning the
rotation - of-factors case , for which the model is not identifiable .

R1 :
> ... in order to corroborate the quantitative results , authors
> should have reported some visual experiments in order to assess
> whether a change in c_j really correspond to a change in the
> corresponding factor of variation z_i according to the learnt monomial
> matrix .

Please see Figure 6 , as per the original submission .

R3 :
> 3 . the actual sources of variation are interpretable and explicit
> measurable quantities here . However , oftentimes a source of variation
> can be a variable that is hard or impossible to express in a simple
> vector z ( for instance the sentiment of a scene ) even when these
> factors are known . How do the authors propose to move past narrow
> definitions of factors of variation and handle more complex variables ?
> Arguably , disentangling is a step towards concept learning and
> concepts might be harder to formalize than the approach taken here
> where in the experiment the variables are well - behaved and relatively
> easy to quantify since they relate to image formation physics .

It is vital to be able to quantify disentangling wrt what R3 calls a
" simple " vector z . The contribution of the paper is to do this . There
may well be more complex sources of latent structure , such as the
inter - relationship of different objects in a scene . In our view
this can likely be handled by an appropriate hierarchical
model with a vector of z's at the highest level , but this is
an issue for future work .

R2 :
> Tables 1 and 2 would be easier to unpack if the authors were to list
> the names of the variables ( i.e. azimuth instead of z_0 ) or at least
> list what each variable is in the caption .

fixed .

This paper studies the adjustment of dropout rates which is a useful tool to prevent the overfitting of deep neural networks . The authors derive a generalization error bound in terms of dropout rates . Based on this , the authors propose a regularization framework to adaptively select dropout rates . Experimental results are also given to verify the theory .

Major comments :
( 1 ) The Empirical Rademacher complexity is not defined . For completeness , it would be better to define it at least in the appendix .
( 2 ) I can not follow the inequality ( 5 ) . Especially , according to the main text , f^L is a vector-valued function . Therefore , it is not clear to me the meaning of \sum\sigma_if^ L ( x_i , w ) in ( 5 ) .
( 3 ) I can also not see clearly the third equality in ( 9 ) . Note that f^l is a vector-valued function . It is not clear to me how it is related to a summation over j there .
( 4 ) There is a linear dependency on the number of classes in Theorem 3.1 . Is it possible to further improve this dependency ?

Minor comments :
( 1 ) Section 4 : 1e-3,1e - 4,1e - 5 is not consistent with 1e^{ - 3 } , 1e^{ - 4 } ,1e^{ -5 }
( 2 ) Abstract : there should be a space before " Experiments " .
( 3 ) It would be better to give more details ( e.g. , page , section ) in citing a book in the proof of Theorem 3.1

Summary :
The mathematical analysis in the present version is not rigorous . The authors should improve the mathematical analysis .

----------------------------
After Rebuttal :
Thank you for revising the paper . I think there are still some possible problems .
Let us consider eq ( 12 ) in the appendix on the contraction property of Rademacher complexity ( RC ) .
( 1 ) Since you consider a variant of RC with absolute value inside the supermum , to my best knowledge , the contraction property ( 12 ) should involve an additional factor of 2 , see , e.g. , Theorem 12 of " Rademacher and Gaussian Complexities : Risk Bounds and Structural Results " by Bartlett and Mendelson . Since you need to apply this contraction property L times , there should be a factor of 2 ^ L in the error bound . This make the bound not appealing for neural networks with a moderate L.
( 2 ) Second , the function g involves an expectation w.r.t. r before the activation function . I am not sure whether this existence of expectation w.r.t. r would make the contraction property applicable in this case .

Thanks very much for your review and comments .

About your major comments

( 1 ) :
Thanks for your suggestion , we will include the definition of empirical Rademacher complexity in our revision .

( 2 ) and ( 3 ) :
As we stated in the first paragraph of our proof 6.1 , we treat the functions fed into the neurons of the l-th layer as one class of functions . Therefore , f^L ( x ; W ) is a vector as you correctly pointed out , but f^L ( x ; w ) is a scalar . So each dimension of f^L ( x ; W ) is viewed as one instance coming from the same function class f^L ( x ; w ) . Similar ways of proof have been adopted in Wan et al . ( 2013 ) . We are sorry about the confusion . We will add more descriptions about it to make that clear in our revision .

( 4 )
It is a good question . The dependency on the number of classes comes from the contraction lemma . However , what we proved is only a weak bound on the Rademacher complexity . We are still working on further tightening the bound . For now , we are not sure if we can reduce the dependency on the number of classes to sub-linear . We hope this work will also open additional research directions and future extensions to the community . You are always welcome to add to it .

About your minor comments :

( 1 ) ( 2 ) Thanks for the careful examination . We will fix the typos in the next version .

( 3 ) Thanks for the comments .
Contraction lemma ( Shalev - Shwartz & Ben-David , 2014 ) is a variant of the Lemma 26.9 located on page 381 , Chapter 26 .
Lemma 26.11 in Shalev - Shwartz & Ben-David ( 2014 ) is located on page 383 , Chapter 26.2 .
We will add the chapters and pages to the proof .


Thanks very much for your careful examination . We do appreciate it .
( 1 ) If you look at the final Rademacher complexity bound we are proving , it has no absolute value inside the supremum . The contraction lemma is applied to the Rademacher complexity without absolute value . That is why equation ( 7 ) comes after the contraction . We understand this is confusing . We will make it clear in the next version .
( 2 ) As you mentioned , if we take expectation with respect to r , then f^L is not a function of r any more . Actually in our definition , the final prediction function f^L is a deterministic function ( since we take the expectation w.r.t. r ) .

== Main comments

The authors connect dropout parameters to a bound of the Rademacher complexity ( Rad ) of the network . While it is great to see deep learning techniques inspired by learning theory , I think the paper makes too many leaps and the Rad story is ultimately unconvincing . Perhaps it is better to start with the resulting regularizer , and the interesting direct optimization of dropout parameters . In its current form , the following leaps problematic and were not addressed in the paper :

1 ) Why is is adding Rad as a regularizer reasonable ? Rad is usually hard to compute , and most useful for bounding the generalization error . It would be interesting if it also turns out to be a good regularizer , but the authors do not say why nor cite anything . Like the VC dimension , Rad itself depends on the model class , and can not be directly optimized . Even if you can somehow optimize over the model class , these quantities give very loose bounds , and do not equal to generalization error . For example , I feel even just adding the actual generalization error bound is more natural . Would it make sense to just add Rad to the objective in this way for a linear model ?

2 ) Why is it reasonable to go from a regularizer based on RC to a loose bound of Rad ? The actual resulting regularizer turns out to be a weight penalty but this seems to be a rather loose bound that might not have too much to do with Rad anymore . There should be some analysis on how loose this bound is , and if this looseness matter at all .

The empirical results themselves seem reasonable , but the results are not actually better than simpler methods in the corresponding tasks , the interpretation is less confident . Afterall , it seems that the proposed method had several parameters that were turned , where the analogous parameters are not present in the competing methods . And the per unit dropout rates are themselves additional parameters , but are they actually good use of parameters ?

== Minor comments

The optimization is perhaps also not quite right , since this requires taking the gradient of the dropout parameter in the original objective . While the authors point out that one can use the mean , but that is more problematic for the gradient than for normal forward predictions . The gradient used for regular learning is not based on the mean prediction , but rather the samples .

tiny columns surrounding figures are ugly and hard to read


Thanks very much for your valuable comments and helpful suggestions .

Q : Why adding Rad as a regularizer reasonable ? Why is it reasonable to go from a regularizer based on RC to a loose bound of Rad ?
A : These are great questions . We agree we do not have a rigorous way to prove adding an approximate upper bound to the objective can lead to any theoretical guarantee as you correctly pointed out . The theorem of the upper bound in the paper is rigorous but why adding the upper bound to the objective can help is heuristic and empirical .

On the other hand , adding an approximate term that is related to the upper bound of the Rademacher complexity is not something new . For example , the squared L2 norm regularizer used in the ridge regression , though there are explanations such as Bayesian priors , can be interpreted as a term related to the upper bound of the Rademacher complexity of linear classes . People are already using it . Similarly , the L1 regularizer used in LASSO can also be interpreted as a term related to the Rademacher complexity bound . We have put a section in the Appendix ( Section 6.3 ) to somewhat justify it in a heuristic way .

Q : The actual resulting regularizer turns out to be … rather loose bound …
A : We agree that the bound proved in the paper could be a bit loose . Still in some extreme cases it is tight . For example , as we indicated in the paragraph before Section 3.3 , if the retain rates in one layer are all zeros , the model always makes random guess for prediction . In this case the empirical Rademacher complexity is zero and our bound is tight . In general , even if the bound is loose , it still gives some justification on the norms used in today ’s neural network regularizations . Additionally , it leads to a systematic way of weighting the norms as well as the retain rates .

Minor comments :
Q : While the authors point out that one can use the mean , but that is more problematic for the gradient than for normal forward predictions . After all , the gradient used for regular learning is not based on the mean prediction , but rather the samples .
A : This is an excellent question . As we stated in Section 3.3 , “ this is an approximation to the true f^L ( x ; W , θ ) ” . Using the mean is purely an approximation used for the sake of optimization efficiency . By design we should use the samples . However empirically we found that optimizing based on the mean ( instead of the actual sampling ) still leads to a decrease of the objective . We will add additional figures to better illustrate the point in our next revision .

Q : tiny columns surrounding figures are ugly and hard to read
A : Thanks for the suggestion . We will fix it in our revision .

Q : dropout rate is perhaps more common than retain rate
A : We use the retain rate instead just to make the upper bound look less messy .

An important contribution . The paper is well written . Some questions that needs to be better answered are listed here .
1 . The theorem is difficult to decipher . Some remarks needs to be included explaining the terms on the right and what they mean with respect to learnability or complexity .
2 . How does the regularization term in eq ( 2 ) relate to the existing ( currently used ) norm based regularizers in deep network learning ? It may be straight forward but some small simulation / plots explaining this is important .
3 . Apart from the accuracy results , the change in computational time for working with eq ( 2 ) , rather than using existing state - of - the - art deep network optimization needs to be reported ? How does this change vary with respect to dataset and network size ( beyond the description of scaled regularization in section 4 ) ?
4 . Confidence intervals needs to be computed for the retain -rates ( reported as a function of epoch ) . This is critical both to evaluate the stability of regularizers as well as whether the bound from theorem is strong .
5 . Did the evaluations show some patterns on the retain rates across different layers ? It seems from Figure 3 , 4 that retain rates in lower layers are more closer to 1 and they decrease to 0.5 as depth increases . Is this a general pattern ?
6 . It has been long known that dropout relates to non- negative weighted averaging of partially learned neural networks and dropout rate of 0.5 provides best dymanics . The evaluations say that clearly 0.5 for all units / layers us not correct . What does this mean in terms of network architecture ? Is it that some layers are easy to average ( nothing is learned there , so dropped networks have small variance ) , while some other layers are sensitive ?
7 . What are some simple guidelines for choosing the values of p and q? Again it appears p=q=2 is the best , but need confidence intervals here to say anything substantial .

Thanks very much for your encouraging comments and helpful suggestions .

1 . The upper bound suggests that layers affect the complexity in a multiplicative way . An extreme case as we described in the last paragraph of section 3.2 is , if the dropout retain rates for one layer are all zeros , then the empirical Rademacher complexity for the whole network is zero since the network is doing random guess for predictions . In this case the bound is tight . We will put more descriptions about the terms in our bound .

2 . This is an interesting suggestion . Norm based regularizers currently used are imposed on the weights of each layer without considering the retain rates and the regularization is done on each layer independently . We suggest organizing them in a systematic way .

3 . In terms of running time , the proposed framework takes one additional backpropagation compared to its standard deep network counterpart . In practice , we find the running time per epoch after introducing the regularizer is approximately 1.6 to 1.9 times that of the current standard deep network .

4 : Thanks for the great suggestions . There are some potential issues with drawing the confidence intervals for the retain rate of a particular neuron . For example , permuting the neurons does not change the network structure but it may lead to some identifiability issues . Instead to demo the stability of the algorithm we may add a plot showing the histograms of the theta with different initializations .

5 . This is an excellent question ! In fact , we are conducting additional evaluations to verify this pattern . We had some preliminary empirical observations that , as the layer goes higher , fewer neurons get high retain rates . This is somewhat consistent with the fact that people tend to set the number of neurons smaller for higher layers . We still need more experiments to tell if this is a general pattern .

6 . This is another great question . It is also related to an on-going follow - up work we are currently investigating as stated in the conclusion and future work section of our paper . If we use the setting of p=\infty and q=1 , the L1 norm regularizer may produce sparse retain rates . Subsequently , we could prune the corresponding neuron . Therefore we could use the algorithm as a way to determine the number of neurons used in hidden layers , i.e. , we can use the regularizer to tune the network architecture . Similarly , if we use p=1 and q=\infty , then we can expect sparse coefficients on W due to the property of the L1 norm , in this way the regularizer can also be used to prune the internal neural connections .

7 . Currently we do not have any theory for choosing p and q. As we stated above , one way is to choose p and q based on the sparsity desire . If we would like to impose sparsity on the number of neurons to fire , we may set q=1 to promote sparse retain rates . On the other hand , if we would like to impose sparsity on the number of internal connections , i.e. , have a sparse coefficient matrix W , we may set p=1 instead .

It is well known that the original GAN ( Goodfellow et al. ) suffers from instability and mode collapsing . Indeed , existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies ( for the minmax game ) . The present paper proposes to obtain mixed strategy through an online learning approach . Online learning ( no regret ) algorithms have been used in finding an equilibrium for zero sum game . However , most theoretical convergence results are known for convex - concave loss . One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network ( and concave in M ) . In particular , the concave player plays the FTRL algorithm with standard L2 regularization term . The regret of concave player can be bounded using existing result for FTRL . The regret for the other player is more interesting : it uses the fact the adversary 's strategy does n't change too drastically . Then a lemma by Kalai and Vempala can be used . The theory part of the paper is reasonable and quite well written .

Based on the theory developed , the paper presents a practical algorithm . Compared to the standard GAN training , the new algorithm returns mixed strategy and examine several previous models ( instead of the latest ) in each iteration . The paper claims that this may help to prevent model collapsing .

However , the experimental part is less satisfying . From figure 2 , I do n't see much advantage of Checkhov GAN . In other experiments , I do n't see much improvement neither ( CIFAR10 and CELEBA ) . The paper did n't really compare other popular GAN models , especially WGAN and its improved version , which is already quite popular by now and should be compared with .

Overall , I think it is a borderline paper .

-------------------------
I read the response and the new experimental results regarding WGAN .
The experimental results make more sense now .
It would be interesting to see whether the idea can be applied to more recent GAN models and still perform better .
I raised my score to 7 .



Dear Reviewer ,

We thank you for your feedback and for your positive review regarding the theoretical part of the paper . In the following we answer your concerns and requests .

Your review indicates that you believe the experimental part does not demonstrate the benefits of our approach . We respectfully disagree and we kindly ask you to look again at the experiments . Moreover , 1 ) as you requested , we have added new results further highlighting the benefits of our approach compared to the baselines , and 2 ) we improved the visibility of Figure 2 which we believe might have confused the reviewer in the original submission since some colors might not have been easily readable .

To be specific :
- In Figure 2 ( toy example ) , you can see that after 50 K steps our method captures all of the modes while the standard GAN is missing one mode .
- In Table 1 ( MNIST ) , you can see that our method ( 10 states ) generates 26 % more classes compared to standard GAN training , and stabilizes the training which can be observed through the reduced variance . Please note that our approach also does better with respect to the reverse KL measure .
- In Table 2 ( CIFAR 10 ) , you can see that our methods consistently outperform the standard GAN training with respect to the MSE measure . Concretely , the MSE of our method ( 25 states ) is lower by 20 % than the MSE of the standard training method . In terms of number of images from the test / training set with the lowest reconstruction , GAN achieves 0 % , whereas our best variant achieves 82.43 % and 80.33 % , respectively .
- In Table 4 ( CelebA ) , you can see that our method outperforms the standard GAN training with respect to the number of modes that are classified by the auxiliary discriminator as not real .
Concretely , our method improves by a factor of 50 % .
This means that using an auxiliary discriminator based on our method , only 1400 images of the true test set were recognized by the auxiliary discriminator as fake .
Conversely , using an auxiliary discriminator on the standard method , there were 3000 images recognized by the auxiliary discriminator as fake .
- In addition , we have now added a new section with experiments showcasing Inception Score and Frechet Dirichlet Distance ( FID ) for our method and show improvement over several strong baselines ( Section 5.3 ) .

“ From figure 2 , I do n't see much advantage of Checkhov GAN ”
See our answer above , light colors indicate a mode with less mass . We therefore clearly observe that the normal GAN suffers from mode collapse , while our approach finds all the modes . We hope that by going again over the experimental part , you will find that our method does improve over the standard GAN training .

Regarding comparison to other methods : WGAN is a different loss function rather than a different GAN training method . Our method can be applied to other GAN loss functions , including WGAN . Thus , WGAN is not a competitor to our approach , but rather another setup where our approach could be applied . We do agree that a comparison to WGAN is valuable and we have now added the requested results to the paper ( please see Table 5 , Figure 3 , Figure 4 and Figure 11 ) .
* Applying our method on top of WGAN ( denoted Chekhov WGAN in the paper ) , outperforms WGAN across all epochs consistently in terms of both metrics ( inception score and FID ) .


This is an interesting paper , exploring GAN dynamics using ideas from online learning , in particular the pioneering " sparring " follow - the-regularized leader analysis of Freund and Schapire ( using what is listed here as Lemma 4 ) . By restricting the discriminator to be a single layer , the maximum player plays over a concave ( parameter ) space which stabilizes the full sequence of losses so that Lemma 3 can be proved , allowing proof of the dynamics ' convergence to a Nash equilibrium . The analysis suggests a practical ( heuristic ) algorithm incorporating two features which emerge from the theory : L2 regularization and keeping a history of past models . A very simple queue for the latter is shown to do quite competitively in practice .

This paper merits acceptance on theoretical merits alone , because the FTRL analysis for convex - concave games is a very robust tool from theory ( see also the more recent sequel [ Syrgkanis et al . 2016 " Fast convergence of regularized learning in games " ] ) that is natural to employ to gain insight on the much more brittle GAN case . The practical aspects are also interesting , because the incorporation of added randomness into the mixed generation strategy is an area where theoretical justifications do motivate practical performance gains ; these ideas could clearly be developed in future work .

Dear Reviewer ,

Thank you for your detailed and supportive review !


The paper applies tools from online learning to GANs . In the case of a shallow discriminator , the authors proved some results on the convergence of their proposed algorithm ( an adaptation of FTRL ) in GAN games , by leveraging the fact that when D update is small , the problem setup meets the ideal conditions for no - regret algorithms . The paper then takes the intuition from the semi-shallow case and propose a heuristic training procedure for deep GAN game .

Overall the paper is very well written . The theory is significant to the GAN literature , probably less so to the online learning community . In practice , with deep D , trained by single gradient update steps for G and D , instead of the " argmin " in Algo 1. , the assumptions of the theory break . This is OK as long as sufficient experiment results verify that the intuitions suggested by the theory still qualitatively hold true . However , this is where I have issues with the work :

1 ) In all quantitative results , Chekhov GAN do not significantly beat unrolled GAN . Unrolled GAN looks at historical D 's through unrolled optimization , but not the history of G . So this lack of significant difference in results raise the question of whether any improvement of Chekhov GAN is coming from the online learning perspective for D and G , or simply due to the fact that it considers historical D models ( which could be motivated by sth other than the online learning theory ) .

2 ) The mixture GAN approach suggested in Arora et al. ( 2017 ) is very related to this work , as acknowledged in Sec. 2.1 , but no in - depth analysis is carried out . I suggest the authors to either discuss why Chekhov GAN is obviously superior and hence no experiments are needed , or compare them experimentally .

3 ) In the current state , it is hard to place the quantitative results in context with other common methods in the recent literature such as WGAN with gradient penalty . I suggest the authors to either report some results in terms of inception scores on cifar10 with similar architectures used in other methods for comparison . Alternatively please show WGAN - GP and / or other method results in at least one or two experiments using the evaluation methods in the paper .

In summary , almost all the experiments in the paper are trying to establish improvement over basic GAN , which would be OK if the gap between theory and practice is small . But in this case , it is not . So it is not entirely convincing that the practical Algo 2 works better for the reason suggested by the theory , nor it drastically improves practical results that it could become the standard technique in the literature .

Dear Reviewer ,

Thank you for your valuable feedback and the positive review regarding the theoretical part of the paper . We have added a significant number of experimental results to address your concerns and suggestions about the experimental part of the paper . We detail the changes made to the paper below .

1 ) Unrolled GAN : We first would like to point out that , unlike what you mentioned in your review , unrolled GAN does not look at historical D ’s but rather it looks at ** future * * discriminator ’s updates by unrolling a few optimization steps , thus making the two algorithms very different . The unrolling procedure comes with significant drawbacks as one needs to compute future steps which will not be used to update the model parameters . In contrast , our approach makes use of past steps and is therefore less wasteful in terms of computation ( note that this could be further sped - up using parallel computations , which cannot be done with Unrolled GAN ) . Experiments on toy datasets show the benefits of using the history ( Chekhov GAN ) in comparison to Unrolled GAN ( Figure 7 ) . We have now added more experimental results comparing to Unrolled GAN in terms of Inception Score and FID , where we can clearly see that our approach achieves significantly better scores ( Table 5 , Figure 4 and Figure 11 ) .

2 ) The mixture GAN approach suggested in Arora et al : We agree with the reviewer that a comparison to this work is valuable and we have now added the requested results to the updated version of the paper ( Table 5 ) . These results demonstrate that our approach achieves better results than Arora et al . when both methods are applied on top of WGAN , despite having 5 times less trainable parameters . Applying both methods on top of GAN yields comparable scores , but with notably less variance for Chekhov GAN .


3 ) WGAN & WGAN - GP : As requested by the reviewer , we have added a comparison to WGAN and WGAN GP in Table 5 in the paper where we clearly see better scores for Chekhov GAN . Note that although we originally developed our approach for the vanilla GAN , a similar algorithm can be applied to the Wasserstein objective which is also a min-max objective . As a proof of concept , we also provide empirical results for our approach with the Wasserstein objective and show that it also consistently improves upon the baseline .

In summary , Chekhov GAN outperforms GAN over various metrics ( MSE , number of missing modes , reverse KL divergence , number of generated classes , missing modes due to catastrophic forgetting ) and achieves comparable performance in terms of Inception Score and FID , with reduced variance . At the same time , we successfully apply our algorithm on top of WGAN and achieve consistent improvement . We further show improvement across several other baselines as well .

As you have mentioned , our algorithm has theoretical guarantees which we believe are of significant interest for the GAN literature . The practical algorithm is strongly inspired by the theory ( using the history of generators and discriminators and updating the parameters by taking a gradient step guided by the FTRL objective ) and outperforms the baselines across several metrics and datasets . Closing the gap between the theory and practice would be an interesting direction for future work as there are clear theoretical benefits .


The paper provides an approach to learning reward functions in high - dimensional domains , showing that it performs comparably to other recent approaches to this problem in the imitation - learning setting . It also argues that a key property to learning generalizable reward functions is for them to depend on state , but not state-action or state-action -state . It uses this property to produce " disentangled rewards " , demonstrating that they transfer well to the same task under different transition dynamics .

The need for " state - only " rewards is a useful insight and is covered fairly well in the paper . The need for an " adversarial " approach is not justified as fully , but perhaps is a consequence of recent work . The experiments are thorough , although the connection to the motivation in the abstract ( wanting to avoid reward engineering ) is weak .

Detailed feedback :

" deployed in at test - time on environments " -> " deployed at test time in environments " ?

" which can effectively recover disentangle the goals " -> " which can effectively disentangle the goals " ?

" it allows for sub-optimality in demonstrations , and removes ambiguity between demonstrations and the expert policy " : I am not certain what is being described here and it does n't appear to come up again in the paper . Perhaps remove it ?

" r high -dimensional ( Finn et al. , 2016 b ) Wulfmeier " -> " r high -dimensional ( Finn et al. , 2016 b ) . Wulfmeier " .

" also consider learning cost function with " -> " also consider learning cost functions with " ?

" o learn nonlinear cost function have " -> " o learn nonlinear cost functions have " .

" are not robust the environment changes " -> " are not robust to environment changes " ?

" We present a short proof sketch " : It is unclear to me what is being proven here . Please state the theorem .

" In the method presented in Section 4 , we can not learn a state - only reward function " : I 'm not seeing that . Or , maybe I 'm confused between rewards depending on s vs. s, a vs. s , a , s '. Again , an explicit theorem statement might remove some confusion here .

" AIRLperforms " -> " AIRL performs " .

Figure 2 : The blue and green colors look very similar to me . I 'd recommend reordering the legend to match the order of the lines ( random on the bottom ) to make it easier to interpret .

" must reach to goal " -> " must reach the goal " ?

" pointmass " -> " point mass " . ( Multiple times . )

Amin , Jiang , and Singh 's work on efficiently learning a transferable reward function seems relevant here . ( Although , it might not be published yet : https://arxiv.org/pdf/1705.05427.pdf.)

Perhaps the final experiment should have included state - only runs . I 'm guessing that they did n't work out too well , but it would still be good to know how they compare .


Thank you for the detailed feedback . We have included all of the typo corrections and clarifications , as well as included state - only runs in the imitation learning experiments ( Section 7.3 ) . As detailed below , we believe that we have addressed all of the issues raised in your review , but we would appreciate any further feedback you might offer .

> The need for an " adversarial " approach is not justified as fully , but perhaps is a consequence of recent work .

Adversarial approaches are an inherent consequence of using sampling - based methods for training energy - based models , and we ’ve edited Section 2 , paragraph 2 to make this more clear . There is in fact no other ( known ) choice for doing this : any method that does maxent IRL and generates samples ( rather than assuming known dynamics ) must be adversarial in nature , as shown by Finn16a . Traditional methods like tabular MaxEnt IRL [ Ziebart 08 ] have an adversarial nature as they must alternate between an inner-loop RL problem ( the sampler ) and updating the reward function ( the discriminator ) .

> Although the connection to the motivation in the abstract ( wanting to avoid reward engineering ) is weak .

We ’ve slightly modified the paragraph before section 7.1 to make this connection more clear . We use environments where a reward function is available for the purpose of easily collecting demonstrations ( otherwise we would need to resort to motion capture or teleoperation ) . However the experimental setup after demo collection is exactly the same as one would encounter while using IRL when a ground truth reward is not available .

> Amin , Jiang , and Singh 's work on efficiently learning a transferable reward function seems relevant here . ( Although , it might not be published yet : https://arxiv.org/pdf/1705.05427.pdf.)

Amin , Jian & Singh ’s work is indeed relevant and we have also included it in the related work section .

> Perhaps the final experiment should have included state - only runs . I 'm guessing that they did n't work out too well , but it would still be good to know how they compare .

We ’ve included these in the experiments . State -only runs perform slightly worse as expected , since the true reward has torque penalty terms which depend on the action , and can not be captured by the model . However the performance is n’t so bad that the agent fails to solve the task .


SUMMARY :
This paper considers the Inverse Reinforcement Learning ( IRL ) problem , and particularly suggests a method that obtains a reward function that is robust to the change of dynamics of the MDP .

It starts from formulating the problem within the MaxEnt IRL framework of Ziebart et al . ( 2008 ) . The challenge of MaxEnt IRL is the computation of a partition function . Guided Cost Learning ( GCL ) of Finn et al. ( 2016 b ) is an approximation of MaxEnt IRL that uses an adaptive importance sampler to estimate the partition function . This can be shown to be a form of GAN , obtained by using a specific discriminator [ Finn et al. ( 2016 a ) ] .

If the discriminator directly works with trajectories tau , the result would be GAN -GCL . But this leads to high variance estimates , so the paper suggests using a single state-action formulation , in which the discriminator f_theta ( s , a ) is a function of ( s , a ) instead of the trajectory . The optimal solution of this discriminator is to have f( s , a ) = A ( s , a ) — the advantage function .
The paper , however , argues that the advantage function is “ entangled ” with the dynamics , and this is undesirable . So it modified the discriminator to learn a function that is a combination of two terms , one only depends on state-action and the other depends on state , and has the form of shaped reward transformation .


EVALUATION :

This is an interesting paper with good empirical results . As I am not very familiar with the work of Finn et al. ( 2016a ) and Finn et al . ( 2016 b ) , I have not verified the detail of derivations of this new paper very closely . That being said , I have some comments and questions :


* The MaxEnt IRL formulation of this work , which assumes that p_theta ( tau ) is proportional to exp ( r_theta ( tau ) ) , comes from
[ Ziebart et al. , 2008 ] and assumes a deterministic dynamics . Ziebart ’s PhD dissertation [ Ziebart , 2010 ] or the following paper show that the formulation is different for stochastic dynamics :

Ziebart , Bagnell , Dey , “ The Principle of Maximum Causal Entropy for Estimating Interacting Processes , ” IEEE Trans. on IT , 2013 .

Is it still a reasonable thing to develop based on this earlier , an inaccurate , formulation ?


* I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants .
It is suggested that since the only items on both sides of the equation on top of p. 13 depend on s’ are h* and V , they should be equal .
This would be true if s’ could be chosen arbitrararily . But s’ would be uniquely determined by s for a deterministic dynamics . In that case , this conclusion is not obvious anymore .

Consider the state space to be integers 0 , 1 , 2 , 3 , … .
Suppose the dynamics is that whenever we are at state s ( which is an integer ) , at the next time step the state decreases toward 1 , that is s’ = phi( s , a ) = s - 1 ; unless s = 0 , which we just stay at s’ = s = 0 . This is independent of actions .
Also define r( s ) = 1 / s for s>=1 and r ( 0 ) = 0.
Suppose the discount factor is gamma = 1 ( note that in Appendix B.1 , the undiscounted case is studied , so I assume gamma = 1 is acceptable ) .

With this choices , the value function V ( s ) = 1 / s + 1 / ( s - 1 ) + … + 1 / 1 = H_s , i.e. , the Harmonic function .
The advantage function is zero . So we can choose g* ( s ) = 0 , and h* ( s ) = h* ( s ’ ) = 1.
This is in contrast to the conclusion that h* ( s ’ ) = V ( s ’ ) + c , which would be H_s + c , and g* ( s ) = r( s ) = 1/s.
( In fact , nothing is special about this choice of reward and dynamics . )

Am I missing something obvious here ?

Also please discuss how ergodicity leads to the conclusion that spaces of s’ and s are identical . What does “ space of s ” mean ? Do you mean the support of s ? Please make the argument more rigorous .


* Please make the argument of Section 5.1 more rigorous .

Thank you for the constructive feedback . We ’ve incorporated your comments and clarified certain points of the paper below . Please let us know if there are other additional issues which need clarification .

> The MaxEnt IRL formulation of this work , which assumes that p_theta ( tau ) is proportional to exp ( r_theta ( tau ) ) , comes from
[ Ziebart et al. , 2008 ] and assumes a deterministic dynamics . Ziebart ’s PhD dissertation [ Ziebart , 2010 ] or the following paper show that the formulation is different for stochastic dynamics .
Is it still a reasonable thing to develop based on this earlier , an inaccurate , formulation ?

We have updated the background ( section 3 ) and appendix ( section A ) to use the maximum causal entropy framework rather than the earlier maximum entropy framework of [ Ziebart 08 ] . Our algorithm requires no changes since the causal entropy framework more accurately describes what we were doing in the first place ( our old derivations were valid in the deterministic case , where MaxEnt and MaxCausal Ent are identical , but in the stochastic case , our approach in fact matches MaxCausalEnt ) .

> * I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants .
Also please discuss how ergodicity leads to the conclusion that spaces of s’ and s are identical . What does “ space of s ” mean ? Do you mean the support of s ? Please make the argument more rigorous .
* Please make the argument of Section 5.1 more rigorous .

We ’ve provided more formal proofs for Section 5 and the appendix . In order to fix the statements , we ’ve changed the condition on the dynamics - a major component is that it requires that each state be reachable from > 1 other state within one step . Ergodicity is neither a sufficient nor necessary condition on the dynamics , but special cases such as an ergodic MDP with self - transitions at each state satisfies the new condition ( though the minimum necessary conditions are less restrictive ) .


This paper revisits the generative adversarial network guided cost learning ( GAN - GCL ) algorithm presented last year . The authors argue learning rewards from sampled trajectories has a high variance . Instead , they propose to learn a generative model wherein actions are sampled as a function of states . The same energy model is used for sampling actions : the probability of an action is proportional to the exponential of its reward . To avoid overfitting the expert 's demonstrations ( by mimicking the actions directly instead of learning a reward that can be generalized to different dynamics ) , the authors propose to learn rewards that depend only on states , and not on actions . Also , the proposed reward function includes a shaping term , in order to cover all possible transformations of the reward function that could have been behind the expert 's actions . The authors argue formally that this is necessary to disentangle the reward function from the dynamics . Th paper also demonstrates this argument empirically ( e.g. Figure 1 ) .

This paper is well - written and technically sound . The empirical evaluations seem to be supporting the main claims of the paper . The paper lacks a little bit in novelty since it is basically a variante of GAN - GCL , but it makes it up with the inclusion of a shaping term in the rewards and with the related formal arguments . The empirical evaluations could also be strengthened with experiments in higher - dimensional systems ( like video games ) .

" Under maximum entropy IRL , we assume the demonstrations are drawn from an optimal policy p( \tau ) \ propto exp ( r ( tau ) ) " This is not an assumption , it 's the form of the solution we get by maximizing the entropy ( for regularization ) .


Thank you for the thoughtful feedback . We ’ve incorporated the suggestions to the best of our ability , and clarified portions of the paper , as described below .

> " Under maximum entropy IRL , we assume the demonstrations are drawn from an optimal policy p( \tau ) \ propto exp ( r ( tau ) ) " This is not an assumption , it 's the form of the solution we get by maximizing the entropy ( for regularization ) .

We ’ve modified Section 3 to remove this ambiguity ( note that we ’ve also modified the section to use the causal entropy framework as requested by another reviewer ) . This statement was referring to the fact that we are assuming the expert is drawing samples from the distribution p( tau ) , not the fact that p( tau ) \ propto exp ( r ( tau ) ) .

> " The paper lacks a little bit in novelty since it is basically a variant of GAN - GCL , but it makes it up with the inclusion of a shaping term in the rewards and with the related formal arguments . "

In regard to GAN - GCL , we would note that , although the method draws heavily on the theory in this workshop paper , it is unpublished and does not describe an implementation of any actual algorithm -- the GAN - GCL paper simply describes a theoretical connection between GANs and IRL . Our implementation of the algorithm that is closest to the one suggested by the theory in the GAN -GCL workshop paper does not perform very well in practice ( Section 7.3 ) .


This paper proposes to use a mixture of continuous spikes propto 1 / abs ( w_ij-c_k ) as prior for a Bayesian neural network and demonstrates good performance with relatively sparsified convnets for minist and cifar - 10 . The paper is building quite a lot upon Kingma et al 2015 and Molchanov et al 2017 .

The paper is of good quality , clearly written with an ok level of originality and significance .

Pros :
1 . Demonstrates a sparse Bayesian approach that scales .
2 . Really a relevant research area for being able to make more efficient and compact deployment .
Cons :
1 . Somewhat incremental relative to the papers mentioned above .
2 . Could have taken the experimental part further . For example can we learn something about what part of the network has the biggest potential for being pruned and use that to come up with modifications of the architecture ?


Regarding 2 . : Connecting network compression with principled architecture search / optimization is a very interesting topic which has not received enough attention in the literature so far and the authors agree that there is promising potential . Unfortunately , our method might only be suitable for rather coarse statements . In order to provide interesting statements about parts of layers or even single neurons / convolutional filters , the method would need to be extended to include group- constraints as was done in Bayesian Compression or Structured Bayesian Pruning . This would allow statements about the relevance of certain sub-parts of networks . In contrast , our method only allows reporting sparsity - rates per layer , which could perhaps be used for high - level architecture exploration ( layers with high sparsity can probably be made smaller ) .

This paper presents Variational Network Quantization ; a variational Bayesian approach for quantising neural network weights to ternary values post- training in a principled way . This is achieved by a straightforward extension of the scale mixture of Gaussians perspective of the log-uniform prior proposed at [ 1 ] . The authors posit a mixture of delta peaks hyperprior over the locations of the Gaussian distribution , where each peak can be seen as the specific target value for quantisation ( including zero to induce sparsity ) . They then further propose an approximation for the KL - divergence , necessary for the variational objective , from this multimodal prior to a factorized Gaussian posterior by appropriately combining the approximation given at [ 2 ] for each of the modes . At test - time , the variational posterior for each weight is replaced by the target quantisation value that is closest , w.r.t. the squared distance , to the mean of the Gaussian variational posterior . Encouraging experimental results are shown with performance comparable to the state - of - the- art for ternary weight neural networks .

This paper presented a straightforward extension of the work done at [ 1 , 2 ] for ternary networks through a multimodal quantising prior . It is generally well - written , with extensive preliminaries and clear equations . The visualizations also serve as a nice way to convey the behaviour of the proposed approach . The idea is interesting and well executed so I propose for acceptance . I only have a couple of minor questions :
- For the KL - divergence approximation you report a maximum difference of 1 nat per weight that seems a bit high ; did you experiment with the `naive` Monte Carlo approximation of the bound ( e.g. as done at Bayes By Backprop ) during optimization ? If yes , was there a big difference in performance ?
- Was pre-training necessary to obtain the current results for MNIST ? As far as I know , [ 1 ] and [ 2 ] did not need pre-training for the MNIST results ( but did employ pre-training for CIFAR 10 ) .
- How necessary was each one of the constraints during optimization ( and what did they prevent ) ?
- Did you ever observe posterior means that do not settle at one of the prior modes but rather stay in between ? Or did you ever had issues of the variance growing large enough , so that q( w ) captures multiple modes of the prior ( maybe the constraints prevent this ) ? How sensitive is the quantisation scheme ?

Other minor comments / typos :
( 1 ) 7th line of section 2.1 page 2 , ‘a unstructured data ’ -> ‘ unstructured data ’
( 2 ) 5th line on page 3 , remove ‘ compare Eq . ( 1 ) ’ ( or rephrase it appropriately ) .
( 3 ) Section 2.2 , ’ Kullback - Leibler divergence between the true and the approximate posterior ’ ; between implies symmetry ( and the KL is n’t symmetric ) so I suggest to change it to e.g. ‘ from the true to the approximate posterior ’ to avoid confusion . Same for the first line of Section 3.3.
( 4 ) Footnote 2 , the distribution of the noise depends on the random variable so I would suggest to change it to a general \epsilon \sim p( \epsilon ) .
( 5 ) Equation 4 is confusing .

[ 1 ] Louizos , Ullrich & Welling , Bayesian Compression for Deep Learning .
[ 2 ] Molchanov , Ashukha & Vetrov , Variational Dropout Sparsifies Deep Neural Networks .

We address the reveiewer 's questions in their original order ( due to limit in number of characters we respond with two separate entries )

Did we try naive MC approximation of the bound ?
We ran additional experiments to compare our results against a naive MC approximation of the KL divergence . To keep computational complexity comparable to our method , we use a single sample for the MC approximation . On MNIST we get the same accuracy and even higher pruning rates , however on CIFAR - 10 we get catastrophic accuracy after quantization and even the non-quantized network has significantly lower accuracy . We have added these results to the appendix A 3.1 , including a new table and two figures .

Was pre-training necessary on MNIST ?
We follow the same learning schedule as Sparse VD and train the first five epochs of a randomly initialized network without the KL penalization term and then gradually switch it on over the next epochs . We call the network after these first five epochs the " pre-trained " network , since five epochs suffice to get a decent MNIST classifier . We have run an additional experiment where we have a non-zero weight for the KL term already in the first epoch of training to start from a truly random network . Results were added to Table 1 , training from scratch gets the same accuracy but slightly better pruning rates .

How necessary was each of the constraints ?
Lower-bounding the log-variance helps avoiding numerical issues , upper-bounding the log-variance leads to higher accuracy during training - Bayesian Compression and the Multiplicative Normalizing Flows paper also report upper-bounding the posterior variance as it " helps avoiding bad local optima of the variational objective " . Clipping the non-zero codebook levels at an absolute value of 0.05 to avoid getting collapsing codebooks was important since the objective implicitly favors close - to - zero codebook levels - particularly in the early stages of training such a collapse of the codebook needed to be prevented via clipping . Clipping weights that lie left to the left - most funnel or right to the right - most funnel helped with keeping accuracy after quantization . Without this clipping a small number of ( seemingly important ) weights are drawn to very large positive or negative values ( particularly in the first layer ) . Since it is just a small number of weights , the impact on the objective is small , however quantizing such weights leads to significant accuracy loss . By clipping , the algorithm seems to find an alternative weight configuration that does not require such weights with large absolute values .

Did we observe posterior means that do not settle at one of the prior modes ?
Yes , such cases can be seen in our experiments Fig. 1 b ( conv_1 ) and more pronounced in the first and last layer of DenseNet ( top -left and bottom - right panel of Fig. 3 in the appendix ) . A small number of weights ( blue dots ) do not lie on the prior modes ( outside the " funnels " in the low-variance regime ) . During early stages of training , the number of such weights is typically higher and quantizing such a network leads to poor accuracy . After sufficient training , we find in our experiments that a small number of such weights is tolerable without much loss in accuracy .


Did we observe that the posterior variance of weights grows large enough to cover multiple prior modes ?
Yes . Weights which are close to the upper \log \sigma clipping boundary ( see Figure 1 b and 3 ) have a comparatively large posterior variance such that all prior modes have a non-negligible likelihood . Empirically we find that this is not problematic for our method since such large variance weights are pruned after training ( via thresholding \alpha , see Eq. 9 and the following sentence ) . A speculative explanation could be that these high - variance weights can essentially have arbitrary values since the information that they convey is discarded anyway downstream in the sparse network .

How sensitive is the quantization scheme ?
We found that training on MNIST typically worked quite robustly and was not severely affected by different initializations or changes in the learning rate etc . Training on CIFAR - 10 was more sensitive regarding the learning rate . Probably the most crucial aspects were the clipping constraints and using a lower learning - rate for learning the codebook levels . One interesting aspect about the probabilistic soft-quantization is that weights with large posterior variance can essentially have any value that has sufficiently high likelihood under the posterior - this could be beneficial for improving robustness against hardware errors ( rounding errors , limited precision , analog effects ) . In theory this should also translate into being more robust against noisy activations ( or even network input ) which could be very interesting . We think this question would require proper investigation beyond the scope of this paper .


Response to minor comments :
( 1 ) Done .

( 2 ) Done .

( 3 ) Thanks for pointing it out , we have fixed this throughout the paper .

( 4 ) Done .

( 5 ) Another glitch , the equation should have been arranged differently ( it should make more sense then ) - we have updated the equation in the paper .


The goal of this work is to infer weights of a neural network , constrained to a discrete set , where each weight can be represented by a few bits . This is a quite important and hot topic in deep learning . As a direct optimization would lead to a highly nontrivial combinatorial optimization problem , the authors propose a so-called ' quantizing prior ' ( actually a relaxed spike and slab prior to induce a sparsity enforcing heavy tail prior ) over weights and derive a differentiable variational KL approximation . One important advantage of the current method is that this approach does not require fine- tuning after quantization . The paper presents ternary quantization for LeNet-5 ( MNIST ) and DenseNet - 121 ( CIFAR - 10 ) .

The paper is mostly well written and cites carefully the recent relevant literature . While there are a few glitches here and there in the writing , overall the paper is easy to follow . One exception is that in section 2 , many ideas are presented in a sequence without providing any guidance where all this will lead .
The idea is closely related to sparse Bayesian learning but the variational approximation is achieved via the local reparametrization trick of Kingma 2015 , with the key idea presented in section 3.3.



Minor

In the introduction , the authors write " ... weights with a large variance can be pruned as they do not contribute much to the overall computation " . What does this mean ? Is this the marginal posterior variance as in ARD ?

The authors write : " Additionally , variational Bayesian inference is known to automatically reduce parameter redundancy by penalizing overly complex models . " I would argue that
it is Bayesian inference ; variational inference sometimes retains this property , but not always .

In Eq ( 10 ) , z needs also subscripts , as otherwise the notation may suggest parameter tying . Alternatively , drop the indices entirely , as later in the paper .

Sec. 3.2. is not very well written . This seems to be the MAP of the product of the marginals ,
or the mode of the variational distribution , not the true MAP configuration of the weight posterior . Please be more precise .

The abbreviation P&Q ( probably Post-training Quantization ) seems to be not defined in the paper .


We address the reivewer 's comments in the order in which they appear in the original review


Section 2 : no guidance where this will lead to - we added a short introduction to section 2 to tie the section together and provide an outline as a guidance to the reader . We also rewrote section 2.1 to be more focused .

Minor comments :

Intro : we write " ... weights with a large variance can be pruned as they do not contribute much to the overall computation " . What does this mean ? Is this the marginal posterior variance as in ARD ?
Yes , in that sentence we refer to the marginal ( approximate ) posterior variance which is also the pruning criterion in ARD - however in ARD typically parameters with low variance ( or high precision ) are pruned . This is due to the fact that ARD assumes a zero-mean Gaussian prior over weights ( with a different precision per parameter or group of parameters , that is adjusted during training and regularized by a hyper-prior ) . Weights that differ significantly from zero get assigned a high variance or , dually , weights with low variance are very likely to lie close to zero ( the prior mean ) and can thus be pruned . ARD is very similar to the situation where we only have the central funnel ( a zero-mean prior ) which is the case in Sparse Variational Dropout ( compare Eq. 10 in our paper ) . However in the latter , as in our method , the pruning criterion takes into account both , the marginal posterior mean and variance ( see Eq. 9 ) and also large - variance weights are pruned as long as the posterior mean is small enough ( the intuition is that a high - variance weight can essentially have arbitrary values which implies that it most probably does not do anything sensible and can be pruned ) . To visualize the difference between the pruning criteria , consider the central funnel in the top-row plots of Figure 1 : Sparse Variational Dropout and our method prune everything that lies within the area marked by the red dotted funnel . In contrast , thresholding the marginal posterior variance as in classical ARD would correspond to pruning everything that lies below a horizontal line in the " funnel plots " ( which for the central funnel are precisely weights that lie close to zero ) . Note that of course different pruning criteria can also be used in ARD .


Intro : Bayesian inference penalizes overly complex models , variational Bayesian inference does not necessarily do so - agreed , we have changed the sentence accordingly .

Eq 10 . - z needs subscripts - agreed , we have added sub-scripts throughout the paper .

Section 3.2 : do not refer to ' MAP ' but be more precise - agreed , we rephrased our writing to refer to ' maximizing likelihood under the approximate posterior ' .

Clarify P&Q - P&Q refers to ' Pruning ' and ' Quantization ' , we have clarified this in the corresponding table legends .

This paper presents an image - to-image cross domain translation framework based on generative adversarial networks . The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved . The results show that the proposed method is superior for the task of exact correspondence identification and that AN - GAN rivals the performance of pix2 pix with strong supervision .


Negatives :
1 . ) The task of exact correspondence identification seems contrived . It is not clear which real - world problems have this property of having both all inputs and all outputs in the dataset , with just the correspondence information between inputs and outputs missing .
2 . ) The supervised vs unsupervised experiment on Facades - > Labels ( Table 3 ) is only one scenario where applying a supervised method on top of AN - GAN ’s matches is better than an unsupervised method . More transfer experiments of this kind would greatly benefit the paper and support the conclusion that “ our self - supervised method performs similarly to the fully supervised method . ”

Positives :
1 . ) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework
2 . ) The paper shows promising results on applying a supervised method on top of AN - GAN ’s matches .

Minor comments :
1 . The paper sometimes uses L1 and sometimes L_1 , it should be L_1 in all cases .
2 . DiscoGAN should have the Kim et al citation , right after the first time it is used . I had to look up DiscoGAN to realize it is just Kim et al .

We thank you for highlighting the novelty and successful motivation of the exemplar- based matching loss .

We think that the exact - analogy problem is very important . Please refer to our comment to AnonReviewer2 for an extensive discussion .

Following your request , we have added AN - GAN supervised experiments for the edges2 shoes and edges2handbags datasets . The results as for the Facades case are very good .

Thank you for highlighting the inconsistency in L_1 notation and the confusing reference . This has been fixed in the revised version .


The paper presents a method for finding related images ( analogies ) from different domains based on matching - by-synthesis . The general idea is interesting and the results show improvements over previous approaches , such as CycleGAN ( with different initializations , pre-learned or not ) . The algorithm is tested on three datasets .

While the approach has some strong positive points , such as good experiments and theoretical insights ( the idea to match by synthesis and the proposed loss which is novel , and combines the proposed concepts ) , the paper lacks clarity and sufficient details .

Instead of the longer intro and related work discussion , I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments . Also , the matching part , which is discussed at the theoretical level , could be better explained and presented at a more visual level . It is hard to understand sufficiently well what the formalism means without more insight .

Also , the experiments need more details . For example , it is not clear what the numbers in Table 2 mean .





Thank you for your positive feedback on the theoretical and experimental merits of this paper .

Following your feedback on the clarity of presentation of the method . we included a diagram ( including example images ) illustrating the algorithm . To help keep the length under control , we shortened the introduction and related work section as you suggested .

We further clarified the text of the experiments . Specifically the numbers in Tab 2 are the top - 1 accuracy for both directions ( A to B and B to A ) when 0 % , 10 % and 25 % of examples do not have matches in the other domain . If some details remain unclear , we would be glad to clarify them .

We hope that your positive opinion of the content of the paper with the improvement in clarity of presentation will merit an acceptance .


This paper adds an interesting twist on top of recent unpaired image translation work . A domain - level translation function is jointly optimized with an instance - level matching objective . This yields the ability to extract corresponding image pairs out of two unpaired datasets , and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches . I think this is a promising direction , but the current paper has unconvincing results , and it ’s not clear if the method is really solving an important problem yet .

My main criticism is with the experiments and results . The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets . Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches . This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches . It feels rather contrived to focus so much on the datasets with exact matches since , 1 ) these datasets actually come as paired data and , in actual practice , supervised translation can be run directly , 2 ) it ’s hard to imagine datasets that have exact but unknown matches ( I welcome the authors to put forward some such scenarios ) , 3 ) when exact matches exist , simpler methods may be sufficient , such as matching edges . There is no comparison to any such simple baselines .

I think finding analogies that are not exact matches is much more compelling . Quantifying performance in this case may be hard , and the current paper only offers a few qualitative results . I ’d like to see far more results , and some attempt at a metric . One option would be to run user studies where humans judge the quality of the matches . The results shown in Figure 2 do n’t convince me , not just because they are qualitative and few , but also because I ’m not sure I even agree that the proposed method is producing better results : for example , the DiscoGAN results have some artifacts but capture the texture better in row 3 .

I was also not convinced by the supervised second step in Section 4.3 . Given that the first step achieves 97 % alignment accuracy , it ’s no surprised that running an off- the-shelf supervised method on top of this will match the performance of running on 100 % correct data . In other words , this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful .

What I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches . For example , the shoes and handbags dataset or even better , domain adaptation datasets like sim to real .

I ’d like to see more discussion of why the second stage supervised problem is beneficial . Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one - hot and T is simply training against a supervised objective ( Equation 7 ) ?

Minor comments :
1 . In the intro , it would be useful to have a clear definition of “ analogy ” for the present context .
2 . Page 2 : a link should be provided for the Putin example , as it is not actually in Zhu et al. 2017 .
3 . Page 3 : “ Weakly Supervised Mapping ” — I would n’t call this weakly supervised . Rather , I ’d say it ’s just another constraint / prior , similar to cycle-consistency , which was referred to under the “ Unsupervised ” section .
4 . Page 4 and throughout : It ’s hard to follow which variables are being optimized over when . For example , in Eqn. 7 , it would be clearer to write out the min over optimization variables .
5 . Page 6 : The Maps dataset was introduced in Isola et al. 2017 , not Zhu et al. 2017 .
6 . Page 7 : The following sentence is confusing and should be clarified : “ This shows that the distribution matching is able to map source images that are semantically similar in the target domain . ”
7 . Page 7 : “ This shows that a good initialization is important for this task . ” — Is n’t this more than initialization ? Rather , removing the distributional and cycle constraints changes the overall objective being optimized .
8 . In Figure 2 , are the outputs the matched training images , or are they outputs of the translation function ?
9 . Throughout the paper , some citations are missing enclosing parentheses .

We thank the reviewer for the extensive style and reference comments . They have been fixed in the revised version :
1 . A definition of “ analogy ” for the present context added to intro .
2 . Putin example removed for need of space .
3 . “ Weakly Supervised Mapping ” previous work section removed and references merged for need of space .
4 . Optimization variables have been explicitly added to equations .
5 . Maps dataset citation was changed to Isola et al . 2017
6 . Removed confusing comment : “ This shows that the distribution matching is able to map source images that are semantically similar in the target domain . ”
7 . “ This shows that a good initialization is important for this task . ” : one way of looking at it , is that the exemplar loss optimizes the matching problem that we care about but is a hard optimization task . The two other losses are auxiliary losses that help optimization converge . Clarification added in text .
8 . The results shown for inexact matching are as follows : For alpha iterations and ANGAN we show the matches recovered by our methods , The DiscoGAN results are the outputs of the translation function .
9 . Parentheses added to all citations .

We hope that this has convinced the reviewer of the importance of this work and are keen to answer any further questions .


Thank you for the detailed and constructive review . It highlighted motivation and experimental protocols that were further clarified in the revised version .

This paper is focused on exact analogy identification . A core question in the reviews was the motivation for the scenario of exact matching , and we were challenged by the reviewer to find real world applications for it .

We believe that finding exact matches is an important problem and occurs in multiple real - world problems . Exact or near- exact matching occurs in :
* 3D point cloud matching .
* Matching between different cameras panning the same scene in different trajectories ( hard if they are in different modalities such as RGB and IR ) .
* Matching between the audio samples of two speakers uttering the same set of sentences .
* Two repeats of the same scripted activity ( recipe , physics experiment , theatrical show )
* Two descriptions of the same news event in different styles ( at the sentence level or at the story level ) .
* Matching parallel dictionary definitions and visual collections .
* Learning to play one racket sport after knowing to play another , building on the existing set of acquired movements and skills .

In all these cases , there are exact or near exact analogies that could play a major rule in forming unsupervised links between the domains .

We note that on a technical level , most numerical benchmarks in cross domain translation are already built using exact matches , and many of the unsupervised techniques could be already employing this information , even if implicitly . We show that our method is more effective at it than other methods .

On a more theoretical level , cognitive theories of analogy - based reasoning mostly discuss exact analogies from memory ( see , e.g. , G. Fauconnier , and M. Turner , “ The way we think ” , 2002 ) . For example , a new situation is dealt with by retrieving and adopting a motor action that was performed before . Here , the chances of finding such analogies are high since the source domain is heavily populated due to life experiences .

Regarding experiments . We believe that in some cases the requests are conflicting : we cannot provide numerical results in places for which there are no analogies and no metrics for success . We provide a large body of experiments for exact matches and show that our method far surpasses everything else . We have compared with multiple baselines covering all the reasonable successful approaches for matching between domains .

The experiments regarding cases without exact matches are , admittedly , less extensive , added for completeness , and not the focus of this paper .

The reviewer wondered if matching will likely work better with simpler methods . Our baselines test precisely this possibility and show that the simpler methods do not perform well . Specifically edge-based matches are well covered by the more general VGG feature baseline ( which uses also low level maps - not just fc7 ) . AN - GAN has easily outperformed this method . If it is possible to hand - craft a successful method for each task individually , these hand - crafted features are unlikely to generalize as well as the multi-scale VGG features or AN -GAN .

We put further clarification in the paper for the motivation for the second “ supervised ” step . In unsupervised semantic matching , larger neural architecture have been theoretically and practically shown to be less successful ( due to overfitting and finding it less easy to recover the correct transformation ) . The distribution matching loss function ( e.g. CycleGAN ) is adversarial and is therefore less stable and might not optimize the quantity we care about ( e.g. L1 / L2 loss ) . Once the datasets are aligned and analogies are identified , however , the cross domain translation becomes a standard supervised deep learning problem where large architectures do well and standard loss functions can be used . This is the reason for the two steps . It might be possible to include the increase in architecture into the alpha-iterations but it ’s non-trivial and we did n’t find it necessary .


Updates : thanks for the authors ' hard rebuttal work , which addressed some of my problems / concerns . But still , without the analysis of the temporal ensembling trick [ Samuli & Timo , 2017 ] and data augmentation , it is difficult to figure out the real effectiveness of the proposed GAN . I would insist my previous argument and score .

Original review :
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
This paper presented an improved approach for training WGANs , by applying some Lipschitz constraint close to the real manifold in the pixel level . The framework can also be integrated to boost the SSL performances . In experiments , the generated data showed very good qualities , measured by inception score . Meanwhile , the SSL - GANs results were impressive on MNIST and CIFAR - 10 , demonstrating its effectiveness .

However , the paper has the following weakness :

Missing citations : the most related work of this one is the DRAGAN work . However , it did not cite it . I think the author should cite it , make a clear justification for the comparison and emphasize the main contribution of the method . Also , it suggested that the paper should discuss its relation to other important work , [ Arjovsky & Bottou 2017 ] , [ Wu et al. 2016 ] .

Experiments : as for the experimental part , it is not solid . Firstly , although the SSL results are very good , it is guaranteed the proposed GAN is good [ Dai & Almahairi , et al. 2017 ] . Secondly , the paper missed several details , such as settings , model configuration , hyper - parameters , making it is difficult to justify which part of the model works . Since the paper using the temporal ensembling trick [ Samuli & Timo , 2017 ] , most of the gain might be from there . Data augmentation might also help to improve . Finally , except CIFAR - 10 , it is better to evaluate it on more datasets .

Given the above reason , I think this paper is not ready to be published in ICLR . The author can submit it to the workshop and prepare for next conference .

We are pleased to see that the reviewer thinks our " generated data showed very good qualities " and " the SSL - GANs results were impressive " .

= Q1=
" Missing citations : the most related work of this one is the DRAGAN "

We would consider WGAN and WGAN - GP as the most related works to ours and DRAGAN ranks after them . As a matter of fact , DRAGAN is an unpublished work and has not been peer-reviewed . As another matter of fact , the gradient penalty in DRAGAN is the same as in WGAN - GP except that it is imposed around the real data while WGAN - GP applies it to the points sampled between the real and the generated ones .

Next , we highlight some key differences between DRAGAN and ours .

We propose to improve Wasserstein GAN , while DRAGAN works with GAN .

DRAGAN aims to reduce the non-optimal saddle points in the minmax two - player training of GANs . In contrast , we propose an approach to enforcing the 1 - Lipschitz continuity over the critic of WGANs .

One of our key observations is that it blurs the generated samples if we add noise directly to the data points , as done in DRAGAN . Instead , we perturb the hidden layers of the discriminator .

DRAGAN perturbs a data point once while we do it twice in each iteration . After the perturbations , DRAGAN penalizes the gradients while we enforce the consistency of the outputs .

One of the most distinct features of our approach is that it seamlessly integrates the semi-supervised learning method by Laine & Aila ( 2016 ) with GANs .

= Q2=
" the paper should discuss ... [ Arjovsky & Bottou 2017 ] , [ Wu et al. 2016 ] "

We had included both in our paper . Arjovsky & Bottou 2017 analyzes some distribution divergences and their effects in training GANs . Wu et al . 2016 propose to quantitatively evaluate the decoder - based generative models by annealed importance sampling . In our paper , we focus on a different subject , i.e. , to design an algorithmic solution to the difficulty of training GANs .

=Q3=
" the paper missed several details "

Please see either Appendices A and B of the revised paper or the following for our answer to this question .

Given the context of the question , we believe it is about SSL . We follow the experiment setups in the prior works so that our results are directly comparable to theirs . Please see below for more details . If you are interested , you may also check out our code : https://github.com/biuyq/CT-GAN/blob/master/CT-GANs/Theano_classifier/CT_CIFAR-10_TE.py.

MNIST : There are 60,000 images in total . We randomly choose 10 data points for each digit as the labeled set . No data augmentation is used .

CIFAR - 10 : There are 50,000 image in total . We randomly choose 400 images for each class as the labeled set . We augment the data by horizontally flipping the images and randomly translating the images within [ - 2 , 2 ] pixels . No ZCA whitening is used .

Model Configurations : We had included them in the appendix .

Hyper-parameters : We set lambda = 1.0 in Eq. ( 7 ) in all our experiments . For CIFAR - 10 , the number of training epochs is set to 1,000 with a constant learning rate of 0.0003 . For MNIST , the number of training epochs is set to 300 with a constant learning rate of 0.003 . The other hyper - parameters are exactly the same as in the improved GAN ( Salimans et al. , 2016 ) .

= Q4=
" ... which part of the model works "

Please see either Appendix C of the revised paper or the following for our answer to this question .

We have done some ablation studies about our semi-supervised learning approach on CIFAR - 10 .
Method , Error
w / o CT , 15.0
w / o GAN ( note 1 ) , 12.0
w batch norm ( note 2 ) , --
w / o D_ , 10.7
OURS , 10.0

Note 1 : This almost reduces to TE ( Laine & Aila , 2016 ) . All the settings here are the same as TE except that we use the extra regularization ( $ D\_ ( . , . ) $ in CT ) over the second - to - last layer .
Note 2 : We use the weight normalization as in ( Salimans et al. , 2016 ) , which becomes a core constituent of our approach . The batch normalization would actually invalidate the feature matching in ( Salimans et al. , 2016 ) .

We can see that both GAN and the temporal ensembling effectively contribute to our final results . The results without our consistency regularization ( w / o CT ) drop more than those without GAN . We are running the experiments without any data augmentation and will include the corresponding results in the paper .

= Q5=
" ... it is better to evaluate it on more datasets "

We have run some new experiments on the SVHN dataset . Ours is the best among all the GAN based semi-supervised learning methods , and is on par with the state of the arts .
Method , Error
PI Laine & Aila 2016 , 4.8
TE Laine & Aila 2016 , 4.4
Tarvainen & Valpola 2017 , 4.0
Miyato et al. 2017 , 3.86
Salimans et al. 2016 , 8.1
Dumoulin et al. 2016 , 7.4
Kumar et al. 2017 , 5.9
Ours , 4.2

Following ( Laine & Aila , 2016 , Miyato et al. , 2017 , Tarvainen & Valpola , 2017 ) , we do not apply any augmentation to MNIST and yet augment the CIFAR10 images in the following way . We flip the images horizontally and randomly translate the images within [ - 2 , 2 ] pixels horizontally .

Samuli Laine and Timo Aila . Temporal ensembling for semi-supervised learning . arXiv preprint arXiv:1610.02242 , 2016 .
Takeru Miyato , Shin-ichi Maeda , Masanori Koyama , and Shin Ishii . Virtual adversarial training : a regularization method for supervised and semi-supervised learning . arXiv preprint arXiv:1704.03976 , 2017 .
Antti Tarvainen and Harri Valpola . Weight -averaged consistency targets improve semi-supervised deep learning results . arXiv preprint arXiv:1703.01780 , 2017 .

AnonReviewer2 : " But still , without the analysis of the temporal ensembling trick [ Samuli & Timo , 2017 ] "

We have actually reported the ablation study about this temporal ensemebling technique in the rebuttal . Please read our answer to Q4 in the rebuttal .

= Q4=
" ... which part of the model works "

Please see either Appendix C of the revised paper or the following for our answer to this question .

We have done some ablation studies about our semi-supervised learning approach on CIFAR - 10 .
Method , Error
w / o CT , 15.0
w / o GAN ( note 1 ) , 12.0
w batch norm ( note 2 ) , --
w / o D_ , 10.7
OURS , 10.0

Note 1 : This almost reduces to TE ( Laine & Aila , 2016 ) . All the settings here are the same as TE except that we use the extra regularization ( $ D\_ ( . , . ) $ in CT ) over the second - to - last layer .
Note 2 : We use the weight normalization as in ( Salimans et al. , 2016 ) , which becomes a core constituent of our approach . The batch normalization would actually invalidate the feature matching in ( Salimans et al. , 2016 ) .

We can see that both GAN and the temporal ensembling effectively contribute to our final results . The results without our consistency regularization ( w / o CT ) drop more than those without GAN . We are running the experiments without any data augmentation and will include the corresponding results in the paper .

Summary :

The paper proposes a new regularizer for wgans , to be combined with the traditional gradient penalty . The theoretical motivation is bleak , and the analysis contains some important mistakes . The results are very good , as noticed by the comments , the fact that the method is also less susceptible to overfitting is also an important result , though this might be purely due to dropout . One of the main problems is that the largest dataset used is CIFAR , which is small . Experiments on something like bedrooms or imagenet would make the paper much stronger .

If the authors fix the theoretical analysis and add evidence in a larger dataset I will raise the score .

Detailed comments :

- The motivation of 1.2 and the sentence " Arguably , it is fairly safe to limit our scope to the manifold that supports the real data distribution P_r and its surrounding regions " are incredibly wrong . First of all , it should be noted that the duality uses 1 - Lip in the entire space between Pr and Pg , not in Pr alone . If the manifolds are not extremely close ( such as in the beginning of training ) , then the discriminator can be almost exactly 1 in the real data , and 0 on the fake . Thus the discriminator would be almost exactly constant ( 0 - Lip ) near the real manifold , but will fail to be 1 - lip in the decision boundary , this is where interpolations fix this issue . See Figure 2 of the wgan paper for example , in this simple example an almost perfect discriminator would have almost 0 penalty .

- In the ' Potential caveats ' section , the implication that 1 - Lip may not be enforced in non - examined samples is checkable by an easy experiment , which is to look for samples that have gradients of the critic wrt the input with norm > 1 . I performed the exp in figure 8 and saw that by taking a slightly higher lambda , one reaches gradients that are as close to 1 as with ct -gan . Since ct - gan uses an extra regularizer , I think the authors need some stronger evidence to support the claim that ct - gan better battles this ' potential caveat ' .

- It 's important to realize that the CT regularizer with M ' = 1 ( 1 - Lip constraint ) will only be positive for an almost 1 - Lip function if x and x ' are sampled when x - x ' has a very similar direction than the gradient at x . This is very hard in high dimensional spaces , and when I implemented a CT regularizer indeed the ration of eq ( 4 ) was quite less than the norm of the gradient . It would be useful to plot the value of the CT regularizer ( the eq 4 version ) as the training iterations progresses . Thus the CT regularizer works as an overall Lipschitz penalty , as opposed to penalizing having more than 1 for the Lipschitz constant . This difference is non-trivial and should be discussed .

- Line 11 of the algorithm is missing L^( i ) inside the sum .

- One should n't use MNIST for anything else than deliberately testing an overfitting problem . Figure 4 is thus relevant , but the semi-supervised results of MNIST or the sample quality experiments give hardly any evidence to support the method .

- The overfitting result is very important , but one should disambiguate this from being due to dropout . Comparing with wgangp + dropout is thus important in this experiment .

- The authors should provide experiments in at least one larger dataset like bedrooms or imagenet ( not faces , which is known to be very easy ) . This would strengthen the paper quite a bit .

We thank the reviewer for the insightful comments and suggestions ! The paper has been revised accordingly . Next , we answer the questions in detail .

== Q1 : The motivation ==
We acknowledge that the duality uses 1 - Lipschitz continuity in the entire space between Pr and Pg , and it is impossible to visit everywhere of the space in the experiments . We instead focus on the region around the real data manifold to complement the region checked by GP - WGAN --- the gradient penalty term is kept in our overall approach . We have clarified this point by the following in the revised paper .

Arguably , it is fairly safe to limit our scope to the manifold that supports the real data distribution $ \mathbb{ P}_r $ and its surrounding regions mainly for two reasons . First , we keep the gradient penalty term and improve it by the proposed consistency term in our overall approach . While the former enforces the continuity over the points sampled between the real and generated points , the latter complement the former by focusing on the region around the real data manifold instead . Second , the distribution of the generative model $ \mathbb{ P}_G $ is virtually desired to be as close as possible to $ \mathbb{ P}_r$ .


== Q2 : That 1 - Lip may not be enforced in non- examined samples is checkable ==
The non -examined samples can refer to all the possible samples in the continuous space which cannot be traversed in a discrete manner . Figure 8 plots the norm of the gradients ( of the critic with respect to the input ) over the real data points only . In other words , Figure 8 is only part of the consequence , and certainly not the cause , of the discriminators trained by GP - WGAN and our CT - GAN , respectively . It is not surprising that the norms by CT - GAN are closer to 1 than by GP - WGAN because we explicitly enforce the continuity around the real data .

We have run more experiments with larger \lambda values in GP - GAN , and found the gradient norms can indeed reach those of CT - GAN when the \lambda is four times larger than the original one used in the authors ’ code . However , the inception score on CIFAR - 10 drops a little , and the overfitting remains .

Stronger evidence ? In addition to the gradient norm , we have also examined the 1 - Lipschitz continuity of the critic using the basic definition . For any two inputs x and x ' , the difference of the critic 's outputs should be no more than M*|x - x '| . This notion is captured by our CT term defined in eq . ( 4 ) . We plot the CT versus the training iterations as Figure 9 in the revised paper . In particular , for every 100 iterations , we randomly pick up 64 real examples and split them into two subsets of the same size . We compute d( D ( x1 ) - D ( x2 ) ) / d ( x1 - x2 ) for all the ( x1 , x2 ) pairs , where x 1 is from the first subset and x2 is from the second . The maximum of d( D ( x1 ) - D ( x2 ) ) / d ( x1 - x2 ) is plotted in Figure 9 . We can see that the CT - GAN curve converges under a certain value much faster than GP-WGAN .

== Q3 : Plot the value of the CT regularizer ==
Please see Figures 9 and 10 in the revised paper for the plots . Note that M’ has absorbed the term d( x ’ , x ’’ ) in the final consistency term ( eq. ( 5 ) ) , so we have to tune its value as opposed to fixing it to 1 . Also , because of this fact , we agree with the comment that “ Thus the CT regularizer works as an overall Lipschitz penalty , as opposed to penalizing having more than 1 for the Lipschitz constant . ” We will clarify this part in the final paper , if it is accepted .

== Q4 : Line 11 ==
It is correct and is another way of denoting the gradient .

== Q5 : MNIST ==
We understand your concern with the use of MNIST and appreciate that you agree the overfitting experiments ( Figure 4 ) are relevant . The other results ( e.g. , the generated samples and the test error in semi-supervised learning ) can give the readers a concrete understanding about our model , but we agree one should not use MNIST to compare different algorithms .


== Q6 : GP - WGAN + Dropout ==
Please see Appendix E for the experimental results of GP - WGAN + Dropout on CIFAR - 10 using 1000 training images . The corresponding inception score is better than GP - WGAN and yet still significantly lower than ours ( 2.98+-0.11 vs. 4.29+-0.12 vs. 5.13+-0.12 ) . Figure 12 , which is about the convergence curves of the discriminator cost over both training and testing sets , shows that dropout is indeed able to reduce the overfitting , but it is not as effective as ours .


== Q7 : Experiments in larger datasets ==
In Appendix F of the revised paper , we present results on the ImageNet and LSUN bedroom datasets following the experiment setup of GP - WGAN . After 200,000 generator iterations on ImageNet , the inception score of CT - GAN is 10.27+-0.15 , whereas GP - WGAN 's is 9.85+-0.17 . Since there is only one class in LSUN bedroom , the inception score is not a proper evaluation metric for the experiments on this dataset . Visually , there is no clear difference between the generated samples of GP - WGAN and CT - GAN up to the 124,000th generator iteration .


This paper continues a trend of incremental improvements to Wasserstein GANs ( WGAN ) , where the latter were proposed in order to alleviate the difficulties encountered in training GANs . Originally , Arjovsky et al . [ 1 ] argued that the Wasserstein distance was superior to many others typically used for GANs . An important feature of WGANs is the requirement for the discriminator to be 1 - Lipschitz , which [ 1 ] achieved simply by clipping the network weights . Recently , Gulrajani et al . [ 2 ] proposed a gradient penalty " encouraging " the discriminator to be 1 - Lipschitz . However , their approach estimated continuity on points between the generated and the real samples , and thus could fail to guarantee Lipschitz - ness at the early training stages . The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples . Together with various technical improvements , this leads to state - of - the- art practical performance both in terms of generated images and in semi-supervised learning .

In terms of novelty , the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs . The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator ’s response on the perturbed points . The proposed method is used in eq . ( 6 ) together with the gradient penalty from [ 2 ] . The authors found that directly perturbing the data with Gaussian noise led to inferior results and therefore propose to perturb the hidden layers using dropout . For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10 . They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation .

The authors do an excellent comparative job in presenting their experiments . They compare numerous techniques ( e.g. , Gaussian noise , dropout ) and demonstrates the applicability of the approach for a wide range of tasks . They use several criteria to evaluate their performance ( images , inception score , semi-supervised learning , overfitting , weight histogram ) and compare against a wide range of competing papers .

Where the paper could perhaps be slightly improved is writing clarity . In particular , the discussion of M and M ' is vital to the point of the paper , but could be written in a more transparent manner . The same goes for the semi-supervised experiment details and the CIFAR - 10 augmentation process . Finally , the title seems uninformative . Almost all progress is incremental , and the authors modestly give credit to both [ 1 ] and [ 2 ] , but the title is neither memorable nor useful in expressing the novel idea .
[ 1 ] Martin Arjovsky , Soumith Chintala , and Leon Bottou . Wasserstein gan .

[ 2 ] Ishaan Gulrajani , Faruk Ahmed , Martin Arjovsky , Vincent Dumoulin , and Aaron Courville . Improved training of wasserstein gans .



We thank the reviewer for the very positive and affirmative comments about our work .

We also appreciate the suggestions for improving the writing clarify of the paper . The following has been incorporated in the revised paper .

== M vs. M' ==
We use the notation $ M$ in eq . ( 3 ) and a different $ M '$ in eq . ( 4 ) to reflect the fact that the continuity will be checked only sparsely at some data points in practice . ... ... Note that , however , it becomes impossible to compute the distance $ d ( \bm{x} ' , \bm{x } ' ' ) $ between the two virtual data points . In this work , we assume it is bounded by a constant and absorb the constant to $ M '$ . Accordingly , we tune $ M '$ in our experiments to take account of this unknown constant ; the best results are obtained between $ M'=0 $ and $ M'=0.2 $ .

== Semi-supervised experiment details and the CIFAR - 10 augmentation process ==

MNIST : There are 60,000 images in total . We randomly choose 10 data points for each digit as the labeled set . No data augmentation is used .

CIFAR - 10 : There are 50,000 image in total . We randomly choose 400 images for each class as the labeled set . We augment the data by horizontally flipping the images and randomly translating the images within [ - 2 , 2 ] pixels . No ZCA whitening is used .

Model Configuration

Table 1 : MNIST
--------------
Classifier C | Generator G
Input : Labels y , 28*28 Images x | Input : Noise 100 z
Gaussian noise 0.3 , MLP 1000 , ReLU | MLP 500 , Softplus , Batch norm
Gaussian noise 0.5 , MLP 500 , ReLU | MLP 500 , Softplus , Batch norm
Gaussian noise 0.5 , MLP 250 , ReLU | MLP 784 , Sigmoid , Weight norm
Gaussian noise 0.5 , MLP 250 , ReLU |
Gaussian noise 0.5 , MLP 250 , ReLU |
Gaussian noise 0.5 , MLP 10 , Softmax |

Table 2 : CIFAR - 10
-----------------
Input : Labels y , 32*32*3 Colored Image x , | Input : Noise 50 z
------------------------------------------------------------------------------
0.2 Dropout | MLP 8192 , ReLU , BN
3*3 conv. 128 , Pad = 1 , Stride = 1 , lReLU , Weight norm | Reshape 512*4*4
3*3 conv. 128 , Pad = 1 , Stride = 1 , lReLU , Weight norm | 5*5 deconv . 256*8*8 ,
3*3 conv. 128 , Pad = 1 , Stride = 2 , lReLU , Weight norm | ReLU , Batch norm
------------------------------------------------------------------------------
0.5 Dropout |
3*3 conv. 256 , Pad = 1 , Stride = 1 , lReLU , Weight norm |
3*3 conv. 256 , Pad = 1 , Stride = 1 , lReLU , Weight norm | 5*5 deconv . 128*16*16 ,
3*3 conv. 256 , Pad = 1 , Stride =2 , lReLU , Weight norm | ReLU , Batch norm
------------------------------------------------------------------------------
0.5 Dropout |
3*3 conv. 512 , Pad =0 , Stride = 1 , lReLU , Weight norm |
3*3 conv. 256 , Pad =0 , Stride = 1 , lReLU , Weight norm | 5*5 deconv . 3*32*32 ,
3*3 conv. 128 , Pad =0 , Stride = 1 , lReLU , Weight norm | Tanh , Weight norm
-----------------------------------------------------------------------------
Global pool |
MLP 10 , Weight norm , Softmax |

== Hyper-parameters ==
We set \lambda = 1.0 in Eq. ( 7 ) in all our experiments . For CIFAR - 10 , the number of training epochs is set to 1,000 with a constant learning rate of 0.0003 . For MNIST , the number of training epochs is set to 300 with a constant learning rate of 0.003 . The other hyper - parameters are exactly the same as in the improved GAN ( Salimans et al. , 2016 ) .

== New Title ==
Improving the Improved Training of Wasserstein GANs : A Consistency Term and Its Dual Effect


The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the limitation of truncated backpropagation through time ( BPTT ) of providing biased gradient estimates .

The idea is to use a forward computation of the gradient as in Williams and Zipser ( 1989 ) with an unbiased approximation of Delta s_t / Delta theta to reduce the memory and computational cost .

The proposed approach , called UORO , is tested on a few artificial datasets .

The approach is interesting and could potentially be very useful . However , the paper lacks in providing a substantial experimental evaluation and comparison with other methods .
Rather than with truncated BPTT with smaller truncation than required , which is easy to outperform , I would have expected a comparison with some of the other methods mentioned in the Related Work Section , such as NBT , ESNs , Decoupled Neural Interfaces , etc . Also the evaluation should be extended to other challenging tasks .

I have increased the score to 6 based on the comments and revisions from the authors .

Thank you for your comments and suggestions .

1 / Regarding comparison to other online methods such as NoBackTrack and Echo State Networks . For plain , fully connected RNNs , NoBackTrack and UORO turn out to be mathematically identical ( though implemented quite differently ) , so they will perform the same . On the contrary , for LSTMs , NoBack Track is extremely difficult to implement ( to our knowledge , it has never been done ) ; this was one of the motivations for UORO , but it makes the comparison difficult .

For Echo State Networks : ESNs amount in great part to not learning the internal weights , only the output weights ( together with a carefully tuned initialization ) . As much as we are aware , they are not known to fare particularly well on the kind of task we consider , but we may have missed relevant references .

2 / We have included a few more tasks and tests , although this remains relatively small -scale .
